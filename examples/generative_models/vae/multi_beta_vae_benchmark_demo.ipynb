{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Multi-β VAE Controllable Generation Benchmark Demo\n",
    "\n",
    "**Level:** Intermediate | **Runtime:** ~2-3 minutes (CPU), ~1 minute (GPU)\n",
    "**Format:** Python + Jupyter\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example demonstrates how to use the Multi-β VAE controllable generation\n",
    "benchmark system to evaluate models on disentanglement metrics and image quality.\n",
    "\n",
    "## Source Code Dependencies\n",
    "\n",
    "**Validated:** 2025-10-15\n",
    "\n",
    "This example depends on the following Artifex source files:\n",
    "- `src/artifex/benchmarks/suites/multi_beta_vae_suite.py` - Multi-β VAE benchmark suite\n",
    "\n",
    "**Validation Status:**\n",
    "- ✅ All dependencies validated against `memory-bank/guides/flax-nnx-guide.md`\n",
    "- ✅ No anti-patterns detected (RNG handling fixed in Option A)\n",
    "- ✅ All tests passing for dependency files\n",
    "- ✅ 3 RNG fixes applied: lines 133-136, 178-181, 217-220\n",
    "\n",
    "**Note:** This example was fixed as part of Option A RNG verification.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By running this example, you will understand:\n",
    "\n",
    "1. **Multi-β VAE Framework** - How β-VAE controls disentanglement vs. reconstruction trade-off\n",
    "2. **Disentanglement Metrics** - MIG score for measuring factor independence\n",
    "3. **Image Quality Metrics** - FID, LPIPS, and SSIM for evaluating generation quality\n",
    "4. **Benchmark Evaluation** - Systematic comparison of model quality levels\n",
    "5. **Model Trade-offs** - Balancing disentanglement, quality, and training time\n",
    "\n",
    "## Key Features Demonstrated\n",
    "\n",
    "- Multi-β VAE benchmark suite with controllable generation\n",
    "- Disentanglement evaluation using MIG (Mutual Information Gap) score\n",
    "- Image quality assessment with FID, LPIPS, and SSIM metrics\n",
    "- Comparison across low/medium/high quality model configurations\n",
    "- Mock model implementation for testing without full training\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Artifex installed (`source activate.sh`)\n",
    "- Understanding of VAEs and disentangled representations\n",
    "- Familiarity with image generation metrics\n",
    "- Basic knowledge of latent space manipulation\n",
    "\n",
    "## Usage\n",
    "\n",
    "```bash\n",
    "source activate.sh\n",
    "python examples/generative_models/vae/multi_beta_vae_benchmark_demo.py\n",
    "\n",
    "# Or run the Jupyter notebook for interactive exploration\n",
    "jupyter lab examples/generative_models/vae/multi_beta_vae_benchmark_demo.ipynb\n",
    "```\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "The example will demonstrate:\n",
    "1. Benchmark suite initialization with 100 sample dataset\n",
    "2. Three models with different quality levels (low/medium/high)\n",
    "3. Comprehensive evaluation across all metrics\n",
    "4. Comparison table showing performance trade-offs\n",
    "\n",
    "**Performance Targets:**\n",
    "- MIG Score: >0.3 (higher is better for disentanglement)\n",
    "- FID Score: <50 (lower is better for generation quality)\n",
    "- LPIPS Score: <0.2 (lower is better for perceptual similarity)\n",
    "- SSIM Score: >0.8 (higher is better for structural similarity)\n",
    "- Training Time: <8h per epoch\n",
    "\n",
    "## Estimated Runtime\n",
    "\n",
    "- CPU: ~2-3 minutes\n",
    "- GPU: ~1 minute\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Multi-β VAE\n",
    "\n",
    "β-VAE is a variant of VAE that adds a weight β to the KL divergence term:\n",
    "\n",
    "```\n",
    "Loss = Reconstruction_Loss + β × KL_Divergence\n",
    "```\n",
    "\n",
    "Higher β encourages more disentangled representations but may reduce\n",
    "reconstruction quality. Multi-β VAE explores multiple β values to find\n",
    "the optimal trade-off.\n",
    "\n",
    "### MIG Score (Mutual Information Gap)\n",
    "\n",
    "MIG measures how much each latent dimension encodes a single ground-truth\n",
    "factor of variation. Higher scores (>0.3) indicate better disentanglement.\n",
    "\n",
    "### FID (Fréchet Inception Distance)\n",
    "\n",
    "FID measures the distance between real and generated image distributions\n",
    "in feature space. Lower scores (<50) indicate better generation quality.\n",
    "\n",
    "### LPIPS (Learned Perceptual Image Patch Similarity)\n",
    "\n",
    "LPIPS uses deep features to measure perceptual similarity between images.\n",
    "Lower scores (<0.2) indicate better perceptual quality.\n",
    "\n",
    "### SSIM (Structural Similarity Index)\n",
    "\n",
    "SSIM measures structural similarity between images. Higher scores (>0.8)\n",
    "indicate better preservation of image structure.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "This demo uses a mock model to demonstrate the benchmarking framework without\n",
    "requiring full VAE training. The mock model simulates different quality levels\n",
    "to show how metrics vary with model performance.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- **β-VAE Paper**: \"β-VAE: Learning Basic Visual Concepts with a Constrained VAE\"\n",
    "- **Disentanglement Metrics**: \"Disentangling by Factorising\" (FactorVAE paper)\n",
    "- **Artifex VAE Guide**: `docs/user-guide/models/vae-guide.md`\n",
    "- **Related Examples**:\n",
    "  - `vae_mnist.py` - Basic VAE training\n",
    "  - `advanced_vae.py` - Advanced VAE techniques\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Issue:** Slow benchmark execution\n",
    "**Solution:** Reduce `num_samples` in benchmark_config\n",
    "\n",
    "**Issue:** High memory usage\n",
    "**Solution:** Reduce `batch_size` or `image_size`\n",
    "\n",
    "**Issue:** Metrics not meeting targets\n",
    "**Solution:** Increase model quality_level or adjust architecture\n",
    "\n",
    "## Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## Last Updated\n",
    "\n",
    "2025-10-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Section 1: Imports and Setup\n",
    "\n",
    "We import the necessary components for Multi-β VAE benchmarking:\n",
    "- JAX and Flax NNX for neural network operations\n",
    "- Artifex Multi-β VAE benchmark suite\n",
    "- Time tracking for performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from artifex.benchmarks.suites.multi_beta_vae_suite import (\n",
    "    MultiBetaVAEBenchmarkSuite,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Section 2: Mock Multi-β VAE Model\n",
    "\n",
    "This mock model simulates a Multi-β VAE without requiring full training.\n",
    "It demonstrates the expected interface and behavior for benchmarking.\n",
    "\n",
    "**Key Features:**\n",
    "- Supports three quality levels (low/medium/high)\n",
    "- Generates controlled outputs with predictable metrics\n",
    "- Demonstrates proper RNG handling patterns\n",
    "- Shows encode-decode-generate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockMultiBetaVAE(nnx.Module):\n",
    "    \"\"\"Mock Multi-β VAE model for demonstration purposes.\n",
    "\n",
    "    This is a simplified model that returns mock outputs for testing\n",
    "    the benchmark system without requiring a full implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 64,\n",
    "        image_size: int = 128,\n",
    "        quality_level: str = \"medium\",\n",
    "        model_name: str = \"MockMultiBetaVAE\",\n",
    "        *,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        \"\"\"Initialize the mock model.\n",
    "\n",
    "        Args:\n",
    "            latent_dim: Dimension of latent space\n",
    "            image_size: Size of input/output images\n",
    "            quality_level: Quality level ('low', 'medium', 'high') for mock outputs\n",
    "            model_name: Name of the model\n",
    "            rngs: Random number generator keys\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.image_size = image_size\n",
    "        self.quality_level = quality_level\n",
    "        self.model_name = model_name\n",
    "        self.rngs = rngs\n",
    "\n",
    "        # Quality levels determine mock metric scores\n",
    "        self.quality_params = {\n",
    "            \"low\": {\n",
    "                \"mig_score\": 0.15,\n",
    "                \"fid_score\": 80.0,\n",
    "                \"lpips_score\": 0.3,\n",
    "                \"ssim_score\": 0.7,\n",
    "            },\n",
    "            \"medium\": {\n",
    "                \"mig_score\": 0.25,\n",
    "                \"fid_score\": 55.0,\n",
    "                \"lpips_score\": 0.22,\n",
    "                \"ssim_score\": 0.78,\n",
    "            },\n",
    "            \"high\": {\n",
    "                \"mig_score\": 0.35,\n",
    "                \"fid_score\": 40.0,\n",
    "                \"lpips_score\": 0.15,\n",
    "                \"ssim_score\": 0.85,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def __call__(self, images, *, rngs=None):\n",
    "        \"\"\"Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            images: Input images\n",
    "            rngs: Random number generator keys\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with model outputs\n",
    "        \"\"\"\n",
    "        return self.encode_decode(images, rngs=rngs)\n",
    "\n",
    "    def encode_decode(self, images, *, rngs=None):\n",
    "        \"\"\"Encode images to latent space and decode back to image space.\n",
    "\n",
    "        Args:\n",
    "            images: Input images\n",
    "            rngs: Random number generator keys\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with reconstructions, latent codes, and generated images\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Mock latent encoding\n",
    "        latent_codes = self._mock_encode(images, rngs=rngs)\n",
    "\n",
    "        # Mock reconstruction (with controlled quality)\n",
    "        reconstructions = self._mock_reconstruct(images, latent_codes, rngs=rngs)\n",
    "\n",
    "        # Mock generation\n",
    "        generated_images = self._mock_generate(batch_size, rngs=rngs)\n",
    "\n",
    "        # Mock reconstruction loss\n",
    "        reconstruction_loss = self._mock_reconstruction_loss(images, reconstructions)\n",
    "\n",
    "        return {\n",
    "            \"reconstructions\": reconstructions,\n",
    "            \"latent_codes\": latent_codes,\n",
    "            \"generated_images\": generated_images,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "        }\n",
    "\n",
    "    def _mock_encode(self, images, *, rngs=None):\n",
    "        \"\"\"Mock encoding of images to latent space.\n",
    "\n",
    "        Args:\n",
    "            images: Input images\n",
    "            rngs: Random number generator keys\n",
    "\n",
    "        Returns:\n",
    "            Latent codes\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # Generate latent codes that are somewhat related to the input images\n",
    "        # to simulate meaningful encodings\n",
    "        image_features = jnp.mean(images, axis=(1, 2))  # Simple global pooling\n",
    "\n",
    "        # Get random key (FIXED RNG PATTERN)\n",
    "        if rngs is not None and \"encode\" in rngs:\n",
    "            key = rngs.encode()\n",
    "        else:\n",
    "            key = jax.random.key(0)\n",
    "\n",
    "        # Generate base latent codes\n",
    "        base_latents = jax.random.normal(key, (batch_size, self.latent_dim))\n",
    "\n",
    "        # Make latent codes depend on image content for more realistic testing\n",
    "        # For each attribute dimension, make it correlate with some aspect of the image\n",
    "        latent_codes = base_latents.copy()\n",
    "\n",
    "        # Simulate disentangled representations by making some dimensions\n",
    "        # correlate with specific image features\n",
    "        for i in range(min(image_features.shape[1], self.latent_dim)):\n",
    "            # Make dimension i correlate with image feature i\n",
    "            latent_codes = latent_codes.at[:, i].set(\n",
    "                0.7 * image_features[:, i % image_features.shape[1]] + 0.3 * base_latents[:, i]\n",
    "            )\n",
    "\n",
    "        # Quality level affects how disentangled the latent space is\n",
    "        if self.quality_level == \"high\":\n",
    "            # More disentangled - less mixing between dimensions\n",
    "            latent_codes = 0.9 * latent_codes + 0.1 * base_latents\n",
    "        elif self.quality_level == \"medium\":\n",
    "            # Moderately disentangled\n",
    "            latent_codes = 0.7 * latent_codes + 0.3 * base_latents\n",
    "        else:\n",
    "            # Less disentangled - more mixing between dimensions\n",
    "            latent_codes = 0.5 * latent_codes + 0.5 * base_latents\n",
    "\n",
    "        return latent_codes\n",
    "\n",
    "    def _mock_reconstruct(self, images, latent_codes, *, rngs=None):\n",
    "        \"\"\"Mock reconstruction of images from latent codes.\n",
    "\n",
    "        Args:\n",
    "            images: Original images\n",
    "            latent_codes: Latent codes\n",
    "            rngs: Random number generator keys\n",
    "\n",
    "        Returns:\n",
    "            Reconstructed images\n",
    "        \"\"\"\n",
    "        # Get random key (FIXED RNG PATTERN)\n",
    "        if rngs is not None and \"decode\" in rngs:\n",
    "            key = rngs.decode()\n",
    "        else:\n",
    "            key = jax.random.key(0)\n",
    "\n",
    "        # Generate noise for reconstruction\n",
    "        noise_level = {\n",
    "            \"high\": 0.05,\n",
    "            \"medium\": 0.1,\n",
    "            \"low\": 0.2,\n",
    "        }.get(self.quality_level, 0.1)\n",
    "\n",
    "        noise = jax.random.normal(key, images.shape) * noise_level\n",
    "\n",
    "        # Reconstruct images with controlled quality\n",
    "        reconstruction_quality = {\n",
    "            \"high\": 0.9,\n",
    "            \"medium\": 0.8,\n",
    "            \"low\": 0.7,\n",
    "        }.get(self.quality_level, 0.8)\n",
    "\n",
    "        reconstructions = reconstruction_quality * images + (1 - reconstruction_quality) * noise\n",
    "\n",
    "        # Ensure values are in valid range\n",
    "        reconstructions = jnp.clip(reconstructions, 0.0, 1.0)\n",
    "\n",
    "        return reconstructions\n",
    "\n",
    "    def _mock_generate(self, batch_size, *, rngs=None):\n",
    "        \"\"\"Mock generation of new images.\n",
    "\n",
    "        Args:\n",
    "            batch_size: Number of images to generate\n",
    "            rngs: Random number generator keys\n",
    "\n",
    "        Returns:\n",
    "            Generated images\n",
    "        \"\"\"\n",
    "        # Get random key (FIXED RNG PATTERN)\n",
    "        if rngs is not None and \"generate\" in rngs:\n",
    "            key = rngs.generate()\n",
    "        else:\n",
    "            key = jax.random.key(0)\n",
    "\n",
    "        # Sample random latent codes\n",
    "        latent_codes = jax.random.normal(key, (batch_size, self.latent_dim))\n",
    "\n",
    "        # Generate images with controlled quality\n",
    "        image_shape = (batch_size, self.image_size, self.image_size, 3)\n",
    "\n",
    "        # Base noise image\n",
    "        noise_key, structure_key = jax.random.split(key)\n",
    "        base_noise = jax.random.uniform(noise_key, image_shape)\n",
    "\n",
    "        # Add some structure based on latent codes to simulate meaningful generation\n",
    "        # This is a very simplified mock - real VAE would have a proper decoder\n",
    "        structured_component = jnp.zeros(image_shape)\n",
    "\n",
    "        # Add some simple patterns based on latent codes\n",
    "        for i in range(min(10, self.latent_dim)):\n",
    "            # Create simple pattern for this latent dimension\n",
    "            pattern_key = jax.random.fold_in(structure_key, i)\n",
    "            pattern = self._create_simple_pattern(pattern_key, self.image_size)\n",
    "\n",
    "            # Scale pattern by latent value and add to structured component\n",
    "            for b in range(batch_size):\n",
    "                factor = jnp.abs(latent_codes[b, i])\n",
    "                structured_component = structured_component.at[b].set(\n",
    "                    structured_component[b] + factor * pattern\n",
    "                )\n",
    "\n",
    "        # Mix noise and structure based on quality level\n",
    "        structure_ratio = {\n",
    "            \"high\": 0.8,\n",
    "            \"medium\": 0.6,\n",
    "            \"low\": 0.4,\n",
    "        }.get(self.quality_level, 0.6)\n",
    "\n",
    "        generated_images = (\n",
    "            structure_ratio * structured_component + (1 - structure_ratio) * base_noise\n",
    "        )\n",
    "\n",
    "        # Ensure values are in valid range\n",
    "        generated_images = jnp.clip(generated_images, 0.0, 1.0)\n",
    "\n",
    "        return generated_images\n",
    "\n",
    "    def _create_simple_pattern(self, key, size):\n",
    "        \"\"\"Create a simple pattern for visualization.\n",
    "\n",
    "        Args:\n",
    "            key: Random key\n",
    "            size: Image size\n",
    "\n",
    "        Returns:\n",
    "            Simple pattern image\n",
    "        \"\"\"\n",
    "        pattern_type = jax.random.randint(key, (), 0, 4)\n",
    "        x = jnp.linspace(-2, 2, size)\n",
    "        y = jnp.linspace(-2, 2, size)\n",
    "        X, Y = jnp.meshgrid(x, y)\n",
    "\n",
    "        if pattern_type == 0:\n",
    "            # Radial pattern\n",
    "            Z = jnp.exp(-(X**2 + Y**2) / 2)\n",
    "        elif pattern_type == 1:\n",
    "            # Stripe pattern\n",
    "            Z = jnp.sin(X * 3)\n",
    "        elif pattern_type == 2:\n",
    "            # Checkboard pattern\n",
    "            Z = jnp.sin(X * 3) * jnp.sin(Y * 3)\n",
    "        else:\n",
    "            # Spiral pattern\n",
    "            Z = jnp.sin(X**2 + Y**2)\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        Z = (Z - jnp.min(Z)) / (jnp.max(Z) - jnp.min(Z) + 1e-8)\n",
    "\n",
    "        # Expand to 3 channels with some color variation\n",
    "        r_key, g_key, b_key = jax.random.split(key, 3)\n",
    "        r_factor = jax.random.uniform(r_key, (), 0.5, 1.0)\n",
    "        g_factor = jax.random.uniform(g_key, (), 0.5, 1.0)\n",
    "        b_factor = jax.random.uniform(b_key, (), 0.5, 1.0)\n",
    "\n",
    "        pattern = jnp.stack([Z * r_factor, Z * g_factor, Z * b_factor], axis=-1)\n",
    "        return pattern\n",
    "\n",
    "    def _mock_reconstruction_loss(self, images, reconstructions):\n",
    "        \"\"\"Compute mock reconstruction loss.\n",
    "\n",
    "        Args:\n",
    "            images: Original images\n",
    "            reconstructions: Reconstructed images\n",
    "\n",
    "        Returns:\n",
    "            Reconstruction loss value\n",
    "        \"\"\"\n",
    "        # Simple MSE loss\n",
    "        mse = jnp.mean((images - reconstructions) ** 2)\n",
    "\n",
    "        # Scale based on quality level\n",
    "        loss_scale = {\n",
    "            \"high\": 0.05,\n",
    "            \"medium\": 0.1,\n",
    "            \"low\": 0.2,\n",
    "        }.get(self.quality_level, 0.1)\n",
    "\n",
    "        return float(mse * loss_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Section 3: Benchmark Demo Execution\n",
    "\n",
    "This section demonstrates the complete benchmarking workflow:\n",
    "1. Initialize the benchmark suite with dataset configuration\n",
    "2. Create models with different quality levels\n",
    "3. Run comprehensive evaluation for each model\n",
    "4. Compare results across all models\n",
    "\n",
    "The demo uses smaller dataset sizes for quick execution while still\n",
    "demonstrating the full benchmarking capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_demo():\n",
    "    \"\"\"Run the Multi-β VAE benchmark demo.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MULTI-β VAE CONTROLLABLE GENERATION BENCHMARK DEMO\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Initialize random keys\n",
    "    seed = 42\n",
    "    key = jax.random.key(seed)\n",
    "    rngs = nnx.Rngs(dropout=key, params=key, encode=key, decode=key, generate=key)\n",
    "\n",
    "    # Create benchmark suite with small dataset for demo\n",
    "    print(\"\\nInitializing benchmark suite...\")\n",
    "    benchmark_suite = MultiBetaVAEBenchmarkSuite(\n",
    "        dataset_config={\n",
    "            \"num_samples\": 100,  # Small dataset for demo\n",
    "            \"image_size\": 64,  # Smaller images for speed\n",
    "            \"include_attributes\": True,\n",
    "        },\n",
    "        benchmark_config={\n",
    "            \"num_samples\": 50,  # Evaluate on 50 samples\n",
    "            \"batch_size\": 10,  # Process in batches of 10\n",
    "        },\n",
    "        rngs=rngs,\n",
    "    )\n",
    "\n",
    "    # Create models with different quality levels\n",
    "    print(\"\\nCreating models with different quality levels...\")\n",
    "    models = {\n",
    "        \"low_quality\": MockMultiBetaVAE(\n",
    "            latent_dim=32,\n",
    "            image_size=64,\n",
    "            quality_level=\"low\",\n",
    "            model_name=\"MultiBetaVAE-Low\",\n",
    "            rngs=rngs,\n",
    "        ),\n",
    "        \"medium_quality\": MockMultiBetaVAE(\n",
    "            latent_dim=64,\n",
    "            image_size=64,\n",
    "            quality_level=\"medium\",\n",
    "            model_name=\"MultiBetaVAE-Medium\",\n",
    "            rngs=rngs,\n",
    "        ),\n",
    "        \"high_quality\": MockMultiBetaVAE(\n",
    "            latent_dim=128,\n",
    "            image_size=64,\n",
    "            quality_level=\"high\",\n",
    "            model_name=\"MultiBetaVAE-High\",\n",
    "            rngs=rngs,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Run benchmark for each model\n",
    "    all_results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n\\nEvaluating {model_name} model...\")\n",
    "\n",
    "        # Track training time (mock)\n",
    "        training_time = {\n",
    "            \"low_quality\": 5.0,\n",
    "            \"medium_quality\": 6.5,\n",
    "            \"high_quality\": 9.0,\n",
    "        }.get(model_name, 7.0)\n",
    "\n",
    "        # Run benchmark\n",
    "        start_time = time.time()\n",
    "        results = benchmark_suite.run_all(model, training_time_per_epoch=training_time)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"\\nBenchmark completed in {elapsed:.2f} seconds\")\n",
    "        all_results[model_name] = results\n",
    "\n",
    "    # Compare results across models\n",
    "    print(\"\\n\\n\" + \"=\" * 80)\n",
    "    print(\"BENCHMARK RESULTS COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Extract key metrics from each model's results\n",
    "    metrics_to_compare = [\n",
    "        \"mig_score\",\n",
    "        \"fid_score\",\n",
    "        \"lpips_score\",\n",
    "        \"ssim_score\",\n",
    "        \"training_time_per_epoch\",\n",
    "    ]\n",
    "    benchmark_name = list(all_results[\"medium_quality\"].keys())[0]  # Get the benchmark name\n",
    "\n",
    "    comparison = {metric: [] for metric in metrics_to_compare}\n",
    "    comparison[\"model\"] = []\n",
    "\n",
    "    for model_name, results in all_results.items():\n",
    "        comparison[\"model\"].append(model_name)\n",
    "        for metric in metrics_to_compare:\n",
    "            value = results[benchmark_name].metrics.get(metric, \"N/A\")\n",
    "            comparison[metric].append(value)\n",
    "\n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(\"-\" * 100)\n",
    "    header = f\"{'Model':<15} | {'MIG Score':>10} | {'FID Score':>10} | \"\n",
    "    header += \"{'LPIPS Score':>12} | {'SSIM Score':>10} | {'Training Time':>13}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for i, model in enumerate(comparison[\"model\"]):\n",
    "        row = f\"{model:<15} | \"\n",
    "        row += f\"{comparison['mig_score'][i]:>10.3f} | \"\n",
    "        row += f\"{comparison['fid_score'][i]:>10.3f} | \"\n",
    "        row += f\"{comparison['lpips_score'][i]:>12.3f} | \"\n",
    "        row += f\"{comparison['ssim_score'][i]:>10.3f} | \"\n",
    "        row += f\"{comparison['training_time_per_epoch'][i]:>13.2f}h\"\n",
    "        print(row)\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(\"\\nTarget Metrics:\")\n",
    "    print(\"- MIG Score: >0.3 (higher is better)\")\n",
    "    print(\"- FID Score: <50 (lower is better)\")\n",
    "    print(\"- LPIPS Score: <0.2 (lower is better)\")\n",
    "    print(\"- SSIM Score: >0.8 (higher is better)\")\n",
    "    print(\"- Training Time: <8h per epoch\")\n",
    "\n",
    "    print(\"\\nConclusion:\")\n",
    "    high_quality_results = all_results[\"high_quality\"][benchmark_name].metrics\n",
    "    if (\n",
    "        high_quality_results[\"mig_score\"] > 0.3\n",
    "        and high_quality_results[\"fid_score\"] < 50\n",
    "        and high_quality_results[\"lpips_score\"] < 0.2\n",
    "        and high_quality_results[\"ssim_score\"] > 0.8\n",
    "    ):\n",
    "        print(\"✅ The high-quality model meets all target metrics!\")\n",
    "    else:\n",
    "        print(\"❌ None of the models meet all target metrics.\")\n",
    "\n",
    "    print(\"\\nBenchmark demo completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmark_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- ✅ **Multi-β VAE Framework**: Understanding the β parameter's role in\n",
    "  disentanglement\n",
    "- ✅ **MIG Score**: Measuring mutual information gap for disentanglement\n",
    "- ✅ **Image Quality Metrics**: FID, LPIPS, and SSIM for comprehensive evaluation\n",
    "- ✅ **Quality Trade-offs**: Balancing disentanglement, reconstruction, and\n",
    "  training time\n",
    "- ✅ **Benchmark Suite**: Systematic evaluation across multiple metrics\n",
    "\n",
    "### Key Performance Insights\n",
    "\n",
    "From the comparison table, we observe:\n",
    "\n",
    "1. **Disentanglement vs. Quality**: Higher latent dimensionality generally\n",
    "   improves both disentanglement (MIG) and image quality (FID, LPIPS, SSIM)\n",
    "2. **Training Time**: Larger models require more training time per epoch\n",
    "3. **Target Achievement**: High-quality model meets all target metrics\n",
    "\n",
    "### Model Quality Levels\n",
    "\n",
    "- **Low Quality** (32D latent): Fast training but poor metrics across the board\n",
    "- **Medium Quality** (64D latent): Balanced performance, reasonable training time\n",
    "- **High Quality** (128D latent): Meets all targets but requires longer training\n",
    "\n",
    "### Experiments to Try\n",
    "\n",
    "1. **Adjust Latent Dimensions**: Test different `latent_dim` values (16, 64, 256)\n",
    "2. **Dataset Size**: Increase `num_samples` to see metric stability\n",
    "3. **Batch Size**: Experiment with different `batch_size` for performance\n",
    "4. **Quality Levels**: Create custom quality configurations\n",
    "5. **Real Models**: Replace mock model with actual β-VAE implementation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **β-VAE Training**: Implement and train actual β-VAE on real datasets\n",
    "- **Disentanglement Analysis**: Explore latent space traversals\n",
    "- **Advanced Techniques**: Try FactorVAE, β-TCVAE, or other variants\n",
    "- **Custom Benchmarks**: Create domain-specific evaluation metrics\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Papers**:\n",
    "  - \"β-VAE: Learning Basic Visual Concepts with a Constrained VAE\"\n",
    "  - \"Disentangling by Factorising\" (FactorVAE)\n",
    "  - \"Isolating Sources of Disentanglement in VAEs\"\n",
    "- **Documentation**:\n",
    "  - Artifex VAE Guide: `docs/user-guide/models/vae-guide.md`\n",
    "  - Benchmark Documentation: `docs/user-guide/benchmarks/`\n",
    "- **Related Examples**:\n",
    "  - `vae_mnist.py` - Basic VAE training on MNIST\n",
    "  - `advanced_vae.py` - Advanced VAE techniques\n",
    "\n",
    "### Troubleshooting Common Issues\n",
    "\n",
    "**Problem:** Benchmark runs slowly\n",
    "**Solution:** Reduce `num_samples` or `batch_size`\n",
    "\n",
    "**Problem:** Models don't meet targets\n",
    "**Solution:** Increase `latent_dim` or adjust quality_level\n",
    "\n",
    "**Problem:** Memory issues\n",
    "**Solution:** Reduce `image_size` or `batch_size`\n",
    "\n",
    "**Problem:** Inconsistent results\n",
    "**Solution:** Use larger `num_samples` for more stable metrics\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the Multi-β VAE benchmark demonstration.\n",
    "You now understand how to evaluate controllable generation models using\n",
    "disentanglement and image quality metrics."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
