{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Simple Text Generation with Character-Level Models\n",
    "\n",
    "This example demonstrates text generation using character-level language modeling\n",
    "with JAX/Flax NNX. Learn how to build a simple recurrent text generator that\n",
    "processes sequences one character at a time.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- [ ] Understand character-level text generation fundamentals\n",
    "- [ ] Implement embedding layers for character representations\n",
    "- [ ] Build recurrent-style networks with sequential processing\n",
    "- [ ] Apply temperature-based sampling for generation diversity\n",
    "- [ ] Handle variable-length sequence generation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of neural networks\n",
    "- Familiarity with sequence modeling concepts\n",
    "- Knowledge of JAX/Flax NNX module patterns\n",
    "- Understanding of text tokenization basics\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Character-Level Language Modeling\n",
    "\n",
    "Character-level models process text one character at a time, making them simple\n",
    "yet effective for learning text patterns:\n",
    "\n",
    "$$P(x_t | x_{<t}) = \\\\text{softmax}(f_\\\\theta(x_{<t}))$$\n",
    "\n",
    "where $x_t$ is the character at position $t$ and $f_\\\\theta$ is the neural network.\n",
    "\n",
    "### Temperature Sampling\n",
    "\n",
    "Temperature controls generation randomness:\n",
    "\n",
    "$$P_T(x_t) = \\\\frac{\\\\exp(z_t / T)}{\\\\sum_i \\\\exp(z_i / T)}$$\n",
    "\n",
    "- Low temperature ($T < 1$): More deterministic, conservative\n",
    "- High temperature ($T > 1$): More random, creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/env python\n",
    "\"\"\"Simple text generation example using the Artifex framework.\n",
    "\n",
    "This example demonstrates basic text generation using character-level\n",
    "language modeling with JAX/Flax.\n",
    "\n",
    "Source Code Dependencies:\n",
    "    - flax.nnx: Neural network modules (Embed, Linear, Sequential)\n",
    "    - jax.numpy: Array operations\n",
    "    - jax.random: Sampling operations\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model Architecture\n",
    "\n",
    "The `SimpleTextGenerator` implements a character-level language model with:\n",
    "\n",
    "1. **Embedding Layer**: Maps character IDs to dense vectors\n",
    "2. **Recurrent Network**: Processes sequences position-by-position\n",
    "3. **Output Projection**: Maps hidden states to vocabulary logits\n",
    "\n",
    "This architecture demonstrates fundamental sequence processing patterns\n",
    "while remaining simple enough to understand and modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleTextGenerator(nnx.Module):\n",
    "    \"\"\"Simple character-level text generator.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 128,  # ASCII characters\n",
    "        embed_dim: int = 64,\n",
    "        hidden_dim: int = 128,\n",
    "        seq_length: int = 32,\n",
    "        *,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        \"\"\"Initialize the text generator.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embed_dim: Embedding dimension\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            seq_length: Maximum sequence length\n",
    "            rngs: Random number generators\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
    "\n",
    "        # Simple RNN-like network (using dense layers for simplicity)\n",
    "        self.rnn = nnx.Sequential(\n",
    "            nnx.Linear(embed_dim, hidden_dim, rngs=rngs),\n",
    "            nnx.relu,\n",
    "            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs),\n",
    "            nnx.relu,\n",
    "            nnx.Linear(hidden_dim, vocab_size, rngs=rngs),\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        self.output_layer = nnx.Linear(vocab_size, vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, input_ids):\n",
    "        \"\"\"Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input token IDs [batch, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Logits for next token prediction [batch, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # Embed input tokens\n",
    "        x = self.embedding(input_ids)  # [batch, seq_len, embed_dim]\n",
    "\n",
    "        # Process through RNN-like network\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # Process each position\n",
    "            h = self.rnn(x[:, i, :])  # [batch, vocab_size]\n",
    "            outputs.append(h)\n",
    "\n",
    "        # Stack outputs\n",
    "        logits = jnp.stack(outputs, axis=1)  # [batch, seq_len, vocab_size]\n",
    "\n",
    "        # Apply output layer\n",
    "        logits = self.output_layer(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self, prompt: str = \"\", max_length: int = 100, temperature: float = 1.0, *, rngs: nnx.Rngs\n",
    "    ):\n",
    "        \"\"\"Generate text from a prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: Starting text prompt\n",
    "            max_length: Maximum length of generated text\n",
    "            temperature: Sampling temperature\n",
    "            rngs: Random number generators\n",
    "\n",
    "        Returns:\n",
    "            Generated text string\n",
    "        \"\"\"\n",
    "        # Convert prompt to token IDs (simple ASCII encoding)\n",
    "        if prompt:\n",
    "            input_ids = jnp.array([ord(c) % self.vocab_size for c in prompt])\n",
    "        else:\n",
    "            # Start with a random character\n",
    "            key = rngs.sample()\n",
    "            input_ids = jax.random.randint(key, (1,), 0, self.vocab_size)\n",
    "\n",
    "        generated = list(input_ids)\n",
    "\n",
    "        for _ in range(max_length - len(generated)):\n",
    "            # Prepare input (pad or truncate to seq_length)\n",
    "            current_seq = jnp.array(generated[-self.seq_length :])\n",
    "            if len(current_seq) < self.seq_length:\n",
    "                # Pad with zeros\n",
    "                padding = jnp.zeros(self.seq_length - len(current_seq), dtype=jnp.int32)\n",
    "                current_seq = jnp.concatenate([padding, current_seq])\n",
    "\n",
    "            # Get predictions\n",
    "            logits = self(current_seq[None, :])  # Add batch dimension\n",
    "\n",
    "            # Sample next token\n",
    "            next_logits = logits[0, -1, :] / temperature\n",
    "            key = rngs.sample()\n",
    "            next_token = jax.random.categorical(key, next_logits)\n",
    "\n",
    "            generated.append(int(next_token))\n",
    "\n",
    "            # Stop if we generate a newline (optional)\n",
    "            if next_token == ord(\"\\n\"):\n",
    "                break\n",
    "\n",
    "        # Convert back to text\n",
    "        text = \"\".join([chr(t % 128) for t in generated])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Training Data Creation\n",
    "\n",
    "For demonstration purposes, create simple repetitive text patterns\n",
    "that allow the model to learn basic character relationships quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    \"\"\"Create simple training data for demonstration.\"\"\"\n",
    "    # Simple repetitive patterns for easy learning\n",
    "    patterns = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Hello world! This is a text generation example.\",\n",
    "        \"JAX and Flax make neural networks easy.\",\n",
    "        \"Machine learning with Python is fun.\",\n",
    "        \"Generative models can create text.\",\n",
    "    ]\n",
    "\n",
    "    # Repeat patterns to create more data\n",
    "    text = \" \".join(patterns * 10)\n",
    "\n",
    "    # Convert to token IDs (simple ASCII encoding)\n",
    "    token_ids = jnp.array([ord(c) % 128 for c in text])\n",
    "\n",
    "    return text, token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Text Generation Demonstration\n",
    "\n",
    "This section demonstrates the text generation capabilities including:\n",
    "\n",
    "- Model initialization with RNG handling\n",
    "- Forward pass testing with batched sequences\n",
    "- Temperature-based sampling with various prompts\n",
    "- Batch processing for multiple sequences\n",
    "\n",
    "Each generation uses different temperature values to show how this\n",
    "parameter affects output diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def demonstrate_text_generation():\n",
    "    \"\"\"Demonstrate text generation capabilities.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Simple Text Generation Example\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set random seed\n",
    "    seed = 42\n",
    "    key = jax.random.key(seed)\n",
    "    rngs = nnx.Rngs(params=key, sample=key)\n",
    "\n",
    "    # Create text generator\n",
    "    print(\"\\nCreating text generator...\")\n",
    "    generator = SimpleTextGenerator(\n",
    "        vocab_size=128, embed_dim=64, hidden_dim=128, seq_length=32, rngs=rngs\n",
    "    )\n",
    "\n",
    "    print(f\"Vocabulary size: {generator.vocab_size}\")\n",
    "    print(f\"Embedding dimension: {generator.embed_dim}\")\n",
    "    print(f\"Hidden dimension: {generator.hidden_dim}\")\n",
    "    print(f\"Sequence length: {generator.seq_length}\")\n",
    "\n",
    "    # Create training data\n",
    "    print(\"\\nCreating training data...\")\n",
    "    text_data, token_ids = create_training_data()\n",
    "    print(f\"Training text length: {len(text_data)} characters\")\n",
    "    print(f\"Sample text: '{text_data[:50]}...'\")\n",
    "\n",
    "    # Test forward pass\n",
    "    print(\"\\nTesting forward pass...\")\n",
    "    # Create a batch of sequences\n",
    "    seq_len = 16\n",
    "    batch_size = 4\n",
    "    test_input = token_ids[: batch_size * seq_len].reshape(batch_size, seq_len)\n",
    "    logits = generator(test_input)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    print(f\"Output logits shape: {logits.shape}\")\n",
    "\n",
    "    # Generate text with different temperatures\n",
    "    print()\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Text Generation Examples\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    prompts = [\"The \", \"Hello \", \"Machine \", \"\"]\n",
    "    temperatures = [0.5, 0.8, 1.0, 1.5]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: '{prompt}'\")\n",
    "        for temp in temperatures:\n",
    "            # Create new RNG for each generation\n",
    "            gen_key = jax.random.key(np.random.randint(0, 10000))\n",
    "            gen_rngs = nnx.Rngs(sample=gen_key)\n",
    "\n",
    "            generated = generator.generate(\n",
    "                prompt=prompt, max_length=50, temperature=temp, rngs=gen_rngs\n",
    "            )\n",
    "\n",
    "            # Clean up output for display\n",
    "            generated_clean = generated.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "            if len(generated_clean) > 60:\n",
    "                generated_clean = generated_clean[:60] + \"...\"\n",
    "\n",
    "            print(f\"  Temp {temp:.1f}: {generated_clean}\")\n",
    "\n",
    "    # Demonstrate batch generation\n",
    "    print()\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Batch Processing Example\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Process multiple sequences\n",
    "    batch_prompts = [\"The \", \"Hello \", \"JAX \"]\n",
    "    batch_ids = []\n",
    "\n",
    "    for prompt in batch_prompts:\n",
    "        ids = [ord(c) % 128 for c in prompt]\n",
    "        # Pad to seq_length\n",
    "        if len(ids) < seq_len:\n",
    "            ids = [0] * (seq_len - len(ids)) + ids\n",
    "        else:\n",
    "            ids = ids[-seq_len:]\n",
    "        batch_ids.append(ids)\n",
    "\n",
    "    batch_input = jnp.array(batch_ids)\n",
    "    batch_logits = generator(batch_input)\n",
    "\n",
    "    print(f\"\\nBatch input shape: {batch_input.shape}\")\n",
    "    print(f\"Batch output shape: {batch_logits.shape}\")\n",
    "\n",
    "    # Get predictions for next character\n",
    "    next_char_logits = batch_logits[:, -1, :]\n",
    "    next_chars = jnp.argmax(next_char_logits, axis=-1)\n",
    "\n",
    "    print(\"\\nNext character predictions:\")\n",
    "    for i, (prompt, next_id) in enumerate(zip(batch_prompts, next_chars)):\n",
    "        next_char = chr(int(next_id) % 128)\n",
    "        print(f\"  '{prompt}' -> '{next_char}'\")\n",
    "\n",
    "    print(\"\\nSimple text generation example completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "This example demonstrated fundamental text generation concepts:\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Character-Level Modeling**: Process text one character at a time\n",
    "2. **Embedding Layers**: Map discrete tokens to continuous representations\n",
    "3. **Sequential Processing**: Handle variable-length sequences\n",
    "4. **Temperature Sampling**: Control generation diversity\n",
    "5. **Batch Processing**: Efficient multi-sequence handling\n",
    "\n",
    "### Experiments to Try\n",
    "\n",
    "1. **Architecture Modifications**:\n",
    "   - Increase embedding and hidden dimensions\n",
    "   - Add more RNN layers or use LSTM/GRU\n",
    "   - Implement attention mechanisms\n",
    "\n",
    "2. **Training Improvements**:\n",
    "   - Use real training data from text corpora\n",
    "   - Implement proper training loop with optimization\n",
    "   - Add regularization (dropout, weight decay)\n",
    "\n",
    "3. **Generation Techniques**:\n",
    "   - Try different sampling strategies (top-k, nucleus)\n",
    "   - Implement beam search for better quality\n",
    "   - Add length penalties and repetition controls\n",
    "\n",
    "4. **Advanced Features**:\n",
    "   - Switch from character to word/subword tokenization\n",
    "   - Implement conditional generation\n",
    "   - Add style control mechanisms\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Explore related examples:\n",
    "- **Transformer Models**: Modern attention-based architectures\n",
    "- **Text Compression**: Information-theoretic approaches\n",
    "- **Sequence-to-Sequence**: Translation and summarization tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the text generation example.\"\"\"\n",
    "    demonstrate_text_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
