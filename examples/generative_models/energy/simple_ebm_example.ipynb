{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Energy-Based Models (EBM) Example\n",
    "\n",
    "**Duration:** 15 minutes | **Level:** Intermediate | **GPU Required:** No\n",
    "(recommended for faster sampling)\n",
    "\n",
    "This example demonstrates how to create, train, and sample from Energy-Based Models (EBMs)\n",
    "using the Artifex framework. You'll learn how EBMs learn probability distributions through\n",
    "energy functions and how to generate samples using MCMC.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this example, you will:\n",
    "1. Understand how Energy-Based Models work\n",
    "2. Learn to compute energy functions and gradients (scores)\n",
    "3. Generate samples using MCMC (Markov Chain Monte Carlo)\n",
    "4. Apply contrastive divergence for efficient training\n",
    "5. Use persistent contrastive divergence with sample buffers\n",
    "6. Create deep EBMs with residual connections\n",
    "\n",
    "## üîç Source Code Dependencies\n",
    "\n",
    "**Validated:** 2025-10-14\n",
    "\n",
    "This example depends on the following Artifex source files:\n",
    "- `src/artifex/generative_models/models/energy/base.py` - Base EBM classes\n",
    "- `src/artifex/generative_models/models/energy/ebm.py` - EBM implementations\n",
    "- `src/artifex/generative_models/models/energy/mcmc.py` - MCMC sampling utilities\n",
    "- `src/artifex/generative_models/core/configuration.py` - Configuration system\n",
    "- `src/artifex/generative_models/factory.py` - Model factory\n",
    "\n",
    "**Validation Status:**\n",
    "- ‚úÖ All dependencies validated against `memory-bank/guides/flax-nnx-guide.md`\n",
    "- ‚úÖ No anti-patterns detected (RNG handling, module init, activations)\n",
    "- ‚úÖ All tests passing for dependency files (Week 0 fixes applied)\n",
    "\n",
    "## üìö Background\n",
    "\n",
    "Energy-Based Models (EBMs) are a powerful class of generative models that learn a function\n",
    "E(x) called the \"energy\" of input x. The probability distribution is defined as:\n",
    "\n",
    "$$p(x) \\\\propto \\\\exp(-E(x))$$\n",
    "\n",
    "**Key Intuition:**\n",
    "- **Low energy** ‚Üí **High probability** (data-like samples)\n",
    "- **High energy** ‚Üí **Low probability** (unlikely samples)\n",
    "\n",
    "The model learns to assign low energy to real data and high energy to fake data.\n",
    "\n",
    "## üîë Key Concepts\n",
    "\n",
    "- **Energy Function E(x):** Maps data to scalar energy values\n",
    "- **Score Function ‚àáE(x):** Gradient of energy w.r.t. input\n",
    "- **MCMC Sampling:** Markov Chain Monte Carlo for generating samples\n",
    "- **Contrastive Divergence:** Efficient training algorithm\n",
    "- **Persistent CD:** Reuses chains across iterations for better mixing\n",
    "- **Sample Buffer:** Stores MCMC samples to improve efficiency\n",
    "\n",
    "## ‚ÑπÔ∏è Prerequisites\n",
    "\n",
    "- Understanding of generative models\n",
    "- Familiarity with MCMC concepts (helpful but not required)\n",
    "- Basic knowledge of JAX and neural networks\n",
    "- Artifex installed (see below)\n",
    "\n",
    "## üì¶ Setup\n",
    "\n",
    "Before running this example, activate the Artifex environment:\n",
    "\n",
    "```bash\n",
    "source activate.sh\n",
    "python examples/generative_models/energy/simple_ebm_example.py\n",
    "```\n",
    "\n",
    "## üé¨ Expected Output\n",
    "\n",
    "This example will:\n",
    "- Create EBMs for MNIST-like data\n",
    "- Compute energy values for test images\n",
    "- Generate samples using MCMC\n",
    "- Demonstrate contrastive divergence loss\n",
    "- Show persistent CD with sample buffers\n",
    "- Create deep EBM with residual connections\n",
    "\n",
    "## ‚è±Ô∏è Estimated Runtime\n",
    "\n",
    "- **CPU:** ~2 minutes\n",
    "- **GPU:** ~30 seconds\n",
    "\n",
    "## üë• Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## üìÖ Last Updated\n",
    "\n",
    "2025-10-14\n",
    "\n",
    "## üìÑ License\n",
    "\n",
    "MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Import Dependencies and Setup\n",
    "\n",
    "We'll use:\n",
    "- **JAX:** For high-performance numerical computing and automatic differentiation\n",
    "- **Flax NNX:** For neural network modules\n",
    "- **Artifex:** For EBM implementations, MCMC utilities, and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "\n",
    "# Optional: Force CPU for testing\n",
    "# import os\n",
    "# os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Energy-Based Model (EBM) Example\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ JAX version: {jax.__version__}\")\n",
    "print(f\"üñ•Ô∏è  Backend: {jax.default_backend()}\")\n",
    "print(f\"üîß Devices: {jax.devices()}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 2. Create a Simple EBM for MNIST\n",
    "\n",
    "Let's start by creating a basic EBM designed for MNIST-like data (28√ó28 grayscale images).\n",
    "\n",
    "The `create_mnist_ebm` factory function creates an EBM with:\n",
    "- CNN architecture optimized for 28√ó28 images\n",
    "- Energy network that outputs scalar values\n",
    "- Built-in MCMC sampling capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from artifex.generative_models.models.energy import create_mnist_ebm\n",
    "\n",
    "\n",
    "print(\"\\nüìä Creating EBM for MNIST-like data...\")\n",
    "\n",
    "# Initialize random number generators\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "# Create the model\n",
    "model = create_mnist_ebm(rngs=rngs)\n",
    "\n",
    "print(\"‚úÖ Created MNIST EBM\")\n",
    "print(f\"   Model type: {type(model).__name__}\")\n",
    "print(\"   Input shape: (28, 28, 1)\")\n",
    "print(\"   Output: Scalar energy values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 3. Compute Energy Values\n",
    "\n",
    "The core of an EBM is the energy function E(x). Let's compute energies for test images.\n",
    "\n",
    "**What's happening:**\n",
    "- Model takes images as input\n",
    "- Outputs a dictionary with:\n",
    "  - `energy`: Scalar values (lower = more data-like)\n",
    "  - `score`: Gradient ‚àáE(x) used for MCMC sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚ö° Testing energy computation...\")\n",
    "\n",
    "# Create test batch (all ones as a simple test)\n",
    "batch_size = 4\n",
    "test_images = jnp.ones((batch_size, 28, 28, 1))\n",
    "\n",
    "# Forward pass: compute energy and score\n",
    "output = model(test_images)\n",
    "\n",
    "print(\"‚úÖ Energy computation successful!\")\n",
    "print(f\"   Energy values shape: {output['energy'].shape}\")\n",
    "print(f\"   Energy values: {output['energy']}\")\n",
    "print(f\"   Score (gradient) shape: {output['score'].shape}\")\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   - Lower energy = model thinks it's more likely\")\n",
    "print(\"   - Score shows direction to move in MCMC sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 4. Generate Samples Using MCMC\n",
    "\n",
    "EBMs generate samples using Markov Chain Monte Carlo (MCMC):\n",
    "1. Start from random noise\n",
    "2. Iteratively move toward lower energy regions\n",
    "3. Use score (gradient) to guide the movement\n",
    "\n",
    "This is called **Langevin dynamics** or **score-based sampling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüé® Generating samples using MCMC...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate samples using built-in MCMC sampler\n",
    "samples = model.generate(\n",
    "    n_samples=4,\n",
    "    shape=(28, 28, 1),\n",
    "    rngs=rngs,\n",
    "    n_steps=50,  # Number of MCMC steps (more = better quality, slower)\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Generated {samples.shape[0]} samples in {elapsed:.2f}s\")\n",
    "print(f\"   Sample shape: {samples.shape}\")\n",
    "print(f\"   Sample range: [{samples.min():.3f}, {samples.max():.3f}]\")\n",
    "print(\"\\nüí° MCMC Process:\")\n",
    "print(\"   1. Start from random noise\")\n",
    "print(\"   2. For each step, move toward lower energy\")\n",
    "print(\"   3. Add noise to avoid getting stuck\")\n",
    "print(\"   4. Result: samples from the learned distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 5. Using the Configuration System\n",
    "\n",
    "Artifex provides a flexible configuration system for creating models.\n",
    "This allows you to:\n",
    "- Define model architecture declaratively\n",
    "- Easily experiment with different configurations\n",
    "- Save and load model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from artifex.generative_models.core.configuration.energy_config import (\n",
    "    DeepEBMConfig,\n",
    "    EnergyNetworkConfig,\n",
    "    MCMCConfig,\n",
    "    SampleBufferConfig,\n",
    ")\n",
    "from artifex.generative_models.models.energy import DeepEBM\n",
    "\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  Creating EBM with custom configuration...\")\n",
    "\n",
    "# Define nested configurations for the EBM components\n",
    "\n",
    "# Energy network configuration (what architecture computes E(x))\n",
    "energy_network_config = EnergyNetworkConfig(\n",
    "    name=\"custom_energy_network\",\n",
    "    hidden_dims=(64, 128, 64),  # CNN channel progression (must be tuple)\n",
    "    activation=\"silu\",  # SiLU/Swish activation\n",
    "    network_type=\"cnn\",  # Use CNN architecture for images\n",
    "    use_bias=True,\n",
    ")\n",
    "\n",
    "# MCMC sampling configuration (how to generate samples)\n",
    "mcmc_config = MCMCConfig(\n",
    "    name=\"custom_mcmc\",\n",
    "    n_steps=60,  # Number of MCMC steps for sampling\n",
    "    step_size=0.01,  # Langevin dynamics step size\n",
    "    noise_scale=0.005,  # Noise added during sampling\n",
    ")\n",
    "\n",
    "# Sample buffer configuration (for persistent contrastive divergence)\n",
    "sample_buffer_config = SampleBufferConfig(\n",
    "    name=\"custom_buffer\",\n",
    "    capacity=4096,\n",
    "    reinit_prob=0.05,\n",
    ")\n",
    "\n",
    "# Create the main DeepEBMConfig with nested configs\n",
    "config = DeepEBMConfig(\n",
    "    name=\"custom_ebm\",\n",
    "    input_shape=(28, 28, 1),  # MNIST-like images\n",
    "    energy_network=energy_network_config,\n",
    "    mcmc=mcmc_config,\n",
    "    sample_buffer=sample_buffer_config,\n",
    "    alpha=0.01,  # Regularization coefficient\n",
    ")\n",
    "\n",
    "# Create model from configuration\n",
    "custom_model = DeepEBM(config=config, rngs=rngs)\n",
    "\n",
    "print(\"‚úÖ Created custom EBM from configuration\")\n",
    "print(f\"   Architecture: {config.energy_network.hidden_dims}\")\n",
    "print(f\"   Activation: {config.energy_network.activation}\")\n",
    "print(f\"   MCMC steps: {config.mcmc.n_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 6. Contrastive Divergence Loss\n",
    "\n",
    "EBMs are trained using **Contrastive Divergence** (CD):\n",
    "- Maximize probability of real data (lower their energy)\n",
    "- Minimize probability of fake/generated data (raise their energy)\n",
    "\n",
    "**Loss Formula:**\n",
    "$$\\\\mathcal{L} = E_{\\\\text{real}}(x) - E_{\\\\text{fake}}(x) + \\\\alpha \\\\|E\\\\|^2$$\n",
    "\n",
    "Where:\n",
    "- First term: Energy of real data (want to minimize)\n",
    "- Second term: Energy of fake data (want to maximize, hence negative)\n",
    "- Third term: Regularization to prevent energy from going to -‚àû"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìâ Computing contrastive divergence loss...\")\n",
    "\n",
    "# Create real and fake data batches\n",
    "real_data = jax.random.normal(rngs.sample(), (4, 28, 28, 1))\n",
    "fake_data = jax.random.normal(rngs.sample(), (4, 28, 28, 1))\n",
    "\n",
    "# Compute CD loss\n",
    "loss_dict = custom_model.contrastive_divergence_loss(\n",
    "    real_data=real_data,\n",
    "    fake_data=fake_data,\n",
    "    alpha=0.01,  # Regularization weight\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Loss computation successful!\")\n",
    "print(f\"   Total loss: {loss_dict['loss']:.4f}\")\n",
    "print(f\"   Real energy: {loss_dict['real_energy_mean']:.4f}\")\n",
    "print(f\"   Fake energy: {loss_dict['fake_energy_mean']:.4f}\")\n",
    "print(\"\\nüí° Training Goal:\")\n",
    "print(\"   - Push real energy DOWN\")\n",
    "print(\"   - Push fake energy UP\")\n",
    "print(\"   - Maximize energy difference between real and fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 7. Persistent Contrastive Divergence\n",
    "\n",
    "**Problem with standard CD:** Starting MCMC from random noise every iteration is slow.\n",
    "\n",
    "**Solution: Persistent CD (PCD)**\n",
    "- Maintain a buffer of samples across iterations\n",
    "- Initialize new chains from buffer instead of noise\n",
    "- Chains can explore the distribution more thoroughly\n",
    "- Much more efficient for training\n",
    "\n",
    "This is a key technique for practical EBM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from artifex.generative_models.models.energy.mcmc import (\n",
    "    persistent_contrastive_divergence,\n",
    "    SampleBuffer,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nüîÑ Using persistent contrastive divergence...\")\n",
    "\n",
    "# Initialize sample buffer for efficient training\n",
    "buffer = SampleBuffer(\n",
    "    capacity=256,  # Maximum number of samples to store\n",
    "    reinit_prob=0.05,  # 5% chance to reinitialize a sample\n",
    ")\n",
    "\n",
    "print(f\"üì¶ Created sample buffer (capacity: {buffer.capacity})\")\n",
    "\n",
    "# Generate samples using persistent CD\n",
    "real_samples = jax.random.normal(rngs.sample(), (8, 28, 28, 1))\n",
    "\n",
    "real_processed, fake_samples = persistent_contrastive_divergence(\n",
    "    energy_fn=custom_model.energy,\n",
    "    real_samples=real_samples,\n",
    "    sample_buffer=buffer,\n",
    "    rng_key=rngs.sample(),\n",
    "    n_mcmc_steps=30,  # Fewer steps needed with persistence\n",
    "    step_size=0.01,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Persistent CD completed!\")\n",
    "print(f\"   Buffer size: {len(buffer.buffer)} / {buffer.capacity}\")\n",
    "print(f\"   Generated fake samples: {fake_samples.shape}\")\n",
    "print(f\"   Fake sample range: [{fake_samples.min():.3f}, {fake_samples.max():.3f}]\")\n",
    "print(\"\\nüí° Why Persistent CD?\")\n",
    "print(\"   - Reuses MCMC chains across iterations\")\n",
    "print(\"   - Chains mix better over time\")\n",
    "print(\"   - Much faster than starting from scratch\")\n",
    "print(\"   - Essential for training on complex data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 8. Deep EBM with Residual Connections\n",
    "\n",
    "For more complex data (e.g., CIFAR-10, ImageNet), we need deeper architectures.\n",
    "\n",
    "The `DeepEBM` class provides:\n",
    "- Multiple residual blocks for deep networks\n",
    "- Spectral normalization for training stability\n",
    "- Support for higher resolution images\n",
    "- More expressive energy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEBM is already imported above, we just need fresh rngs for a new model\n",
    "deep_rngs = nnx.Rngs(42)\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Creating Deep EBM...\")\n",
    "\n",
    "# Configuration for Deep EBM (suitable for CIFAR-10 or similar)\n",
    "# Create nested configs for the deep energy network\n",
    "deep_energy_network_config = EnergyNetworkConfig(\n",
    "    name=\"deep_energy_network\",\n",
    "    hidden_dims=(32, 64, 128),  # Channel progression (must be tuple)\n",
    "    activation=\"silu\",\n",
    "    network_type=\"cnn\",  # CNN for images\n",
    "    use_bias=True,\n",
    "    use_spectral_norm=True,  # Stabilizes training\n",
    "    use_residual=True,  # Residual connections for deep networks\n",
    ")\n",
    "\n",
    "deep_mcmc_config = MCMCConfig(\n",
    "    name=\"deep_mcmc\",\n",
    "    n_steps=100,  # More steps for complex data\n",
    "    step_size=0.005,\n",
    "    noise_scale=0.001,\n",
    ")\n",
    "\n",
    "deep_sample_buffer_config = SampleBufferConfig(\n",
    "    name=\"deep_buffer\",\n",
    "    capacity=8192,  # Larger buffer for complex data\n",
    "    reinit_prob=0.05,\n",
    ")\n",
    "\n",
    "deep_config = DeepEBMConfig(\n",
    "    name=\"deep_ebm\",\n",
    "    input_shape=(32, 32, 3),  # RGB images (CIFAR-10 size)\n",
    "    energy_network=deep_energy_network_config,\n",
    "    mcmc=deep_mcmc_config,\n",
    "    sample_buffer=deep_sample_buffer_config,\n",
    "    alpha=0.001,  # Lower regularization for deep models\n",
    ")\n",
    "\n",
    "# Create Deep EBM\n",
    "deep_ebm = DeepEBM(config=deep_config, rngs=deep_rngs)\n",
    "\n",
    "print(\"‚úÖ Created Deep EBM with residual connections\")\n",
    "print(f\"   Input shape: {deep_config.input_shape}\")\n",
    "print(f\"   Hidden dims: {deep_config.energy_network.hidden_dims}\")\n",
    "print(f\"   Spectral norm: {deep_config.energy_network.use_spectral_norm}\")\n",
    "print(f\"   Residual: {deep_config.energy_network.use_residual}\")\n",
    "\n",
    "# Test on a batch\n",
    "test_batch = jnp.ones((2, 32, 32, 3))\n",
    "deep_output = deep_ebm(test_batch)\n",
    "\n",
    "print(\"\\nüß™ Test inference:\")\n",
    "print(f\"   Deep EBM energy shape: {deep_output['energy'].shape}\")\n",
    "print(f\"   Energy values: {deep_output['energy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### üéì What You Learned\n",
    "\n",
    "In this example, you learned:\n",
    "\n",
    "1. **EBM Fundamentals:** How energy functions define probability distributions\n",
    "2. **Energy Computation:** Computing E(x) and score ‚àáE(x) for any input\n",
    "3. **MCMC Sampling:** Generating samples using Langevin dynamics\n",
    "4. **Contrastive Divergence:** The standard training algorithm for EBMs\n",
    "5. **Persistent CD:** Efficient training with sample buffers\n",
    "6. **Deep Architectures:** Residual connections and spectral normalization\n",
    "\n",
    "### üí° Key Concepts Recap\n",
    "\n",
    "- **Energy Function:** E(x) where p(x) ‚àù exp(-E(x))\n",
    "- **Low Energy = High Probability:** Model assigns low energy to data\n",
    "- **MCMC Sampling:** Iteratively move toward low energy regions\n",
    "- **Contrastive Divergence:** Push down real energy, push up fake energy\n",
    "- **Persistent CD:** Reuse MCMC chains for better mixing\n",
    "- **Sample Buffer:** Stores samples across training iterations\n",
    "\n",
    "### üî¨ Experiments to Try\n",
    "\n",
    "Now that you understand the basics, try these modifications:\n",
    "\n",
    "1. **Adjust MCMC parameters:**\n",
    "   ```python\n",
    "   samples = model.generate(\n",
    "       n_samples=4,\n",
    "       n_steps=100,  # More steps for better samples\n",
    "       step_size=0.02,  # Larger steps (but less stable)\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Change the architecture:**\n",
    "   ```python\n",
    "   config.hidden_dims = [128, 256, 128]  # Deeper network\n",
    "   config.activation = \"gelu\"  # Different activation\n",
    "   ```\n",
    "\n",
    "3. **Modify CD parameters:**\n",
    "   ```python\n",
    "   loss_dict = model.contrastive_divergence_loss(\n",
    "       real_data=real_data,\n",
    "       fake_data=fake_data,\n",
    "       alpha=0.1,  # Stronger regularization\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Experiment with sample buffer:**\n",
    "   ```python\n",
    "   buffer = SampleBuffer(\n",
    "       capacity=512,  # Larger buffer\n",
    "       reinit_prob=0.1,  # More reinitialization\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "To learn more about Energy-Based Models:\n",
    "\n",
    "- **Training EBMs:** See complete training loop with optimization\n",
    "- **Conditional EBMs:** Learn to control generation with class labels\n",
    "- **Score Matching:** Alternative training method to contrastive divergence\n",
    "- **Denoising Score Matching:** Connection to diffusion models\n",
    "\n",
    "### üìñ Additional Resources\n",
    "\n",
    "- **Paper:** [Energy-Based Models (LeCun et al.)](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)\n",
    "- **Paper:** [Training with Contrastive Divergence](https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf)\n",
    "- **Paper:** [Improved Contrastive Divergence Training](https://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf)\n",
    "- **Documentation:** [Artifex EBM Guide](../../../docs/models/energy.md)\n",
    "- **Related Examples:**\n",
    "  - `dit_demo.py` - Diffusion models (related to score-based models)\n",
    "  - `simple_diffusion_example.py` - Understanding diffusion\n",
    "\n",
    "### üêõ Troubleshooting\n",
    "\n",
    "**Problem:** MCMC samples look like noise\n",
    "- **Solution:** Increase `n_steps` or decrease `step_size`\n",
    "\n",
    "**Problem:** Training is too slow\n",
    "- **Solution:** Use persistent CD with a sample buffer\n",
    "\n",
    "**Problem:** Energy values explode or collapse\n",
    "- **Solution:** Increase regularization `alpha` or use spectral normalization\n",
    "\n",
    "**Problem:** Samples don't match data distribution\n",
    "- **Solution:** Train longer, use more MCMC steps, or increase model capacity\n",
    "\n",
    "### üí¨ Feedback\n",
    "\n",
    "Found a bug or have suggestions? Please open an issue on GitHub!\n",
    "\n",
    "---\n",
    "\n",
    "**Example completed successfully! üéâ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚ú® Energy-Based Model Example Complete! ‚ú®\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nüí° Key Takeaways:\")\n",
    "    print(\"   1. EBMs learn energy functions E(x) where p(x) ‚àù exp(-E(x))\")\n",
    "    print(\"   2. Lower energy = higher probability under the model\")\n",
    "    print(\"   3. Training uses contrastive divergence with MCMC sampling\")\n",
    "    print(\"   4. Persistent CD with sample buffers improves efficiency\")\n",
    "    print(\"   5. Deep EBMs with residual connections handle complex data\")\n",
    "    print(\"\\nüîó Next Steps:\")\n",
    "    print(\"   - Try different MCMC parameters (steps, step_size)\")\n",
    "    print(\"   - Experiment with model architectures\")\n",
    "    print(\"   - Explore persistent CD with different buffer sizes\")\n",
    "    print(\"   - Learn about score matching as an alternative to CD\")\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "formats": "py:percent,ipynb",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
