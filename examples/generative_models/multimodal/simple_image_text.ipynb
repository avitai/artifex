{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Simple Image-Text Multimodal Learning\n",
    "\n",
    "This example demonstrates multimodal learning by combining image and text\n",
    "modalities in a unified model. Learn how to build encoders for different\n",
    "modalities, create shared embedding spaces, and perform cross-modal retrieval.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- [ ] Understand multimodal model architectures\n",
    "- [ ] Implement separate encoders for different modalities\n",
    "- [ ] Create shared embedding spaces for multiple modalities\n",
    "- [ ] Compute cross-modal similarities\n",
    "- [ ] Perform cross-modal retrieval tasks\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of CNNs for image processing\n",
    "- Familiarity with text embeddings and RNNs\n",
    "- Knowledge of similarity metrics\n",
    "- Basic understanding of representation learning\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Multimodal Learning\n",
    "\n",
    "Multimodal models learn joint representations from multiple input modalities (e.g., images and text).\n",
    "The goal is to create a shared embedding space where semantically similar inputs from different\n",
    "modalities are close together.\n",
    "\n",
    "### Contrastive Learning\n",
    "\n",
    "The model learns by maximizing similarity between matching pairs (e.g., an image and its caption)\n",
    "while minimizing similarity between non-matching pairs:\n",
    "\n",
    "$$\\\\mathcal{L} = -\\\\log \\\\frac{\\\\exp(\\\\text{sim}(f_I(I), f_T(T)) / \\\\tau)}{\\\\sum_{i=1}^N \\\\exp(\\\\text{sim}(f_I(I), f_T(T_i)) / \\\\tau)}$$\n",
    "\n",
    "where $f_I$ and $f_T$ are image and text encoders, and $\\\\tau$ is the temperature parameter.\n",
    "\n",
    "### Cross-Modal Retrieval\n",
    "\n",
    "Given a query in one modality, retrieve relevant items from another modality\n",
    "by computing similarities in the shared embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/env python\n",
    "\"\"\"Simple image-text multimodal example using the Artifex framework.\n",
    "\n",
    "This example demonstrates how to use the Artifex framework's modality\n",
    "system to create multimodal models.\n",
    "\n",
    "Source Code Dependencies:\n",
    "    - flax.nnx: Neural network modules (Conv, Linear, Embed, Sequential)\n",
    "    - jax.numpy: Array operations\n",
    "    - matplotlib.pyplot: Visualization\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from flax import nnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Model Architecture\n",
    "\n",
    "The multimodal model consists of three main components:\n",
    "\n",
    "1. **Image Encoder**: CNN-based encoder for images → embeddings\n",
    "2. **Text Encoder**: Embedding + MLP for text → embeddings\n",
    "3. **Fusion Layer**: Combines modalities for joint predictions\n",
    "\n",
    "Each encoder maps its input to a shared embedding space where\n",
    "cross-modal similarities can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleImageEncoder(nnx.Module):\n",
    "    \"\"\"Simple image encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, image_size=32, embed_dim=128, *, rngs: nnx.Rngs):\n",
    "        \"\"\"Initialize the image encoder.\n",
    "\n",
    "        Args:\n",
    "            image_size: Size of input images.\n",
    "            embed_dim: Dimension of output embeddings.\n",
    "            rngs: Random number generators.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Simple CNN encoder\n",
    "        self.encoder = nnx.Sequential(\n",
    "            nnx.Conv(3, 32, kernel_size=(3, 3), rngs=rngs),\n",
    "            nnx.relu,\n",
    "            nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs),\n",
    "            nnx.relu,\n",
    "            lambda x: jnp.mean(x, axis=(1, 2)),  # Global average pooling\n",
    "            nnx.Linear(64, embed_dim, rngs=rngs),\n",
    "        )\n",
    "\n",
    "    def __call__(self, images):\n",
    "        \"\"\"Encode images to embeddings.\n",
    "\n",
    "        Args:\n",
    "            images: Input images.\n",
    "\n",
    "        Returns:\n",
    "            Image embeddings.\n",
    "        \"\"\"\n",
    "        return self.encoder(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleTextEncoder(nnx.Module):\n",
    "    \"\"\"Simple text encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=128, embed_dim=128, *, rngs: nnx.Rngs):\n",
    "        \"\"\"Initialize the text encoder.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary.\n",
    "            embed_dim: Dimension of output embeddings.\n",
    "            rngs: Random number generators.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nnx.Embed(vocab_size, embed_dim, rngs=rngs)\n",
    "        self.encoder = nnx.Sequential(\n",
    "            nnx.Linear(embed_dim, 64, rngs=rngs), nnx.relu, nnx.Linear(64, embed_dim, rngs=rngs)\n",
    "        )\n",
    "\n",
    "    def __call__(self, text_ids):\n",
    "        \"\"\"Encode text to embeddings.\n",
    "\n",
    "        Args:\n",
    "            text_ids: Input text token IDs.\n",
    "\n",
    "        Returns:\n",
    "            Text embeddings.\n",
    "        \"\"\"\n",
    "        # Embed and average pool\n",
    "        embedded = self.embedding(text_ids)\n",
    "        pooled = jnp.mean(embedded, axis=1)  # Average over sequence\n",
    "        return self.encoder(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SimpleMultimodalModel(nnx.Module):\n",
    "    \"\"\"Simple multimodal model combining image and text.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, image_size=32, vocab_size=128, embed_dim=128, output_dim=10, *, rngs: nnx.Rngs\n",
    "    ):\n",
    "        \"\"\"Initialize the multimodal model.\n",
    "\n",
    "        Args:\n",
    "            image_size: Size of input images\n",
    "            vocab_size: Size of text vocabulary\n",
    "            embed_dim: Dimension of shared embedding space\n",
    "            output_dim: Dimension of output\n",
    "            rngs: Random number generators\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Separate encoders for each modality\n",
    "        self.image_encoder = SimpleImageEncoder(image_size, embed_dim, rngs=rngs)\n",
    "        self.text_encoder = SimpleTextEncoder(vocab_size, embed_dim, rngs=rngs)\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion = nnx.Sequential(\n",
    "            nnx.Linear(embed_dim * 2, embed_dim, rngs=rngs),\n",
    "            nnx.relu,\n",
    "            nnx.Linear(embed_dim, output_dim, rngs=rngs),\n",
    "        )\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        \"\"\"Encode images to embeddings.\"\"\"\n",
    "        return self.image_encoder(images)\n",
    "\n",
    "    def encode_text(self, text_ids):\n",
    "        \"\"\"Encode text to embeddings.\"\"\"\n",
    "        return self.text_encoder(text_ids)\n",
    "\n",
    "    def __call__(self, images, text_ids):\n",
    "        \"\"\"Forward pass combining both modalities.\n",
    "\n",
    "        Args:\n",
    "            images: Input images [batch, h, w, c]\n",
    "            text_ids: Input text token IDs [batch, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Output predictions [batch, output_dim]\n",
    "        \"\"\"\n",
    "        # Encode each modality\n",
    "        image_features = self.encode_image(images)\n",
    "        text_features = self.encode_text(text_ids)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined = jnp.concatenate([image_features, text_features], axis=-1)\n",
    "\n",
    "        # Apply fusion\n",
    "        output = self.fusion(combined)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_similarity(self, images, text_ids):\n",
    "        \"\"\"Compute similarity between images and text.\n",
    "\n",
    "        Args:\n",
    "            images: Input images\n",
    "            text_ids: Input text token IDs\n",
    "\n",
    "        Returns:\n",
    "            Similarity scores\n",
    "        \"\"\"\n",
    "        image_features = self.encode_image(images)\n",
    "        text_features = self.encode_text(text_ids)\n",
    "\n",
    "        # Normalize features\n",
    "        image_features = image_features / (\n",
    "            jnp.linalg.norm(image_features, axis=-1, keepdims=True) + 1e-8\n",
    "        )\n",
    "        text_features = text_features / (\n",
    "            jnp.linalg.norm(text_features, axis=-1, keepdims=True) + 1e-8\n",
    "        )\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = jnp.sum(image_features * text_features, axis=-1)\n",
    "\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Data Generation and Visualization\n",
    "\n",
    "Create synthetic image-text pairs for demonstration and visualize\n",
    "the learned embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_synthetic_data(batch_size=8, image_size=32, seq_len=10):\n",
    "    \"\"\"Create synthetic image-text pairs for demonstration.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Number of samples\n",
    "        image_size: Size of images\n",
    "        seq_len: Length of text sequences\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (images, text_ids, labels)\n",
    "    \"\"\"\n",
    "    key = jax.random.key(np.random.randint(0, 10000))\n",
    "\n",
    "    # Create random images\n",
    "    img_key, text_key, label_key = jax.random.split(key, 3)\n",
    "    images = jax.random.uniform(img_key, (batch_size, image_size, image_size, 3))\n",
    "\n",
    "    # Create random text (token IDs)\n",
    "    text_ids = jax.random.randint(text_key, (batch_size, seq_len), 0, 128)\n",
    "\n",
    "    # Create random labels for classification\n",
    "    labels = jax.random.randint(label_key, (batch_size,), 0, 10)\n",
    "\n",
    "    return images, text_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_multimodal_embeddings(model, images, text_ids):\n",
    "    \"\"\"Visualize the embedding space of the multimodal model.\n",
    "\n",
    "    Args:\n",
    "        model: Multimodal model\n",
    "        images: Input images\n",
    "        text_ids: Input text token IDs\n",
    "    \"\"\"\n",
    "    # Get embeddings\n",
    "    image_embeddings = model.encode_image(images)\n",
    "    text_embeddings = model.encode_text(text_ids)\n",
    "\n",
    "    # Reduce to 2D for visualization using PCA-like projection\n",
    "    key = jax.random.key(42)\n",
    "    projection = jax.random.normal(key, (model.embed_dim, 2))\n",
    "\n",
    "    image_2d = image_embeddings @ projection\n",
    "    text_2d = text_embeddings @ projection\n",
    "\n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Plot image embeddings\n",
    "    ax1.scatter(image_2d[:, 0], image_2d[:, 1], c=\"blue\", label=\"Image\", alpha=0.6)\n",
    "    ax1.set_title(\"Image Embeddings\")\n",
    "    ax1.set_xlabel(\"Dim 1\")\n",
    "    ax1.set_ylabel(\"Dim 2\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot text embeddings\n",
    "    ax2.scatter(text_2d[:, 0], text_2d[:, 1], c=\"red\", label=\"Text\", alpha=0.6)\n",
    "    ax2.set_title(\"Text Embeddings\")\n",
    "    ax2.set_xlabel(\"Dim 1\")\n",
    "    ax2.set_ylabel(\"Dim 2\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot both together\n",
    "    ax3.scatter(image_2d[:, 0], image_2d[:, 1], c=\"blue\", label=\"Image\", alpha=0.6)\n",
    "    ax3.scatter(text_2d[:, 0], text_2d[:, 1], c=\"red\", label=\"Text\", alpha=0.6)\n",
    "\n",
    "    # Draw lines connecting pairs\n",
    "    for i in range(len(image_2d)):\n",
    "        ax3.plot(\n",
    "            [image_2d[i, 0], text_2d[i, 0]],\n",
    "            [image_2d[i, 1], text_2d[i, 1]],\n",
    "            \"gray\",\n",
    "            alpha=0.3,\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "    ax3.set_title(\"Joint Embedding Space\")\n",
    "    ax3.set_xlabel(\"Dim 1\")\n",
    "    ax3.set_ylabel(\"Dim 2\")\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Main Demonstration\n",
    "\n",
    "Demonstrates:\n",
    "- Model creation and initialization\n",
    "- Forward pass with both modalities\n",
    "- Individual encoder testing\n",
    "- Cross-modal similarity computation\n",
    "- Embedding space visualization\n",
    "- Cross-modal retrieval (image-to-text and text-to-image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the multimodal example.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Simple Image-Text Multimodal Example\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set random seed\n",
    "    seed = 42\n",
    "    key = jax.random.key(seed)\n",
    "    rngs = nnx.Rngs(params=key)\n",
    "\n",
    "    # Create multimodal model\n",
    "    print(\"\\nCreating multimodal model...\")\n",
    "    model = SimpleMultimodalModel(\n",
    "        image_size=32, vocab_size=128, embed_dim=128, output_dim=10, rngs=rngs\n",
    "    )\n",
    "\n",
    "    print(\"Image encoder initialized\")\n",
    "    print(\"Text encoder initialized\")\n",
    "    print(f\"Embedding dimension: {model.embed_dim}\")\n",
    "\n",
    "    # Create synthetic data\n",
    "    print(\"\\nCreating synthetic data...\")\n",
    "    batch_size = 16\n",
    "    images, text_ids, labels = create_synthetic_data(\n",
    "        batch_size=batch_size, image_size=32, seq_len=10\n",
    "    )\n",
    "\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Text IDs shape: {text_ids.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "    # Test forward pass\n",
    "    print(\"\\nTesting forward pass...\")\n",
    "    outputs = model(images, text_ids)\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "\n",
    "    # Test individual encoders\n",
    "    print(\"\\nTesting individual encoders...\")\n",
    "    image_features = model.encode_image(images)\n",
    "    text_features = model.encode_text(text_ids)\n",
    "    print(f\"Image features shape: {image_features.shape}\")\n",
    "    print(f\"Text features shape: {text_features.shape}\")\n",
    "\n",
    "    # Compute similarities\n",
    "    print(\"\\nComputing image-text similarities...\")\n",
    "    similarities = model.compute_similarity(images, text_ids)\n",
    "    print(f\"Similarities shape: {similarities.shape}\")\n",
    "    print(f\"Mean similarity: {jnp.mean(similarities):.3f}\")\n",
    "    print(f\"Max similarity: {jnp.max(similarities):.3f}\")\n",
    "    print(f\"Min similarity: {jnp.min(similarities):.3f}\")\n",
    "\n",
    "    # Visualize embeddings\n",
    "    print(\"\\nVisualizing embedding space...\")\n",
    "    fig = visualize_multimodal_embeddings(model, images, text_ids)\n",
    "\n",
    "    # Save figure\n",
    "    import os\n",
    "\n",
    "    output_dir = \"examples_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    fig.savefig(os.path.join(output_dir, \"multimodal_embeddings.png\"))\n",
    "    print(f\"Embeddings visualization saved to {output_dir}/multimodal_embeddings.png\")\n",
    "\n",
    "    # Demonstrate cross-modal retrieval\n",
    "    print()\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Cross-Modal Retrieval Example\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Create a batch of queries and gallery\n",
    "    query_images, query_text, _ = create_synthetic_data(5, 32, 10)\n",
    "    gallery_images, gallery_text, _ = create_synthetic_data(20, 32, 10)\n",
    "\n",
    "    # Image-to-text retrieval\n",
    "    print(\"\\nImage-to-Text Retrieval:\")\n",
    "    query_img_features = model.encode_image(query_images[:1])  # Use first image as query\n",
    "    gallery_text_features = model.encode_text(gallery_text)\n",
    "\n",
    "    # Compute similarities\n",
    "    query_img_norm = query_img_features / (\n",
    "        jnp.linalg.norm(query_img_features, axis=-1, keepdims=True) + 1e-8\n",
    "    )\n",
    "    gallery_text_norm = gallery_text_features / (\n",
    "        jnp.linalg.norm(gallery_text_features, axis=-1, keepdims=True) + 1e-8\n",
    "    )\n",
    "\n",
    "    sims = query_img_norm @ gallery_text_norm.T\n",
    "    top_5 = jnp.argsort(sims[0])[-5:][::-1]\n",
    "\n",
    "    print(f\"Top 5 text matches for image query: {top_5}\")\n",
    "    print(f\"Similarity scores: {sims[0][top_5]}\")\n",
    "\n",
    "    # Text-to-image retrieval\n",
    "    print(\"\\nText-to-Image Retrieval:\")\n",
    "    query_text_features = model.encode_text(query_text[:1])  # Use first text as query\n",
    "    gallery_img_features = model.encode_image(gallery_images)\n",
    "\n",
    "    # Compute similarities\n",
    "    query_text_norm = query_text_features / (\n",
    "        jnp.linalg.norm(query_text_features, axis=-1, keepdims=True) + 1e-8\n",
    "    )\n",
    "    gallery_img_norm = gallery_img_features / (\n",
    "        jnp.linalg.norm(gallery_img_features, axis=-1, keepdims=True) + 1e-8\n",
    "    )\n",
    "\n",
    "    sims = query_text_norm @ gallery_img_norm.T\n",
    "    top_5 = jnp.argsort(sims[0])[-5:][::-1]\n",
    "\n",
    "    print(f\"Top 5 image matches for text query: {top_5}\")\n",
    "    print(f\"Similarity scores: {sims[0][top_5]}\")\n",
    "\n",
    "    print(\"\\nSimple multimodal example completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "This example demonstrated multimodal learning fundamentals:\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Separate Encoders**: Different architectures for different modalities\n",
    "2. **Shared Embedding Space**: Unified representation for multiple modalities\n",
    "3. **Cross-Modal Similarity**: Computing relationships between modalities\n",
    "4. **Retrieval Tasks**: Finding relevant items across modalities\n",
    "5. **Visualization**: Understanding learned embedding spaces\n",
    "\n",
    "### Experiments to Try\n",
    "\n",
    "1. **Architecture Improvements**:\n",
    "   - Add attention mechanisms for better feature aggregation\n",
    "   - Use pre-trained encoders (ResNet for images, BERT for text)\n",
    "   - Implement transformer-based fusion\n",
    "\n",
    "2. **Training Enhancements**:\n",
    "   - Add contrastive loss (InfoNCE, SimCLR)\n",
    "   - Implement hard negative mining\n",
    "   - Use temperature-scaled training\n",
    "\n",
    "3. **Advanced Features**:\n",
    "   - Multi-head attention for fusion\n",
    "   - Cross-modal attention mechanisms\n",
    "   - Hierarchical embeddings\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Explore related examples:\n",
    "- **CLIP-style Models**: Large-scale image-text models\n",
    "- **Visual Question Answering**: Advanced multimodal reasoning\n",
    "- **Image Captioning**: Generating text from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
