{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Advanced Training Example\n",
    "\n",
    "**Author:** Artifex Team\n",
    "**Last Updated:** 2025-10-22\n",
    "**Difficulty:** Intermediate\n",
    "**Runtime:** ~2 minutes\n",
    "**Format:** Dual (.py script | .ipynb notebook)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This comprehensive example demonstrates production-ready training patterns using the\n",
    "Artifex framework. Learn how to implement robust training loops, manage optimizer\n",
    "configurations, track metrics, and implement checkpointing strategies.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this example, you will understand:\n",
    "\n",
    "- [ ] How to implement a complete training pipeline with proper validation\n",
    "- [ ] Optimizer and learning rate scheduler configuration\n",
    "- [ ] Metrics tracking and visualization during training\n",
    "- [ ] Checkpoint management and model persistence\n",
    "- [ ] Best practices for training loop organization\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of neural network training\n",
    "- Familiarity with JAX and Flax NNX\n",
    "- Understanding of gradient descent and backpropagation\n",
    "- Knowledge of learning rate scheduling concepts\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Training Loop Components\n",
    "\n",
    "A production training loop requires several key components:\n",
    "\n",
    "1. **Data Management**: Efficient batching and shuffling\n",
    "2. **Optimization**: Gradient computation and parameter updates\n",
    "3. **Metrics Tracking**: Monitor training and validation performance\n",
    "4. **Checkpointing**: Save model state for recovery and deployment\n",
    "5. **Validation**: Monitor generalization to unseen data\n",
    "\n",
    "### Learning Rate Scheduling\n",
    "\n",
    "Learning rate schedules improve training by:\n",
    "\n",
    "- **Warmup**: Gradual increase to avoid instability\n",
    "- **Decay**: Reduce learning rate as training progresses\n",
    "- **Cosine Annealing**: Smooth decrease with periodic restarts\n",
    "\n",
    "The formula for cosine decay is:\n",
    "\n",
    "$$\n",
    "\\\\eta_t = \\\\eta_{\\\\text{min}} + \\\\frac{1}{2}(\\\\eta_{\\\\text{max}} - \\\\eta_{\\\\text{min}})\n",
    "\\\\left(1 + \\\\cos\\\\left(\\\\frac{t}{T}\\\\pi\\\\right)\\\\right)\n",
    "$$\n",
    "\n",
    "where $\\\\eta_t$ is the learning rate at step $t$, and $T$ is the total number of steps.\n",
    "\n",
    "## Installation\n",
    "\n",
    "This example requires the Artifex library with standard dependencies:\n",
    "\n",
    "```bash\n",
    "pip install artifex[examples]\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "Run the Python script directly:\n",
    "\n",
    "```bash\n",
    "python examples/generative_models/advanced_training_example.py\n",
    "```\n",
    "\n",
    "Or open the notebook:\n",
    "\n",
    "```bash\n",
    "jupyter notebook examples/generative_models/advanced_training_example.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Import required modules from JAX, Flax NNX, and Artifex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "from artifex.generative_models.core.configuration import (\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    SchedulerConfig,\n",
    "    TrainingConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Data Utilities\n",
    "\n",
    "Create synthetic datasets and data loaders for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset(num_samples=1000, input_dim=784, num_classes=10):\n",
    "    \"\"\"Create synthetic dataset for demonstration.\n",
    "\n",
    "    Args:\n",
    "        num_samples: Number of samples\n",
    "        input_dim: Input dimension\n",
    "        num_classes: Number of classes for classification\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_data, val_data, test_data)\n",
    "    \"\"\"\n",
    "    key = jax.random.key(42)\n",
    "    keys = jax.random.split(key, 6)\n",
    "\n",
    "    # Create train/val/test splits\n",
    "    train_x = jax.random.normal(keys[0], (num_samples, input_dim))\n",
    "    train_y = jax.random.randint(keys[1], (num_samples,), 0, num_classes)\n",
    "\n",
    "    val_x = jax.random.normal(keys[2], (num_samples // 5, input_dim))\n",
    "    val_y = jax.random.randint(keys[3], (num_samples // 5,), 0, num_classes)\n",
    "\n",
    "    test_x = jax.random.normal(keys[4], (num_samples // 5, input_dim))\n",
    "    test_y = jax.random.randint(keys[5], (num_samples // 5,), 0, num_classes)\n",
    "\n",
    "    return (train_x, train_y), (val_x, val_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data, batch_size=32, shuffle=True):\n",
    "    \"\"\"Create a simple data loader.\n",
    "\n",
    "    Args:\n",
    "        data: Tuple of (features, labels)\n",
    "        batch_size: Batch size\n",
    "        shuffle: Whether to shuffle data\n",
    "\n",
    "    Yields:\n",
    "        Batches of data\n",
    "    \"\"\"\n",
    "    x, y = data\n",
    "    num_samples = len(x)\n",
    "    indices = jnp.arange(num_samples)\n",
    "\n",
    "    if shuffle:\n",
    "        key = jax.random.key(np.random.randint(0, 10000))\n",
    "        indices = jax.random.permutation(key, indices)\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_indices = indices[i : i + batch_size]\n",
    "        yield x[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Metrics Tracker\n",
    "\n",
    "Track and visualize training and validation metrics over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMetrics:\n",
    "    \"\"\"Simple metrics tracker for training.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the metrics tracker.\"\"\"\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_acc\": [],\n",
    "        }\n",
    "\n",
    "    def update(self, metrics: dict[str, float]):\n",
    "        \"\"\"Update metrics.\"\"\"\n",
    "        for key, value in metrics.items():\n",
    "            if key in self.history:\n",
    "                self.history[key].append(float(value))\n",
    "\n",
    "    def plot(self, save_path=None):\n",
    "        \"\"\"Plot training curves.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        # Plot loss\n",
    "        if self.history[\"train_loss\"]:\n",
    "            ax1.plot(self.history[\"train_loss\"], label=\"Train\")\n",
    "        if self.history[\"val_loss\"]:\n",
    "            ax1.plot(self.history[\"val_loss\"], label=\"Validation\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_title(\"Training Loss\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # Plot accuracy\n",
    "        if self.history[\"train_acc\"]:\n",
    "            ax2.plot(self.history[\"train_acc\"], label=\"Train\")\n",
    "        if self.history[\"val_acc\"]:\n",
    "            ax2.plot(self.history[\"val_acc\"], label=\"Validation\")\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.set_title(\"Training Accuracy\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Model Definition\n",
    "\n",
    "Define a simple classifier using Flax NNX for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nnx.Module):\n",
    "    \"\"\"Simple classifier for demonstration.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, *, rngs: nnx.Rngs):\n",
    "        \"\"\"Initialize the classifier.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Input dimension.\n",
    "            hidden_dims: List of hidden layer dimensions.\n",
    "            num_classes: Number of output classes.\n",
    "            rngs: Random number generators.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers: list[nnx.Module] = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nnx.Linear(prev_dim, hidden_dim, rngs=rngs))\n",
    "            layers.append(nnx.relu)\n",
    "            layers.append(nnx.Dropout(rate=0.1, rngs=rngs))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nnx.Linear(prev_dim, num_classes, rngs=rngs))\n",
    "\n",
    "        self.net = nnx.Sequential(*layers)\n",
    "\n",
    "    def __call__(self, x, *, training=False):\n",
    "        \"\"\"Forward pass through the classifier.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "            training: Whether in training mode.\n",
    "\n",
    "        Returns:\n",
    "            Logits for each class.\n",
    "        \"\"\"\n",
    "        # Note: In real code, you'd properly handle dropout training mode\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Optimizer and Scheduler Creation\n",
    "\n",
    "Create optimizers and learning rate schedulers from configuration objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(config: OptimizerConfig):\n",
    "    \"\"\"Create optimizer from configuration.\n",
    "\n",
    "    Args:\n",
    "        config: Optimizer configuration\n",
    "\n",
    "    Returns:\n",
    "        Optax optimizer\n",
    "    \"\"\"\n",
    "    if config.optimizer_type == \"adam\":\n",
    "        return optax.adam(\n",
    "            learning_rate=config.learning_rate,\n",
    "            b1=config.beta1,\n",
    "            b2=config.beta2,\n",
    "        )\n",
    "    elif config.optimizer_type == \"sgd\":\n",
    "        return optax.sgd(\n",
    "            learning_rate=config.learning_rate,\n",
    "            momentum=config.momentum,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {config.optimizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scheduler(base_optimizer, scheduler_config: SchedulerConfig):\n",
    "    \"\"\"Create learning rate scheduler.\n",
    "\n",
    "    Args:\n",
    "        base_optimizer: Base optimizer\n",
    "        scheduler_config: Scheduler configuration\n",
    "\n",
    "    Returns:\n",
    "        Optimizer with scheduler\n",
    "    \"\"\"\n",
    "    if scheduler_config is None:\n",
    "        return base_optimizer\n",
    "\n",
    "    if scheduler_config.scheduler_type == \"cosine\":\n",
    "        schedule = optax.cosine_decay_schedule(\n",
    "            init_value=1.0,\n",
    "            decay_steps=scheduler_config.total_steps or 10000,\n",
    "        )\n",
    "    elif scheduler_config.scheduler_type == \"exponential\":\n",
    "        schedule = optax.exponential_decay(\n",
    "            init_value=1.0,\n",
    "            transition_steps=scheduler_config.decay_steps,\n",
    "            decay_rate=scheduler_config.decay_rate,\n",
    "        )\n",
    "    else:\n",
    "        return base_optimizer\n",
    "\n",
    "    # Combine with base optimizer\n",
    "    return optax.chain(\n",
    "        optax.scale_by_schedule(schedule),\n",
    "        base_optimizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Training and Evaluation Functions\n",
    "\n",
    "Implement the core training step and evaluation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, batch_x, batch_y, loss_fn):\n",
    "    \"\"\"Single training step.\n",
    "\n",
    "    Args:\n",
    "        model: Model to train\n",
    "        optimizer: Optimizer\n",
    "        batch_x: Input batch\n",
    "        batch_y: Target batch\n",
    "        loss_fn: Loss function\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (loss, accuracy)\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(model):\n",
    "        logits = model(batch_x, training=True)\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch_y)\n",
    "        loss = jnp.mean(loss)\n",
    "\n",
    "        # Accuracy\n",
    "        predictions = jnp.argmax(logits, axis=-1)\n",
    "        accuracy = jnp.mean(predictions == batch_y)\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    (loss, accuracy), grads = nnx.value_and_grad(compute_loss, has_aux=True)(model)\n",
    "    optimizer.update(model, grads)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    \"\"\"Evaluate model on dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Model to evaluate\n",
    "        data_loader: Data loader\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (average_loss, average_accuracy)\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        logits = model(batch_x, training=False)\n",
    "\n",
    "        # Loss\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch_y)\n",
    "        loss = jnp.mean(loss)\n",
    "\n",
    "        # Accuracy\n",
    "        predictions = jnp.argmax(logits, axis=-1)\n",
    "        accuracy = jnp.mean(predictions == batch_y)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_acc += accuracy\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / num_batches, total_acc / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Checkpointing\n",
    "\n",
    "Save model checkpoints during training for recovery and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, checkpoint_dir):\n",
    "    \"\"\"Save training checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model: Model to save\n",
    "        optimizer: Optimizer state\n",
    "        epoch: Current epoch\n",
    "        checkpoint_dir: Directory to save checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # In real implementation, you'd use orbax or similar\n",
    "    print(f\"Checkpoint saved at epoch {epoch} to {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Main Training Loop\n",
    "\n",
    "Execute the complete training pipeline with all components integrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the advanced training example.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Advanced Training Example\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Configuration\n",
    "    print(\"\\n1. Setting up configuration...\")\n",
    "\n",
    "    # Model configuration\n",
    "    model_config = ModelConfig(\n",
    "        name=\"classifier\",\n",
    "        model_class=\"simple_classifier\",\n",
    "        input_dim=784,\n",
    "        hidden_dims=[256, 128],\n",
    "        output_dim=10,\n",
    "        dropout_rate=0.1,\n",
    "        parameters={},\n",
    "    )\n",
    "\n",
    "    # Optimizer configuration\n",
    "    optimizer_config = OptimizerConfig(\n",
    "        name=\"training_optimizer\",\n",
    "        optimizer_type=\"adam\",\n",
    "        learning_rate=1e-3,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        weight_decay=1e-4,\n",
    "    )\n",
    "\n",
    "    # Scheduler configuration\n",
    "    scheduler_config = SchedulerConfig(\n",
    "        name=\"cosine_scheduler\",\n",
    "        scheduler_type=\"cosine\",\n",
    "        total_steps=1000,\n",
    "        warmup_steps=100,\n",
    "    )\n",
    "\n",
    "    # Training configuration\n",
    "    training_config = TrainingConfig(\n",
    "        name=\"training\",\n",
    "        batch_size=32,\n",
    "        num_epochs=10,\n",
    "        optimizer=optimizer_config,\n",
    "        scheduler=scheduler_config,\n",
    "        checkpoint_dir=\"./checkpoints/advanced_example\",\n",
    "        save_frequency=5,\n",
    "    )\n",
    "\n",
    "    print(f\"  Model: {model_config.name}\")\n",
    "    print(f\"  Optimizer: {optimizer_config.optimizer_type}\")\n",
    "    print(f\"  Scheduler: {scheduler_config.scheduler_type}\")\n",
    "    print(f\"  Epochs: {training_config.num_epochs}\")\n",
    "    print(f\"  Batch size: {training_config.batch_size}\")\n",
    "\n",
    "    # Create dataset\n",
    "    print(\"\\n2. Creating synthetic dataset...\")\n",
    "    train_data, val_data, test_data = create_synthetic_dataset(\n",
    "        num_samples=1000, input_dim=784, num_classes=10\n",
    "    )\n",
    "    print(f\"  Train samples: {len(train_data[0])}\")\n",
    "    print(f\"  Validation samples: {len(val_data[0])}\")\n",
    "    print(f\"  Test samples: {len(test_data[0])}\")\n",
    "\n",
    "    # Create model\n",
    "    print(\"\\n3. Creating model...\")\n",
    "    key = jax.random.key(42)\n",
    "    rngs = nnx.Rngs(params=key, dropout=key)\n",
    "\n",
    "    model = SimpleClassifier(input_dim=784, hidden_dims=[256, 128], num_classes=10, rngs=rngs)\n",
    "    print(f\"  Model created with {len(model_config.hidden_dims)} hidden layers\")\n",
    "\n",
    "    # Create optimizer\n",
    "    print(\"\\n4. Setting up optimizer and scheduler...\")\n",
    "    base_optimizer = create_optimizer(optimizer_config)\n",
    "    optimizer_with_schedule = create_scheduler(base_optimizer, scheduler_config)\n",
    "    optimizer = nnx.Optimizer(model, optimizer_with_schedule, wrt=nnx.All(nnx.Param))\n",
    "\n",
    "    # Training metrics\n",
    "    metrics = TrainingMetrics()\n",
    "\n",
    "    # Training loop\n",
    "    print(\"\\n5. Starting training...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for epoch in range(training_config.num_epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        train_loader = create_data_loader(\n",
    "            train_data, batch_size=training_config.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            loss, acc = train_step(model, optimizer, batch_x, batch_y, None)\n",
    "            train_loss += loss\n",
    "            train_acc += acc\n",
    "            num_train_batches += 1\n",
    "\n",
    "        train_loss /= num_train_batches\n",
    "        train_acc /= num_train_batches\n",
    "\n",
    "        # Validation\n",
    "        val_loader = create_data_loader(\n",
    "            val_data, batch_size=training_config.batch_size, shuffle=False\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        # Update metrics\n",
    "        metrics.update(\n",
    "            {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch + 1}/{training_config.num_epochs}\")\n",
    "        print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % training_config.save_frequency == 0:\n",
    "            save_checkpoint(model, optimizer, epoch + 1, training_config.checkpoint_dir)\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    print(\"\\n6. Evaluating on test set...\")\n",
    "    test_loader = create_data_loader(\n",
    "        test_data, batch_size=training_config.batch_size, shuffle=False\n",
    "    )\n",
    "    test_loss, test_acc = evaluate(model, test_loader)\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    print(\"\\n7. Plotting training curves...\")\n",
    "    output_dir = \"examples_output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    metrics.plot(save_path=f\"{output_dir}/training_curves.png\")\n",
    "    print(f\"  Training curves saved to {output_dir}/training_curves.png\")\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… Advanced training example completed successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nKey takeaways:\")\n",
    "    print(\"- Use configuration objects for all settings\")\n",
    "    print(\"- Implement proper training and validation loops\")\n",
    "    print(\"- Track metrics throughout training\")\n",
    "    print(\"- Save checkpoints regularly\")\n",
    "    print(\"- Evaluate on held-out test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "This example demonstrated:\n",
    "\n",
    "1. **Configuration Management**: Use Pydantic configuration objects for all settings\n",
    "2. **Training Pipeline**: Implement proper training and validation loops\n",
    "3. **Metrics Tracking**: Monitor performance throughout training\n",
    "4. **Checkpointing**: Save model state regularly for recovery\n",
    "5. **Learning Rate Scheduling**: Apply adaptive learning rate strategies\n",
    "\n",
    "### Experiments to Try\n",
    "\n",
    "1. **Different Optimizers**: Compare Adam, SGD, AdamW performance\n",
    "2. **Scheduler Variations**: Test exponential decay vs cosine annealing\n",
    "3. **Architecture Changes**: Experiment with different hidden layer sizes\n",
    "4. **Regularization**: Add L2 regularization or different dropout rates\n",
    "5. **Early Stopping**: Implement early stopping based on validation loss\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore more advanced models (VAEs, GANs, Diffusion Models)\n",
    "- Learn about distributed training strategies\n",
    "- Study advanced optimization techniques (gradient clipping, mixed precision)\n",
    "- Implement custom callbacks and monitoring tools\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Artifex Documentation](https://docs.artifex.ai)\n",
    "- [Flax NNX Guide](https://flax.readthedocs.io/en/latest/)\n",
    "- [Optax Documentation](https://optax.readthedocs.io/)\n",
    "- [JAX Training Best Practices](https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
