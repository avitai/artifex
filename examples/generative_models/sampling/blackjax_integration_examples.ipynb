{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# BlackJAX Integration Examples with Artifex Distributions\n",
    "\n",
    "This example demonstrates advanced integration patterns between BlackJAX samplers and\n",
    "Artifex's distribution framework.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this example, you will:\n",
    "\n",
    "- [ ] Understand how to use BlackJAX sampler classes with Artifex distributions\n",
    "- [ ] Learn to use both class-based and functional sampling APIs\n",
    "- [ ] Apply samplers to Artifex distributions (Normal, Mixture)\n",
    "- [ ] Compare class-based vs functional sampling approaches\n",
    "- [ ] Handle memory constraints in NUTS sampling\n",
    "- [ ] Sample from mixture distributions using MCMC\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of MCMC sampling concepts\n",
    "- Familiarity with Artifex distributions module\n",
    "- Basic knowledge of HMC, MALA, and NUTS algorithms\n",
    "- Artifex core sampling and distributions modules\n",
    "\n",
    "## Integration Approaches\n",
    "\n",
    "Artifex supports two ways to integrate with BlackJAX:\n",
    "\n",
    "### 1. Direct BlackJAX API (This Example)\n",
    "Use BlackJAX's native API directly with Artifex distributions:\n",
    "- Create log probability function from Artifex distribution\n",
    "- Use `blackjax.hmc()`, `blackjax.mala()`, `blackjax.nuts()` directly\n",
    "- Full control over sampling parameters and state management\n",
    "- Useful when you need maximum flexibility\n",
    "\n",
    "### 2. Artifex Functional API\n",
    "Use Artifex's convenience functions like `hmc_sampling()`, `mala_sampling()`:\n",
    "- Single function call for complete sampling workflow\n",
    "- Automatic burn-in and state management\n",
    "- Simplified interface for common use cases\n",
    "- Recommended for most applications\n",
    "\n",
    "## Example Overview\n",
    "\n",
    "This example includes:\n",
    "\n",
    "1. **Normal Distribution with Direct BlackJAX HMC**: Using BlackJAX HMC API directly\n",
    "2. **Normal Distribution with hmc_sampling**: Using Artifex's functional API\n",
    "3. **Normal Distribution with Direct BlackJAX MALA**: Using BlackJAX MALA API directly\n",
    "4. **Univariate Normal with Direct BlackJAX NUTS**: NUTS on 1D distribution (memory-aware)\n",
    "5. **Multimodal Distribution Comparison**: Teaching example comparing MALA vs NUTS\n",
    "   - 5a: Mixture with MALA (demonstrates local sampler limitations)\n",
    "   - 5b: Mixture with NUTS (demonstrates Hamiltonian dynamics success)\n",
    "\n",
    "## Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\"\"\"Import required libraries.\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "import blackjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from artifex.generative_models.core.distributions import Mixture, Normal\n",
    "from artifex.generative_models.core.sampling.blackjax_samplers import (\n",
    "    hmc_sampling,\n",
    "    mala_sampling,\n",
    "    nuts_sampling,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Helper Function: Plotting Samples\n",
    "\n",
    "We define a helper function to visualize MCMC samples for 1D and 2D distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples, title=\"MCMC Samples\", filename=None, true_params=None):\n",
    "    \"\"\"Plot MCMC samples.\n",
    "\n",
    "    Args:\n",
    "        samples: Array of samples with shape [n_samples, d]\n",
    "        title: Plot title\n",
    "        filename: If provided, save plot to this file\n",
    "        true_params: Optional tuple of (mean, scale) for true distribution\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # For 1D data, plot histogram\n",
    "    if samples.shape[1] == 1:\n",
    "        plt.hist(samples, bins=50, density=True, alpha=0.7)\n",
    "\n",
    "        # Plot true density if provided\n",
    "        if true_params:\n",
    "            true_mean, true_scale = true_params\n",
    "            if true_mean is not None and true_scale is not None:\n",
    "                x = jnp.linspace(jnp.min(samples) - 1, jnp.max(samples) + 1, 1000)\n",
    "                density = jnp.exp(-0.5 * ((x - true_mean) / true_scale) ** 2) / (\n",
    "                    true_scale * jnp.sqrt(2 * jnp.pi)\n",
    "                )\n",
    "                plt.plot(x, density, \"r-\", linewidth=2, label=\"True Density\")\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "\n",
    "    # For 2D data, plot scatter\n",
    "    elif samples.shape[1] == 2:\n",
    "        plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=5)\n",
    "\n",
    "        # Plot true mean if provided\n",
    "        if true_params:\n",
    "            true_mean, _ = true_params\n",
    "            if hasattr(true_mean, \"__len__\") and len(true_mean) == 2:\n",
    "                plt.plot(true_mean[0], true_mean[1], \"r*\", markersize=15, label=\"True Mean\")\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.axis(\"equal\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "\n",
    "    if filename:\n",
    "        import os\n",
    "\n",
    "        output_dir = \"examples_output\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath)\n",
    "        print(f\"Plot saved as '{filepath}'\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    sample_mean = jnp.mean(samples, axis=0)\n",
    "    sample_std = jnp.std(samples, axis=0)\n",
    "\n",
    "    print(f\"\\n{title} - Statistics:\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Print true parameters if provided\n",
    "    if true_params:\n",
    "        true_mean, true_scale = true_params\n",
    "        if true_mean is None and true_scale is None:\n",
    "            # No true parameters - just print sample statistics\n",
    "            print(f\"Sample Mean: {sample_mean}\")\n",
    "            print(f\"Sample Std:  {sample_std}\")\n",
    "        elif true_scale is not None:\n",
    "            # Format as table\n",
    "            print(f\"{'Statistic':<15} {'True Value':<30} {'Sample Value':<30}\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "            # Handle scalar vs vector\n",
    "            if hasattr(true_mean, \"__len__\"):\n",
    "                # Vector format (handles all arrays including 1D)\n",
    "                mean_true_str = \"[\" + \", \".join(f\"{float(x):>7.4f}\" for x in true_mean) + \"]\"\n",
    "                mean_sample_str = \"[\" + \", \".join(f\"{float(x):>7.4f}\" for x in sample_mean) + \"]\"\n",
    "                print(f\"{'Mean':<15} {mean_true_str:<30} {mean_sample_str:<30}\")\n",
    "\n",
    "                std_true_str = \"[\" + \", \".join(f\"{float(x):>7.4f}\" for x in true_scale) + \"]\"\n",
    "                std_sample_str = \"[\" + \", \".join(f\"{float(x):>7.4f}\" for x in sample_std) + \"]\"\n",
    "                print(f\"{'Std':<15} {std_true_str:<30} {std_sample_str:<30}\")\n",
    "            else:\n",
    "                # Scalar format (true scalars only)\n",
    "                mean_true = f\"{float(true_mean):>8.4f}\"\n",
    "                mean_sample = f\"{float(sample_mean):>8.4f}\"\n",
    "                print(f\"{'Mean':<15} {mean_true:<30} {mean_sample:<30}\")\n",
    "                std_true = f\"{float(true_scale):>8.4f}\"\n",
    "                std_sample = f\"{float(sample_std):>8.4f}\"\n",
    "                print(f\"{'Std':<15} {std_true:<30} {std_sample:<30}\")\n",
    "        else:\n",
    "            # Only mean provided (for mixture)\n",
    "            print(f\"{'Statistic':<15} {'True Value':<30} {'Sample Value':<30}\")\n",
    "            print(\"-\" * 70)\n",
    "            if hasattr(true_mean, \"__len__\"):\n",
    "                mean_true_str = \"[\" + \", \".join(f\"{float(x):>7.4f}\" for x in true_mean) + \"]\"\n",
    "                mean_sample_str = \"[\" + \", \".join(f\"{float(x):>7.4f}\" for x in sample_mean) + \"]\"\n",
    "                print(f\"{'Mean':<15} {mean_true_str:<30} {mean_sample_str:<30}\")\n",
    "\n",
    "                std_sample_str = \"[\" + \", \".join(f\"{float(x):>7.4f}\" for x in sample_std) + \"]\"\n",
    "                print(f\"{'Std':<15} {'N/A':<30} {std_sample_str:<30}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"{'Mean':<15} {float(true_mean):>8.4f}              {float(sample_mean):>8.4f}\"\n",
    "                )\n",
    "                print(f\"{'Std':<15} {'N/A':<30} {float(sample_std):>8.4f}\")\n",
    "    else:\n",
    "        print(f\"Sample Mean: {sample_mean}\")\n",
    "        print(f\"Sample Std:  {sample_std}\")\n",
    "\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 1: Normal Distribution with Direct BlackJAX HMC\n",
    "\n",
    "This example demonstrates using BlackJAX's HMC sampler directly with a Artifex\n",
    "Normal distribution. This shows the low-level BlackJAX API for maximum control.\n",
    "\n",
    "Key points:\n",
    "- Create log probability function from Artifex distribution\n",
    "- Initialize BlackJAX HMC sampler with proper parameters\n",
    "- Manually manage sampler state and run burn-in\n",
    "- Use JAX random keys correctly for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_normal_hmc():\n",
    "    \"\"\"Sample from a normal distribution using BlackJAX HMC directly.\"\"\"\n",
    "    print()\n",
    "    print(\"===== Example: Normal Distribution with Direct BlackJAX HMC =====\")\n",
    "\n",
    "    # Create distribution\n",
    "    true_mean = jnp.array([3.0, -2.0])\n",
    "    true_scale = jnp.array([1.5, 0.8])\n",
    "    normal_dist = Normal(loc=true_mean, scale=true_scale)\n",
    "\n",
    "    # Create log probability function that returns scalar\n",
    "    def logdensity_fn(x):\n",
    "        log_prob = normal_dist.log_prob(x)\n",
    "        # Ensure scalar output by summing if needed\n",
    "        if hasattr(log_prob, \"shape\") and len(log_prob.shape) > 0:\n",
    "            return jnp.sum(log_prob)\n",
    "        return log_prob\n",
    "\n",
    "    # Initial position and parameters\n",
    "    init_position = jnp.zeros(2)\n",
    "    step_size = 0.1\n",
    "    inverse_mass_matrix = jnp.ones(2)\n",
    "    num_integration_steps = 10\n",
    "\n",
    "    # Create HMC sampler\n",
    "    hmc = blackjax.hmc(\n",
    "        logdensity_fn,\n",
    "        step_size,\n",
    "        inverse_mass_matrix,\n",
    "        num_integration_steps,\n",
    "    )\n",
    "\n",
    "    # Initialize sampler state\n",
    "    state = hmc.init(init_position)\n",
    "\n",
    "    # JIT compile the step function for speed\n",
    "    step_fn = jax.jit(hmc.step)\n",
    "\n",
    "    # Warm-up JIT compilation (don't count this in timing)\n",
    "    key = jax.random.key(0)\n",
    "    warmup_key = jax.random.fold_in(key, 999999)\n",
    "    _, _ = step_fn(warmup_key, state)\n",
    "\n",
    "    # Number of samples to collect\n",
    "    n_samples = 10000\n",
    "    n_burnin = 2000\n",
    "\n",
    "    # Collect samples\n",
    "    samples = jnp.zeros((n_samples, 2))\n",
    "\n",
    "    # Run burn-in with progress bar\n",
    "    print(\"Running burn-in...\")\n",
    "    start_time = time.time()\n",
    "    for i in tqdm(range(n_burnin), desc=\"Burn-in\", ncols=80):\n",
    "        key = jax.random.fold_in(key, i)\n",
    "        state, _ = step_fn(key, state)\n",
    "    burnin_time = time.time() - start_time\n",
    "\n",
    "    # Collect samples with progress bar\n",
    "    print(\"Collecting samples...\")\n",
    "    start_time = time.time()\n",
    "    for i in tqdm(range(n_samples), desc=\"Sampling\", ncols=80):\n",
    "        key = jax.random.fold_in(key, n_burnin + i)\n",
    "        state, _ = step_fn(key, state)\n",
    "        samples = samples.at[i].set(state.position)\n",
    "    sampling_time = time.time() - start_time\n",
    "\n",
    "    # Print timing info\n",
    "    total_time = burnin_time + sampling_time\n",
    "    samples_per_sec = n_samples / sampling_time\n",
    "    print(f\"\\nTiming: {total_time:.2f}s total ({samples_per_sec:.1f} samples/sec)\")\n",
    "\n",
    "    # Plot results\n",
    "    plot_samples(\n",
    "        samples,\n",
    "        \"Normal Distribution - Direct BlackJAX HMC\",\n",
    "        \"normal_blackjax_hmc.png\",\n",
    "        (true_mean, true_scale),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 2: Normal Distribution with hmc_sampling (Functional)\n",
    "\n",
    "This example demonstrates using the `hmc_sampling()` function, which provides a\n",
    "simplified interface for the same task. This is the recommended approach for most\n",
    "use cases.\n",
    "\n",
    "Key points:\n",
    "- Single function call replaces manual state management\n",
    "- Automatic burn-in handling\n",
    "- Direct integration with Artifex distributions\n",
    "- Cleaner, more concise code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_normal_hmc_function():\n",
    "    \"\"\"Sample from a normal distribution using hmc_sampling function.\"\"\"\n",
    "    print()\n",
    "    print(\"===== Example: Normal Distribution with hmc_sampling =====\")\n",
    "\n",
    "    # Create distribution\n",
    "    true_mean = jnp.array([3.0, -2.0])\n",
    "    true_scale = jnp.array([1.5, 0.8])\n",
    "    normal_dist = Normal(loc=true_mean, scale=true_scale)\n",
    "\n",
    "    # Initial position\n",
    "    init_position = jnp.zeros(2)\n",
    "\n",
    "    # Set sampling parameters\n",
    "    key = jax.random.key(1)\n",
    "    n_samples = 10000\n",
    "    n_burnin = 5000\n",
    "\n",
    "    # Warm-up run to compile JIT (don't count in timing)\n",
    "    print(\"Warming up JIT compilation...\")\n",
    "    _ = hmc_sampling(\n",
    "        normal_dist,\n",
    "        init_position,\n",
    "        jax.random.key(999999),\n",
    "        n_samples=10,\n",
    "        n_burnin=5,\n",
    "        step_size=0.1,\n",
    "        num_integration_steps=10,\n",
    "    )\n",
    "\n",
    "    print(f\"Sampling {n_samples} samples with {n_burnin} burn-in...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the functional API (already JIT-compiled from warm-up)\n",
    "    samples = hmc_sampling(\n",
    "        normal_dist,\n",
    "        init_position,\n",
    "        key,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=n_burnin,\n",
    "        step_size=0.1,\n",
    "        num_integration_steps=10,\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    samples_per_sec = n_samples / total_time\n",
    "    print(f\"Timing: {total_time:.2f}s total ({samples_per_sec:.1f} samples/sec)\")\n",
    "\n",
    "    # Plot results\n",
    "    plot_samples(\n",
    "        samples,\n",
    "        \"Normal Distribution - hmc_sampling Function\",\n",
    "        \"normal_hmc_function.png\",\n",
    "        (true_mean, true_scale),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 3: Normal Distribution with Direct BlackJAX MALA\n",
    "\n",
    "This example demonstrates using BlackJAX's MALA sampler directly. MALA is often\n",
    "faster per iteration than HMC but may require more iterations for the same effective\n",
    "sample size.\n",
    "\n",
    "MALA uses gradient information through Langevin dynamics, making it efficient for\n",
    "smooth distributions with well-behaved gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_normal_mala():\n",
    "    \"\"\"Sample from a normal distribution using BlackJAX MALA directly.\"\"\"\n",
    "    print()\n",
    "    print(\"===== Example: Normal Distribution with Direct BlackJAX MALA =====\")\n",
    "\n",
    "    # Create distribution\n",
    "    true_mean = jnp.array([3.0, -2.0])\n",
    "    true_scale = jnp.array([1.5, 0.8])\n",
    "    normal_dist = Normal(loc=true_mean, scale=true_scale)\n",
    "\n",
    "    # Create log probability function that returns scalar\n",
    "    def logdensity_fn(x):\n",
    "        log_prob = normal_dist.log_prob(x)\n",
    "        if hasattr(log_prob, \"shape\") and len(log_prob.shape) > 0:\n",
    "            return jnp.sum(log_prob)\n",
    "        return log_prob\n",
    "\n",
    "    # Initial position and parameters\n",
    "    init_position = jnp.zeros(2)\n",
    "    step_size = 0.1\n",
    "\n",
    "    # Create MALA sampler\n",
    "    mala = blackjax.mala(logdensity_fn, step_size)\n",
    "\n",
    "    # Initialize sampler state\n",
    "    state = mala.init(init_position)\n",
    "\n",
    "    # JIT compile the step function for speed\n",
    "    step_fn = jax.jit(mala.step)\n",
    "\n",
    "    # Warm-up JIT compilation (don't count this in timing)\n",
    "    key = jax.random.key(2)\n",
    "    warmup_key = jax.random.fold_in(key, 999999)\n",
    "    _, _ = step_fn(warmup_key, state)\n",
    "\n",
    "    # Number of samples to collect\n",
    "    n_samples = 10000\n",
    "    n_burnin = 2000\n",
    "\n",
    "    # Collect samples\n",
    "    samples = jnp.zeros((n_samples, 2))\n",
    "\n",
    "    # Run burn-in with progress bar\n",
    "    print(\"Running burn-in...\")\n",
    "    start_time = time.time()\n",
    "    for i in tqdm(range(n_burnin), desc=\"Burn-in\", ncols=80):\n",
    "        key = jax.random.fold_in(key, i)\n",
    "        state, _ = step_fn(key, state)\n",
    "    burnin_time = time.time() - start_time\n",
    "\n",
    "    # Collect samples with progress bar\n",
    "    print(\"Collecting samples...\")\n",
    "    start_time = time.time()\n",
    "    for i in tqdm(range(n_samples), desc=\"Sampling\", ncols=80):\n",
    "        key = jax.random.fold_in(key, n_burnin + i)\n",
    "        state, _ = step_fn(key, state)\n",
    "        samples = samples.at[i].set(state.position)\n",
    "    sampling_time = time.time() - start_time\n",
    "\n",
    "    # Print timing info\n",
    "    total_time = burnin_time + sampling_time\n",
    "    samples_per_sec = n_samples / sampling_time\n",
    "    print(f\"\\nTiming: {total_time:.2f}s total ({samples_per_sec:.1f} samples/sec)\")\n",
    "\n",
    "    # Plot results\n",
    "    plot_samples(\n",
    "        samples,\n",
    "        \"Normal Distribution - Direct BlackJAX MALA\",\n",
    "        \"normal_blackjax_mala.png\",\n",
    "        (true_mean, true_scale),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 4: Univariate Normal with Direct BlackJAX NUTS\n",
    "\n",
    "This example demonstrates NUTS sampling on a univariate (1D) normal distribution.\n",
    "NUTS automatically tunes the HMC trajectory length, but can be memory-intensive.\n",
    "\n",
    "Note: We use a 1D distribution and fewer samples to avoid memory issues. NUTS stores\n",
    "trajectory information which scales with dimensionality and number of doublings.\n",
    "\n",
    "The example includes error handling to gracefully handle potential memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_univariate_normal_nuts():\n",
    "    \"\"\"Sample from a univariate normal distribution using BlackJAX NUTS directly.\"\"\"\n",
    "    print()\n",
    "    print(\"===== Example: Univariate Normal with Direct BlackJAX NUTS =====\")\n",
    "    print(\"Note: This may fail if memory is limited\")\n",
    "\n",
    "    # Create univariate normal distribution\n",
    "    true_mean = jnp.array([2.0])\n",
    "    true_scale = jnp.array([1.0])\n",
    "    normal_dist = Normal(loc=true_mean, scale=true_scale)\n",
    "\n",
    "    try:\n",
    "        # Create log probability function that returns scalar\n",
    "        def logdensity_fn(x):\n",
    "            log_prob = normal_dist.log_prob(x)\n",
    "            if hasattr(log_prob, \"shape\") and len(log_prob.shape) > 0:\n",
    "                return jnp.sum(log_prob)\n",
    "            return log_prob\n",
    "\n",
    "        # Initial position and parameters\n",
    "        init_position = jnp.zeros(1)\n",
    "        step_size = 0.1\n",
    "        inverse_mass_matrix = jnp.ones(1)\n",
    "\n",
    "        # Create NUTS sampler\n",
    "        nuts = blackjax.nuts(logdensity_fn, step_size, inverse_mass_matrix)\n",
    "\n",
    "        # Initialize sampler state\n",
    "        state = nuts.init(init_position)\n",
    "\n",
    "        # JIT compile the step function for speed\n",
    "        step_fn = jax.jit(nuts.step)\n",
    "\n",
    "        # Warm-up JIT compilation (don't count this in timing)\n",
    "        key = jax.random.key(3)\n",
    "        warmup_key = jax.random.fold_in(key, 999999)\n",
    "        _, _ = step_fn(warmup_key, state)\n",
    "\n",
    "        # Number of samples to collect\n",
    "        n_samples = 5000  # Fewer samples for NUTS due to memory\n",
    "        n_burnin = 1000\n",
    "\n",
    "        # Collect samples\n",
    "        samples = jnp.zeros((n_samples, 1))\n",
    "\n",
    "        # Run burn-in with progress bar\n",
    "        print(\"Running burn-in...\")\n",
    "        start_time = time.time()\n",
    "        for i in tqdm(range(n_burnin), desc=\"Burn-in\", ncols=80):\n",
    "            key = jax.random.fold_in(key, i)\n",
    "            state, _ = step_fn(key, state)\n",
    "        burnin_time = time.time() - start_time\n",
    "\n",
    "        # Collect samples with progress bar\n",
    "        print(\"Collecting samples...\")\n",
    "        start_time = time.time()\n",
    "        for i in tqdm(range(n_samples), desc=\"Sampling\", ncols=80):\n",
    "            key = jax.random.fold_in(key, n_burnin + i)\n",
    "            state, _ = step_fn(key, state)\n",
    "            samples = samples.at[i].set(state.position)\n",
    "        sampling_time = time.time() - start_time\n",
    "\n",
    "        # Print timing info\n",
    "        total_time = burnin_time + sampling_time\n",
    "        samples_per_sec = n_samples / sampling_time\n",
    "        print(f\"\\nTiming: {total_time:.2f}s total ({samples_per_sec:.1f} samples/sec)\")\n",
    "\n",
    "        # Plot results\n",
    "        plot_samples(\n",
    "            samples,\n",
    "            \"Univariate Normal - Direct BlackJAX NUTS\",\n",
    "            \"univariate_blackjax_nuts.png\",\n",
    "            (true_mean, true_scale),\n",
    "        )\n",
    "    except (RuntimeError, MemoryError) as e:\n",
    "        print(f\"NUTS sampling failed due to: {e}\")\n",
    "        print(\"This is likely due to memory constraints.\")\n",
    "        print(\"Try using HMC or MALA instead, or reduce parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 5: Mixture of Gaussians with mala_sampling\n",
    "\n",
    "This example demonstrates sampling from a mixture distribution using the functional API.\n",
    "Mixture distributions are multimodal and can be challenging for MCMC samplers.\n",
    "\n",
    "We create a mixture of two Gaussians with different weights, means, and scales, then\n",
    "use MALA to sample from the mixture distribution. The Artifex Mixture class handles\n",
    "the log probability calculation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_mixture_mala_function():\n",
    "    \"\"\"Sample from a mixture of Gaussians using mala_sampling function.\n",
    "\n",
    "    This example demonstrates MALA's limitations with multimodal distributions.\n",
    "    MALA is a local sampler that takes small gradient-guided steps, making it\n",
    "    difficult to jump between distant modes separated by low-probability regions.\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"===== Example 5a: Mixture of Gaussians with MALA (Demonstrating Limitations) =====\")\n",
    "\n",
    "    # Use 1D mixture with widely separated modes\n",
    "    weights = jnp.array([0.6, 0.4])\n",
    "    means = jnp.array([[-2.0], [8.0]])  # 10 units apart - very distant\n",
    "    scales = jnp.array([[0.8], [0.8]])\n",
    "\n",
    "    components = [Normal(loc=means[0], scale=scales[0]), Normal(loc=means[1], scale=scales[1])]\n",
    "\n",
    "    mixture = Mixture(components, weights)\n",
    "\n",
    "    # Initial position\n",
    "    init_position = jnp.array([-2.0])  # Start at first mode\n",
    "\n",
    "    # Set sampling parameters\n",
    "    key = jax.random.key(4)\n",
    "    n_samples = 10000\n",
    "    n_burnin = 5000\n",
    "\n",
    "    # Warm-up run to compile JIT (don't count in timing)\n",
    "    print(\"Warming up JIT compilation...\")\n",
    "    _ = mala_sampling(\n",
    "        mixture,\n",
    "        init_position,\n",
    "        jax.random.key(999999),\n",
    "        n_samples=10,\n",
    "        n_burnin=5,\n",
    "        step_size=0.05,\n",
    "    )\n",
    "\n",
    "    print(f\"Sampling {n_samples} samples with {n_burnin} burn-in...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the functional API (already JIT-compiled from warm-up)\n",
    "    samples = mala_sampling(\n",
    "        mixture,  # Directly pass the mixture distribution\n",
    "        init_position,\n",
    "        key,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=n_burnin,\n",
    "        step_size=0.05,\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    samples_per_sec = n_samples / total_time\n",
    "    print(f\"Timing: {total_time:.2f}s total ({samples_per_sec:.1f} samples/sec)\")\n",
    "\n",
    "    # Print mixture information\n",
    "    sep_dist = abs(float(means[1][0] - means[0][0]))\n",
    "    m1, s1 = float(means[0][0]), float(scales[0][0])\n",
    "    m2, s2 = float(means[1][0]), float(scales[1][0])\n",
    "    print(\"\\nMixture Distribution Info (1D):\")\n",
    "    print(f\"  Component 1 (weight={weights[0]:.1f}): mean={m1:.1f}, scale={s1:.1f}\")\n",
    "    print(f\"  Component 2 (weight={weights[1]:.1f}): mean={m2:.1f}, scale={s2:.1f}\")\n",
    "    print(f\"  Separation: {sep_dist:.1f} units (widely separated)\")\n",
    "    print(f\"  Step size: {0.05}\")\n",
    "    print(\"\\n⚠️  Educational Note:\")\n",
    "    print(\"  MALA is a LOCAL sampler - it takes small gradient-guided steps.\")\n",
    "    print(f\"  With step_size=0.05 and modes {sep_dist:.0f} units apart, MALA will\")\n",
    "    print(f\"  get stuck in one mode (starting at {m1:.1f}).\")\n",
    "    print(\"  This demonstrates why algorithm choice matters for multimodal distributions!\")\n",
    "    print(\"  See Example 5b: NUTS handles MODERATE separation better.\\n\")\n",
    "\n",
    "    # Calculate sample statistics\n",
    "    sample_mean = jnp.mean(samples, axis=0)\n",
    "    sample_std = jnp.std(samples, axis=0)\n",
    "\n",
    "    # Check mode occupancy (1D)\n",
    "    dist_to_mode1 = jnp.abs(samples[:, 0] - means[0][0])\n",
    "    dist_to_mode2 = jnp.abs(samples[:, 0] - means[1][0])\n",
    "    in_mode1 = (dist_to_mode1 < dist_to_mode2).sum()\n",
    "    in_mode2 = (dist_to_mode2 <= dist_to_mode1).sum()\n",
    "    pct_mode1 = 100 * in_mode1 / n_samples\n",
    "    pct_mode2 = 100 * in_mode2 / n_samples\n",
    "\n",
    "    print(\"\\nSample Statistics:\")\n",
    "    print(f\"  Sample mean: {float(sample_mean[0]):>7.4f}\")\n",
    "    print(f\"  Sample std:  {float(sample_std[0]):>7.4f}\")\n",
    "    w1_pct = weights[0] * 100\n",
    "    w2_pct = weights[1] * 100\n",
    "    print(f\"\\nMode occupancy (should be [{w1_pct:.0f}%, {w2_pct:.0f}%]):\")\n",
    "    print(f\"  Mode 1 at {m1:.1f}: {pct_mode1:.1f}% of samples\")\n",
    "    print(f\"  Mode 2 at {m2:.1f}: {pct_mode2:.1f}% of samples\")\n",
    "    if pct_mode1 > 95 or pct_mode2 > 95:\n",
    "        stuck_mode = 1 if pct_mode1 > 95 else 2\n",
    "        print(f\"  ✗ MALA stuck in mode {stuck_mode} - too distant for local sampler\")\n",
    "    else:\n",
    "        print(\"  ⚠ Partial exploration - better than expected for MALA!\")\n",
    "\n",
    "    # Plot results (no true_mean for mixture)\n",
    "    plot_samples(\n",
    "        samples,\n",
    "        \"Example 5a: 1D Mixture with MALA (Wide Separation)\",\n",
    "        \"mixture_mala_limitation.png\",\n",
    "        (None, None),  # Don't show \"true\" statistics for mixture\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 5b: Mixture of Gaussians with NUTS (Better but Not Perfect)\n",
    "\n",
    "Now let's demonstrate NUTS on a multimodal distribution with moderately-separated modes.\n",
    "NUTS uses Hamiltonian dynamics for better exploration than MALA, but still faces\n",
    "challenges with very distant modes due to energy conservation constraints.\n",
    "\n",
    "This example uses **closer modes** (5.7 units apart) vs Example 5a (14.1 units),\n",
    "showing that NUTS can handle moderate multimodality better than local samplers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_mixture_nuts_function():\n",
    "    \"\"\"Sample from a mixture of Gaussians using nuts_sampling function.\n",
    "\n",
    "    This example demonstrates NUTS's improved (but not perfect) performance on\n",
    "    moderately-separated multimodal distributions. NUTS uses Hamiltonian dynamics\n",
    "    for better exploration than MALA, but energy constraints still limit mode-switching\n",
    "    when modes are very far apart. For extreme multimodality, parallel tempering or\n",
    "    SMC methods are needed.\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"===== Example 5b: Mixture with NUTS (Better Exploration, Closer Modes) =====\")\n",
    "\n",
    "    # Use 1D mixture for clearer demonstration\n",
    "    # Note: Even NUTS struggles with very distant modes due to energy constraints\n",
    "    weights = jnp.array([0.6, 0.4])\n",
    "    means = jnp.array([[-2.0], [3.0]])  # 5 units apart in 1D\n",
    "    scales = jnp.array([[0.8], [0.8]])\n",
    "\n",
    "    components = [Normal(loc=means[0], scale=scales[0]), Normal(loc=means[1], scale=scales[1])]\n",
    "\n",
    "    mixture = Mixture(components, weights)\n",
    "\n",
    "    # Initial position\n",
    "    init_position = jnp.array([-2.0])  # Start at first mode\n",
    "\n",
    "    # Set sampling parameters\n",
    "    key = jax.random.key(5)\n",
    "    n_samples = 10000\n",
    "    n_burnin = 5000\n",
    "\n",
    "    # Warm-up run to compile JIT (don't count in timing)\n",
    "    print(\"Warming up JIT compilation...\")\n",
    "    _ = nuts_sampling(\n",
    "        mixture,\n",
    "        init_position,\n",
    "        jax.random.key(999999),\n",
    "        n_samples=10,\n",
    "        n_burnin=5,\n",
    "        step_size=0.5,  # Initial step size; NUTS adapts this\n",
    "    )\n",
    "\n",
    "    print(f\"Sampling {n_samples} samples with {n_burnin} burn-in...\")\n",
    "    print(\"(NUTS automatically adapts step size and trajectory length)\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the functional API (already JIT-compiled from warm-up)\n",
    "    samples = nuts_sampling(\n",
    "        mixture,  # Directly pass the mixture distribution\n",
    "        init_position,\n",
    "        key,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=n_burnin,\n",
    "        step_size=0.5,  # Initial step size; NUTS adapts during burn-in\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    samples_per_sec = n_samples / total_time\n",
    "    print(f\"Timing: {total_time:.2f}s total ({samples_per_sec:.1f} samples/sec)\")\n",
    "\n",
    "    # Print mixture information\n",
    "    sep_dist = abs(float(means[1][0] - means[0][0]))\n",
    "    m1, s1 = float(means[0][0]), float(scales[0][0])\n",
    "    m2, s2 = float(means[1][0]), float(scales[1][0])\n",
    "    print(\"\\nMixture Distribution Info (1D for clarity):\")\n",
    "    print(f\"  Component 1 (weight={weights[0]:.1f}): mean={m1:.1f}, scale={s1:.1f}\")\n",
    "    print(f\"  Component 2 (weight={weights[1]:.1f}): mean={m2:.1f}, scale={s2:.1f}\")\n",
    "    print(f\"  Separation: {sep_dist:.1f} units (vs 14.1 in Example 5a)\")\n",
    "    print(\"\\n✓ Educational Note:\")\n",
    "    print(\"  NUTS uses HAMILTONIAN DYNAMICS - momentum allows better exploration than MALA.\")\n",
    "    print(\"  With MODERATELY-separated modes (5 units), NUTS can transition between modes.\")\n",
    "    print(\"  However, even NUTS struggles with VERY distant modes due to energy constraints:\")\n",
    "    print(\"  Maximum potential energy increase is bounded by initial kinetic energy.\")\n",
    "    print(\"  For extreme multimodality: use parallel tempering, SMC, or tempered transitions.\")\n",
    "    print(\"  Compare with Example 5a: NUTS explores moderate separation better than MALA.\\n\")\n",
    "\n",
    "    # Calculate sample statistics\n",
    "    sample_mean = jnp.mean(samples, axis=0)\n",
    "    sample_std = jnp.std(samples, axis=0)\n",
    "\n",
    "    # Check mode occupancy (1D)\n",
    "    dist_to_mode1 = jnp.abs(samples[:, 0] - means[0][0])\n",
    "    dist_to_mode2 = jnp.abs(samples[:, 0] - means[1][0])\n",
    "    # Assign each sample to nearest mode\n",
    "    in_mode1 = (dist_to_mode1 < dist_to_mode2).sum()\n",
    "    in_mode2 = (dist_to_mode2 <= dist_to_mode1).sum()\n",
    "    pct_mode1 = 100 * in_mode1 / n_samples\n",
    "    pct_mode2 = 100 * in_mode2 / n_samples\n",
    "\n",
    "    print(\"\\nSample Statistics:\")\n",
    "    print(f\"  Sample mean: {float(sample_mean[0]):>7.4f}\")\n",
    "    print(f\"  Sample std:  {float(sample_std[0]):>7.4f}\")\n",
    "    w1_pct = weights[0] * 100\n",
    "    w2_pct = weights[1] * 100\n",
    "    print(f\"\\nMode occupancy (should match weights [{w1_pct:.0f}%, {w2_pct:.0f}%]):\")\n",
    "    print(f\"  Mode 1 at {m1:.1f}: {pct_mode1:.1f}% of samples\")\n",
    "    print(f\"  Mode 2 at {m2:.1f}: {pct_mode2:.1f}% of samples\")\n",
    "    expected_pct1 = weights[0] * 100\n",
    "    expected_pct2 = weights[1] * 100\n",
    "    if abs(pct_mode1 - expected_pct1) < 15 and abs(pct_mode2 - expected_pct2) < 15:\n",
    "        print(\"  ✓ NUTS successfully explored both modes!\")\n",
    "    elif pct_mode1 > 95 or pct_mode2 > 95:\n",
    "        print(\"  ✗ NUTS got stuck in one mode - modes too far apart for energy constraints\")\n",
    "    else:\n",
    "        print(\"  ⚠ Partial exploration - NUTS found both modes but proportions are off\")\n",
    "\n",
    "    # Plot results (no true_mean for mixture)\n",
    "    plot_samples(\n",
    "        samples,\n",
    "        \"Example 5b: 1D Mixture with NUTS (Moderate Separation)\",\n",
    "        \"mixture_nuts_moderate.png\",\n",
    "        (None, None),  # Don't show \"true\" statistics for mixture\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Running All Examples\n",
    "\n",
    "Now let's run all integration examples to see both class-based and functional APIs\n",
    "in action, including the multimodal distribution comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"===== Testing BlackJAX integration with Artifex distributions =====\")\n",
    "\n",
    "    # Run examples with BlackJAX Direct API\n",
    "    example_normal_hmc()\n",
    "    example_normal_mala()\n",
    "    example_univariate_normal_nuts()\n",
    "\n",
    "    # Run examples with Artifex Functional API\n",
    "    example_normal_hmc_function()\n",
    "\n",
    "    # Run multimodal distribution comparison\n",
    "    example_mixture_mala_function()  # Example 5a: MALA struggles\n",
    "    example_mixture_nuts_function()  # Example 5b: NUTS succeeds\n",
    "\n",
    "    print()\n",
    "    print(\"All examples complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "After running this example, you should understand:\n",
    "\n",
    "1. **Integration Approaches**:\n",
    "   - Direct BlackJAX API: Maximum flexibility and control\n",
    "   - Artifex Functional API: Simplified interface, recommended for most use cases\n",
    "\n",
    "2. **Distribution Integration**: Artifex distributions work seamlessly with BlackJAX\n",
    "   through their `log_prob` methods - just wrap them to return scalars\n",
    "\n",
    "3. **Key BlackJAX Pattern**:\n",
    "   - Create sampler: `sampler = blackjax.hmc(logdensity_fn, step_size, ...)`\n",
    "   - Initialize state: `state = sampler.init(position)`\n",
    "   - Step with random key: `state, info = sampler.step(key, state)`\n",
    "\n",
    "4. **Memory Management**: NUTS requires careful memory management due to trajectory\n",
    "   storage. Use lower dimensions and fewer samples when memory is limited\n",
    "\n",
    "5. **Mixture Sampling**: Artifex's Mixture class enables easy MCMC sampling from\n",
    "   multimodal distributions\n",
    "\n",
    "6. **Random Key Management**: Always use `jax.random.fold_in()` to generate unique\n",
    "   keys for each sampling step\n",
    "\n",
    "## Experiments to Try\n",
    "\n",
    "1. **Compare APIs**: Time class-based vs functional approaches for the same task\n",
    "2. **Higher dimensions**: Try 5D or 10D normal distributions to see scalability\n",
    "3. **Complex mixtures**: Create mixtures with 3+ components and varying weights\n",
    "4. **Custom distributions**: Create custom distributions by defining log_prob functions\n",
    "5. **Convergence diagnostics**: Implement R-hat and effective sample size calculations\n",
    "6. **Adaptive tuning**: Experiment with different step sizes and integration steps\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Review `blackjax_example.py` for simpler introductory examples\n",
    "- Explore `blackjax_sampling_examples.py` for more sampling algorithms\n",
    "- Read Artifex distributions documentation for available distributions\n",
    "- Check BlackJAX documentation for additional sampler options\n",
    "- Learn about advanced diagnostics for MCMC convergence assessment"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
