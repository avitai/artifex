{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# BlackJAX Integration Example\n",
    "\n",
    "This example demonstrates how to use BlackJAX samplers with Artifex's distribution framework.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this example, you will:\n",
    "\n",
    "- [ ] Understand how to use BlackJAX samplers (HMC, NUTS, MALA) with Artifex\n",
    "- [ ] Learn to sample from multimodal distributions using different MCMC methods\n",
    "- [ ] Implement Bayesian regression using NUTS sampling\n",
    "- [ ] Compare different sampling algorithms for the same problem\n",
    "- [ ] Visualize and interpret MCMC sampling results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of MCMC sampling concepts\n",
    "- Basic knowledge of Bayesian inference\n",
    "- Familiarity with probability distributions\n",
    "- Artifex core sampling module\n",
    "\n",
    "## What is BlackJAX?\n",
    "\n",
    "BlackJAX is a library of samplers for JAX that provides state-of-the-art MCMC algorithms.\n",
    "Artifex integrates BlackJAX to offer advanced sampling capabilities including:\n",
    "\n",
    "- **HMC (Hamiltonian Monte Carlo)**: Uses gradient information for efficient sampling\n",
    "- **NUTS (No-U-Turn Sampler)**: Automatically tunes HMC step size and trajectory length\n",
    "- **MALA (Metropolis-Adjusted Langevin Algorithm)**: Gradient-based Metropolis method\n",
    "\n",
    "These algorithms are particularly useful for:\n",
    "- Sampling from complex, high-dimensional distributions\n",
    "- Bayesian inference and parameter estimation\n",
    "- Comparing sampling efficiency across methods\n",
    "\n",
    "## Example Overview\n",
    "\n",
    "This example includes two demonstrations:\n",
    "\n",
    "1. **Multimodal Distribution Sampling**: Compare different samplers on a bimodal Gaussian mixture\n",
    "2. **Bayesian Regression**: Use NUTS to infer parameters in a linear regression model\n",
    "\n",
    "## Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required libraries.\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from artifex.generative_models.core.sampling import (\n",
    "    hmc_sampling,\n",
    "    mala_sampling,\n",
    "    mcmc_sampling,\n",
    "    nuts_sampling,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Multimodal Distribution Example\n",
    "\n",
    "This example demonstrates sampling from a bimodal distribution using different MCMC methods.\n",
    "We'll compare the performance of:\n",
    "\n",
    "- Metropolis-Hastings (our basic implementation)\n",
    "- HMC (BlackJAX)\n",
    "- NUTS (BlackJAX)\n",
    "- MALA (BlackJAX)\n",
    "\n",
    "The target distribution is a mixture of two Gaussians centered at x=-2 and x=+2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_distribution_example():\n",
    "    \"\"\"Example with a multimodal distribution.\"\"\"\n",
    "    print(\"Running multimodal distribution example...\")\n",
    "\n",
    "    # Define a multi-modal log probability function (mixture of two Gaussians)\n",
    "    def log_prob_fn(x):\n",
    "        # Create bimodal distribution (mixture of Gaussians)\n",
    "        log_prob1 = -0.5 * ((x - 2.0) ** 2) / 0.5\n",
    "        log_prob2 = -0.5 * ((x + 2.0) ** 2) / 0.5\n",
    "        return jnp.logaddexp(log_prob1, log_prob2)\n",
    "\n",
    "    # Initial state and key\n",
    "    init_state = jnp.array(0.0)\n",
    "    key = jax.random.key(0)\n",
    "\n",
    "    # Sample using different methods\n",
    "    n_samples = 2000\n",
    "\n",
    "    # Regular Metropolis-Hastings (our implementation)\n",
    "    mh_samples = mcmc_sampling(\n",
    "        log_prob_fn=log_prob_fn,\n",
    "        init_state=init_state,\n",
    "        key=key,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=500,\n",
    "        step_size=0.5,\n",
    "    )\n",
    "\n",
    "    # HMC sampling (BlackJAX)\n",
    "    hmc_samples = hmc_sampling(\n",
    "        log_prob_fn=log_prob_fn,\n",
    "        init_state=init_state,\n",
    "        key=key,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=500,\n",
    "        step_size=0.1,\n",
    "        num_integration_steps=10,\n",
    "    )\n",
    "\n",
    "    # NUTS sampling (BlackJAX)\n",
    "    nuts_samples = nuts_sampling(\n",
    "        log_prob_fn=log_prob_fn,\n",
    "        init_state=init_state,\n",
    "        key=key,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=500,\n",
    "    )\n",
    "\n",
    "    # MALA sampling (BlackJAX)\n",
    "    mala_samples = mala_sampling(\n",
    "        log_prob_fn=log_prob_fn,\n",
    "        init_state=init_state,\n",
    "        key=key,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=500,\n",
    "        step_size=0.1,\n",
    "    )\n",
    "\n",
    "    # Plot histograms of samples\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(mh_samples, bins=50, alpha=0.7, density=True)\n",
    "    plt.title(\"Metropolis-Hastings\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(hmc_samples, bins=50, alpha=0.7, density=True)\n",
    "    plt.title(\"Hamiltonian Monte Carlo (BlackJAX)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(nuts_samples, bins=50, alpha=0.7, density=True)\n",
    "    plt.title(\"NUTS (BlackJAX)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(mala_samples, bins=50, alpha=0.7, density=True)\n",
    "    plt.title(\"MALA (BlackJAX)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"blackjax_multimodal_comparison.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Multimodal sampling complete. Results saved to blackjax_multimodal_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Bayesian Regression Example\n",
    "\n",
    "This example demonstrates using NUTS to perform Bayesian linear regression.\n",
    "We'll:\n",
    "\n",
    "1. Generate synthetic data with known parameters\n",
    "2. Define a Bayesian model with priors on coefficients and noise\n",
    "3. Use NUTS to sample from the posterior distribution\n",
    "4. Visualize the posterior distributions and compare to true values\n",
    "\n",
    "NUTS is particularly well-suited for Bayesian regression because it:\n",
    "- Efficiently explores high-dimensional parameter spaces\n",
    "- Automatically adapts step size and trajectory length\n",
    "- Provides robust sampling without manual tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_regression_example():\n",
    "    \"\"\"Example with Bayesian regression.\"\"\"\n",
    "    print(\"Running Bayesian regression example...\")\n",
    "\n",
    "    # Generate synthetic data\n",
    "    key = jax.random.key(0)\n",
    "    key, subkey = jax.random.split(key)\n",
    "\n",
    "    n_samples = 100\n",
    "    n_features = 5\n",
    "\n",
    "    # True parameters\n",
    "    true_beta = jnp.array([0.5, -0.3, 0.8, -0.2, 0.4])\n",
    "    noise_scale = 0.1\n",
    "\n",
    "    # Generate features\n",
    "    X = jax.random.normal(key, (n_samples, n_features))\n",
    "\n",
    "    # Generate target with noise\n",
    "    key, subkey = jax.random.split(key)\n",
    "    y = X @ true_beta + noise_scale * jax.random.normal(subkey, (n_samples,))\n",
    "\n",
    "    # Define log probability function for Bayesian regression\n",
    "    def log_prob_fn(params):\n",
    "        # Unpack parameters\n",
    "        beta = params[\"beta\"]\n",
    "        log_sigma = params[\"log_sigma\"]\n",
    "        sigma = jnp.exp(log_sigma)\n",
    "\n",
    "        # Prior\n",
    "        prior_beta = jnp.sum(jax.scipy.stats.norm.logpdf(beta, loc=0.0, scale=1.0))\n",
    "        prior_sigma = jax.scipy.stats.norm.logpdf(log_sigma, loc=-2.0, scale=1.0)\n",
    "\n",
    "        # Likelihood\n",
    "        y_pred = X @ beta\n",
    "        likelihood = jnp.sum(jax.scipy.stats.norm.logpdf(y, loc=y_pred, scale=sigma))\n",
    "\n",
    "        return prior_beta + prior_sigma + likelihood\n",
    "\n",
    "    # Initial state\n",
    "    init_state = {\n",
    "        \"beta\": jnp.zeros(n_features),\n",
    "        \"log_sigma\": jnp.array(0.0),\n",
    "    }\n",
    "\n",
    "    # Sample using NUTS (the best choice for this problem)\n",
    "    key = jax.random.key(1)  # New key for sampling\n",
    "    nuts_samples = nuts_sampling(\n",
    "        log_prob_fn=log_prob_fn,\n",
    "        init_state=init_state,\n",
    "        key=key,\n",
    "        n_samples=2000,\n",
    "        n_burnin=1000,\n",
    "    )\n",
    "\n",
    "    # Extract samples\n",
    "    beta_samples = nuts_samples[\"beta\"]\n",
    "    sigma_samples = jnp.exp(nuts_samples[\"log_sigma\"])\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot coefficient distributions\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(n_features):\n",
    "        beta_label = f\"$\\\\beta_{i}$ (true: {true_beta[i]:.2f})\"\n",
    "        plt.hist(beta_samples[:, i], bins=30, alpha=0.6, label=beta_label)\n",
    "    plt.axvline(x=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.title(\"Posterior Distributions of Coefficients\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot sigma distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(sigma_samples, bins=30, alpha=0.6)\n",
    "    sigma_label = f\"True $\\\\sigma$: {noise_scale:.2f}\"\n",
    "    plt.axvline(x=noise_scale, color=\"r\", linestyle=\"--\", label=sigma_label)\n",
    "    plt.title(\"Posterior Distribution of Noise Scale ($\\\\sigma$)\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"blackjax_regression_example.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Bayesian regression complete. Results saved to blackjax_regression_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Running the Examples\n",
    "\n",
    "Now let's run both examples to see the different sampling methods in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the examples.\"\"\"\n",
    "    multimodal_distribution_example()\n",
    "    print()\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "    bayesian_regression_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "After running this example, you should understand:\n",
    "\n",
    "1. **BlackJAX Integration**: Artifex provides seamless integration with BlackJAX samplers\n",
    "2. **Sampler Selection**: Different samplers have different strengths:\n",
    "   - NUTS: Best for complex posteriors, automatic tuning\n",
    "   - HMC: Good balance of efficiency and control\n",
    "   - MALA: Gradient-based Metropolis for smooth distributions\n",
    "   - MH: Simple baseline, useful for comparison\n",
    "3. **Bayesian Inference**: NUTS excels at Bayesian parameter estimation\n",
    "4. **Multimodal Distributions**: All samplers can handle multimodal targets, but with\n",
    "   varying efficiency\n",
    "\n",
    "## Experiments to Try\n",
    "\n",
    "1. **Change the distribution**: Modify the bimodal distribution to have three modes or\n",
    "   varying widths\n",
    "2. **Tune hyperparameters**: Experiment with different step sizes, integration steps,\n",
    "   and burn-in periods\n",
    "3. **Compare convergence**: Track acceptance rates and effective sample sizes across\n",
    "   methods\n",
    "4. **Higher dimensions**: Extend the Bayesian regression to more features\n",
    "   (e.g., 20-50 dimensions)\n",
    "5. **Different priors**: Try different prior distributions on the regression\n",
    "   coefficients\n",
    "6. **Visualize traces**: Plot MCMC traces to check for convergence and mixing\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore `blackjax_sampling_examples.py` for more sampling algorithms\n",
    "- Learn about advanced diagnostics in `blackjax_integration_examples.py`\n",
    "- Check out the full BlackJAX documentation for more sampler options"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
