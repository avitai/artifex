{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# BlackJAX Sampling Examples for Generative Models\n",
    "\n",
    "This example demonstrates comprehensive usage of BlackJAX samplers integrated with Artifex's\n",
    "generative modeling framework.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completing this example, you will:\n",
    "\n",
    "- [ ] Understand different MCMC sampling algorithms (HMC, MALA, NUTS)\n",
    "- [ ] Learn to use Artifex's BlackJAX integration API\n",
    "- [ ] Compare Artifex's sampler wrappers with direct BlackJAX usage\n",
    "- [ ] Apply MCMC sampling to mixture distributions\n",
    "- [ ] Visualize and interpret sampling results\n",
    "- [ ] Handle memory constraints in NUTS sampling\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of MCMC sampling fundamentals\n",
    "- Familiarity with probability distributions\n",
    "- Basic knowledge of Hamiltonian Monte Carlo\n",
    "- Artifex core distributions and sampling modules\n",
    "\n",
    "## MCMC Algorithms Overview\n",
    "\n",
    "This example demonstrates three state-of-the-art MCMC algorithms:\n",
    "\n",
    "### HMC (Hamiltonian Monte Carlo)\n",
    "- Uses gradient information to propose efficient moves\n",
    "- Simulates Hamiltonian dynamics for exploration\n",
    "- Requires tuning of step size and number of integration steps\n",
    "- Excellent for smooth, continuous distributions\n",
    "\n",
    "### MALA (Metropolis-Adjusted Langevin Algorithm)\n",
    "- Gradient-based Metropolis method\n",
    "- Uses Langevin dynamics for proposals\n",
    "- Single step per iteration (faster than HMC)\n",
    "- Good for smooth posteriors with strong gradients\n",
    "\n",
    "### NUTS (No-U-Turn Sampler)\n",
    "- Automatically tunes HMC trajectory length\n",
    "- No manual tuning of integration steps needed\n",
    "- Adaptive step size selection\n",
    "- State-of-the-art for Bayesian inference\n",
    "\n",
    "## Example Overview\n",
    "\n",
    "This example includes:\n",
    "\n",
    "1. **Artifex HMC**: Using Artifex's HMC wrapper on mixture distribution\n",
    "2. **Artifex MALA**: Using Artifex's MALA wrapper on mixture distribution\n",
    "3. **Artifex NUTS**: Using Artifex's NUTS wrapper on standard normal\n",
    "4. **Direct BlackJAX**: Using BlackJAX HMC API directly for comparison\n",
    "\n",
    "## Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required libraries.\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import blackjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from flax import nnx\n",
    "\n",
    "from artifex.generative_models.core.distributions import Normal\n",
    "from artifex.generative_models.core.sampling.blackjax_samplers import (\n",
    "    hmc_sampling,\n",
    "    mala_sampling,\n",
    "    nuts_sampling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up output directory and random seed.\"\"\"\n",
    "\n",
    "EXAMPLES_DIR = Path(__file__).parent.parent.parent.parent / \"examples_output\"\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "We define helper functions to create target distributions and visualize sampling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mixture_logprob():\n",
    "    \"\"\"Create a simple 2D mixture of Gaussians log probability function.\n",
    "\n",
    "    This function creates a mixture of two Gaussian distributions located\n",
    "    at [3,3] and [-3,-3] with unit standard deviations.\n",
    "\n",
    "    Returns:\n",
    "        A function that computes the log probability of a point.\n",
    "    \"\"\"\n",
    "    # Define two Gaussian components for the mixture\n",
    "    mean1 = jnp.array([3.0, 3.0])\n",
    "    mean2 = jnp.array([-3.0, -3.0])\n",
    "\n",
    "    # Create log probability function\n",
    "    def log_prob_fn(x):\n",
    "        # Create two normal distributions\n",
    "        dist1 = Normal(loc=mean1, scale=jnp.array([1.0, 1.0]))\n",
    "        dist2 = Normal(loc=mean2, scale=jnp.array([1.0, 1.0]))\n",
    "\n",
    "        # Sum the log probabilities for each component\n",
    "        log_prob1 = jnp.sum(dist1.log_prob(x))\n",
    "        log_prob2 = jnp.sum(dist2.log_prob(x))\n",
    "\n",
    "        # Mixture log probability\n",
    "        return jnp.logaddexp(log_prob1, log_prob2) - jnp.log(2.0)\n",
    "\n",
    "    return log_prob_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normal_logprob():\n",
    "    \"\"\"Create a simple 2D normal distribution.\n",
    "\n",
    "    This function creates a standard normal distribution centered at origin.\n",
    "\n",
    "    Returns:\n",
    "        A function that computes the log probability of a point.\n",
    "    \"\"\"\n",
    "    # Just a single normal distribution\n",
    "    mean = jnp.array([0.0, 0.0])\n",
    "    scale = jnp.array([1.0, 1.0])\n",
    "\n",
    "    # Create log probability function\n",
    "    def log_prob_fn(x):\n",
    "        # Create a normal distribution\n",
    "        dist = Normal(loc=mean, scale=scale)\n",
    "        # Sum the log probabilities (ensure scalar output)\n",
    "        return jnp.sum(dist.log_prob(x))\n",
    "\n",
    "    return log_prob_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples, title=\"MCMC Samples\", filename=\"mcmc_samples.png\"):\n",
    "    \"\"\"Plot the sampling results.\n",
    "\n",
    "    Args:\n",
    "        samples: Array of samples with shape [n_samples, 2]\n",
    "        title: Plot title\n",
    "        filename: Filename to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, s=5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.grid(True)\n",
    "    plt.axis(\"equal\")\n",
    "    # Ensure output directory exists\n",
    "    EXAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(EXAMPLES_DIR / filename)\n",
    "    print(f\"Plot saved as '{EXAMPLES_DIR / filename}'\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    mean = jnp.mean(samples, axis=0)\n",
    "    std = jnp.std(samples, axis=0)\n",
    "\n",
    "    print()\n",
    "    print(\"Sample Statistics:\")\n",
    "    print(f\"Mean: {mean}\")\n",
    "    print(f\"Std: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 1: Artifex HMC Sampling\n",
    "\n",
    "This example demonstrates using Artifex's HMC wrapper to sample from a bimodal mixture\n",
    "of Gaussians. HMC uses Hamiltonian dynamics to propose moves, making it efficient for\n",
    "exploring smooth, continuous distributions.\n",
    "\n",
    "The mixture has two modes at [3,3] and [-3,-3], which tests the sampler's ability to\n",
    "explore multiple modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_hmc_artifex():\n",
    "    \"\"\"Example using artifex's HMC implementation.\"\"\"\n",
    "    print()\n",
    "    print(\"===== Example: Artifex HMC Sampling =====\")\n",
    "\n",
    "    # Create log probability function\n",
    "    mixture_logprob = create_mixture_logprob()\n",
    "\n",
    "    # Initial state\n",
    "    init_state = jnp.zeros(2)\n",
    "\n",
    "    # Set parameters\n",
    "    n_samples = 1000\n",
    "    n_burnin = 500\n",
    "\n",
    "    print(f\"Running {n_samples} samples with {n_burnin} burn-in steps...\")\n",
    "\n",
    "    # Run HMC sampling\n",
    "    key_hmc = jax.random.fold_in(key, 0)\n",
    "    hmc_samples = hmc_sampling(\n",
    "        mixture_logprob,\n",
    "        init_state,\n",
    "        key_hmc,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=n_burnin,\n",
    "        step_size=0.1,\n",
    "        num_integration_steps=10,\n",
    "    )\n",
    "    plot_samples(hmc_samples, \"Artifex HMC Sampling\", \"hmc_artifex_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 2: Artifex MALA Sampling\n",
    "\n",
    "MALA (Metropolis-Adjusted Langevin Algorithm) is a gradient-based sampler that uses\n",
    "Langevin dynamics for proposals. It's faster than HMC per iteration but may require\n",
    "more iterations to achieve the same effective sample size.\n",
    "\n",
    "This example uses the same bimodal mixture to compare MALA's performance with HMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_mala_artifex():\n",
    "    \"\"\"Example using artifex's MALA implementation.\"\"\"\n",
    "    print()\n",
    "    print(\"===== Example: Artifex MALA Sampling =====\")\n",
    "\n",
    "    # Create log probability function\n",
    "    mixture_logprob = create_mixture_logprob()\n",
    "\n",
    "    # Initial state\n",
    "    init_state = jnp.zeros(2)\n",
    "\n",
    "    # Set parameters\n",
    "    n_samples = 1000\n",
    "    n_burnin = 500\n",
    "\n",
    "    print(f\"Running {n_samples} samples with {n_burnin} burn-in steps...\")\n",
    "\n",
    "    # Run MALA sampling\n",
    "    key_mala = jax.random.fold_in(key, 1)\n",
    "    mala_samples = mala_sampling(\n",
    "        mixture_logprob,\n",
    "        init_state,\n",
    "        key_mala,\n",
    "        n_samples=n_samples,\n",
    "        n_burnin=n_burnin,\n",
    "        step_size=0.05,\n",
    "    )\n",
    "    plot_samples(mala_samples, \"Artifex MALA Sampling\", \"mala_artifex_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 3: Artifex NUTS Sampling\n",
    "\n",
    "NUTS (No-U-Turn Sampler) automatically tunes the HMC trajectory length, eliminating the\n",
    "need to manually set the number of integration steps. This makes it particularly useful\n",
    "for complex, high-dimensional posteriors.\n",
    "\n",
    "Note: This example uses a simpler standard normal distribution to avoid memory issues\n",
    "that can occur with NUTS on mixture distributions. NUTS stores trajectory information\n",
    "which can be memory-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_nuts_artifex():\n",
    "    \"\"\"Example using artifex's NUTS implementation.\n",
    "\n",
    "    Note: This example uses a simpler distribution to avoid memory issues.\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"===== Example: Artifex NUTS Sampling =====\")\n",
    "\n",
    "    # Create log probability function (using simpler distribution)\n",
    "    simple_logprob = create_normal_logprob()\n",
    "\n",
    "    # Initial state\n",
    "    init_state = jnp.zeros(2)\n",
    "\n",
    "    # Set parameters (use fewer samples to reduce memory)\n",
    "    n_samples = 500\n",
    "    n_burnin = 200\n",
    "\n",
    "    print(f\"Running {n_samples} samples with {n_burnin} burn-in steps...\")\n",
    "\n",
    "    # Run NUTS sampling with simple parameters\n",
    "    key_nuts = jax.random.fold_in(key, 2)\n",
    "    try:\n",
    "        nuts_samples = nuts_sampling(\n",
    "            simple_logprob,\n",
    "            init_state,\n",
    "            key_nuts,\n",
    "            n_samples=n_samples,\n",
    "            n_burnin=n_burnin,\n",
    "            step_size=0.8,\n",
    "            max_num_doublings=5,  # Lower value to reduce memory usage\n",
    "        )\n",
    "        plot_samples(nuts_samples, \"Artifex NUTS Sampling\", \"nuts_artifex_samples.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"NUTS sampling failed with error: {e}\")\n",
    "        print(\"This may be due to memory constraints on your system.\")\n",
    "        print(\"Try reducing n_samples, n_burnin, or max_num_doublings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 4: Direct BlackJAX HMC\n",
    "\n",
    "This example demonstrates using BlackJAX's HMC API directly, without Artifex's wrapper.\n",
    "This is useful when you need more fine-grained control over the sampling process or want\n",
    "to implement custom sampling logic.\n",
    "\n",
    "Comparing this with Artifex's HMC wrapper shows how Artifex simplifies the API while\n",
    "maintaining flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_direct_blackjax_hmc():\n",
    "    \"\"\"Example using BlackJAX's HMC implementation directly.\"\"\"\n",
    "    print()\n",
    "    print(\"===== Example: Direct BlackJAX HMC =====\")\n",
    "\n",
    "    # Create log probability function\n",
    "    mixture_logprob = create_mixture_logprob()\n",
    "\n",
    "    # Initial state\n",
    "    init_state = jnp.zeros(2)\n",
    "\n",
    "    # Set parameters\n",
    "    n_samples = 1000\n",
    "    n_burnin = 500\n",
    "    step_size = 0.1\n",
    "    num_integration_steps = 10\n",
    "\n",
    "    print(f\"Running {n_samples} samples with {n_burnin} burn-in steps...\")\n",
    "\n",
    "    # Initialize the HMC algorithm\n",
    "    inverse_mass_matrix = jnp.eye(2)  # Identity matrix for 2D problem\n",
    "    hmc = blackjax.hmc(\n",
    "        mixture_logprob,\n",
    "        step_size=step_size,\n",
    "        inverse_mass_matrix=inverse_mass_matrix,\n",
    "        num_integration_steps=num_integration_steps,\n",
    "    )\n",
    "\n",
    "    # Initialize the sampling state\n",
    "    key_hmc = jax.random.fold_in(key, 3)\n",
    "    initial_state = hmc.init(init_state)\n",
    "\n",
    "    # Run sampling\n",
    "    @nnx.jit\n",
    "    def one_step(state, key):\n",
    "        state, _ = hmc.step(key, state)\n",
    "        return state, state\n",
    "\n",
    "    # Burn-in\n",
    "    key_hmc, subkey = jax.random.split(key_hmc)\n",
    "    state = initial_state\n",
    "    for _ in range(n_burnin):\n",
    "        key_hmc, subkey = jax.random.split(key_hmc)\n",
    "        state, _ = one_step(state, subkey)\n",
    "\n",
    "    # Collect samples\n",
    "    key_hmc, subkey = jax.random.split(key_hmc)\n",
    "    state, samples = jax.lax.scan(one_step, state, jax.random.split(subkey, n_samples))\n",
    "    samples = samples.position\n",
    "\n",
    "    plot_samples(samples, \"Direct BlackJAX HMC\", \"direct_blackjax_hmc_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Running All Examples\n",
    "\n",
    "Now let's run all four examples to compare different sampling approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_examples():\n",
    "    \"\"\"Run all sampling examples.\"\"\"\n",
    "    print(\"Running BlackJAX sampling examples...\")\n",
    "    print()\n",
    "\n",
    "    example_hmc_artifex()\n",
    "    example_mala_artifex()\n",
    "    example_nuts_artifex()\n",
    "    example_direct_blackjax_hmc()\n",
    "\n",
    "    print()\n",
    "    print(\"All examples completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_all_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "After running this example, you should understand:\n",
    "\n",
    "1. **Algorithm Selection**:\n",
    "   - HMC: Best for smooth distributions, requires tuning\n",
    "   - MALA: Faster per iteration, gradient-based\n",
    "   - NUTS: Automatic tuning, excellent for complex posteriors\n",
    "\n",
    "2. **Artifex Integration**: Artifex provides simple wrappers around BlackJAX that\n",
    "   handle common use cases while maintaining access to advanced features\n",
    "\n",
    "3. **Memory Considerations**: NUTS can be memory-intensive due to trajectory storage.\n",
    "   Use `max_num_doublings` to control memory usage\n",
    "\n",
    "4. **Direct API Access**: When needed, you can use BlackJAX directly for maximum control\n",
    "\n",
    "## Experiments to Try\n",
    "\n",
    "1. **Compare convergence**: Plot autocorrelation for each sampler to assess mixing\n",
    "2. **Tune hyperparameters**: Experiment with step sizes and integration steps\n",
    "3. **Higher dimensions**: Extend to 10D or 20D mixtures to see scalability\n",
    "4. **Different targets**: Try non-Gaussian distributions (e.g., Rosenbrock, funnel)\n",
    "5. **Acceptance rates**: Track and compare acceptance rates across samplers\n",
    "6. **Effective sample size**: Compute ESS to measure sampling efficiency\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore `blackjax_integration_examples.py` for advanced integration patterns\n",
    "- Check out `blackjax_example.py` for simpler introductory examples\n",
    "- Read BlackJAX documentation for more sampler options (e.g., SGLD, SGHMC)\n",
    "- Learn about diagnostics (R-hat, ESS) for assessing convergence"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
