{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"Diffusion Model on MNIST - DDPM Training and Generation Example\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example demonstrates how to use Artifex's DDPM (Denoising Diffusion Probabilistic Model)\n",
    "on MNIST to generate realistic handwritten digits. It showcases both training and various\n",
    "sampling techniques including DDIM for fast generation.\n",
    "\n",
    "**Key Artifex Components Used:**\n",
    "- `DDPMModel` - Artifex's DDPM implementation with noise scheduling\n",
    "- `DDPMConfig` - Frozen dataclass configuration with nested configs\n",
    "- `NoiseScheduleConfig` - Noise schedule configuration\n",
    "- `UNetBackboneConfig` - UNet backbone configuration\n",
    "\n",
    "## Source Code Dependencies\n",
    "\n",
    "**Validated:** 2026-01-13\n",
    "\n",
    "This example depends on the following Artifex source files:\n",
    "- `src/artifex/generative_models/models/diffusion/ddpm.py` - DDPMModel class\n",
    "- `src/artifex/generative_models/models/diffusion/base.py` - DiffusionModel base\n",
    "- `src/artifex/generative_models/core/configuration/diffusion_config.py` - DDPMConfig\n",
    "- `src/artifex/generative_models/core/configuration/diffusion_config.py` - NoiseScheduleConfig\n",
    "- `src/artifex/generative_models/core/configuration/backbone_config.py` - UNetBackboneConfig\n",
    "\n",
    "**Validation Status:**\n",
    "- âœ… All dependencies validated against `memory-bank/guides/flax-nnx-guide.md`\n",
    "- âœ… No anti-patterns detected (RNG handling checked 2025-10-16)\n",
    "- âœ… All patterns follow Flax NNX best practices\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- [ ] How to configure and create a DDPM model using Artifex\n",
    "- [ ] Understanding DDPM forward diffusion (adding noise)\n",
    "- [ ] Model forward pass for noise prediction\n",
    "- [ ] Generating samples with DDPM (slow, high quality)\n",
    "- [ ] Generating samples with DDIM (fast, good quality)\n",
    "- [ ] Visualizing the denoising process step-by-step\n",
    "- [ ] Comparing sampling speeds and quality\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Artifex installed (run `source activate.sh`)\n",
    "- Basic understanding of diffusion models\n",
    "- Familiarity with JAX and Flax NNX\n",
    "- ~30 minutes on GPU, ~2 hours on CPU for full training\n",
    "\n",
    "## Usage\n",
    "\n",
    "```bash\n",
    "source activate.sh\n",
    "python examples/generative_models/image/diffusion/diffusion_mnist.py\n",
    "```\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "The example will:\n",
    "1. Create synthetic MNIST-like data (for quick demonstration)\n",
    "2. Build DDPM model with Artifex's DDPMModel\n",
    "3. Demonstrate forward diffusion (adding noise)\n",
    "4. Show model forward pass (predicting noise)\n",
    "5. Generate samples with DDPM sampling (1000 steps)\n",
    "6. Generate samples with DDIM sampling (50 steps, 20x faster!)\n",
    "7. Visualize the progressive denoising process\n",
    "8. Save visualizations to `examples_output/diffusion_mnist_*.png`\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Denoising Diffusion Probabilistic Models (DDPM)\n",
    "\n",
    "DDPM learns to reverse a gradual noising process:\n",
    "\n",
    "**Forward Process (q):** Gradually adds Gaussian noise to data\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
    "\n",
    "**Reverse Process (p):** Learns to denoise, generating data from noise\n",
    "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
    "\n",
    "**Training Objective:** Predict the noise added at each step\n",
    "$$L = \\mathbb{E}_{t, x_0, \\epsilon} \\left[ \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2 \\right]$$\n",
    "\n",
    "### Artifex's Modular Design\n",
    "\n",
    "Artifex provides:\n",
    "- **DDPMModel**: Full DDPM with configurable noise schedules\n",
    "- **Noise schedules**: Linear, cosine, and custom schedules\n",
    "- **Fast sampling**: DDIM integration for 20-50x speedup\n",
    "- **Flexible backbone**: Works with any neural network architecture\n",
    "\n",
    "### DDIM: Fast Sampling\n",
    "\n",
    "DDIM (Denoising Diffusion Implicit Models) enables:\n",
    "- Deterministic sampling (same seed â†’ same output)\n",
    "- 50 steps instead of 1000 (20x faster!)\n",
    "- Comparable quality to DDPM\n",
    "- Enables interpolation in latent space\n",
    "\n",
    "## Estimated Runtime\n",
    "\n",
    "- **CPU**: ~5-10 minutes (demo mode, synthetic data)\n",
    "- **GPU**: ~2-3 minutes (demo mode, synthetic data)\n",
    "\n",
    "For full MNIST training:\n",
    "- **CPU**: ~2 hours\n",
    "- **GPU**: ~30 minutes\n",
    "\n",
    "## Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## Last Updated\n",
    "\n",
    "2025-10-16\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Diffusion Model on MNIST\n",
    "\n",
    "This notebook demonstrates DDPM (Denoising Diffusion Probabilistic Models) on MNIST\n",
    "using Artifex's modular diffusion components.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this example, you will understand:\n",
    "1. How to configure and use Artifex's DDPMModel\n",
    "2. The forward diffusion process (adding noise)\n",
    "3. The reverse process (denoising / generation)\n",
    "4. DDPM vs DDIM sampling trade-offs\n",
    "5. Visualizing the denoising trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import Dependencies\n",
    "r\"\"\"\n",
    "Import Artifex components:\n",
    "- DDPMModel: Artifex's DDPM implementation\n",
    "- DDPMConfig: Frozen dataclass configuration for DDPM\n",
    "- NoiseScheduleConfig: Noise schedule configuration\n",
    "- UNetBackboneConfig: UNet backbone architecture configuration\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "from tqdm import tqdm\n",
    "\n",
    "from artifex.generative_models.core.configuration.backbone_config import UNetBackboneConfig\n",
    "from artifex.generative_models.core.configuration.diffusion_config import (\n",
    "    DDPMConfig,\n",
    "    NoiseScheduleConfig,\n",
    ")\n",
    "from artifex.generative_models.models.diffusion.ddpm import DDPMModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Setup\n",
    "\n",
    "Initialize the environment, device, and random number generators.\n",
    "\n",
    "### Device Management\n",
    "\n",
    "JAX automatically detects and uses available GPUs. If no GPU is available,\n",
    "it falls back to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup and Device Configuration\n",
    "print(\"=\" * 80)\n",
    "print(\"DDPM MNIST Example - Using Artifex's DDPMModel\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check device\n",
    "device = jax.default_backend()\n",
    "print(f\"ðŸ–¥ï¸  JAX backend: {device}\")\n",
    "print(f\"ðŸ–¥ï¸  Available devices: {jax.device_count()}\")\n",
    "print()\n",
    "\n",
    "# Initialize RNG streams\n",
    "# We need separate streams for different random operations\n",
    "seed = 42\n",
    "print(f\"ðŸŽ² Random seed: {seed}\")\n",
    "print()\n",
    "\n",
    "# Create RNG streams\n",
    "# - params: For model parameter initialization\n",
    "# - noise: For adding noise in forward diffusion\n",
    "# - sample: For sampling operations\n",
    "# - dropout: For dropout layers (if used)\n",
    "rngs = nnx.Rngs(\n",
    "    params=seed,\n",
    "    noise=seed + 1,\n",
    "    sample=seed + 2,\n",
    "    dropout=seed + 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "For this demonstration, we create synthetic MNIST-like data. This allows the example\n",
    "to run quickly without requiring data downloads.\n",
    "\n",
    "**In production**, you would load real MNIST:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "```\n",
    "\n",
    "**Data Format:**\n",
    "- Images: 28Ã—28Ã—1 (grayscale)\n",
    "- Values: [-1, 1] range (normalized for diffusion models)\n",
    "- Shape: (batch_size, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Loading Function\n",
    "def load_mnist_data():\n",
    "    \"\"\"Load MNIST dataset.\n",
    "\n",
    "    In this demo, we use synthetic data for quick execution.\n",
    "    Replace with real MNIST loading for production.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, test_images) in [-1, 1] range\n",
    "\n",
    "    Note:\n",
    "        Real MNIST loading:\n",
    "        ```python\n",
    "        import tensorflow as tf\n",
    "        (train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "        train_images = (train_images / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "        ```\n",
    "    \"\"\"\n",
    "    # Create synthetic MNIST-like data\n",
    "    print(\"ðŸ“Š Loading data...\")\n",
    "    key = jax.random.key(42)\n",
    "    train_key, test_key = jax.random.split(key)\n",
    "\n",
    "    # Synthetic 28Ã—28Ã—1 images in [-1, 1] range\n",
    "    # Real MNIST has 60,000 train + 10,000 test images\n",
    "    train_images = jax.random.uniform(train_key, (1000, 28, 28, 1), minval=-1, maxval=1)\n",
    "    test_images = jax.random.uniform(test_key, (100, 28, 28, 1), minval=-1, maxval=1)\n",
    "\n",
    "    print(f\"  âœ… Train data shape: {train_images.shape}\")\n",
    "    print(f\"  âœ… Test data shape: {test_images.shape}\")\n",
    "    print(f\"  âœ… Data range: [{train_images.min():.2f}, {train_images.max():.2f}]\")\n",
    "    print()\n",
    "\n",
    "    return train_images, test_images\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_images, test_images = load_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Model Creation\n",
    "\n",
    "Now we'll create Artifex's DDPMModel with proper configuration.\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "- **noise_steps**: Number of diffusion timesteps (1000 is standard)\n",
    "- **beta_start**: Initial noise level (small value, typically 1e-4)\n",
    "- **beta_end**: Final noise level (larger value, typically 0.02)\n",
    "- **beta_schedule**: Noise schedule type ('linear' or 'cosine')\n",
    "\n",
    "The beta schedule controls how noise is added across timesteps. Linear is simpler\n",
    "but cosine often works better for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create DDPM Model\n",
    "print(\"ðŸ”§ Creating DDPM model using Artifex APIs...\")\n",
    "print()\n",
    "\n",
    "# Configure the backbone network (UNet for image generation)\n",
    "backbone_config = UNetBackboneConfig(\n",
    "    name=\"unet_backbone\",\n",
    "    hidden_dims=(64, 128, 256),  # Encoder hidden dimensions (tuple for frozen dataclass)\n",
    "    activation=\"gelu\",\n",
    "    in_channels=1,  # MNIST is grayscale (1 channel)\n",
    "    out_channels=1,  # Output same number of channels\n",
    "    time_embedding_dim=128,  # Time embedding dimension\n",
    "    attention_resolutions=(16, 8),  # Apply attention at these resolutions\n",
    "    num_res_blocks=2,  # Number of residual blocks per level\n",
    "    channel_mult=(1, 2, 4),  # Channel multipliers for each level\n",
    "    dropout_rate=0.0,  # No dropout for demo\n",
    ")\n",
    "\n",
    "# Configure the noise schedule\n",
    "noise_schedule_config = NoiseScheduleConfig(\n",
    "    name=\"linear_schedule\",\n",
    "    schedule_type=\"linear\",  # Linear noise schedule\n",
    "    num_timesteps=1000,  # Number of diffusion timesteps\n",
    "    beta_start=1e-4,  # Initial noise level\n",
    "    beta_end=2e-2,  # Final noise level\n",
    ")\n",
    "\n",
    "# Configure the DDPM model with nested configs\n",
    "config = DDPMConfig(\n",
    "    name=\"ddpm_mnist\",\n",
    "    backbone=backbone_config,\n",
    "    noise_schedule=noise_schedule_config,\n",
    "    input_shape=(28, 28, 1),  # MNIST image dimensions (H, W, C)\n",
    "    loss_type=\"mse\",  # Mean squared error loss\n",
    "    clip_denoised=True,  # Clip denoised samples to [-1, 1]\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Model Configuration:\")\n",
    "print(f\"  - Name: {config.name}\")\n",
    "print(f\"  - Input shape: {config.input_shape}\")\n",
    "print(f\"  - Noise steps: {config.noise_schedule.num_timesteps}\")\n",
    "print(f\"  - Beta range: [{config.noise_schedule.beta_start}, {config.noise_schedule.beta_end}]\")\n",
    "print(f\"  - Beta schedule: {config.noise_schedule.schedule_type}\")\n",
    "print()\n",
    "\n",
    "# Create the DDPM model\n",
    "# Artifex automatically initializes the noise schedule and backbone network\n",
    "model = DDPMModel(config, rngs=rngs)\n",
    "\n",
    "print(\"âœ… DDPMModel created successfully!\")\n",
    "print(f\"  - Model type: {type(model).__name__}\")\n",
    "print(f\"  - Noise steps: {model.noise_steps}\")\n",
    "print(f\"  - Input channels: {model.in_channels}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Forward Diffusion Process\n",
    "\n",
    "The forward diffusion process gradually adds noise to data. At step t=0, we have\n",
    "clean data; at t=T (noise_steps), we have pure noise.\n",
    "\n",
    "**Mathematical formulation:**\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $\\bar{\\alpha}_t$ = cumulative product of (1 - Î²) values\n",
    "- $\\epsilon \\sim \\mathcal{N}(0, I)$ is Gaussian noise\n",
    "\n",
    "Let's visualize how an image becomes noisy across different timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Demonstrate Forward Diffusion\n",
    "print(\"ðŸ” Demonstrating forward diffusion (adding noise)...\")\n",
    "print()\n",
    "\n",
    "# Take a test image\n",
    "test_img = train_images[0:1]  # Shape: (1, 28, 28, 1)\n",
    "print(f\"Original image shape: {test_img.shape}\")\n",
    "\n",
    "# Add noise at different timesteps\n",
    "timesteps_to_show = [0, 250, 500, 750, 999]\n",
    "noisy_images = []\n",
    "\n",
    "for t in timesteps_to_show:\n",
    "    # Create timestep tensor\n",
    "    t_tensor = jnp.array([t])\n",
    "\n",
    "    # Apply forward diffusion\n",
    "    # This is what happens during training: we add noise to clean images\n",
    "    noisy_x, added_noise = model.forward_diffusion(test_img, t_tensor)\n",
    "\n",
    "    noisy_images.append(noisy_x[0])  # Remove batch dimension\n",
    "    print(f\"  t={t:4d}: noise_level={jnp.mean(added_noise**2):.4f}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Visualize Forward Diffusion\n",
    "\n",
    "Let's see how the image progressively becomes noisier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Plot Forward Diffusion Process\n",
    "def visualize_diffusion_process(original, noisy_images, timesteps, title=\"Forward Diffusion\"):\n",
    "    \"\"\"Visualize the forward diffusion process.\n",
    "\n",
    "    Args:\n",
    "        original: Original clean image\n",
    "        noisy_images: List of noisy images at different timesteps\n",
    "        timesteps: List of timestep values\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    n_images = len(noisy_images) + 1\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 2, 2))\n",
    "\n",
    "    # Plot original\n",
    "    img = (original.squeeze() + 1) / 2  # Convert from [-1,1] to [0,1]\n",
    "    axes[0].imshow(img, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Plot noisy versions\n",
    "    for i, (noisy, t) in enumerate(zip(noisy_images, timesteps)):\n",
    "        img = (noisy.squeeze() + 1) / 2  # Convert from [-1,1] to [0,1]\n",
    "        axes[i + 1].imshow(img, cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[i + 1].set_title(f\"t={t}\")\n",
    "        axes[i + 1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=14, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š Visualizing forward diffusion...\")\n",
    "fig = visualize_diffusion_process(test_img[0], noisy_images, timesteps_to_show)\n",
    "\n",
    "# Save figure\n",
    "output_dir = \"examples_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"diffusion_mnist_forward.png\")\n",
    "fig.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"  âœ… Saved to {output_path}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Model Forward Pass (Noise Prediction)\n",
    "\n",
    "During training, the model learns to predict the noise that was added at each timestep.\n",
    "Let's test the model's forward pass to see how it predicts noise.\n",
    "\n",
    "**Training objective:**\n",
    "$$L = \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2$$\n",
    "\n",
    "where $\\epsilon_\\theta$ is our neural network that predicts the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test Model Forward Pass\n",
    "print(\"ðŸ§ª Testing model forward pass (noise prediction)...\")\n",
    "print()\n",
    "\n",
    "# Create a batch of noisy images at random timesteps\n",
    "batch_size = 4\n",
    "test_batch = train_images[:batch_size]\n",
    "\n",
    "# Sample random timesteps for each image in batch\n",
    "t_batch = jax.random.randint(\n",
    "    jax.random.key(123),\n",
    "    (batch_size,),\n",
    "    0,\n",
    "    model.noise_steps,\n",
    ")\n",
    "\n",
    "print(f\"Test batch shape: {test_batch.shape}\")\n",
    "print(f\"Timesteps: {t_batch}\")\n",
    "print()\n",
    "\n",
    "# Forward pass: model predicts noise\n",
    "# This is what happens during training\n",
    "outputs = model(test_batch, t_batch)\n",
    "\n",
    "# Extract predicted noise\n",
    "predicted_noise = outputs.get(\"predicted_noise\", outputs.get(\"output\"))\n",
    "print(\"âœ… Model forward pass successful!\")\n",
    "print(f\"  - Output shape: {predicted_noise.shape}\")\n",
    "print(f\"  - Output keys: {list(outputs.keys())}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Sampling with DDPM (Slow but High Quality)\n",
    "\n",
    "Now comes the exciting part: generating new images from pure noise!\n",
    "\n",
    "DDPM sampling performs the full reverse diffusion process:\n",
    "1. Start with random noise: $x_T \\sim \\mathcal{N}(0, I)$\n",
    "2. Denoise iteratively for T steps: $x_{t-1} = f(x_t, t)$\n",
    "3. Return clean sample: $x_0$\n",
    "\n",
    "**This is slow** (1000 steps) but produces high-quality samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generate Samples with DDPM\n",
    "print(\"ðŸŽ¨ Generating samples with DDPM sampling (1000 steps)...\")\n",
    "print(\"   âš ï¸  This will take a while (1000 denoising steps)...\")\n",
    "print()\n",
    "\n",
    "# Generate 8 samples\n",
    "n_samples = 8\n",
    "\n",
    "# DDPM sampling: full 1000 steps\n",
    "# This is the original DDPM algorithm\n",
    "samples_ddpm = model.sample(\n",
    "    n_samples_or_shape=n_samples,\n",
    "    scheduler=\"ddpm\",  # Use DDPM scheduler\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated {n_samples} samples with DDPM\")\n",
    "print(f\"  - Sample shape: {samples_ddpm.shape}\")\n",
    "print(f\"  - Value range: [{samples_ddpm.min():.2f}, {samples_ddpm.max():.2f}]\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Sampling with DDIM (Fast and Good Quality)\n",
    "\n",
    "DDIM (Denoising Diffusion Implicit Models) enables **much faster sampling**!\n",
    "\n",
    "Instead of 1000 steps, DDIM can generate comparable quality with just **50 steps**\n",
    "(20x speedup!).\n",
    "\n",
    "**Key advantages:**\n",
    "- 20-50x faster than DDPM\n",
    "- Deterministic (same seed â†’ same output)\n",
    "- Enables interpolation in latent space\n",
    "- Comparable quality to DDPM\n",
    "\n",
    "This makes diffusion models practical for real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Generate Samples with DDIM\n",
    "print(\"âš¡ Generating samples with DDIM sampling (50 steps, 20x faster!)...\")\n",
    "print()\n",
    "\n",
    "# DDIM sampling: only 50 steps instead of 1000!\n",
    "samples_ddim = model.sample(\n",
    "    n_samples_or_shape=n_samples,\n",
    "    scheduler=\"ddim\",  # Use DDIM scheduler\n",
    "    steps=50,  # Only 50 steps!\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated {n_samples} samples with DDIM\")\n",
    "print(f\"  - Sample shape: {samples_ddim.shape}\")\n",
    "print(f\"  - Value range: [{samples_ddim.min():.2f}, {samples_ddim.max():.2f}]\")\n",
    "print(\"  - Speedup: ~20x faster than DDPM!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Visualize Generated Samples\n",
    "\n",
    "Let's compare samples from DDPM and DDIM side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualize Samples\n",
    "def visualize_samples(samples, title=\"Generated Samples\", n_cols=4):\n",
    "    \"\"\"Visualize a grid of generated samples.\n",
    "\n",
    "    Args:\n",
    "        samples: Generated images (N, H, W, C)\n",
    "        title: Plot title\n",
    "        n_cols: Number of columns in grid\n",
    "\n",
    "    Returns:\n",
    "        matplotlib figure\n",
    "    \"\"\"\n",
    "    n_samples = len(samples)\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "    axes = axes.flatten() if n_samples > 1 else [axes]\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i < n_samples:\n",
    "            # Convert from [-1, 1] to [0, 1] for display\n",
    "            img = (samples[i].squeeze() + 1) / 2\n",
    "            img = np.clip(img, 0, 1)\n",
    "\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        else:\n",
    "            # Hide unused subplots\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š Visualizing generated samples...\")\n",
    "\n",
    "# Plot DDPM samples\n",
    "fig_ddpm = visualize_samples(samples_ddpm, title=\"DDPM Samples (1000 steps)\")\n",
    "output_path_ddpm = os.path.join(output_dir, \"diffusion_mnist_ddpm_samples.png\")\n",
    "fig_ddpm.savefig(output_path_ddpm, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"  âœ… DDPM samples saved to {output_path_ddpm}\")\n",
    "\n",
    "# Plot DDIM samples\n",
    "fig_ddim = visualize_samples(samples_ddim, title=\"DDIM Samples (50 steps)\")\n",
    "output_path_ddim = os.path.join(output_dir, \"diffusion_mnist_ddim_samples.png\")\n",
    "fig_ddim.savefig(output_path_ddim, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"  âœ… DDIM samples saved to {output_path_ddim}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Progressive Denoising Visualization\n",
    "\n",
    "Let's visualize the denoising process step-by-step to see how the model transforms\n",
    "noise into a clean image.\n",
    "\n",
    "This helps build intuition for how diffusion models work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Progressive Denoising\n",
    "def generate_with_trajectory(model, n_samples=1, save_every=100):\n",
    "    \"\"\"Generate samples and save intermediate steps.\n",
    "\n",
    "    Args:\n",
    "        model: Diffusion model\n",
    "        n_samples: Number of samples to generate\n",
    "        save_every: Save every N steps\n",
    "\n",
    "    Returns:\n",
    "        List of intermediate images during denoising\n",
    "    \"\"\"\n",
    "    # Start from pure noise\n",
    "    shape = (n_samples, 28, 28, 1)\n",
    "    x = jax.random.normal(rngs.sample(), shape)\n",
    "\n",
    "    # Store trajectory\n",
    "    trajectory = [x.copy()]\n",
    "\n",
    "    # Denoise step by step\n",
    "    steps = list(range(model.noise_steps - 1, -1, -save_every))\n",
    "    if steps[-1] != 0:\n",
    "        steps.append(0)  # Ensure we save the final image\n",
    "\n",
    "    print(f\"Denoising over {len(steps)} snapshots...\")\n",
    "\n",
    "    for i, t in enumerate(tqdm(range(model.noise_steps - 1, -1, -1), desc=\"Denoising\")):\n",
    "        # Create timestep for all samples\n",
    "        t_batch = jnp.full((n_samples,), t, dtype=jnp.int32)\n",
    "\n",
    "        # Get model prediction\n",
    "        outputs = model(x, t_batch)\n",
    "        predicted_noise = outputs.get(\"predicted_noise\", outputs.get(\"output\"))\n",
    "\n",
    "        # Denoise one step\n",
    "        x = model.denoise_step(x, t_batch, predicted_noise, clip_denoised=True)\n",
    "\n",
    "        # Save snapshot\n",
    "        if t % save_every == 0 or t == 0:\n",
    "            trajectory.append(x.copy())\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "print(\"ðŸŽ¬ Generating progressive denoising trajectory...\")\n",
    "print()\n",
    "\n",
    "# Generate trajectory for one sample\n",
    "trajectory = generate_with_trajectory(model, n_samples=1, save_every=200)\n",
    "\n",
    "print(f\"âœ… Captured {len(trajectory)} snapshots\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Visualize Progressive Denoising\n",
    "\n",
    "Watch how the model transforms pure noise into a digit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Plot Progressive Denoising\n",
    "def plot_trajectory(trajectory, title=\"Progressive Denoising\"):\n",
    "    \"\"\"Plot the denoising trajectory.\n",
    "\n",
    "    Args:\n",
    "        trajectory: List of images at different denoising steps\n",
    "        title: Plot title\n",
    "\n",
    "    Returns:\n",
    "        matplotlib figure\n",
    "    \"\"\"\n",
    "    n_steps = len(trajectory)\n",
    "    fig, axes = plt.subplots(1, n_steps, figsize=(n_steps * 2, 2))\n",
    "\n",
    "    for i, img in enumerate(trajectory):\n",
    "        # Convert from [-1, 1] to [0, 1]\n",
    "        img_display = (img[0].squeeze() + 1) / 2\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "        if n_steps > 1:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            ax = axes\n",
    "\n",
    "        ax.imshow(img_display, cmap=\"gray\", vmin=0, vmax=1)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Compute step number (counting backwards from noise_steps)\n",
    "        step = (n_steps - i - 1) * 200\n",
    "        if step < 0:\n",
    "            step = 0\n",
    "        ax.set_title(f\"t={step}\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š Visualizing progressive denoising...\")\n",
    "fig_traj = plot_trajectory(trajectory)\n",
    "output_path_traj = os.path.join(output_dir, \"diffusion_mnist_trajectory.png\")\n",
    "fig_traj.savefig(output_path_traj, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"  âœ… Saved to {output_path_traj}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "âœ… **What We Learned:**\n",
    "- How to configure and use Artifex's DDPMModel\n",
    "- The forward diffusion process (q) adds noise progressively\n",
    "- The reverse process (p) learns to denoise and generate\n",
    "- DDPM sampling is slow (1000 steps) but high quality\n",
    "- DDIM sampling is fast (50 steps, 20x speedup) with comparable quality\n",
    "- Diffusion models transform noise into structured data iteratively\n",
    "\n",
    "ðŸ’¡ **Key Insights:**\n",
    "- Diffusion models work by learning to reverse a gradual noising process\n",
    "- The noise schedule controls how noise is added across timesteps\n",
    "- DDIM makes diffusion models practical for real-time applications\n",
    "- Artifex provides modular, easy-to-use diffusion components\n",
    "- The same framework works for images, audio, and other modalities\n",
    "\n",
    "ðŸ“Š **Results:**\n",
    "- Successfully demonstrated forward diffusion\n",
    "- Generated samples with both DDPM and DDIM\n",
    "- Visualized the progressive denoising process\n",
    "- All visualizations saved to examples_output/\n",
    "\n",
    "ðŸ”§ **Artifex APIs Used:**\n",
    "- `DDPMModel`: Full DDPM implementation with noise scheduling\n",
    "- `DDPMConfig`: Frozen dataclass configuration for DDPM\n",
    "- `NoiseScheduleConfig`: Noise schedule configuration\n",
    "- `UNetBackboneConfig`: UNet backbone architecture configuration\n",
    "\n",
    "ðŸ”¬ **Next Steps:**\n",
    "- Train on real MNIST for realistic digit generation\n",
    "- Experiment with different noise schedules (cosine vs linear)\n",
    "- Try different numbers of timesteps (500, 2000, etc.)\n",
    "- Implement conditional generation (class-conditional DDPM)\n",
    "- Explore latent diffusion for higher resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"DDPM MNIST Example Completed Successfully!\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"ðŸ’¡ Key Takeaways:\")\n",
    "print(\"  - Diffusion models transform noise into data through iterative denoising\")\n",
    "print(\"  - DDPM: 1000 steps, high quality, slow\")\n",
    "print(\"  - DDIM: 50 steps, good quality, 20x faster!\")\n",
    "print(\"  - Artifex provides easy-to-use diffusion components\")\n",
    "print()\n",
    "print(\"ðŸ“ Output files:\")\n",
    "print(f\"  - {output_path}\")\n",
    "print(f\"  - {output_path_ddpm}\")\n",
    "print(f\"  - {output_path_ddim}\")\n",
    "print(f\"  - {output_path_traj}\")\n",
    "print()\n",
    "print(\"ðŸ”¬ Experiments to Try:\")\n",
    "print(\"  - Load real MNIST data for better results\")\n",
    "print(\"  - Try different noise schedules (schedule_type='cosine' in NoiseScheduleConfig)\")\n",
    "print(\"  - Experiment with different step counts (DDIM steps=20, steps=100)\")\n",
    "print(\"  - Compare generation quality vs speed tradeoffs\")\n",
    "print()\n",
    "print(\"ðŸ“š Related Examples:\")\n",
    "print(\"  - simple_diffusion_example.py: Basic diffusion concepts\")\n",
    "print(\"  - dit_demo.py: Diffusion Transformer architecture\")\n",
    "print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"âœ¨ Example complete! âœ¨\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
