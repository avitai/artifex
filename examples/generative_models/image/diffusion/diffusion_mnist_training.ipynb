{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9af67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set memory env vars BEFORE importing TensorFlow or JAX\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress TF warnings\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  # Don't pre-allocate GPU memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"  # JAX: don't pre-allocate\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\"  # JAX: use 80% of GPU memory\n",
    "\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: py:percent,ipynb\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"Training a Diffusion Model on MNIST - Complete End-to-End Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial provides a complete, production-ready example of training a DDPM\n",
    "(Denoising Diffusion Probabilistic Model) on the MNIST dataset. You'll learn how\n",
    "to train from scratch, evaluate quality, and generate realistic handwritten digits.\n",
    "\n",
    "**Key Artifex Components Used:**\n",
    "- `DDPMModel` - Artifex's DDPM implementation\n",
    "- `DiffusionTrainer` - Training utilities with SOTA techniques\n",
    "- Cosine noise schedule, Huber loss, warmup + cosine LR decay\n",
    "\n",
    "## Training Best Practices Applied\n",
    "\n",
    "Based on research (HuggingFace Annotated Diffusion, labml.ai DDPM):\n",
    "- 1000 timesteps with **cosine noise schedule** (smoother than linear)\n",
    "- Image padding to 32x32 for optimal UNet downsampling\n",
    "- **Learning rate warmup** (1000 steps) + **cosine decay**\n",
    "- **Huber loss** for stable training (more robust than MSE)\n",
    "- Uniform timestep sampling for training stability\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- **Training time:** ~30 minutes (50 epochs, GPU)\n",
    "- **Final loss:** ~0.027 (benchmark: 0.021 for quality digits)\n",
    "- **Sample quality:** Clear, readable handwritten digits\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install Artifex with CUDA support (recommended)\n",
    "uv sync --extra cuda-dev\n",
    "\n",
    "# Or CPU-only\n",
    "uv sync\n",
    "```\n",
    "\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2706a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# DataRax imports for data loading\n",
    "from datarax import from_source\n",
    "from datarax.sources import TfdsDataSourceConfig, TFDSSource\n",
    "from flax import nnx\n",
    "from tqdm import tqdm\n",
    "\n",
    "from artifex.generative_models.core.configuration.backbone_config import UNetBackboneConfig\n",
    "from artifex.generative_models.core.configuration.diffusion_config import (\n",
    "    DDPMConfig,\n",
    "    NoiseScheduleConfig,\n",
    ")\n",
    "from artifex.generative_models.core.noise_schedule import create_noise_schedule\n",
    "from artifex.generative_models.models.diffusion.ddpm import DDPMModel\n",
    "from artifex.generative_models.training.trainers.diffusion_trainer import (\n",
    "    DiffusionTrainer,\n",
    "    DiffusionTrainingConfig,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Artifex Diffusion Training - MNIST\")\n",
    "print(\"Using: DiffusionTrainer, DDPMModel, nnx.jit\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbebfae",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Training configuration based on research best practices for diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3870024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (tuned based on research best practices)\n",
    "SEED = 42\n",
    "NUM_EPOCHS = 50  # Research shows 40-100 epochs needed for quality results\n",
    "BATCH_SIZE = 256  # Balance between memory and gradient stability\n",
    "NUM_TIMESTEPS = 1000\n",
    "IMAGE_SIZE = 32  # Pad MNIST to 32x32 (original DDPM used 32x32 images)\n",
    "\n",
    "# Learning rate with warmup schedule\n",
    "BASE_LR = 1e-4  # Conservative LR (labml.ai uses 2e-5 at batch 64)\n",
    "WARMUP_STEPS = 500  # ~2 epochs of warmup (234 batches/epoch)\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}x{IMAGE_SIZE} (MNIST padded)\")\n",
    "print(f\"  Timesteps: {NUM_TIMESTEPS}\")\n",
    "print(f\"  Learning rate: {BASE_LR} (with {WARMUP_STEPS} warmup steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08c50e",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Use DataRax to load MNIST and create a batched data pipeline.\n",
    "Pad to 32x32 for optimal UNet downsampling (32 -> 16 -> 8 -> 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543cc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNG for data loading\n",
    "data_rngs = nnx.Rngs(SEED)\n",
    "\n",
    "# Configure MNIST data source using DataRax\n",
    "train_source_config = TfdsDataSourceConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train\",\n",
    "    shuffle=True,  # Shuffle training data\n",
    "    shuffle_buffer_size=10000,  # Buffer size for shuffling\n",
    ")\n",
    "train_source = TFDSSource(train_source_config, rngs=data_rngs)\n",
    "\n",
    "print(f\"\\nðŸ“Š MNIST train dataset loaded: {len(train_source)} samples\")\n",
    "\n",
    "# Create training pipeline with batching and JIT compilation\n",
    "train_pipeline = from_source(train_source, batch_size=BATCH_SIZE, jit_compile=True)\n",
    "\n",
    "# Calculate number of batches per epoch\n",
    "n_batches = len(train_source) // BATCH_SIZE\n",
    "print(f\"  âœ… Training pipeline created: {n_batches} batches per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63847e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    \"\"\"Preprocess MNIST batch for diffusion training.\n",
    "\n",
    "    Args:\n",
    "        batch: Dictionary with 'image' key (uint8, shape: [B, 28, 28, 1])\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with normalized and padded images (float32, shape: [B, 32, 32, 1])\n",
    "    \"\"\"\n",
    "    image = batch[\"image\"]\n",
    "\n",
    "    # Convert to float and normalize to [-1, 1]\n",
    "    image = image.astype(jnp.float32)\n",
    "    image = (image / 127.5) - 1.0\n",
    "\n",
    "    # Pad 28x28 to 32x32 (2 pixels on each side)\n",
    "    # This enables clean UNet downsampling: 32 -> 16 -> 8 -> 4\n",
    "    image = jnp.pad(\n",
    "        image,\n",
    "        ((0, 0), (2, 2), (2, 2), (0, 0)),  # Batch, height, width, channels\n",
    "        mode=\"constant\",\n",
    "        constant_values=-1.0,  # Background value after normalization\n",
    "    )\n",
    "\n",
    "    return {\"image\": image}\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"  Testing pipeline...\")\n",
    "for raw_batch in train_pipeline:\n",
    "    batch = preprocess_batch(raw_batch)\n",
    "    print(f\"  âœ… Batch shape: {batch['image'].shape}\")\n",
    "    print(\n",
    "        f\"  âœ… Value range: [{float(batch['image'].min()):.2f}, {float(batch['image'].max()):.2f}]\"\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f88f5b",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## 3. Model Creation\n",
    "\n",
    "Configure the DDPM model with cosine noise schedule and Huber loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e401fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNGs\n",
    "key = jax.random.key(SEED)\n",
    "params_key, noise_key, sample_key, dropout_key, timestep_key = jax.random.split(key, 5)\n",
    "rngs = nnx.Rngs(\n",
    "    params=params_key,\n",
    "    noise=noise_key,\n",
    "    sample=sample_key,\n",
    "    dropout=dropout_key,\n",
    "    timestep=timestep_key,\n",
    ")\n",
    "\n",
    "# UNet backbone (memory-efficient version)\n",
    "backbone_config = UNetBackboneConfig(\n",
    "    name=\"unet_backbone\",\n",
    "    hidden_dims=(64, 128, 256),  # 3 levels for 32x32 images\n",
    "    activation=\"gelu\",\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    time_embedding_dim=128,\n",
    "    attention_resolutions=(8,),  # Attention at 8x8 resolution only\n",
    "    num_res_blocks=2,\n",
    "    channel_mult=(1, 2, 4),\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "\n",
    "# Noise schedule (cosine schedule for smoother training)\n",
    "noise_schedule_config = NoiseScheduleConfig(\n",
    "    name=\"cosine_schedule\",\n",
    "    schedule_type=\"cosine\",  # Cosine: smoother noise progression, better gradients\n",
    "    num_timesteps=NUM_TIMESTEPS,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    ")\n",
    "\n",
    "# DDPM config with Huber loss\n",
    "ddpm_config = DDPMConfig(\n",
    "    name=\"ddpm_mnist\",\n",
    "    backbone=backbone_config,\n",
    "    noise_schedule=noise_schedule_config,\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1),  # 32x32 padded MNIST\n",
    "    loss_type=\"huber\",  # Huber loss for stable training (recommended)\n",
    "    clip_denoised=True,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = DDPMModel(ddpm_config, rngs=rngs)\n",
    "\n",
    "print(\"\\nâœ… DDPMModel created:\")\n",
    "print(f\"   UNet: hidden_dims={backbone_config.hidden_dims}\")\n",
    "print(f\"   Channel mults: {backbone_config.channel_mult}\")\n",
    "print(f\"   Noise schedule: {noise_schedule_config.schedule_type}\")\n",
    "print(f\"   Loss type: {ddpm_config.loss_type}\")\n",
    "print(f\"   Timesteps: {NUM_TIMESTEPS}\")\n",
    "print(f\"   JAX backend: {jax.default_backend()}\")\n",
    "print(f\"   Devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906321c",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "Configure optimizer with warmup + cosine decay learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noise schedule for trainer\n",
    "noise_schedule = create_noise_schedule(noise_schedule_config)\n",
    "\n",
    "# Calculate total training steps for learning rate schedule\n",
    "total_steps = NUM_EPOCHS * n_batches\n",
    "print(f\"   Total training steps: {total_steps}\")\n",
    "\n",
    "# Learning rate schedule: warmup + cosine decay\n",
    "# This prevents early training instability and allows gradual learning\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=BASE_LR,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    decay_steps=total_steps,\n",
    "    end_value=BASE_LR * 0.01,  # End at 1% of peak\n",
    ")\n",
    "\n",
    "# Optimizer with warmup schedule and gradient clipping\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(lr_schedule, weight_decay=1e-5),  # AdamW with weight decay\n",
    "    ),\n",
    "    wrt=nnx.Param,\n",
    ")\n",
    "print(f\"   Optimizer: AdamW with warmup ({WARMUP_STEPS} steps) + cosine decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637434f5",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## 5. Initialize Trainer\n",
    "\n",
    "Use Artifex's DiffusionTrainer with uniform timestep sampling for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_config = DiffusionTrainingConfig(\n",
    "    prediction_type=\"epsilon\",  # Classic DDPM epsilon prediction\n",
    "    timestep_sampling=\"uniform\",  # Uniform sampling (more stable for training)\n",
    "    loss_weighting=\"uniform\",  # Start with uniform weighting for stability\n",
    "    ema_decay=0.9999,  # EMA as per original DDPM paper\n",
    "    ema_update_every=10,\n",
    ")\n",
    "\n",
    "trainer = DiffusionTrainer(noise_schedule, diffusion_config)\n",
    "\n",
    "# JIT-compile training step\n",
    "jit_train_step = nnx.jit(trainer.train_step)\n",
    "\n",
    "print(\"\\nâœ… DiffusionTrainer initialized:\")\n",
    "print(f\"   Prediction: {diffusion_config.prediction_type}\")\n",
    "print(f\"   Timestep sampling: {diffusion_config.timestep_sampling}\")\n",
    "print(f\"   Loss weighting: {diffusion_config.loss_weighting}\")\n",
    "print(\"   Training step JIT-compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa1fc6",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Train the model with progress tracking and learning rate monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a79ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"step\": [], \"loss\": [], \"epoch\": [], \"lr\": []}\n",
    "train_key = jax.random.key(999)\n",
    "global_step = 0\n",
    "\n",
    "print(f\"\\nTraining for {NUM_EPOCHS} epochs ({total_steps} steps)...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "\n",
    "    # DataRax pipeline handles batching and shuffling\n",
    "    pbar = tqdm(train_pipeline, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\", total=n_batches)\n",
    "    for raw_batch in pbar:\n",
    "        train_key, step_key = jax.random.split(train_key)\n",
    "\n",
    "        # Preprocess batch (normalize and pad)\n",
    "        batch = preprocess_batch(raw_batch)\n",
    "\n",
    "        # Training step (JIT-compiled)\n",
    "        loss, metrics = jit_train_step(model, optimizer, batch, step_key)\n",
    "\n",
    "        # Update EMA (outside JIT)\n",
    "        if global_step % diffusion_config.ema_update_every == 0:\n",
    "            trainer.update_ema(model)\n",
    "\n",
    "        # Get current learning rate from schedule\n",
    "        current_lr = float(lr_schedule(global_step))\n",
    "\n",
    "        epoch_losses.append(float(loss))\n",
    "        history[\"step\"].append(global_step)\n",
    "        history[\"loss\"].append(float(loss))\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"lr\"].append(current_lr)\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        # Show loss and LR in progress bar\n",
    "        pbar.set_postfix({\"loss\": f\"{loss:.4f}\", \"lr\": f\"{current_lr:.2e}\"})\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    current_lr = float(lr_schedule(global_step - 1))\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}: avg_loss = {avg_loss:.4f}, lr = {current_lr:.2e}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11098cb5",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## 7. Generate Samples\n",
    "\n",
    "Use DDIM for fast sampling with good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating samples...\")\n",
    "n_samples = 16\n",
    "\n",
    "# Use DDIM for faster sampling with more steps for better quality\n",
    "samples = model.sample(\n",
    "    n_samples_or_shape=n_samples,\n",
    "    scheduler=\"ddim\",\n",
    "    steps=100,  # More steps for better quality\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated {n_samples} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd07c6",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## 8. Visualization\n",
    "\n",
    "Save training curves and generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ae0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"examples_output\", exist_ok=True)\n",
    "\n",
    "\n",
    "def visualize_samples(images, title=\"Samples\", n_cols=4, save_path=None):\n",
    "    \"\"\"Visualize a grid of images.\"\"\"\n",
    "    n_images = len(images)\n",
    "    n_rows = (n_images + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "\n",
    "    for i, (ax, img) in enumerate(zip(axes, images)):\n",
    "        img = (np.array(img) + 1.0) / 2.0\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img.squeeze(), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for i in range(n_images, len(axes)):\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Save samples\n",
    "visualize_samples(\n",
    "    samples,\n",
    "    title=\"DDPM Generated MNIST Digits\",\n",
    "    save_path=\"examples_output/diffusion_samples.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curve with dual axis (loss + learning rate)\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Loss curve (left axis)\n",
    "color_loss = \"tab:blue\"\n",
    "ax1.plot(history[\"step\"], history[\"loss\"], alpha=0.3, linewidth=0.5, color=color_loss)\n",
    "# Smooth curve\n",
    "if len(history[\"loss\"]) > 100:\n",
    "    window = 100\n",
    "    smoothed = np.convolve(history[\"loss\"], np.ones(window) / window, mode=\"valid\")\n",
    "    ax1.plot(\n",
    "        history[\"step\"][window - 1 :],\n",
    "        smoothed,\n",
    "        linewidth=2,\n",
    "        label=\"Loss (smoothed)\",\n",
    "        color=color_loss,\n",
    "    )\n",
    "ax1.set_xlabel(\"Step\")\n",
    "ax1.set_ylabel(\"Loss\", color=color_loss)\n",
    "ax1.tick_params(axis=\"y\", labelcolor=color_loss)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate curve (right axis)\n",
    "ax2 = ax1.twinx()\n",
    "color_lr = \"tab:orange\"\n",
    "ax2.plot(history[\"step\"], history[\"lr\"], linewidth=1.5, color=color_lr, label=\"Learning Rate\")\n",
    "ax2.set_ylabel(\"Learning Rate\", color=color_lr)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color_lr)\n",
    "\n",
    "# Combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n",
    "\n",
    "ax1.set_title(\n",
    "    f\"Diffusion Training ({NUM_EPOCHS} epochs, warmup + cosine decay)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"examples_output/diffusion_training_curve.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Saved: examples_output/diffusion_training_curve.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052f4fe",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Training Results\n",
    "\n",
    "- **Final loss:** ~0.027 (close to benchmark 0.021)\n",
    "- **Sample quality:** Clear, readable digits\n",
    "- **Training time:** ~30 minutes on GPU\n",
    "\n",
    "### Key Techniques Used\n",
    "\n",
    "1. **Cosine noise schedule** - Smoother than linear, better gradients\n",
    "2. **Huber loss** - More robust than MSE\n",
    "3. **LR warmup + cosine decay** - Prevents early instability\n",
    "4. **32x32 padding** - Optimal UNet downsampling\n",
    "5. **Uniform timestep sampling** - Training stability\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try conditional generation (class labels)\n",
    "- Experiment with v-prediction instead of epsilon\n",
    "- Apply to other datasets (Fashion-MNIST, CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final loss: {history['loss'][-1]:.4f}\")\n",
    "print(f\"Total steps: {global_step}\")\n",
    "print(\"Samples saved: examples_output/diffusion_samples.png\")\n",
    "print(\"Training curve: examples_output/diffusion_training_curve.png\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
