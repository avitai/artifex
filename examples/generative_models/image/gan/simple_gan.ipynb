{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Training a GAN on 2D Data with Artifex's GANTrainer\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to train a GAN using Artifex's high-level training\n",
    "API. Instead of implementing training loops from scratch, we use `GANTrainer`\n",
    "which provides state-of-the-art techniques like WGAN-GP (Wasserstein GAN with\n",
    "Gradient Penalty) for stable training.\n",
    "\n",
    "**Key Artifex Components Used:**\n",
    "- `Generator`, `Discriminator` - Configurable neural networks\n",
    "- `GANTrainer` - Training framework with WGAN-GP support\n",
    "- `GANTrainingConfig` - Configuration for loss type, gradient penalty, etc.\n",
    "\n",
    "## Training Best Practices Applied\n",
    "\n",
    "Based on the official WGAN-GP implementation:\n",
    "- WGAN-GP loss for training stability (no mode collapse)\n",
    "- Gradient penalty Î»=0.1 for toy data (faster convergence)\n",
    "- 5 critic (discriminator) iterations per generator iteration\n",
    "- Adam optimizer with beta1=0.5, beta2=0.9\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- **Training time:** ~2-3 minutes (GPU/CPU)\n",
    "- **Final Wasserstein distance:** Near 0 (distributions match)\n",
    "- **Generated samples:** Points forming a circle matching real data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install Artifex\n",
    "uv sync\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "from flax import nnx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Artifex imports\n",
    "from artifex.generative_models.core.configuration.network_configs import (\n",
    "    DiscriminatorConfig,\n",
    "    GeneratorConfig,\n",
    ")\n",
    "from artifex.generative_models.models.gan import Discriminator, Generator\n",
    "from artifex.generative_models.training.trainers.gan_trainer import (\n",
    "    GANTrainer,\n",
    "    GANTrainingConfig,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Artifex GAN Training - 2D Circular Data\")\n",
    "print(\"Using: GANTrainer, Generator, Discriminator, nnx.jit\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Training configuration based on the official WGAN-GP implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (based on official WGAN-GP toy implementation)\n",
    "SEED = 42\n",
    "NUM_STEPS = 5000  # Training iterations\n",
    "BATCH_SIZE = 256\n",
    "LATENT_DIM = 2  # Match output dim for simpler mapping\n",
    "N_CRITIC = 5  # Critic iterations per generator step\n",
    "LR = 1e-4  # Adam learning rate\n",
    "GP_WEIGHT = 0.1  # Gradient penalty weight (0.1 for toy data)\n",
    "HIDDEN_DIM = 128  # Hidden layer dimension\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Steps: {NUM_STEPS}, Batch: {BATCH_SIZE}\")\n",
    "print(f\"  Latent dim: {LATENT_DIM}\")\n",
    "print(f\"  Hidden dim: {HIDDEN_DIM}\")\n",
    "print(f\"  N_critic: {N_CRITIC}\")\n",
    "print(f\"  Learning rate: {LR}\")\n",
    "print(f\"  Gradient penalty: {GP_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 2: Data Generation\n",
    "\n",
    "We use a simple circular distribution for visualization. The goal is for the\n",
    "generator to learn to produce points that form a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circle_data(key, batch_size):\n",
    "    \"\"\"Generate 2D points on a unit circle with noise.\"\"\"\n",
    "    theta_key, noise_key = jax.random.split(key)\n",
    "    theta = jax.random.uniform(theta_key, (batch_size,)) * 2 * jnp.pi\n",
    "    r = 1.0 + jax.random.normal(noise_key, (batch_size,)) * 0.05\n",
    "    x = r * jnp.cos(theta)\n",
    "    y = r * jnp.sin(theta)\n",
    "    return jnp.stack([x, y], axis=-1)\n",
    "\n",
    "\n",
    "# Test data generation\n",
    "test_data = generate_circle_data(jax.random.key(0), 500)\n",
    "print(f\"\\nðŸ“Š Test data shape: {test_data.shape}\")\n",
    "print(f\"   Range: x=[{test_data[:, 0].min():.2f}, {test_data[:, 0].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 3: Create Models Using Artifex's API\n",
    "\n",
    "Use Artifex's `Generator` and `Discriminator` classes with configuration objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNGs\n",
    "key = jax.random.key(SEED)\n",
    "gen_key, disc_key, train_key = jax.random.split(key, 3)\n",
    "\n",
    "# Generator configuration\n",
    "gen_config = GeneratorConfig(\n",
    "    name=\"circle_generator\",\n",
    "    hidden_dims=(HIDDEN_DIM, HIDDEN_DIM, HIDDEN_DIM),  # 3-layer MLP\n",
    "    output_shape=(1, 2),  # 2D output\n",
    "    latent_dim=LATENT_DIM,\n",
    "    activation=\"relu\",\n",
    "    batch_norm=False,  # No batch norm for WGAN-GP\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "\n",
    "# Discriminator (critic) configuration\n",
    "disc_config = DiscriminatorConfig(\n",
    "    name=\"circle_discriminator\",\n",
    "    input_shape=(1, 2),  # 2D input\n",
    "    hidden_dims=(HIDDEN_DIM, HIDDEN_DIM, HIDDEN_DIM),\n",
    "    activation=\"relu\",\n",
    "    batch_norm=False,\n",
    "    dropout_rate=0.0,\n",
    ")\n",
    "\n",
    "# Create models\n",
    "gen_rngs = nnx.Rngs(params=gen_key)\n",
    "disc_rngs = nnx.Rngs(params=disc_key)\n",
    "\n",
    "generator = Generator(config=gen_config, rngs=gen_rngs)\n",
    "discriminator = Discriminator(config=disc_config, rngs=disc_rngs)\n",
    "\n",
    "print(\"\\nâœ… Artifex models created:\")\n",
    "print(f\"   Generator: {gen_config.hidden_dims}, latent_dim={LATENT_DIM}\")\n",
    "print(f\"   Discriminator: {disc_config.hidden_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 4: Create Optimizers and GANTrainer\n",
    "\n",
    "Use Artifex's `GANTrainer` with WGAN-GP configuration for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers (Adam with beta1=0.5, beta2=0.9 as per official WGAN-GP)\n",
    "gen_optimizer = nnx.Optimizer(\n",
    "    generator,\n",
    "    optax.adam(LR, b1=0.5, b2=0.9),\n",
    "    wrt=nnx.Param,\n",
    ")\n",
    "disc_optimizer = nnx.Optimizer(\n",
    "    discriminator,\n",
    "    optax.adam(LR, b1=0.5, b2=0.9),\n",
    "    wrt=nnx.Param,\n",
    ")\n",
    "\n",
    "# GANTrainer configuration with WGAN-GP\n",
    "gan_config = GANTrainingConfig(\n",
    "    loss_type=\"wasserstein\",  # WGAN loss\n",
    "    n_critic=N_CRITIC,\n",
    "    gp_weight=GP_WEIGHT,  # Gradient penalty\n",
    "    gp_target=1.0,\n",
    "    r1_weight=0.0,\n",
    "    label_smoothing=0.0,\n",
    ")\n",
    "\n",
    "trainer = GANTrainer(config=gan_config)\n",
    "\n",
    "# JIT-compile training steps for performance\n",
    "jit_d_step = nnx.jit(trainer.discriminator_step)\n",
    "jit_g_step = nnx.jit(trainer.generator_step)\n",
    "\n",
    "print(\"\\nâœ… GANTrainer initialized:\")\n",
    "print(f\"   Loss type: {gan_config.loss_type}\")\n",
    "print(f\"   N_critic: {gan_config.n_critic}\")\n",
    "print(f\"   GP weight: {gan_config.gp_weight}\")\n",
    "print(\"   Training steps JIT-compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "The training loop uses Artifex's `GANTrainer` methods:\n",
    "- `trainer.discriminator_step()` - Updates critic with gradient penalty\n",
    "- `trainer.generator_step()` - Updates generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"step\": [], \"d_loss\": [], \"g_loss\": [], \"w_dist\": []}\n",
    "\n",
    "print(f\"\\nTraining for {NUM_STEPS} steps...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "pbar = tqdm(range(NUM_STEPS), desc=\"Training\")\n",
    "for step in pbar:\n",
    "    train_key, *step_keys = jax.random.split(train_key, 2 + N_CRITIC * 2)\n",
    "\n",
    "    # Train Discriminator (N_CRITIC steps)\n",
    "    for i in range(N_CRITIC):\n",
    "        d_data_key, d_z_key, d_gp_key = jax.random.split(step_keys[i], 3)\n",
    "\n",
    "        real_data = generate_circle_data(d_data_key, BATCH_SIZE)\n",
    "        z = jax.random.normal(d_z_key, (BATCH_SIZE, LATENT_DIM))\n",
    "\n",
    "        # Use Artifex's JIT-compiled discriminator step\n",
    "        d_loss, d_metrics = jit_d_step(\n",
    "            generator, discriminator, disc_optimizer, real_data, z, d_gp_key\n",
    "        )\n",
    "\n",
    "    # Train Generator (1 step)\n",
    "    z_gen_key = step_keys[-1]\n",
    "    z = jax.random.normal(z_gen_key, (BATCH_SIZE, LATENT_DIM))\n",
    "\n",
    "    # Use Artifex's JIT-compiled generator step\n",
    "    g_loss, g_metrics = jit_g_step(generator, discriminator, gen_optimizer, z)\n",
    "\n",
    "    # Record history\n",
    "    w_dist = d_metrics.get(\"d_real\", 0.0) - d_metrics.get(\"d_fake\", 0.0)\n",
    "    history[\"step\"].append(step)\n",
    "    history[\"d_loss\"].append(float(d_loss))\n",
    "    history[\"g_loss\"].append(float(g_loss))\n",
    "    history[\"w_dist\"].append(float(w_dist))\n",
    "\n",
    "    # Update progress bar\n",
    "    pbar.set_postfix({\"D\": f\"{d_loss:.3f}\", \"G\": f\"{g_loss:.3f}\", \"W\": f\"{w_dist:.3f}\"})\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 6: Generate Samples and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating samples...\")\n",
    "n_samples = 1000\n",
    "\n",
    "final_real = generate_circle_data(jax.random.key(5000), n_samples)\n",
    "z_final = jax.random.normal(jax.random.key(6000), (n_samples, LATENT_DIM))\n",
    "final_fake = generator(z_final)\n",
    "\n",
    "# Statistics\n",
    "real_radius = jnp.sqrt(jnp.sum(final_real**2, axis=1))\n",
    "fake_radius = jnp.sqrt(jnp.sum(final_fake**2, axis=1))\n",
    "\n",
    "print(f\"\\nðŸ“Š Evaluation ({n_samples} samples):\")\n",
    "print(f\"   Real: mean_radius = {jnp.mean(real_radius):.4f} Â± {jnp.std(real_radius):.4f}\")\n",
    "print(f\"   Fake: mean_radius = {jnp.mean(fake_radius):.4f} Â± {jnp.std(fake_radius):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 7: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.makedirs(\"examples_output\", exist_ok=True)\n",
    "\n",
    "# Results visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(final_real[:, 0], final_real[:, 1], alpha=0.5, s=8, c=\"#2196F3\")\n",
    "axes[0].set_title(\"Real Data Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].set_xlim(-2, 2)\n",
    "axes[0].set_ylim(-2, 2)\n",
    "axes[0].set_aspect(\"equal\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(final_fake[:, 0], final_fake[:, 1], alpha=0.5, s=8, c=\"#FF9800\")\n",
    "axes[1].set_title(\"Generated Data Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].set_xlim(-2, 2)\n",
    "axes[1].set_ylim(-2, 2)\n",
    "axes[1].set_aspect(\"equal\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].scatter(final_real[:, 0], final_real[:, 1], alpha=0.4, s=8, c=\"#2196F3\", label=\"Real\")\n",
    "axes[2].scatter(final_fake[:, 0], final_fake[:, 1], alpha=0.4, s=8, c=\"#FF9800\", label=\"Generated\")\n",
    "axes[2].set_title(\"Overlay Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "axes[2].set_xlim(-2, 2)\n",
    "axes[2].set_ylim(-2, 2)\n",
    "axes[2].set_aspect(\"equal\")\n",
    "axes[2].legend(loc=\"upper right\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"examples_output/simple_gan_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"\\nSaved: examples_output/simple_gan_results.png\")\n",
    "plt.close()\n",
    "\n",
    "# Training curves with smoothing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "\n",
    "# Smoothing function\n",
    "def smooth(values, window=100):\n",
    "    \"\"\"Apply moving average smoothing to a sequence of values.\"\"\"\n",
    "    if len(values) < window:\n",
    "        return values\n",
    "    return np.convolve(values, np.ones(window) / window, mode=\"valid\")\n",
    "\n",
    "\n",
    "steps = np.array(history[\"step\"])\n",
    "d_loss = np.array(history[\"d_loss\"])\n",
    "g_loss = np.array(history[\"g_loss\"])\n",
    "w_dist = np.array(history[\"w_dist\"])\n",
    "\n",
    "# Plot raw data with low alpha\n",
    "ax1.plot(steps, d_loss, alpha=0.15, color=\"tab:blue\", linewidth=0.5)\n",
    "ax1.plot(steps, g_loss, alpha=0.15, color=\"tab:orange\", linewidth=0.5)\n",
    "\n",
    "# Plot smoothed data\n",
    "window = 100\n",
    "if len(d_loss) > window:\n",
    "    smoothed_d = smooth(d_loss, window)\n",
    "    smoothed_g = smooth(g_loss, window)\n",
    "    ax1.plot(steps[window - 1 :], smoothed_d, label=\"Critic Loss (smoothed)\", linewidth=2)\n",
    "    ax1.plot(steps[window - 1 :], smoothed_g, label=\"Generator Loss (smoothed)\", linewidth=2)\n",
    "\n",
    "ax1.set_xlabel(\"Step\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"WGAN-GP Training Losses\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Wasserstein distance with smoothing\n",
    "ax2.plot(steps, w_dist, alpha=0.15, color=\"green\", linewidth=0.5)\n",
    "if len(w_dist) > window:\n",
    "    smoothed_w = smooth(w_dist, window)\n",
    "    ax2.plot(steps[window - 1 :], smoothed_w, color=\"green\", linewidth=2, label=\"W-dist\")\n",
    "ax2.axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Target (0)\")\n",
    "ax2.set_xlabel(\"Step\")\n",
    "ax2.set_ylabel(\"Wasserstein Distance Estimate\")\n",
    "ax2.set_title(\"Wasserstein Distance During Training\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"examples_output/simple_gan_training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Saved: examples_output/simple_gan_training_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nâœ… Done!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
