{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"VAE MNIST Example - Variational Autoencoder on MNIST\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example demonstrates how to build a Variational Autoencoder (VAE) on MNIST using\n",
    "Artifex's modular encoder/decoder architecture. VAEs learn probabilistic latent representations\n",
    "that enable both reconstruction and generation of new samples.\n",
    "\n",
    "**Key Artifex Components Used:**\n",
    "- `MLPEncoder` - Artifex's MLP-based encoder for latent mean and log-variance\n",
    "- `MLPDecoder` - Artifex's MLP-based decoder for reconstruction\n",
    "- `VAE` - Artifex's base VAE class with ELBO loss and sampling\n",
    "\n",
    "## Source Code Dependencies\n",
    "\n",
    "**Validated:** 2025-10-16\n",
    "\n",
    "This example depends on the following Artifex source files:\n",
    "- `src/artifex/generative_models/models/vae/base.py` - VAE base class\n",
    "- `src/artifex/generative_models/models/vae/encoders.py` - MLPEncoder class\n",
    "- `src/artifex/generative_models/models/vae/decoders.py` - MLPDecoder class\n",
    "\n",
    "**Validation Status:**\n",
    "- âœ… All dependencies validated against `memory-bank/guides/flax-nnx-guide.md`\n",
    "- âœ… No anti-patterns detected (RNG handling, module init, activations)\n",
    "- âœ… All patterns follow Flax NNX best practices (no nnx.List issues)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- [x] VAE architecture: encoder (x â†’ Î¼, Ïƒ), reparameterization (z), decoder (z â†’ xÌ‚)\n",
    "- [x] Using Artifex's MLPEncoder and MLPDecoder components\n",
    "- [x] Proper RNG handling in Flax NNX (rngs.sample() pattern)\n",
    "- [x] ELBO loss decomposition: reconstruction + KL divergence\n",
    "- [x] Sample generation from learned prior p(z) = N(0, I)\n",
    "- [x] Visualization of reconstructions and generated samples\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Artifex installed (run `source activate.sh`)\n",
    "- Basic understanding of autoencoders and latent representations\n",
    "- Familiarity with JAX and Flax NNX basics\n",
    "- Understanding of variational inference concepts\n",
    "\n",
    "## Usage\n",
    "\n",
    "```bash\n",
    "source activate.sh\n",
    "python examples/generative_models/image/vae/vae_mnist.py\n",
    "```\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "The example will:\n",
    "1. Create synthetic MNIST-like data (28Ã—28 grayscale images)\n",
    "2. Build VAE with Artifex's MLPEncoder and MLPDecoder\n",
    "3. Perform forward pass to reconstruct inputs\n",
    "4. Generate new samples from random latent vectors\n",
    "5. Visualize original vs reconstructed vs generated images\n",
    "6. Save visualization to `examples_output/vae_mnist_results.png`\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Variational Autoencoder (VAE)\n",
    "\n",
    "A VAE is a generative model that learns to encode data into a latent space and decode it back.\n",
    "Unlike standard autoencoders, VAEs learn a **probabilistic** latent representation:\n",
    "\n",
    "**Mathematical Framework:**\n",
    "- Encoder: $q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi^2(x))$ - Approximate posterior\n",
    "- Decoder: $p_\\theta(x|z)$ - Likelihood of data given latent code\n",
    "- Prior: $p(z) = \\mathcal{N}(0, I)$ - Standard normal prior\n",
    "\n",
    "**VAE Loss (ELBO - Evidence Lower Bound):**\n",
    "$$\\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] -\n",
    "\\text{KL}(q_\\phi(z|x) \\| p(z))$$\n",
    "\n",
    "Where:\n",
    "- **Reconstruction term**: $\\mathbb{E}_{q(z|x)}[\\log p(x|z)] \\approx -\\|x - \\hat{x}\\|^2$ (MSE)\n",
    "- **KL term**: $\\text{KL}(q(z|x) \\| p(z))$ has closed form for Gaussians\n",
    "\n",
    "### Reparameterization Trick\n",
    "\n",
    "To enable backpropagation through stochastic sampling:\n",
    "$$z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "This separates the stochastic component (Îµ) from the learnable parameters (Î¼, Ïƒ),\n",
    "allowing gradients to flow through the sampling operation.\n",
    "\n",
    "### Artifex's Modular Design\n",
    "\n",
    "Artifex provides reusable encoder/decoder components:\n",
    "- **MLPEncoder**: Maps inputs â†’ (mean, log_var)\n",
    "- **MLPDecoder**: Maps latent codes â†’ reconstructions\n",
    "- **VAE**: Combines encoder/decoder with ELBO loss\n",
    "\n",
    "This modular design allows:\n",
    "- Easy swapping between MLP, CNN, ResNet encoders/decoders\n",
    "- Consistent architecture across examples\n",
    "- Proper initialization and RNG handling\n",
    "\n",
    "## Estimated Runtime\n",
    "\n",
    "- **CPU**: ~2-3 minutes (synthetic data, quick demo)\n",
    "- **GPU**: ~30 seconds (if available)\n",
    "\n",
    "## Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## Last Updated\n",
    "\n",
    "2025-10-16\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# VAE MNIST Example\n",
    "\n",
    "This notebook demonstrates Variational Autoencoders (VAEs) on MNIST using Artifex's\n",
    "modular encoder/decoder components.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this example, you will understand:\n",
    "1. VAE architecture and the reparameterization trick\n",
    "2. How to use Artifex's MLPEncoder and MLPDecoder\n",
    "3. Proper RNG handling in Flax NNX\n",
    "4. ELBO loss computation and interpretation\n",
    "5. Generating samples from learned latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import Dependencies\n",
    "\"\"\"\n",
    "Import Artifex components:\n",
    "- MLPEncoder: Encodes inputs to latent distribution parameters (Î¼, log ÏƒÂ²)\n",
    "- MLPDecoder: Decodes latent codes to reconstructions\n",
    "- VAE: Base VAE class with ELBO loss and sampling methods\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from flax import nnx\n",
    "\n",
    "from artifex.generative_models.core.configuration.network_configs import (\n",
    "    DecoderConfig,\n",
    "    EncoderConfig,\n",
    ")\n",
    "from artifex.generative_models.core.configuration.vae_config import VAEConfig\n",
    "from artifex.generative_models.models.vae import VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "For this example, we create synthetic MNIST-like data. In production, you would use:\n",
    "- `tensorflow_datasets` for real MNIST\n",
    "- `torch.utils.data.DataLoader` for PyTorch datasets\n",
    "- Artifex's data loaders for standardized pipelines\n",
    "\n",
    "**Data Format:**\n",
    "- Images: 28Ã—28Ã—1 (grayscale)\n",
    "- Values: [0, 1] range (normalized)\n",
    "- Shape: (batch_size, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading Function\n",
    "def load_mnist_data():\n",
    "    \"\"\"Load MNIST dataset.\n",
    "\n",
    "    In this example, we use synthetic data for quick demonstration.\n",
    "    Replace this with real MNIST loading for production use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_images, test_images)\n",
    "\n",
    "    Note:\n",
    "        Real MNIST loading would look like:\n",
    "        ```python\n",
    "        import tensorflow_datasets as tfds\n",
    "        ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "        images = ds.map(lambda x, y: x / 255.0)  # Normalize to [0, 1]\n",
    "        ```\n",
    "    \"\"\"\n",
    "    # Create synthetic MNIST-like data with proper dimensions\n",
    "    key = jax.random.key(42)\n",
    "    train_key, test_key = jax.random.split(key)\n",
    "\n",
    "    # Create synthetic data: 28Ã—28Ã—1 images in [0, 1] range\n",
    "    train_images = jax.random.uniform(train_key, (1000, 28, 28, 1))\n",
    "    test_images = jax.random.uniform(test_key, (100, 28, 28, 1))\n",
    "\n",
    "    return train_images, test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Visualization\n",
    "\n",
    "The visualization function shows three rows:\n",
    "1. **Original**: Input images from the dataset\n",
    "2. **Reconstructed**: Images reconstructed by the VAE (encoder â†’ decoder)\n",
    "3. **Generated**: New images generated from random latent codes (prior â†’ decoder)\n",
    "\n",
    "This helps assess:\n",
    "- **Reconstruction quality**: How well the model preserves input details\n",
    "- **Generation diversity**: Variety in generated samples\n",
    "- **Latent space coverage**: Quality of the learned representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Visualization Function\n",
    "def visualize_vae_results(original, reconstructed, generated, num_samples=5):\n",
    "    \"\"\"Visualize VAE results side-by-side.\n",
    "\n",
    "    Args:\n",
    "        original: Original images [batch, height, width, channels]\n",
    "        reconstructed: Reconstructed images (same shape as original)\n",
    "        generated: Generated images from random latent codes\n",
    "        num_samples: Number of samples to display (default: 5)\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The created figure\n",
    "\n",
    "    Note:\n",
    "        All images should be in [0, 1] range for proper visualization.\n",
    "        Images are clipped to [0, 1] before display to handle any overshooting.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(12, 7))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Row 1: Original images\n",
    "        axes[0, i].imshow(jnp.clip(original[i, :, :, 0], 0, 1), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[0, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel(\"Original\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        # Row 2: Reconstructed images\n",
    "        axes[1, i].imshow(jnp.clip(reconstructed[i, :, :, 0], 0, 1), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[1, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel(\"Reconstructed\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        # Row 3: Generated images (from random latent codes)\n",
    "        axes[2, i].imshow(jnp.clip(generated[i, :, :, 0], 0, 1), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[2, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[2, i].set_ylabel(\"Generated\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Main Pipeline\n",
    "\n",
    "The main function demonstrates the VAE workflow:\n",
    "\n",
    "1. **Setup**: Initialize RNG for reproducibility\n",
    "2. **Data**: Load MNIST (synthetic in this demo)\n",
    "3. **Encoder**: Create MLPEncoder to map x â†’ (Î¼, log ÏƒÂ²)\n",
    "4. **Decoder**: Create MLPDecoder to map z â†’ xÌ‚\n",
    "5. **VAE**: Combine encoder + decoder into full VAE\n",
    "6. **Inference**: Run forward pass and generation\n",
    "7. **Visualization**: Display results\n",
    "\n",
    "### Why Explicit Component Creation?\n",
    "\n",
    "We explicitly create encoder and decoder to demonstrate:\n",
    "- How to configure Artifex's components\n",
    "- The modular design pattern\n",
    "- How components connect in the VAE\n",
    "- Easy customization (swap MLP â†’ CNN, adjust layers, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Function\n",
    "def main():\n",
    "    \"\"\"Run the VAE MNIST example.\n",
    "\n",
    "    This function demonstrates the complete VAE pipeline using Artifex's\n",
    "    modular encoder/decoder components.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"VAE MNIST Example - Using Artifex's MLPEncoder & MLPDecoder\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Step 1: Setup RNG\n",
    "\n",
    "    In Flax NNX, we use `nnx.Rngs` to manage random number generation.\n",
    "    We need separate streams for:\n",
    "    - `params`: Parameter initialization\n",
    "    - `dropout`: Dropout layers (if used)\n",
    "    - `sample`: Stochastic sampling in VAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set random seed for reproducibility\n",
    "seed = 42\n",
    "key = jax.random.key(seed)\n",
    "params_key, dropout_key, sample_key = jax.random.split(key, 3)\n",
    "\n",
    "# Create RNG streams for different purposes\n",
    "rngs = nnx.Rngs(params=params_key, dropout=dropout_key, sample=sample_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Step 2: Load Data\n",
    "\n",
    "    MNIST consists of 28Ã—28 grayscale images of handwritten digits (0-9).\n",
    "    - Training set: 60,000 images (we use 1,000 synthetic for demo)\n",
    "    - Test set: 10,000 images (we use 100 synthetic for demo)\n",
    "\n",
    "    Images are normalized to [0, 1] range for stable training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load data\n",
    "print()\n",
    "print(\"ðŸ“Š Loading MNIST data...\")\n",
    "train_images, test_images = load_mnist_data()\n",
    "print(f\"  Train data shape: {train_images.shape}\")  # (1000, 28, 28, 1)\n",
    "print(f\"  Test data shape: {test_images.shape}\")  # (100, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Step 3: Create Encoder Configuration\n",
    "\n",
    "    Artifex uses frozen dataclass configs for model components. The `EncoderConfig`\n",
    "    specifies the encoder architecture:\n",
    "    - `hidden_dims=(256, 128)`: Two hidden layers with decreasing dimensions\n",
    "    - `latent_dim=32`: Dimension of latent space z\n",
    "    - `activation=\"relu\"`: ReLU activation between layers\n",
    "    - `input_shape=(28, 28, 1)`: Shape of input images (auto-flattened to 784)\n",
    "\n",
    "    The encoder then maps inputs to latent distribution parameters:\n",
    "    - Input: x (28Ã—28Ã—1 = 784 features after flattening)\n",
    "    - Output: (mean, log_var) for latent distribution q(z|x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create encoder configuration\n",
    "print()\n",
    "print(\"ðŸ”§ Creating VAE components using Artifex APIs...\")\n",
    "\n",
    "latent_dim = 32\n",
    "encoder_config = EncoderConfig(\n",
    "    name=\"mnist_encoder\",\n",
    "    hidden_dims=(256, 128),  # Encoder architecture (tuple, not list)\n",
    "    latent_dim=latent_dim,  # Latent space dimension\n",
    "    activation=\"relu\",  # Activation function\n",
    "    input_shape=(28, 28, 1),  # Input image shape\n",
    ")\n",
    "print(f\"  âœ… Encoder config: hidden_dims=(256, 128), latent_dim={latent_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Step 4: Create Decoder Configuration\n",
    "\n",
    "    The `DecoderConfig` specifies the decoder architecture:\n",
    "    - `hidden_dims=(128, 256)`: Reversed encoder dims (symmetric architecture)\n",
    "    - `output_shape=(28, 28, 1)`: Shape of reconstructed images\n",
    "    - `latent_dim=32`: Dimension of latent space (must match encoder)\n",
    "    - `activation=\"relu\"`: ReLU activation (except final layer uses sigmoid)\n",
    "\n",
    "    Artifex's decoder maps latent codes to reconstructions:\n",
    "    - Input: z (32-dimensional latent vector)\n",
    "    - Output: xÌ‚ (28Ã—28Ã—1 reconstructed image)\n",
    "\n",
    "    **Note:** The decoder automatically applies sigmoid activation to the output\n",
    "    to ensure pixel values are in [0, 1] range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create decoder configuration\n",
    "decoder_config = DecoderConfig(\n",
    "    name=\"mnist_decoder\",\n",
    "    hidden_dims=(128, 256),  # Decoder architecture (reversed, tuple)\n",
    "    output_shape=(28, 28, 1),  # Output image shape\n",
    "    latent_dim=latent_dim,  # Latent space dimension\n",
    "    activation=\"relu\",  # Activation function\n",
    ")\n",
    "print(\"  âœ… Decoder config: hidden_dims=(128, 256), output_shape=(28, 28, 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Step 5: Create VAE Model\n",
    "\n",
    "    Artifex's `VAE` class uses a `VAEConfig` that combines encoder and decoder configs:\n",
    "    - **Forward pass**: x â†’ encoder â†’ (Î¼, log ÏƒÂ²) â†’ sample z â†’ decoder â†’ xÌ‚\n",
    "    - **ELBO loss**: Reconstruction loss + KL divergence\n",
    "    - **Sampling methods**: Generate from prior p(z) = N(0, I)\n",
    "\n",
    "    **VAEConfig Parameters:**\n",
    "    - `encoder`: The EncoderConfig we created above\n",
    "    - `decoder`: The DecoderConfig we created above\n",
    "    - `encoder_type=\"dense\"`: Uses MLP-based encoder/decoder (vs \"cnn\" or \"resnet\")\n",
    "    - `kl_weight=1.0`: Weight for KL term (Î²-VAE uses Î²â‰ 1 for disentanglement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create VAE model with config\n",
    "vae_config = VAEConfig(\n",
    "    name=\"mnist_vae\",\n",
    "    encoder=encoder_config,\n",
    "    decoder=decoder_config,\n",
    "    encoder_type=\"dense\",  # Use MLP encoder/decoder\n",
    "    kl_weight=1.0,  # Standard VAE (Î²=1), increase for Î²-VAE\n",
    ")\n",
    "model = VAE(config=vae_config, rngs=rngs)\n",
    "print(f\"  âœ… VAE model created: latent_dim={model.latent_dim}, kl_weight={model.kl_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Step 6: Forward Pass (Reconstruction)\n",
    "\n",
    "    The forward pass demonstrates the full VAE pipeline:\n",
    "    1. **Encoding**: x â†’ encoder â†’ (Î¼, log ÏƒÂ²)\n",
    "    2. **Reparameterization**: z = Î¼ + Ïƒ âŠ™ Îµ, where Îµ ~ N(0, I)\n",
    "    3. **Decoding**: z â†’ decoder â†’ xÌ‚ (reconstruction)\n",
    "\n",
    "    **Output Dictionary:**\n",
    "    - `reconstructed` or `reconstruction`: Reconstructed images xÌ‚\n",
    "    - `mean`: Latent distribution mean Î¼\n",
    "    - `log_var` or `logvar`: Latent distribution log variance log ÏƒÂ²\n",
    "    - `z`: Sampled latent codes (used for reconstruction)\n",
    "\n",
    "    **RNG Handling:** The VAE uses its internal `rngs` (passed during initialization)\n",
    "    with a `sample` stream for the reparameterization trick's random sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Test the model with a batch\n",
    "print()\n",
    "print(\"ðŸ§ª Testing model forward pass...\")\n",
    "test_batch = train_images[:8]  # Use 8 images for testing\n",
    "\n",
    "# Forward pass - the VAE uses its internal rngs for reparameterization\n",
    "# The 'sample' RNG stream is used internally for the reparameterization trick\n",
    "outputs = model(test_batch)\n",
    "\n",
    "# Extract reconstructions (check both possible keys)\n",
    "reconstructed = outputs.get(\"reconstructed\")\n",
    "if reconstructed is None:\n",
    "    reconstructed = outputs[\"reconstruction\"]\n",
    "print(f\"  âœ… Reconstruction shape: {reconstructed.shape}\")\n",
    "\n",
    "# Extract latent codes\n",
    "latent = outputs.get(\"z\")\n",
    "if latent is None:\n",
    "    latent = outputs[\"latent\"]\n",
    "print(f\"  âœ… Latent shape: {latent.shape}\")\n",
    "\n",
    "# Show latent statistics to verify reasonable values\n",
    "print(\"  ðŸ“Š Latent statistics:\")\n",
    "print(f\"     Mean: {jnp.mean(latent):.4f} (should be near 0)\")\n",
    "print(f\"     Std: {jnp.std(latent):.4f} (should be near 1 for standard normal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Step 7: Generation from Prior\n",
    "\n",
    "    To generate new samples:\n",
    "    1. Sample z ~ N(0, I) from the standard normal prior\n",
    "    2. Decode: x_new = decoder(z)\n",
    "\n",
    "    This tests whether the VAE has learned a meaningful latent space.\n",
    "\n",
    "    **Quality Indicators:**\n",
    "    - **Diversity**: Generated samples should vary (not all identical)\n",
    "    - **Realism**: Samples should resemble training data distribution\n",
    "    - **Smoothness**: Similar z should produce similar x (interpolation works)\n",
    "\n",
    "    **Note:** With synthetic data, generations won't be realistic digits,\n",
    "    but the shapes should match the training distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Generate new samples from the prior\n",
    "print()\n",
    "print(\"ðŸŽ¨ Generating new samples from prior...\")\n",
    "n_samples = 5\n",
    "generated = model.generate(n_samples=n_samples)  # VAE uses internal rngs\n",
    "print(f\"  âœ… Generated shape: {generated.shape}\")\n",
    "print(f\"  ðŸ“Š Generated pixels range: [{jnp.min(generated):.3f}, {jnp.max(generated):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ### Step 8: Visualization\n",
    "\n",
    "    The visualization shows:\n",
    "    - **Top row**: Original input images\n",
    "    - **Middle row**: Reconstructions (tests encoder + decoder quality)\n",
    "    - **Bottom row**: Generated samples (tests learned prior)\n",
    "\n",
    "    **What to look for:**\n",
    "    - Reconstructions should closely match originals (good reconstruction loss)\n",
    "    - Generated samples should look plausible (good latent space)\n",
    "    - Diversity in generated samples indicates good latent space coverage\n",
    "\n",
    "    **Note:** With synthetic random data, reconstructions will be blurry\n",
    "    and generations will be random patterns. With real MNIST, you'd see\n",
    "    clear digit reconstructions and realistic generated digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Visualize results\n",
    "print()\n",
    "print(\"ðŸ“Š Visualizing results...\")\n",
    "fig = visualize_vae_results(\n",
    "    original=test_batch[:n_samples],\n",
    "    reconstructed=reconstructed[:n_samples],\n",
    "    generated=generated[:n_samples],\n",
    ")\n",
    "\n",
    "# Step 9: Save figure\n",
    "import os\n",
    "\n",
    "\n",
    "output_dir = \"examples_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"vae_mnist_results.png\")\n",
    "fig.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"  âœ… Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "    \"\"\"\n",
    "    ## Summary and Key Takeaways\n",
    "\n",
    "    âœ… **What We Learned:**\n",
    "    - VAE architecture: encoder â†’ latent space â†’ decoder\n",
    "    - Reparameterization trick enables backpropagation through sampling\n",
    "    - Artifex's MLPEncoder and MLPDecoder provide modular components\n",
    "    - Proper RNG handling: use rngs with 'sample' stream for stochastic operations\n",
    "    - VAE base class handles ELBO loss computation automatically\n",
    "\n",
    "    ðŸ’¡ **Key Insights:**\n",
    "    - VAEs trade reconstruction quality for smooth, structured latent spaces\n",
    "    - The latent dimension (32) controls representation capacity\n",
    "    - KL weight controls reconstruction vs. regularization tradeoff\n",
    "    - Modular design allows easy swapping (MLP â†’ CNN, different layers, etc.)\n",
    "\n",
    "    ðŸ“Š **Results:**\n",
    "    - Reconstructions preserve input structure (with some blur from MSE loss)\n",
    "    - Generated samples from prior p(z) = N(0, I)\n",
    "    - Visualization saved for inspection\n",
    "\n",
    "    ðŸ”§ **Artifex APIs Used:**\n",
    "    - `MLPEncoder`: Maps inputs â†’ (Î¼, log ÏƒÂ²)\n",
    "    - `MLPDecoder`: Maps latent codes â†’ reconstructions\n",
    "    - `VAE`: Combines encoder/decoder with ELBO loss\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"VAE MNIST Example Completed Successfully!\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"ðŸ’¡ Key Takeaways:\")\n",
    "    print(\"  - VAEs learn probabilistic latent representations\")\n",
    "    print(\"  - Reparameterization trick enables gradient-based training\")\n",
    "    print(\"  - Artifex provides modular MLPEncoder and MLPDecoder components\")\n",
    "    print(\"  - Easy to swap components (MLP â†’ CNN, different architectures)\")\n",
    "    print()\n",
    "    print(\"ðŸ”¬ Next Steps:\")\n",
    "    print(\"  - Try CNN encoder/decoder for better image modeling\")\n",
    "    print(\"  - Experiment with different latent dimensions (16, 64, 128)\")\n",
    "    print(\"  - Try Î²-VAE with higher kl_weight for disentanglement\")\n",
    "    print(\"  - Train on real MNIST data for realistic results\")\n",
    "    print(\"  - Explore latent space interpolation and traversals\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Experiments to Try\n",
    "\n",
    "1. **CNN Architecture**:\n",
    "   ```python\n",
    "   # Use encoder_type=\"cnn\" in VAEConfig for CNN-based encoder/decoder\n",
    "   encoder_config = EncoderConfig(\n",
    "       name=\"cnn_encoder\",\n",
    "       hidden_dims=(32, 64, 128),\n",
    "       latent_dim=32,\n",
    "       activation=\"relu\",\n",
    "       input_shape=(28, 28, 1),\n",
    "   )\n",
    "   decoder_config = DecoderConfig(\n",
    "       name=\"cnn_decoder\",\n",
    "       hidden_dims=(128, 64, 32),\n",
    "       output_shape=(28, 28, 1),\n",
    "       latent_dim=32,\n",
    "       activation=\"relu\",\n",
    "   )\n",
    "   config = VAEConfig(\n",
    "       name=\"cnn_vae\",\n",
    "       encoder=encoder_config,\n",
    "       decoder=decoder_config,\n",
    "       encoder_type=\"cnn\",  # Use CNN encoder/decoder\n",
    "       kl_weight=1.0,\n",
    "   )\n",
    "   model = VAE(config=config, rngs=rngs)\n",
    "   ```\n",
    "   CNNs often work better for image data than MLPs.\n",
    "\n",
    "2. **Latent Dimension Experiments**:\n",
    "   - Try `latent_dim=16`: Smaller capacity, faster training, may lose details\n",
    "   - Try `latent_dim=64`: Larger capacity, better reconstructions\n",
    "   - Try `latent_dim=128`: Very high capacity, risk of overfitting\n",
    "\n",
    "   **Trade-off:** Larger latent dims â†’ better reconstruction but less structured space\n",
    "\n",
    "3. **Î²-VAE for Disentanglement**:\n",
    "   ```python\n",
    "   # Increase kl_weight for disentanglement\n",
    "   config = VAEConfig(\n",
    "       name=\"beta_vae\",\n",
    "       encoder=encoder_config,\n",
    "       decoder=decoder_config,\n",
    "       encoder_type=\"dense\",\n",
    "       kl_weight=4.0,  # Î²=4 encourages disentangled representations\n",
    "   )\n",
    "   model = VAE(config=config, rngs=rngs)\n",
    "   ```\n",
    "   - Higher Î²: More regularization, worse reconstruction, better disentanglement\n",
    "   - Lower Î²: Better reconstruction, less structured latent space\n",
    "   - Î²=1: Standard VAE\n",
    "\n",
    "4. **Architecture Variations**:\n",
    "   ```python\n",
    "   # Deeper network with different activation\n",
    "   encoder_config = EncoderConfig(\n",
    "       name=\"deep_encoder\",\n",
    "       hidden_dims=(512, 256, 128),  # Deeper network\n",
    "       latent_dim=32,\n",
    "       activation=\"gelu\",  # Try different activations\n",
    "       input_shape=(28, 28, 1),\n",
    "   )\n",
    "   ```\n",
    "   - More layers: Higher capacity but slower training\n",
    "   - Different activations: GELU often works better than ReLU\n",
    "   - Batch normalization: Can help with deeper networks\n",
    "\n",
    "5. **Real MNIST Data**:\n",
    "   ```python\n",
    "   import tensorflow_datasets as tfds\n",
    "\n",
    "   # Load real MNIST\n",
    "   ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "\n",
    "   def preprocess(image, label):\n",
    "       image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "       return image\n",
    "\n",
    "   ds = ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "   ```\n",
    "   Real MNIST will give much better results and realistic digit generation.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Issue:** Encoder/decoder creation fails\n",
    "- **Solution**: Check that `input_shape` matches your data shape in EncoderConfig\n",
    "- **Common mistake**: Using list instead of tuple for `hidden_dims`\n",
    "- **Fix**: Use tuples: `hidden_dims=(256, 128)` not `hidden_dims=[256, 128]`\n",
    "\n",
    "**Issue:** Reconstructions are very blurry\n",
    "- **Solution**: This is expected with MSE loss on images\n",
    "- **Explanation**: MSE averages over pixel space, causing blur\n",
    "- **Alternatives**:\n",
    "  - Use CNNEncoder/CNNDecoder instead of MLP\n",
    "  - Try perceptual loss or adversarial training\n",
    "  - Use VQVAE for sharper reconstructions\n",
    "\n",
    "**Issue:** Generated samples look like noise\n",
    "- **Solution**: VAE needs training; this example only demonstrates architecture\n",
    "- **Note**: With synthetic random data, generations won't be meaningful\n",
    "- **Fix**: Train on real MNIST with a proper training loop (see training examples)\n",
    "\n",
    "**Issue:** KL collapse (all latent codes become identical)\n",
    "- **Solution**: Reduce `kl_weight` to allow more latent variance\n",
    "- **Monitoring**: Check `jnp.std(latent)` - should be > 0.5\n",
    "- **Fix**: Use KL annealing schedule (start with kl_weight=0.1, increase gradually)\n",
    "\n",
    "**Issue:** Model output shape mismatch\n",
    "- **Solution**: Ensure encoder `latent_dim` matches decoder `latent_dim` in configs\n",
    "- **Check**: Verify `output_shape` matches `input_shape` for reconstruction\n",
    "- **Note**: VAEConfig validates that latent_dim matches between encoder and decoder\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "**Papers:**\n",
    "1. **Auto-Encoding Variational Bayes** (Kingma & Welling, 2014)\n",
    "   - Original VAE paper\n",
    "   - https://arxiv.org/abs/1312.6114\n",
    "\n",
    "2. **Î²-VAE: Learning Basic Visual Concepts** (Higgins et al., 2017)\n",
    "   - Disentanglement via Î² parameter\n",
    "   - https://openreview.net/forum?id=Sy2fzU9gl\n",
    "\n",
    "3. **Understanding disentangling in Î²-VAE** (Burgess et al., 2018)\n",
    "   - Analysis of Î²-VAE disentanglement\n",
    "   - https://arxiv.org/abs/1804.03599\n",
    "\n",
    "**Documentation:**\n",
    "- Artifex VAE Documentation: `docs/models/vae.md`\n",
    "- Flax NNX Guide: https://flax.readthedocs.io/en/latest/nnx/index.html\n",
    "- VAE Tutorial: `docs/examples/basic/vae-mnist.md`\n",
    "\n",
    "**Related Examples:**\n",
    "- `advanced_vae.py`: Î²-VAE, Conditional VAE, VQ-VAE, Hierarchical VAE\n",
    "- `multi_beta_vae_benchmark_demo.py`: Disentanglement evaluation with MIG score\n",
    "- `simple_gan.py`: Compare VAE vs. GAN generation approaches"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
