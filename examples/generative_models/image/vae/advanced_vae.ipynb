{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"Advanced VAE Examples - Showcase Artifex's Advanced VAE Features\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example demonstrates Artifex's advanced VAE implementations on real MNIST data:\n",
    "1. **β-VAE**: Disentangled representations with β weighting and annealing\n",
    "2. **β-VAE with Capacity Control**: Burgess et al. capacity-based training\n",
    "3. **Conditional VAE**: Label-conditioned generation for controlled sampling\n",
    "4. **VQ-VAE**: Discrete latent codes with vector quantization\n",
    "\n",
    "All models use Artifex's production-ready implementations with proper training\n",
    "on real MNIST data.\n",
    "\n",
    "## Source Code Dependencies\n",
    "\n",
    "**Validated:** 2026-01-13\n",
    "\n",
    "This example uses Artifex's VAE implementations with frozen dataclass configs:\n",
    "- `artifex.generative_models.models.vae.BetaVAE` - β-VAE implementation\n",
    "- `artifex.generative_models.models.vae.BetaVAEWithCapacity` - Capacity control\n",
    "- `artifex.generative_models.models.vae.ConditionalVAE` - Conditional VAE\n",
    "- `artifex.generative_models.models.vae.VQVAE` - Vector-quantized VAE\n",
    "- `artifex.generative_models.core.configuration.vae_config` - BetaVAEConfig, etc.\n",
    "- `artifex.generative_models.core.configuration.network_configs` - EncoderConfig, DecoderConfig\n",
    "\n",
    "**Validation Status:**\n",
    "- ✅ All dependencies use Flax NNX best practices\n",
    "- ✅ Proper RNG handling throughout\n",
    "- ✅ Production-ready implementations from Artifex\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- [x] Using Artifex's BetaVAE with different β values\n",
    "- [x] Implementing β annealing for gradual disentanglement\n",
    "- [x] Capacity control for stable β-VAE training\n",
    "- [x] Conditional generation with ConditionalVAE\n",
    "- [x] Monitoring VQ-VAE codebook usage and perplexity\n",
    "- [x] Training advanced VAEs on real MNIST data\n",
    "- [x] Evaluating and visualizing latent representations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Artifex installed (run `source activate.sh`)\n",
    "- Understanding of standard VAEs (see basic vae-mnist tutorial)\n",
    "- Familiarity with JAX and Flax NNX\n",
    "- Knowledge of variational inference concepts\n",
    "\n",
    "## Usage\n",
    "\n",
    "```bash\n",
    "source activate.sh\n",
    "python examples/generative_models/image/vae/advanced_vae.py\n",
    "```\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "The example will:\n",
    "1. Load real MNIST dataset (60,000 training images)\n",
    "2. Train four advanced VAE variants (β-VAE, Capacity β-VAE, Conditional VAE, VQ-VAE)\n",
    "3. Generate reconstructions and samples from each model\n",
    "4. Save visualizations to `examples_output/advanced_vae/`\n",
    "5. Display training metrics and convergence curves\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### β-VAE\n",
    "\n",
    "β-VAE adds weight β > 1 to KL divergence term for disentanglement:\n",
    "$$\\mathcal{L}_{\\beta} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\beta \\cdot \\text{KL}(q(z|x) \\| p(z))$$\n",
    "\n",
    "Higher β encourages independence between latent dimensions.\n",
    "\n",
    "### β-VAE with Capacity Control\n",
    "\n",
    "Capacity control (Burgess et al.) gradually increases KL capacity:\n",
    "$$\\mathcal{L}_{C} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] + \\gamma \\cdot |KL - C|$$\n",
    "\n",
    "Where C increases from 0 to max_capacity during training.\n",
    "\n",
    "### Conditional VAE\n",
    "\n",
    "Conditional VAE adds label information to encoder and decoder, enabling\n",
    "controlled generation of specific classes.\n",
    "\n",
    "### VQ-VAE\n",
    "\n",
    "VQ-VAE uses discrete latent codes from a learnable codebook, enabling\n",
    "better compression and sharper reconstructions.\n",
    "\n",
    "## Estimated Runtime\n",
    "\n",
    "- **CPU**: ~20-30 minutes total (all 4 variants)\n",
    "- **GPU**: ~5-8 minutes total (if available)\n",
    "\n",
    "## Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## Last Updated\n",
    "\n",
    "2026-01-13\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Advanced VAE Examples\n",
    "\n",
    "This notebook demonstrates Artifex's advanced VAE implementations on real MNIST data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this example, you will understand:\n",
    "1. How to use Artifex's BetaVAE with different β values\n",
    "2. Implementing β annealing and capacity control\n",
    "3. Conditional generation with ConditionalVAE\n",
    "4. Monitoring VQ-VAE codebook usage\n",
    "5. Training and evaluating advanced VAE variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import Dependencies\n",
    "\"\"\"\n",
    "Import Artifex's VAE implementations and utilities for training advanced variants.\n",
    "\"\"\"\n",
    "\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "from tqdm import tqdm\n",
    "\n",
    "from artifex.generative_models.core.configuration.network_configs import (\n",
    "    DecoderConfig,\n",
    "    EncoderConfig,\n",
    ")\n",
    "from artifex.generative_models.core.configuration.vae_config import (\n",
    "    BetaVAEConfig,\n",
    "    BetaVAEWithCapacityConfig,\n",
    "    ConditionalVAEConfig,\n",
    "    VQVAEConfig,\n",
    ")\n",
    "from artifex.generative_models.models.vae import BetaVAE, ConditionalVAE, VQVAE\n",
    "from artifex.generative_models.models.vae.beta_vae import BetaVAEWithCapacity\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = Path(\"examples_output/advanced_vae\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "Load real MNIST dataset using TensorFlow Datasets. This ensures we train on\n",
    "actual handwritten digits with proper training/test splits.\n",
    "\n",
    "**Dataset Properties:**\n",
    "- Training: 60,000 images\n",
    "- Test: 10,000 images\n",
    "- Image size: 28×28×1 (grayscale)\n",
    "- Labels: 0-9 (digit classes)\n",
    "- Values: [0, 1] (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Real MNIST Data Loading with Grain\n",
    "def load_real_mnist(batch_size=128):\n",
    "    \"\"\"Load real MNIST dataset using Grain framework (JAX best practice).\n",
    "\n",
    "    Uses Hugging Face datasets for initial loading, then Grain for efficient\n",
    "    batching and iteration. This follows JAX/Grain best practices for small datasets.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Batch size for training\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader) as Grain DataLoaders\n",
    "    \"\"\"\n",
    "    import grain.python as grain\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    print(\"\\nLoading MNIST dataset...\")\n",
    "\n",
    "    # Load MNIST from Hugging Face (avoids TensorFlow dependency)\n",
    "    ds = load_dataset(\"mnist\")\n",
    "\n",
    "    # Convert to lists of indices (Grain will fetch data via __getitem__)\n",
    "    # We'll create a custom RandomAccessDataSource to handle complex data types\n",
    "    class MNISTDataSource(grain.RandomAccessDataSource):\n",
    "        \"\"\"Custom data source for MNIST that handles numpy/JAX arrays.\"\"\"\n",
    "\n",
    "        def __init__(self, hf_dataset):\n",
    "            self.dataset = hf_dataset\n",
    "            # Pre-convert all images to JAX arrays for efficiency\n",
    "            self.images = []\n",
    "            self.labels = []\n",
    "            for item in hf_dataset:\n",
    "                # Convert PIL image to JAX array\n",
    "                image = jnp.array(item[\"image\"], dtype=jnp.float32)\n",
    "                image = image / 255.0  # Normalize to [0, 1]\n",
    "                image = image[..., jnp.newaxis]  # Add channel: (28, 28, 1)\n",
    "                self.images.append(image)\n",
    "                self.labels.append(item[\"label\"])\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.images)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return {\"image\": self.images[index], \"label\": self.labels[index]}\n",
    "\n",
    "    train_source = MNISTDataSource(ds[\"train\"])\n",
    "    test_source = MNISTDataSource(ds[\"test\"])\n",
    "\n",
    "    print(f\"✓ MNIST loaded: {len(train_source)} training images, {len(test_source)} test images\")\n",
    "\n",
    "    # Create samplers\n",
    "    train_sampler = grain.IndexSampler(\n",
    "        num_records=len(train_source),\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        num_epochs=None,  # Infinite epochs (we'll manually control batches per epoch)\n",
    "    )\n",
    "\n",
    "    test_sampler = grain.IndexSampler(\n",
    "        num_records=len(test_source),\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        num_epochs=1,  # Single pass for evaluation\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders with batching\n",
    "    train_loader = grain.DataLoader(\n",
    "        data_source=train_source,\n",
    "        sampler=train_sampler,\n",
    "        operations=[grain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
    "        worker_count=0,  # Single-process for simplicity\n",
    "    )\n",
    "\n",
    "    test_loader = grain.DataLoader(\n",
    "        data_source=test_source,\n",
    "        sampler=test_sampler,\n",
    "        operations=[grain.Batch(batch_size=batch_size, drop_remainder=False)],\n",
    "        worker_count=0,\n",
    "    )\n",
    "\n",
    "    print(f\"✓ Created Grain DataLoaders (batch_size={batch_size})\")\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 1: β-VAE with β Annealing\n",
    "\n",
    "Demonstrate Artifex's BetaVAE with gradual β annealing from 1.0 to 4.0.\n",
    "This helps avoid posterior collapse while achieving disentanglement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: β-VAE Setup\n",
    "def create_beta_vae(beta=4.0, warmup_steps=1000):\n",
    "    \"\"\"Create a β-VAE model using Artifex's config-based API.\n",
    "\n",
    "    Args:\n",
    "        beta: Weight for KL divergence (β > 1 for disentanglement)\n",
    "        warmup_steps: Steps to anneal β from 0 to beta\n",
    "\n",
    "    Returns:\n",
    "        BetaVAE model\n",
    "    \"\"\"\n",
    "    # Create encoder config\n",
    "    encoder_config = EncoderConfig(\n",
    "        name=\"beta_vae_encoder\",\n",
    "        hidden_dims=(512, 256),  # Tuple for frozen dataclass\n",
    "        latent_dim=10,\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),\n",
    "    )\n",
    "\n",
    "    # Create decoder config\n",
    "    decoder_config = DecoderConfig(\n",
    "        name=\"beta_vae_decoder\",\n",
    "        hidden_dims=(256, 512),  # Tuple for frozen dataclass\n",
    "        latent_dim=10,\n",
    "        output_shape=(28, 28, 1),\n",
    "        activation=\"relu\",\n",
    "    )\n",
    "\n",
    "    # Create β-VAE config with annealing\n",
    "    config = BetaVAEConfig(\n",
    "        name=\"beta_vae_mnist\",\n",
    "        encoder=encoder_config,\n",
    "        decoder=decoder_config,\n",
    "        encoder_type=\"dense\",\n",
    "        beta_default=beta,\n",
    "        beta_warmup_steps=warmup_steps,\n",
    "        reconstruction_loss_type=\"mse\",\n",
    "    )\n",
    "\n",
    "    # Create model from config\n",
    "    model = BetaVAE(config=config, rngs=nnx.Rngs(0))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Common Training Infrastructure\n",
    "@nnx.jit(donate_argnums=(1,))  # Donate optimizer for memory efficiency\n",
    "def train_step(model, optimizer, images, step, labels=None):\n",
    "    \"\"\"Single training step (JIT-compiled for speed).\n",
    "\n",
    "    Following JAX best practices: step is traced (not static) to avoid recompilation.\n",
    "    JIT compilation provides 3-50x speedup on compute-intensive operations.\n",
    "\n",
    "    Args:\n",
    "        model: VAE model (any variant) with internal RNGs\n",
    "        optimizer: NNX optimizer (donated for memory efficiency)\n",
    "        images: Batch of images\n",
    "        step: Current training step (traced, not static - used for β/capacity annealing)\n",
    "        labels: Optional labels for conditional VAE\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of losses from model.loss_fn()\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(model):\n",
    "        # Handle conditional vs non-conditional models\n",
    "        # Models use their internal RNGs, no need to pass rngs parameter\n",
    "        if labels is not None:\n",
    "            outputs = model(images, y=labels)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "\n",
    "        # Compute losses (step parameter needed for β/capacity annealing)\n",
    "        losses = model.loss_fn(x=images, outputs=outputs, step=step)\n",
    "        return losses[\"loss\"], losses\n",
    "\n",
    "    # Compute gradients\n",
    "    (_, losses), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
    "\n",
    "    # Update parameters (NNX 0.11.0+ API)\n",
    "    optimizer.update(model, grads)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: β-VAE Training\n",
    "def train_beta_vae(model, train_ds, num_epochs=5, learning_rate=1e-3, batches_per_epoch=468):\n",
    "    \"\"\"Train β-VAE with monitoring of β value and loss components.\n",
    "\n",
    "    Uses shared JIT-compiled train step for performance (JAX best practice).\n",
    "\n",
    "    Args:\n",
    "        model: BetaVAE model\n",
    "        train_ds: Training dataset (infinite iterator)\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        batches_per_epoch: Number of batches per epoch (default 468 for MNIST with batch_size=128)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training β-VAE (β={model.beta_default}, warmup={model.beta_warmup_steps} steps)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Optimizer (NNX 0.11.0+ API requires wrt parameter)\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n",
    "\n",
    "    history = {\"loss\": [], \"recon_loss\": [], \"kl_loss\": [], \"beta\": []}\n",
    "    step_count = 0\n",
    "\n",
    "    # Set model to training mode (enables dropout, batch norm training, etc.)\n",
    "    model.train()\n",
    "\n",
    "    print(\"Compiling train step with JIT (first iteration will be slower)...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_metrics = {\"loss\": [], \"recon_loss\": [], \"kl_loss\": [], \"beta\": []}\n",
    "\n",
    "        # Limit batches per epoch from infinite iterator\n",
    "        epoch_ds = islice(train_ds, batches_per_epoch)\n",
    "\n",
    "        # Progress bar for batches (leave=True to show progress for each epoch)\n",
    "        pbar = tqdm(\n",
    "            epoch_ds,\n",
    "            desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "            total=batches_per_epoch,\n",
    "            leave=True,\n",
    "            unit=\" batch\",\n",
    "        )\n",
    "        for batch in pbar:\n",
    "            images = jnp.array(batch[\"image\"])  # Convert to JAX array\n",
    "\n",
    "            # Execute JIT-compiled train step (model uses internal RNGs)\n",
    "            losses = train_step(model, optimizer, images, step_count)\n",
    "\n",
    "            # Track metrics\n",
    "            epoch_metrics[\"loss\"].append(float(losses[\"loss\"]))\n",
    "            epoch_metrics[\"recon_loss\"].append(float(losses[\"reconstruction_loss\"]))\n",
    "            epoch_metrics[\"kl_loss\"].append(float(losses[\"kl_loss\"]))\n",
    "            epoch_metrics[\"beta\"].append(float(losses[\"beta\"]))\n",
    "\n",
    "            # Update progress bar with current metrics\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{losses['loss']:.4f}\",\n",
    "                    \"recon\": f\"{losses['reconstruction_loss']:.4f}\",\n",
    "                    \"kl\": f\"{losses['kl_loss']:.4f}\",\n",
    "                    \"β\": f\"{losses['beta']:.3f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "        # Average metrics\n",
    "        avg_loss = np.mean(epoch_metrics[\"loss\"])\n",
    "        avg_recon = np.mean(epoch_metrics[\"recon_loss\"])\n",
    "        avg_kl = np.mean(epoch_metrics[\"kl_loss\"])\n",
    "        avg_beta = np.mean(epoch_metrics[\"beta\"])\n",
    "\n",
    "        history[\"loss\"].append(avg_loss)\n",
    "        history[\"recon_loss\"].append(avg_recon)\n",
    "        history[\"kl_loss\"].append(avg_kl)\n",
    "        history[\"beta\"].append(avg_beta)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
    "            f\"Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, \"\n",
    "            f\"KL: {avg_kl:.4f}, β: {avg_beta:.3f}\"\n",
    "        )\n",
    "\n",
    "    print(\"✓ β-VAE training complete\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 2: β-VAE with Capacity Control\n",
    "\n",
    "Demonstrate Artifex's BetaVAEWithCapacity which uses capacity control\n",
    "(Burgess et al.) for more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Capacity β-VAE Setup\n",
    "def create_capacity_beta_vae(capacity_max=25.0, capacity_steps=5000):\n",
    "    \"\"\"Create a β-VAE with capacity control using config-based API.\n",
    "\n",
    "    Args:\n",
    "        capacity_max: Maximum KL capacity in nats\n",
    "        capacity_steps: Steps to reach max capacity\n",
    "\n",
    "    Returns:\n",
    "        BetaVAEWithCapacity model\n",
    "    \"\"\"\n",
    "    # Create encoder config\n",
    "    encoder_config = EncoderConfig(\n",
    "        name=\"capacity_vae_encoder\",\n",
    "        hidden_dims=(512, 256),  # Tuple for frozen dataclass\n",
    "        latent_dim=10,\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),\n",
    "    )\n",
    "\n",
    "    # Create decoder config\n",
    "    decoder_config = DecoderConfig(\n",
    "        name=\"capacity_vae_decoder\",\n",
    "        hidden_dims=(256, 512),  # Tuple for frozen dataclass\n",
    "        latent_dim=10,\n",
    "        output_shape=(28, 28, 1),\n",
    "        activation=\"relu\",\n",
    "    )\n",
    "\n",
    "    # Create β-VAE with capacity control config\n",
    "    config = BetaVAEWithCapacityConfig(\n",
    "        name=\"capacity_beta_vae_mnist\",\n",
    "        encoder=encoder_config,\n",
    "        decoder=decoder_config,\n",
    "        encoder_type=\"dense\",\n",
    "        beta_default=1.0,  # β fixed at 1.0 when using capacity control\n",
    "        beta_warmup_steps=0,\n",
    "        reconstruction_loss_type=\"mse\",\n",
    "        use_capacity_control=True,\n",
    "        capacity_max=capacity_max,\n",
    "        capacity_num_iter=capacity_steps,\n",
    "        gamma=1000.0,\n",
    "    )\n",
    "\n",
    "    # Create model from config\n",
    "    model = BetaVAEWithCapacity(config=config, rngs=nnx.Rngs(10))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Capacity β-VAE Training\n",
    "def train_capacity_beta_vae(\n",
    "    model, train_ds, num_epochs=5, learning_rate=1e-3, batches_per_epoch=468\n",
    "):\n",
    "    \"\"\"Train β-VAE with capacity control monitoring.\n",
    "\n",
    "    Uses shared JIT-compiled train step for performance.\n",
    "\n",
    "    Args:\n",
    "        model: BetaVAEWithCapacity model\n",
    "        train_ds: Training dataset (infinite iterator)\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        batches_per_epoch: Number of batches per epoch (default 468 for MNIST with batch_size=128)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training β-VAE with Capacity Control (C_max={model.capacity_max} nats)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Optimizer (NNX 0.11.0+ API requires wrt parameter)\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n",
    "\n",
    "    history = {\"loss\": [], \"recon_loss\": [], \"kl_loss\": [], \"capacity\": [], \"capacity_loss\": []}\n",
    "    step_count = 0\n",
    "\n",
    "    # Set model to training mode (enables dropout, batch norm training, etc.)\n",
    "    model.train()\n",
    "\n",
    "    print(\"Compiling train step with JIT (first iteration will be slower)...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_metrics = {\n",
    "            \"loss\": [],\n",
    "            \"recon_loss\": [],\n",
    "            \"kl_loss\": [],\n",
    "            \"capacity\": [],\n",
    "            \"capacity_loss\": [],\n",
    "        }\n",
    "\n",
    "        # Limit batches per epoch from infinite iterator\n",
    "        epoch_ds = islice(train_ds, batches_per_epoch)\n",
    "\n",
    "        # Progress bar for batches (leave=True to show progress for each epoch)\n",
    "        pbar = tqdm(\n",
    "            epoch_ds,\n",
    "            desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "            total=batches_per_epoch,\n",
    "            leave=True,\n",
    "            unit=\" batch\",\n",
    "        )\n",
    "        for batch in pbar:\n",
    "            images = jnp.array(batch[\"image\"])  # Convert to JAX array\n",
    "\n",
    "            # Execute JIT-compiled train step (model uses internal RNGs)\n",
    "            losses = train_step(model, optimizer, images, step_count)\n",
    "\n",
    "            # Track metrics\n",
    "            epoch_metrics[\"loss\"].append(float(losses[\"loss\"]))\n",
    "            epoch_metrics[\"recon_loss\"].append(float(losses[\"reconstruction_loss\"]))\n",
    "            epoch_metrics[\"kl_loss\"].append(float(losses[\"kl_loss\"]))\n",
    "            epoch_metrics[\"capacity\"].append(float(losses[\"current_capacity\"]))\n",
    "            epoch_metrics[\"capacity_loss\"].append(float(losses[\"capacity_loss\"]))\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{losses['loss']:.4f}\",\n",
    "                    \"recon\": f\"{losses['reconstruction_loss']:.4f}\",\n",
    "                    \"C\": f\"{losses['current_capacity']:.2f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "        # Average metrics\n",
    "        avg_loss = np.mean(epoch_metrics[\"loss\"])\n",
    "        avg_recon = np.mean(epoch_metrics[\"recon_loss\"])\n",
    "        avg_kl = np.mean(epoch_metrics[\"kl_loss\"])\n",
    "        avg_cap = np.mean(epoch_metrics[\"capacity\"])\n",
    "        avg_cap_loss = np.mean(epoch_metrics[\"capacity_loss\"])\n",
    "\n",
    "        history[\"loss\"].append(avg_loss)\n",
    "        history[\"recon_loss\"].append(avg_recon)\n",
    "        history[\"kl_loss\"].append(avg_kl)\n",
    "        history[\"capacity\"].append(avg_cap)\n",
    "        history[\"capacity_loss\"].append(avg_cap_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
    "            f\"Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, \"\n",
    "            f\"KL: {avg_kl:.4f}, C: {avg_cap:.2f}\"\n",
    "        )\n",
    "\n",
    "    print(\"✓ Capacity β-VAE training complete\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 3: Conditional VAE\n",
    "\n",
    "Demonstrate Artifex's ConditionalVAE for label-conditioned generation,\n",
    "enabling controlled sampling of specific digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Conditional VAE Setup\n",
    "def create_conditional_vae():\n",
    "    \"\"\"Create a Conditional VAE model using Artifex's config-based API.\n",
    "\n",
    "    The ConditionalVAE automatically handles the encoder input adjustment\n",
    "    and decoder latent adjustment for conditioning by wrapping base encoders\n",
    "    and decoders with ConditionalEncoder and ConditionalDecoder.\n",
    "\n",
    "    Returns:\n",
    "        ConditionalVAE model\n",
    "    \"\"\"\n",
    "    # MNIST image dimensions - base encoder receives flattened images + condition\n",
    "    # ConditionalVAE handles the input size adjustment internally\n",
    "\n",
    "    # Create encoder config (base encoder dimensions before conditioning adjustment)\n",
    "    encoder_config = EncoderConfig(\n",
    "        name=\"cvae_encoder\",\n",
    "        hidden_dims=(512, 256),  # Tuple for frozen dataclass\n",
    "        latent_dim=20,\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),  # Original input shape (conditioning added internally)\n",
    "    )\n",
    "\n",
    "    # Create decoder config (base decoder dimensions)\n",
    "    decoder_config = DecoderConfig(\n",
    "        name=\"cvae_decoder\",\n",
    "        hidden_dims=(256, 512),  # Tuple for frozen dataclass\n",
    "        latent_dim=20,  # Base latent dim (conditioning added internally)\n",
    "        output_shape=(28, 28, 1),\n",
    "        activation=\"relu\",\n",
    "    )\n",
    "\n",
    "    # Create Conditional VAE config\n",
    "    config = ConditionalVAEConfig(\n",
    "        name=\"conditional_vae_mnist\",\n",
    "        encoder=encoder_config,\n",
    "        decoder=decoder_config,\n",
    "        encoder_type=\"dense\",\n",
    "        num_classes=10,  # 10 digit classes for MNIST\n",
    "        condition_dim=10,  # Embedding dimension for class labels\n",
    "        condition_type=\"concat\",\n",
    "    )\n",
    "\n",
    "    # Create model from config (internally wraps encoder/decoder with conditional layers)\n",
    "    model = ConditionalVAE(config=config, rngs=nnx.Rngs(20))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Conditional VAE Training\n",
    "def train_conditional_vae(model, train_ds, num_epochs=5, learning_rate=1e-3, batches_per_epoch=468):\n",
    "    \"\"\"Train Conditional VAE with label conditioning.\n",
    "\n",
    "    Uses shared JIT-compiled train step for performance.\n",
    "\n",
    "    Args:\n",
    "        model: ConditionalVAE model\n",
    "        train_ds: Training dataset (infinite iterator)\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        batches_per_epoch: Number of batches per epoch (default 468 for MNIST with batch_size=128)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training Conditional VAE (condition_dim={model.condition_dim})\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Optimizer (NNX 0.11.0+ API requires wrt parameter)\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n",
    "\n",
    "    history = {\"loss\": [], \"recon_loss\": [], \"kl_loss\": []}\n",
    "    step_count = 0\n",
    "\n",
    "    # Set model to training mode (enables dropout, batch norm training, etc.)\n",
    "    model.train()\n",
    "\n",
    "    print(\"Compiling train step with JIT (first iteration will be slower)...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_metrics = {\"loss\": [], \"recon_loss\": [], \"kl_loss\": []}\n",
    "\n",
    "        # Limit batches per epoch from infinite iterator\n",
    "        epoch_ds = islice(train_ds, batches_per_epoch)\n",
    "\n",
    "        # Progress bar for batches (leave=True to show progress for each epoch)\n",
    "        pbar = tqdm(\n",
    "            epoch_ds,\n",
    "            desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "            total=batches_per_epoch,\n",
    "            leave=True,\n",
    "            unit=\" batch\",\n",
    "        )\n",
    "        for batch in pbar:\n",
    "            images = jnp.array(batch[\"image\"])  # Convert to JAX array\n",
    "            labels = jnp.array(batch[\"label\"])  # Convert to JAX array\n",
    "\n",
    "            # Execute JIT-compiled train step with labels (model uses internal RNGs)\n",
    "            losses = train_step(model, optimizer, images, step_count, labels=labels)\n",
    "\n",
    "            # Track metrics\n",
    "            epoch_metrics[\"loss\"].append(float(losses[\"loss\"]))\n",
    "            epoch_metrics[\"recon_loss\"].append(float(losses[\"reconstruction_loss\"]))\n",
    "            epoch_metrics[\"kl_loss\"].append(float(losses[\"kl_loss\"]))\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{losses['loss']:.4f}\",\n",
    "                    \"recon\": f\"{losses['reconstruction_loss']:.4f}\",\n",
    "                    \"kl\": f\"{losses['kl_loss']:.4f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "        # Average metrics\n",
    "        avg_loss = np.mean(epoch_metrics[\"loss\"])\n",
    "        avg_recon = np.mean(epoch_metrics[\"recon_loss\"])\n",
    "        avg_kl = np.mean(epoch_metrics[\"kl_loss\"])\n",
    "\n",
    "        history[\"loss\"].append(avg_loss)\n",
    "        history[\"recon_loss\"].append(avg_recon)\n",
    "        history[\"kl_loss\"].append(avg_kl)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
    "            f\"Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"✓ Conditional VAE training complete\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Example 4: VQ-VAE\n",
    "\n",
    "Demonstrate Artifex's VQVAE with codebook monitoring and perplexity tracking\n",
    "to ensure healthy codebook usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: VQ-VAE Setup\n",
    "def create_vqvae(num_embeddings=512, embedding_dim=64):\n",
    "    \"\"\"Create a VQ-VAE model using Artifex's config-based API.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings: Size of codebook\n",
    "        embedding_dim: Dimension of each embedding\n",
    "\n",
    "    Returns:\n",
    "        VQVAE model\n",
    "    \"\"\"\n",
    "    # Create encoder config\n",
    "    encoder_config = EncoderConfig(\n",
    "        name=\"vqvae_encoder\",\n",
    "        hidden_dims=(512, 256),  # Tuple for frozen dataclass\n",
    "        latent_dim=embedding_dim,\n",
    "        activation=\"relu\",\n",
    "        input_shape=(28, 28, 1),\n",
    "    )\n",
    "\n",
    "    # Create decoder config\n",
    "    decoder_config = DecoderConfig(\n",
    "        name=\"vqvae_decoder\",\n",
    "        hidden_dims=(256, 512),  # Tuple for frozen dataclass\n",
    "        latent_dim=embedding_dim,\n",
    "        output_shape=(28, 28, 1),\n",
    "        activation=\"relu\",\n",
    "    )\n",
    "\n",
    "    # Create VQ-VAE config\n",
    "    config = VQVAEConfig(\n",
    "        name=\"vqvae_mnist\",\n",
    "        encoder=encoder_config,\n",
    "        decoder=decoder_config,\n",
    "        encoder_type=\"dense\",\n",
    "        num_embeddings=num_embeddings,\n",
    "        embedding_dim=embedding_dim,\n",
    "        commitment_cost=0.25,\n",
    "    )\n",
    "\n",
    "    # Create model from config\n",
    "    model = VQVAE(config=config, rngs=nnx.Rngs(30))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: VQ-VAE Training\n",
    "def train_vqvae(model, train_ds, num_epochs=5, learning_rate=1e-3, batches_per_epoch=468):\n",
    "    \"\"\"Train VQ-VAE with codebook monitoring.\n",
    "\n",
    "    Uses shared JIT-compiled train step for performance.\n",
    "\n",
    "    Args:\n",
    "        model: VQVAE model\n",
    "        train_ds: Training dataset (infinite iterator)\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        batches_per_epoch: Number of batches per epoch (default 468 for MNIST with batch_size=128)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Training VQ-VAE (codebook={model.num_embeddings}, dim={model.embedding_dim})\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Optimizer (NNX 0.11.0+ API requires wrt parameter)\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n",
    "\n",
    "    history = {\"loss\": [], \"recon_loss\": [], \"vq_loss\": [], \"perplexity\": []}\n",
    "    step_count = 0\n",
    "\n",
    "    # Set model to training mode (enables dropout, batch norm training, etc.)\n",
    "    model.train()\n",
    "\n",
    "    print(\"Compiling train step with JIT (first iteration will be slower)...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_metrics = {\"loss\": [], \"recon_loss\": [], \"vq_loss\": [], \"perplexity\": []}\n",
    "\n",
    "        # Limit batches per epoch from infinite iterator\n",
    "        epoch_ds = islice(train_ds, batches_per_epoch)\n",
    "\n",
    "        # Progress bar for batches (leave=True to show progress for each epoch)\n",
    "        pbar = tqdm(\n",
    "            epoch_ds,\n",
    "            desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
    "            total=batches_per_epoch,\n",
    "            leave=True,\n",
    "            unit=\" batch\",\n",
    "        )\n",
    "        for batch in pbar:\n",
    "            images = jnp.array(batch[\"image\"])  # Convert to JAX array\n",
    "\n",
    "            # Execute JIT-compiled train step (model uses internal RNGs)\n",
    "            losses = train_step(model, optimizer, images, step_count)\n",
    "\n",
    "            # Track metrics\n",
    "            epoch_metrics[\"loss\"].append(float(losses[\"loss\"]))\n",
    "            epoch_metrics[\"recon_loss\"].append(float(losses[\"reconstruction_loss\"]))\n",
    "            epoch_metrics[\"vq_loss\"].append(float(losses[\"vq_loss\"]))\n",
    "            epoch_metrics[\"perplexity\"].append(float(losses.get(\"perplexity\", 0.0)))\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"loss\": f\"{losses['loss']:.4f}\",\n",
    "                    \"recon\": f\"{losses['reconstruction_loss']:.4f}\",\n",
    "                    \"vq\": f\"{losses['vq_loss']:.4f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "        # Average metrics\n",
    "        avg_loss = np.mean(epoch_metrics[\"loss\"])\n",
    "        avg_recon = np.mean(epoch_metrics[\"recon_loss\"])\n",
    "        avg_vq = np.mean(epoch_metrics[\"vq_loss\"])\n",
    "        avg_perp = np.mean(epoch_metrics[\"perplexity\"])\n",
    "\n",
    "        history[\"loss\"].append(avg_loss)\n",
    "        history[\"recon_loss\"].append(avg_recon)\n",
    "        history[\"vq_loss\"].append(avg_vq)\n",
    "        history[\"perplexity\"].append(avg_perp)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} - \"\n",
    "            f\"Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, \"\n",
    "            f\"VQ: {avg_vq:.4f}, Perplexity: {avg_perp:.1f}\"\n",
    "        )\n",
    "\n",
    "    print(\"✓ VQ-VAE training complete\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Visualization Functions\n",
    "\n",
    "Create visualizations for each VAE variant showing reconstructions and\n",
    "generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualization Functions\n",
    "def visualize_vae_results(model, test_ds, variant_name, conditional=False):\n",
    "    \"\"\"Visualize VAE reconstructions and generated samples.\n",
    "\n",
    "    Args:\n",
    "        model: Trained VAE model\n",
    "        test_ds: Test dataset\n",
    "        variant_name: Name of the variant (for plot title)\n",
    "        conditional: Whether this is a conditional VAE\n",
    "\n",
    "    Returns:\n",
    "        matplotlib figure\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode (disables dropout, uses running stats for batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    # Get test batch from Grain DataLoader\n",
    "    test_batch = next(iter(test_ds))\n",
    "    test_images = test_batch[\"image\"][:10]  # Already numpy arrays from Grain\n",
    "\n",
    "    # Reconstructions (model uses internal RNGs in eval mode)\n",
    "    if conditional:\n",
    "        test_labels = test_batch[\"label\"][:10]  # Already numpy arrays from Grain\n",
    "        outputs = model(test_images, y=test_labels)\n",
    "    else:\n",
    "        outputs = model(test_images)\n",
    "\n",
    "    reconstructions = outputs.get(\"reconstructed\", outputs.get(\"reconstruction\"))\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(15, 3.5))\n",
    "    for i in range(10):\n",
    "        # Original\n",
    "        axes[0, i].imshow(test_images[i, :, :, 0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[0, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel(\"Original\", fontsize=10)\n",
    "\n",
    "        # Reconstructed\n",
    "        recon_img = reconstructions[i]\n",
    "        if recon_img.shape != test_images[i].shape:\n",
    "            recon_img = recon_img.reshape(test_images[i].shape)\n",
    "        axes[1, i].imshow(recon_img[:, :, 0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[1, i].axis(\"off\")\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel(\"Reconstructed\", fontsize=10)\n",
    "\n",
    "    plt.suptitle(f\"{variant_name} Results\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    filename = variant_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\") + \"_results.png\"\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"✓ Saved visualization to {output_path}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_training_curves(history, variant_name):\n",
    "    \"\"\"Plot training curves for a VAE variant.\n",
    "\n",
    "    Args:\n",
    "        history: Training history dictionary\n",
    "        variant_name: Name of the variant (for plot title)\n",
    "\n",
    "    Returns:\n",
    "        matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Total loss\n",
    "    axes[0].plot(history[\"loss\"], label=\"Total Loss\", linewidth=2, color=\"tab:blue\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Total Loss\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Component losses\n",
    "    axes[1].plot(history[\"recon_loss\"], label=\"Reconstruction\", linewidth=2, color=\"tab:orange\")\n",
    "    if \"kl_loss\" in history:\n",
    "        axes[1].plot(history[\"kl_loss\"], label=\"KL Divergence\", linewidth=2, color=\"tab:green\")\n",
    "    if \"vq_loss\" in history:\n",
    "        axes[1].plot(history[\"vq_loss\"], label=\"VQ Loss\", linewidth=2, color=\"tab:red\")\n",
    "    if \"capacity_loss\" in history:\n",
    "        axes[1].plot(\n",
    "            history[\"capacity_loss\"], label=\"Capacity Loss\", linewidth=2, color=\"tab:purple\"\n",
    "        )\n",
    "\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Loss Component\")\n",
    "    axes[1].set_title(\"Loss Components\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f\"{variant_name} Training Curves\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    filename = variant_name.lower().replace(\" \", \"_\").replace(\"-\", \"_\") + \"_training.png\"\n",
    "    output_path = OUTPUT_DIR / filename\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"✓ Saved training curves to {output_path}\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Main Execution\n",
    "\n",
    "Train and evaluate all four advanced VAE variants using Artifex's\n",
    "production-ready implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Main Execution\n",
    "def main():\n",
    "    \"\"\"Main execution: demonstrate all advanced VAE variants.\"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADVANCED VAE EXAMPLES - Artifex's Advanced Features on MNIST\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Load data\n",
    "    train_ds, test_ds = load_real_mnist(batch_size=128)\n",
    "\n",
    "    # ========== β-VAE ==========\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"1. β-VAE: Disentangled Representations with β Annealing\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    beta_vae = create_beta_vae(beta=4.0, warmup_steps=1000)\n",
    "    beta_history = train_beta_vae(beta_vae, train_ds, num_epochs=5)\n",
    "    plot_training_curves(beta_history, \"β-VAE\")\n",
    "    visualize_vae_results(beta_vae, test_ds, \"β-VAE\")\n",
    "\n",
    "    # ========== Capacity β-VAE ==========\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"2. β-VAE with Capacity Control: Burgess et al. Method\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    capacity_vae = create_capacity_beta_vae(capacity_max=25.0, capacity_steps=5000)\n",
    "    capacity_history = train_capacity_beta_vae(capacity_vae, train_ds, num_epochs=5)\n",
    "    plot_training_curves(capacity_history, \"Capacity β-VAE\")\n",
    "    visualize_vae_results(capacity_vae, test_ds, \"Capacity β-VAE\")\n",
    "\n",
    "    # ========== Conditional VAE ==========\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"3. Conditional VAE: Label-Conditioned Generation\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    cvae = create_conditional_vae()\n",
    "    cvae_history = train_conditional_vae(cvae, train_ds, num_epochs=5)\n",
    "    plot_training_curves(cvae_history, \"Conditional VAE\")\n",
    "    visualize_vae_results(cvae, test_ds, \"Conditional VAE\", conditional=True)\n",
    "\n",
    "    # ========== VQ-VAE ==========\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"4. VQ-VAE: Discrete Latent Codes with Codebook Monitoring\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    vqvae = create_vqvae(num_embeddings=512, embedding_dim=64)\n",
    "    vqvae_history = train_vqvae(vqvae, train_ds, num_epochs=5)\n",
    "    plot_training_curves(vqvae_history, \"VQ-VAE\")\n",
    "    visualize_vae_results(vqvae, test_ds, \"VQ-VAE\")\n",
    "\n",
    "    # Summary\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"✓ Trained 4 advanced VAE variants using Artifex's implementations\")\n",
    "    print(f\"✓ Generated visualizations in {OUTPUT_DIR}\")\n",
    "    print(\"✓ All models trained on real MNIST data (60,000 images)\")\n",
    "    print()\n",
    "    print(\"Variants demonstrated:\")\n",
    "    print(\"  1. β-VAE with β annealing (β=1.0→4.0)\")\n",
    "    print(\"  2. β-VAE with capacity control (C_max=25 nats)\")\n",
    "    print(\"  3. Conditional VAE (10 classes)\")\n",
    "    print(\"  4. VQ-VAE (codebook_size=512)\")\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
