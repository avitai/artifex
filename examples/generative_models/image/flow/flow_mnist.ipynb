{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "# Training a Flow Model on MNIST with RealNVP\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to train a RealNVP normalizing flow model using\n",
    "Artifex's configuration-based API. Instead of implementing flow transformations\n",
    "from scratch, we use Artifex's `RealNVP` class with `RealNVPConfig` and\n",
    "`CouplingNetworkConfig` for clean, production-ready training.\n",
    "\n",
    "**Key Artifex Components Used:**\n",
    "- `RealNVP` - RealNVP flow model implementation\n",
    "- `RealNVPConfig` - Frozen dataclass configuration for RealNVP\n",
    "- `CouplingNetworkConfig` - Coupling network configuration\n",
    "- `DataRax` - Efficient GPU-accelerated data loading\n",
    "\n",
    "## Training Best Practices Applied\n",
    "\n",
    "Based on RealNVP and flow model research:\n",
    "- Dequantization for discrete data (add uniform noise)\n",
    "- 12 coupling layers with 4-layer MLPs (512 units)\n",
    "- Learning rate warmup + cosine decay\n",
    "- Gradient clipping for stability\n",
    "- Maximum likelihood training (negative log-likelihood)\n",
    "- JIT compilation for GPU performance\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- **Training time:** ~50 minutes (GPU), ~4-5 hours (CPU)\n",
    "- **Final NLL:** ~-2500 to -2700 (more negative is better)\n",
    "- **Generated samples:** Clear, recognizable digits\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install Artifex\n",
    "uv sync\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set memory env vars BEFORE importing TensorFlow or JAX\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress TF warnings\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  # Don't pre-allocate GPU memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"  # JAX: don't pre-allocate\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"  # JAX: use 90% of GPU memory\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from datarax import from_source\n",
    "from datarax.sources import TfdsDataSourceConfig, TFDSSource\n",
    "from flax import nnx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Artifex imports\n",
    "from artifex.generative_models.core.configuration.flow_config import (\n",
    "    CouplingNetworkConfig,\n",
    "    RealNVPConfig,\n",
    ")\n",
    "from artifex.generative_models.models.flow.real_nvp import RealNVP\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Artifex RealNVP Training - MNIST\")\n",
    "print(\"Using: RealNVP, RealNVPConfig, CouplingNetworkConfig, DataRax, nnx.jit\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Training configuration based on RealNVP best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (based on RealNVP best practices)\n",
    "SEED = 42\n",
    "NUM_EPOCHS = 100  # 100 epochs for good quality\n",
    "BATCH_SIZE = 512  # Larger batch size for better GPU utilization\n",
    "BASE_LR = 1e-3  # Scale LR with batch size\n",
    "WARMUP_STEPS = 200  # Warmup steps\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {BASE_LR} (with {WARMUP_STEPS} warmup steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Step 2: Data Loading with DataRax\n",
    "\n",
    "Flow models require continuous data. MNIST is discrete (0-255), so we apply\n",
    "dequantization. We use DataRax for efficient GPU-accelerated data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š Loading MNIST data with DataRax...\")\n",
    "\n",
    "# Initialize data RNGs\n",
    "data_key = jax.random.key(SEED + 1)\n",
    "data_rngs = nnx.Rngs(default=data_key)\n",
    "\n",
    "# Configure MNIST data source using DataRax\n",
    "train_source_config = TfdsDataSourceConfig(\n",
    "    name=\"mnist\",\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    shuffle_buffer_size=10000,\n",
    ")\n",
    "train_source = TFDSSource(train_source_config, rngs=data_rngs)\n",
    "\n",
    "print(f\"  âœ… MNIST train dataset loaded: {len(train_source)} samples\")\n",
    "\n",
    "# Create training pipeline with batching and JIT compilation\n",
    "train_pipeline = from_source(train_source, batch_size=BATCH_SIZE, jit_compile=True)\n",
    "\n",
    "# Calculate number of batches per epoch\n",
    "n_batches = len(train_source) // BATCH_SIZE\n",
    "print(f\"  âœ… Training pipeline created: {n_batches} batches per epoch\")\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, key):\n",
    "    \"\"\"Preprocess MNIST batch for flow models with dequantization.\n",
    "\n",
    "    Flow models require continuous data, so we:\n",
    "    1. Normalize to [0, 1]\n",
    "    2. Flatten to 784-dim vectors\n",
    "    3. Apply dequantization (add uniform noise)\n",
    "    4. Scale to [-1, 1]\n",
    "    \"\"\"\n",
    "    # Extract images from batch\n",
    "    images = batch[\"image\"]\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    images = images.astype(jnp.float32) / 255.0\n",
    "\n",
    "    # Flatten to (batch_size, 784)\n",
    "    images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "    # Dequantization: add uniform noise for continuous data\n",
    "    noise = jax.random.uniform(key, images.shape) / 256.0\n",
    "    images = images + noise\n",
    "\n",
    "    # Scale to [-1, 1]\n",
    "    images = (images - 0.5) / 0.5\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 3: Create RealNVP Using Artifex's API\n",
    "\n",
    "Use Artifex's `RealNVP` class with configuration objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNGs\n",
    "key = jax.random.key(SEED)\n",
    "params_key, noise_key, sample_key, dropout_key = jax.random.split(key, 4)\n",
    "rngs = nnx.Rngs(\n",
    "    params=params_key,\n",
    "    noise=noise_key,\n",
    "    sample=sample_key,\n",
    "    dropout=dropout_key,\n",
    ")\n",
    "\n",
    "# Coupling network config (4 hidden layers with 512 units each for better capacity)\n",
    "coupling_config = CouplingNetworkConfig(\n",
    "    name=\"coupling_mlp\",\n",
    "    hidden_dims=(512, 512, 512, 512),  # 4 hidden layers with more capacity\n",
    "    activation=\"relu\",\n",
    "    network_type=\"mlp\",\n",
    "    scale_activation=\"tanh\",\n",
    ")\n",
    "\n",
    "# RealNVP config (12 coupling layers for better expressiveness)\n",
    "flow_config = RealNVPConfig(\n",
    "    name=\"realnvp_mnist\",\n",
    "    coupling_network=coupling_config,\n",
    "    input_dim=784,  # 28*28\n",
    "    base_distribution=\"normal\",\n",
    "    num_coupling_layers=12,  # More layers for better expressiveness\n",
    "    mask_type=\"checkerboard\",\n",
    ")\n",
    "\n",
    "model = RealNVP(flow_config, rngs=rngs)\n",
    "print(\"\\nâœ… RealNVP created:\")\n",
    "print(f\"   Coupling layers: {flow_config.num_coupling_layers}\")\n",
    "print(f\"   Hidden dims: {coupling_config.hidden_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 4: Create Optimizer with LR Schedule\n",
    "\n",
    "Use learning rate warmup with cosine decay for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total training steps for learning rate schedule\n",
    "total_steps = NUM_EPOCHS * n_batches\n",
    "print(f\"   Total training steps: {total_steps}\")\n",
    "\n",
    "# Learning rate schedule: warmup + cosine decay\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=BASE_LR,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    decay_steps=total_steps,\n",
    "    end_value=BASE_LR * 0.01,\n",
    ")\n",
    "\n",
    "# Optimizer with gradient clipping and LR schedule\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.chain(optax.clip_by_global_norm(1.0), optax.adam(lr_schedule)),\n",
    "    wrt=nnx.Param,\n",
    ")\n",
    "print(f\"   Optimizer: Adam with warmup ({WARMUP_STEPS} steps) + cosine decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 5: Training Step\n",
    "\n",
    "Define the training step for maximum likelihood training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, batch):\n",
    "    \"\"\"Training step for RealNVP (maximum likelihood).\"\"\"\n",
    "\n",
    "    def loss_fn(model):\n",
    "        outputs = model(batch, training=True)\n",
    "        log_prob = outputs[\"log_prob\"]\n",
    "        loss = -jnp.mean(log_prob)  # Negative log-likelihood\n",
    "        return loss, {\"nll\": loss, \"log_prob\": jnp.mean(log_prob)}\n",
    "\n",
    "    (loss, metrics), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
    "    optimizer.update(model, grads)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# JIT-compile training step for performance\n",
    "jit_train_step = nnx.jit(train_step)\n",
    "print(\"   Training step JIT-compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 6: Training Loop\n",
    "\n",
    "Train the model for multiple epochs using DataRax pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"step\": [], \"loss\": [], \"log_prob\": [], \"epoch\": [], \"lr\": []}\n",
    "train_key = jax.random.key(999)\n",
    "global_step = 0\n",
    "\n",
    "print(f\"\\nTraining for {NUM_EPOCHS} epochs...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    epoch_log_probs = []\n",
    "\n",
    "    pbar = tqdm(train_pipeline, total=n_batches, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        train_key, dequant_key = jax.random.split(train_key)\n",
    "\n",
    "        # Preprocess batch with dequantization (fresh noise each batch)\n",
    "        batch_processed = preprocess_batch(batch, dequant_key)\n",
    "\n",
    "        # JIT-compiled training step\n",
    "        metrics = jit_train_step(model, optimizer, batch_processed)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = float(lr_schedule(global_step))\n",
    "\n",
    "        epoch_losses.append(float(metrics[\"nll\"]))\n",
    "        epoch_log_probs.append(float(metrics[\"log_prob\"]))\n",
    "        history[\"step\"].append(global_step)\n",
    "        history[\"loss\"].append(float(metrics[\"nll\"]))\n",
    "        history[\"log_prob\"].append(float(metrics[\"log_prob\"]))\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"lr\"].append(current_lr)\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        pbar.set_postfix({\"NLL\": f\"{metrics['nll']:.2f}\", \"lr\": f\"{current_lr:.2e}\"})\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    avg_log_prob = np.mean(epoch_log_probs)\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}: NLL={avg_loss:.2f}, Log-prob={avg_log_prob:.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 7: Generate Samples\n",
    "\n",
    "Generate new digits from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating samples...\")\n",
    "n_samples = 16\n",
    "\n",
    "generated_samples = model.generate(n_samples=n_samples)\n",
    "\n",
    "# Denormalize from [-1, 1] to [0, 1]\n",
    "generated_samples = (generated_samples * 0.5) + 0.5\n",
    "generated_samples = jnp.clip(generated_samples, 0, 1)\n",
    "\n",
    "# Reshape to images\n",
    "generated_images = generated_samples.reshape(n_samples, 28, 28)\n",
    "\n",
    "print(f\"âœ… Generated {n_samples} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_marker": "r\"\"\""
   },
   "source": [
    "## Step 8: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"examples_output\", exist_ok=True)\n",
    "\n",
    "# Samples\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(generated_images):\n",
    "        ax.imshow(np.array(generated_images[i]), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"RealNVP Generated MNIST Digits\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"examples_output/flow_samples.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"\\nSaved: examples_output/flow_samples.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Training curves with smoothing and LR plot\n",
    "def smooth(values, window=100):\n",
    "    \"\"\"Apply moving average smoothing to a sequence of values.\"\"\"\n",
    "    if len(values) < window:\n",
    "        return values\n",
    "    return np.convolve(values, np.ones(window) / window, mode=\"valid\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "steps = np.array(history[\"step\"])\n",
    "loss = np.array(history[\"loss\"])\n",
    "log_prob = np.array(history[\"log_prob\"])\n",
    "lr = np.array(history[\"lr\"])\n",
    "\n",
    "window = 100\n",
    "\n",
    "# NLL Loss\n",
    "axes[0].plot(steps, loss, alpha=0.15, color=\"tab:blue\", linewidth=0.5)\n",
    "if len(loss) > window:\n",
    "    smoothed_loss = smooth(loss, window)\n",
    "    axes[0].plot(\n",
    "        steps[window - 1 :],\n",
    "        smoothed_loss,\n",
    "        linewidth=2,\n",
    "        label=\"NLL (smoothed)\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Negative Log-Likelihood\")\n",
    "axes[0].set_title(f\"Training Loss ({NUM_EPOCHS} epochs)\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log Probability\n",
    "axes[1].plot(steps, log_prob, alpha=0.15, color=\"tab:green\", linewidth=0.5)\n",
    "if len(log_prob) > window:\n",
    "    smoothed_lp = smooth(log_prob, window)\n",
    "    axes[1].plot(\n",
    "        steps[window - 1 :],\n",
    "        smoothed_lp,\n",
    "        linewidth=2,\n",
    "        label=\"Log-prob (smoothed)\",\n",
    "        color=\"tab:green\",\n",
    "    )\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Log Probability\")\n",
    "axes[1].set_title(\"Average Log Probability\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "axes[2].plot(steps, lr, color=\"tab:orange\", linewidth=1.5)\n",
    "axes[2].set_xlabel(\"Step\")\n",
    "axes[2].set_ylabel(\"Learning Rate\")\n",
    "axes[2].set_title(\"Learning Rate Schedule\", fontsize=14, fontweight=\"bold\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"examples_output/flow_training_curve.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Saved: examples_output/flow_training_curve.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nâœ… Done!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
