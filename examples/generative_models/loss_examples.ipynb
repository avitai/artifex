{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Examples of loss functions for generative models.\n",
    "\n",
    "This module demonstrates various loss functions used in generative models,\n",
    "including VAE losses, GAN losses, and custom loss compositions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the improved loss functions\n",
    "from artifex.generative_models.core.losses import (\n",
    "    chamfer_distance,\n",
    "    # Composable framework\n",
    "    CompositeLoss,\n",
    "    # Convenience functions\n",
    "    create_gan_loss_suite,\n",
    "    mae_loss,\n",
    "    MeshLoss,\n",
    "    # Individual losses\n",
    "    mse_loss,\n",
    "    # Specialized losses\n",
    "    PerceptualLoss,\n",
    "    ScheduledLoss,\n",
    "    total_variation_loss,\n",
    "    WeightedLoss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple Functional Usage\n",
    "def example_functional_usage():\n",
    "    \"\"\"Demonstrate simple functional usage of loss functions.\"\"\"\n",
    "\n",
    "    # Generate some dummy data\n",
    "    key = jax.random.key(42)\n",
    "    key1, key2 = jax.random.split(key)\n",
    "\n",
    "    predictions = jax.random.normal(key1, (32, 64, 64, 3))\n",
    "    targets = jax.random.normal(key2, (32, 64, 64, 3))\n",
    "\n",
    "    # Simple MSE loss\n",
    "    content_loss = mse_loss(predictions, targets)\n",
    "    print(f\"Content loss: {content_loss}\")\n",
    "\n",
    "    # MAE loss with custom reduction\n",
    "    mae_content_loss = mae_loss(predictions, targets, reduction=\"sum\")\n",
    "    print(f\"MAE content loss (sum): {mae_content_loss}\")\n",
    "\n",
    "    return content_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Composable Loss with NNX Modules\n",
    "def example_composable_loss():\n",
    "    \"\"\"Demonstrate composable loss using NNX modules.\"\"\"\n",
    "\n",
    "    # Generate dummy data\n",
    "    key = jax.random.key(42)\n",
    "    key1, key2 = jax.random.split(key)\n",
    "\n",
    "    predictions = jax.random.normal(key1, (32, 64, 64, 3))\n",
    "    targets = jax.random.normal(key2, (32, 64, 64, 3))\n",
    "\n",
    "    # Create individual loss components\n",
    "    content_loss = WeightedLoss(mse_loss, weight=1.0, name=\"content\")\n",
    "    style_loss = WeightedLoss(mae_loss, weight=0.1, name=\"style\")\n",
    "\n",
    "    # Combine them\n",
    "    composite_loss = CompositeLoss([content_loss, style_loss], return_components=True)\n",
    "\n",
    "    # Compute loss\n",
    "    total_loss, loss_dict = composite_loss(predictions, targets)\n",
    "\n",
    "    print(f\"Total loss: {total_loss}\")\n",
    "    print(f\"Loss components: {loss_dict}\")\n",
    "\n",
    "    return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: VAE Training with Composable Losses\n",
    "class SimpleVAE(nnx.Module):\n",
    "    \"\"\"Simple VAE implementation for demonstration.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=784, latent_dim=20, hidden_dim=500, *, rngs: nnx.Rngs):\n",
    "        \"\"\"Initialize the VAE model.\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimension of input data\n",
    "            latent_dim: Dimension of latent space\n",
    "            hidden_dim: Dimension of hidden layers\n",
    "            rngs: Random number generator keys\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nnx.Sequential(\n",
    "            nnx.Linear(in_features=input_dim, out_features=hidden_dim, rngs=rngs),\n",
    "            nnx.relu,\n",
    "            nnx.Linear(in_features=hidden_dim, out_features=latent_dim * 2, rngs=rngs),\n",
    "        )\n",
    "        self.decoder = nnx.Sequential(\n",
    "            nnx.Linear(in_features=latent_dim, out_features=hidden_dim, rngs=rngs),\n",
    "            nnx.relu,\n",
    "            nnx.Linear(in_features=hidden_dim, out_features=input_dim, rngs=rngs),\n",
    "            nnx.sigmoid,\n",
    "        )\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent distribution parameters.\n",
    "\n",
    "        Args:\n",
    "            x: Input data\n",
    "\n",
    "        Returns:\n",
    "            Mean and log variance of the latent distribution\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mean, logvar = jnp.split(h, 2, axis=-1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar, *, rngs: nnx.Rngs):\n",
    "        \"\"\"Sample from the latent distribution using reparameterization.\n",
    "\n",
    "        Args:\n",
    "            mean: Mean of the latent distribution\n",
    "            logvar: Log variance of the latent distribution\n",
    "            rngs: Random number generator keys\n",
    "\n",
    "        Returns:\n",
    "            Sampled latent vector\n",
    "        \"\"\"\n",
    "        std = jnp.exp(0.5 * logvar)\n",
    "        eps = jax.random.normal(rngs.sample(), mean.shape)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vector to reconstruction.\n",
    "\n",
    "        Args:\n",
    "            z: Latent vector\n",
    "\n",
    "        Returns:\n",
    "            Reconstructed data\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def __call__(self, x, *, rngs: nnx.Rngs):\n",
    "        \"\"\"Forward pass through the VAE.\n",
    "\n",
    "        Args:\n",
    "            x: Input data\n",
    "            rngs: Random number generator keys\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with reconstruction, mean, and logvar\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar, rngs=rngs)\n",
    "        recon = self.decode(z)\n",
    "        return {\"reconstruction\": recon, \"mean\": mean, \"logvar\": logvar}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_vae_training():\n",
    "    \"\"\"Demonstrate VAE training with composable losses.\"\"\"\n",
    "\n",
    "    # Create model and optimizer\n",
    "    model = SimpleVAE(latent_dim=64, rngs=nnx.Rngs(42))\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.OfType(nnx.Param))\n",
    "\n",
    "    # Training step\n",
    "    @nnx.jit\n",
    "    def train_step(model, optimizer, x, key):\n",
    "        def loss_fn(model):\n",
    "            rngs = nnx.Rngs(sample=key)\n",
    "            outputs = model(x, rngs=rngs)\n",
    "            # Debug: check what the model returns\n",
    "            if isinstance(outputs, dict):\n",
    "                recon = outputs.get(\"reconstruction\", outputs.get(\"recon\"))\n",
    "                mu = outputs.get(\"mean\", outputs.get(\"mu\"))\n",
    "                logvar = outputs.get(\"logvar\", outputs.get(\"log_var\"))\n",
    "            else:\n",
    "                recon, mu, logvar = outputs\n",
    "\n",
    "            # Reconstruction loss\n",
    "            recon_loss = mse_loss(predictions=recon, targets=x)\n",
    "\n",
    "            # KL divergence loss (standard normal prior)\n",
    "            kl_loss = -0.5 * jnp.sum(1 + logvar - mu**2 - jnp.exp(logvar))\n",
    "            kl_loss = kl_loss / x.shape[0]  # Normalize by batch size\n",
    "\n",
    "            # Note: For this example, we compute losses manually\n",
    "            # In practice, you'd structure this to work with the composite loss\n",
    "            total_loss = recon_loss + 0.1 * kl_loss\n",
    "            return total_loss\n",
    "\n",
    "        loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss\n",
    "\n",
    "    # Generate dummy data\n",
    "    key = jax.random.key(42)\n",
    "    key, data_key, train_key = jax.random.split(key, 3)\n",
    "\n",
    "    x = jax.random.normal(data_key, (32, 784))\n",
    "\n",
    "    # Training step\n",
    "    loss = train_step(model, optimizer, x, train_key)\n",
    "    print(f\"VAE training loss: {loss}\")\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: GAN Training with Composable Losses\n",
    "def example_gan_training():\n",
    "    \"\"\"Demonstrate GAN training setup with composable losses.\"\"\"\n",
    "\n",
    "    # Create GAN loss suite\n",
    "    generator_loss, discriminator_loss = create_gan_loss_suite(\n",
    "        generator_loss_type=\"lsgan\", discriminator_loss_type=\"lsgan\"\n",
    "    )\n",
    "\n",
    "    print(f\"Generator loss: {generator_loss}\")\n",
    "    print(f\"Discriminator loss: {discriminator_loss}\")\n",
    "\n",
    "    # Dummy data for demonstration\n",
    "    key = jax.random.key(42)\n",
    "    real_scores = jax.random.uniform(key, (32,)) * 0.1 + 0.9  # Near 1.0\n",
    "    fake_scores = jax.random.uniform(key, (32,)) * 0.1 + 0.1  # Near 0.0\n",
    "\n",
    "    # Compute losses\n",
    "    gen_loss = generator_loss(fake_scores)\n",
    "    disc_loss = discriminator_loss(real_scores, fake_scores)\n",
    "\n",
    "    print(f\"Generator loss value: {gen_loss}\")\n",
    "    print(f\"Discriminator loss value: {disc_loss}\")\n",
    "\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Advanced Scheduling and Progressive Training\n",
    "def example_scheduled_loss():\n",
    "    \"\"\"Demonstrate scheduled loss for curriculum learning.\"\"\"\n",
    "\n",
    "    # Create a perceptual loss that ramps up over time\n",
    "    perceptual_loss = PerceptualLoss(content_weight=0.1, style_weight=0.01)\n",
    "\n",
    "    # Schedule function: start at 0, ramp up to full weight over 1000 steps\n",
    "    def schedule_fn(step):\n",
    "        return jnp.minimum(1.0, step / 1000.0)\n",
    "\n",
    "    # Create scheduled loss\n",
    "    scheduled_loss = ScheduledLoss(perceptual_loss, schedule_fn, name=\"scheduled_perceptual\")\n",
    "\n",
    "    # Simulate training steps\n",
    "    dummy_features = {\"conv1\": jnp.ones((2, 32, 32, 64)), \"conv2\": jnp.ones((2, 16, 16, 128))}\n",
    "\n",
    "    for step in [0, 250, 500, 750, 1000]:\n",
    "        # Note: This would need proper feature extraction in practice\n",
    "        loss_value = scheduled_loss(\n",
    "            pred_images=jnp.ones((2, 64, 64, 3)),\n",
    "            target_images=jnp.zeros((2, 64, 64, 3)),\n",
    "            features_pred=dummy_features,\n",
    "            features_target=dummy_features,\n",
    "            step=step,\n",
    "        )\n",
    "        weight = schedule_fn(step)\n",
    "        print(f\"Step {step}: weight={weight:.3f}, loss={loss_value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: 3D Geometric Losses\n",
    "def example_geometric_losses():\n",
    "    \"\"\"Demonstrate 3D geometric loss functions.\"\"\"\n",
    "\n",
    "    # Point cloud loss\n",
    "    key = jax.random.key(42)\n",
    "    key1, key2 = jax.random.split(key)\n",
    "\n",
    "    pred_points = jax.random.normal(key1, (4, 1000, 3))  # 4 batches, 1000 points each\n",
    "    target_points = jax.random.normal(key2, (4, 1000, 3))\n",
    "\n",
    "    # Chamfer distance\n",
    "    chamfer_loss = chamfer_distance(pred_points, target_points)\n",
    "    print(f\"Chamfer distance: {chamfer_loss}\")\n",
    "\n",
    "    # Mesh loss\n",
    "    mesh_loss = MeshLoss(\n",
    "        vertex_weight=1.0, normal_weight=0.1, edge_weight=0.1, laplacian_weight=0.01\n",
    "    )\n",
    "\n",
    "    # Dummy mesh data\n",
    "    vertices = jax.random.normal(key1, (100, 3))\n",
    "    faces = jax.random.randint(key2, (50, 3), 0, 100)\n",
    "    normals = jax.random.normal(key1, (100, 3))\n",
    "\n",
    "    pred_mesh = (vertices, faces, normals)\n",
    "    target_mesh = (vertices + 0.1, faces, normals)  # Slightly perturbed\n",
    "\n",
    "    mesh_loss_value = mesh_loss(pred_mesh, target_mesh)\n",
    "    print(f\"Mesh loss: {mesh_loss_value}\")\n",
    "\n",
    "    return chamfer_loss, mesh_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: Complete Training Loop Template\n",
    "def example_complete_training():\n",
    "    \"\"\"Template for a complete training loop using composable losses.\"\"\"\n",
    "\n",
    "    # Mock model and data\n",
    "    class SimpleModel(nnx.Module):\n",
    "        def __init__(self, rngs: nnx.Rngs):\n",
    "            super().__init__()\n",
    "            self.conv = nnx.Conv(3, 3, kernel_size=(3, 3), rngs=rngs)\n",
    "\n",
    "        def __call__(self, x):\n",
    "            return nnx.sigmoid(self.conv(x))\n",
    "\n",
    "    model = SimpleModel(nnx.Rngs(42))\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.OfType(nnx.Param))\n",
    "\n",
    "    @nnx.jit\n",
    "    def train_step(model, optimizer, images, targets):\n",
    "        def loss_function(model):\n",
    "            predictions = model(images)\n",
    "\n",
    "            # For this example, we'll use a simplified version\n",
    "            # In practice, you'd need proper feature extraction for perceptual loss\n",
    "            total_loss = mse_loss(predictions, targets)\n",
    "            tv_loss = total_variation_loss(predictions)\n",
    "\n",
    "            return total_loss + 0.001 * tv_loss\n",
    "\n",
    "        loss, grads = nnx.value_and_grad(loss_function)(model)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss\n",
    "\n",
    "    # Training loop\n",
    "    key = jax.random.key(42)\n",
    "    for epoch in range(5):\n",
    "        # Generate dummy batch\n",
    "        key, data_key = jax.random.split(key)\n",
    "        images = jax.random.normal(data_key, (8, 64, 64, 3))\n",
    "        targets = jax.random.normal(data_key, (8, 64, 64, 3))\n",
    "\n",
    "        loss = train_step(model, optimizer, images, targets)\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Loss Function API Usage Examples\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\n1. Functional Usage:\")\n",
    "    example_functional_usage()\n",
    "\n",
    "    print(\"\\n2. Composable Loss:\")\n",
    "    example_composable_loss()\n",
    "\n",
    "    print(\"\\n3. VAE Training:\")\n",
    "    example_vae_training()\n",
    "\n",
    "    print(\"\\n4. GAN Training:\")\n",
    "    example_gan_training()\n",
    "\n",
    "    print(\"\\n5. Scheduled Loss:\")\n",
    "    example_scheduled_loss()\n",
    "\n",
    "    print(\"\\n6. Geometric Losses:\")\n",
    "    example_geometric_losses()\n",
    "\n",
    "    print(\"\\n7. Complete Training:\")\n",
    "    example_complete_training()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"All examples completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
