{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Diffusion Transformer (DiT) Demo\n",
    "\n",
    "**Duration:** 20 minutes | **Level:** Advanced | **GPU Required:** Recommended\n",
    "(CPU will work but slower)\n",
    "\n",
    "This example demonstrates the Diffusion Transformer (DiT) architecture, which combines the power\n",
    "of transformers with diffusion models. DiT replaces the U-Net backbone typically used in diffusion\n",
    "models with a Vision Transformer (ViT) architecture.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this example, you will:\n",
    "1. Understand the DiT architecture and how it differs from traditional diffusion models\n",
    "2. Learn to create and test DiT models of different sizes (S, B, L, XL)\n",
    "3. Implement conditional generation with classifier-free guidance\n",
    "4. Generate and visualize samples from DiT models\n",
    "5. Benchmark DiT performance across different configurations\n",
    "6. Understand patch-based processing and positional embeddings\n",
    "\n",
    "## üîç Source Code Dependencies\n",
    "\n",
    "**Validated:** 2025-10-14\n",
    "\n",
    "This example depends on the following Artifex source files:\n",
    "- `src/artifex/generative_models/core/configuration.py` - Configuration system\n",
    "- `src/artifex/generative_models/models/backbones/dit.py` - DiffusionTransformer backbone\n",
    "- `src/artifex/generative_models/models/diffusion/dit.py` - DiTModel implementation\n",
    "\n",
    "**Validation Status:**\n",
    "- ‚úÖ All dependencies validated against `memory-bank/guides/flax-nnx-guide.md`\n",
    "- ‚úÖ No anti-patterns detected (RNG handling, module init, activations)\n",
    "- ‚úÖ All tests passing for dependency files\n",
    "\n",
    "## üìö Background\n",
    "\n",
    "**What is DiT?**\n",
    "\n",
    "DiT (Diffusion Transformer) replaces the U-Net backbone in diffusion models with a\n",
    "Vision Transformer:\n",
    "- **Traditional Diffusion:** U-Net with convolutional layers and skip connections\n",
    "- **DiT:** Transformer blocks operating on image patches\n",
    "\n",
    "**Key Innovations:**\n",
    "1. **Patch-based processing:** Images divided into patches (like ViT)\n",
    "2. **Self-attention:** Captures long-range dependencies better than convolutions\n",
    "3. **Scalable:** Easily scale model capacity by adding layers/heads\n",
    "4. **Conditional:** Native support for class conditioning via adaptive layer norm\n",
    "\n",
    "**Why DiT?**\n",
    "- Better quality at large scales\n",
    "- More parameter-efficient than U-Nets\n",
    "- Cleaner architecture without skip connections\n",
    "- Easier to condition on multiple signals\n",
    "\n",
    "## üîë Key Concepts\n",
    "\n",
    "- **Patch Embedding:** Divide image into patches and project to embedding space\n",
    "- **Positional Encoding:** Add position information to patches\n",
    "- **DiT Block:** Transformer block with adaptive layer norm for conditioning\n",
    "- **Classifier-Free Guidance (CFG):** Balance between conditional and unconditional generation\n",
    "- **Model Sizes:** S (Small), B (Base), L (Large), XL (Extra Large)\n",
    "\n",
    "## ‚ÑπÔ∏è Prerequisites\n",
    "\n",
    "- Understanding of transformers and attention mechanisms\n",
    "- Familiarity with diffusion models (see simple_diffusion_example.py)\n",
    "- Knowledge of Vision Transformers (ViT) helpful\n",
    "- Artifex installed (see below)\n",
    "\n",
    "## üì¶ Setup\n",
    "\n",
    "Before running this example, activate the Artifex environment:\n",
    "\n",
    "```bash\n",
    "source activate.sh\n",
    "python examples/generative_models/diffusion/dit_demo.py\n",
    "```\n",
    "\n",
    "## üé¨ Expected Output\n",
    "\n",
    "This example will:\n",
    "- Test DiT components (backbone and model)\n",
    "- Create models of different sizes (S, B, L)\n",
    "- Demonstrate conditional generation with CFG\n",
    "- Generate and visualize samples\n",
    "- Benchmark performance metrics\n",
    "\n",
    "## ‚è±Ô∏è Estimated Runtime\n",
    "\n",
    "- **CPU:** ~5 minutes\n",
    "- **GPU:** ~1-2 minutes\n",
    "\n",
    "## üë• Author\n",
    "\n",
    "Artifex Team\n",
    "\n",
    "## üìÖ Last Updated\n",
    "\n",
    "2025-10-14\n",
    "\n",
    "## üìÑ License\n",
    "\n",
    "MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Import Dependencies and Setup\n",
    "\n",
    "We'll use:\n",
    "- **JAX:** For high-performance numerical computing\n",
    "- **Flax NNX:** For transformer modules\n",
    "- **Matplotlib:** For visualization\n",
    "- **Artifex:** For DiT implementations and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "\n",
    "from artifex.generative_models.core.configuration import DiTConfig, NoiseScheduleConfig\n",
    "\n",
    "# Import DiT components directly\n",
    "from artifex.generative_models.models.backbones.dit import DiffusionTransformer\n",
    "from artifex.generative_models.models.diffusion.dit import DiTModel\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DiT (Diffusion Transformer) Implementation Demo\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ JAX version: {jax.__version__}\")\n",
    "print(f\"üñ•Ô∏è  Backend: {jax.default_backend()}\")\n",
    "print(f\"üîß Devices: {jax.device_count()} device(s)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 2. Test DiT Components\n",
    "\n",
    "Let's start by testing the individual components of the DiT architecture:\n",
    "1. **DiffusionTransformer Backbone:** The core transformer that processes image patches\n",
    "2. **DiTModel:** Complete diffusion model wrapping the backbone\n",
    "\n",
    "### Understanding the Architecture\n",
    "\n",
    "**DiffusionTransformer** takes:\n",
    "- `x`: Input image tensor (B, H, W, C)\n",
    "- `t`: Timesteps for diffusion process (B,)\n",
    "- Returns: Predicted noise (same shape as input)\n",
    "\n",
    "The model:\n",
    "1. Divides image into patches\n",
    "2. Projects patches to embedding space\n",
    "3. Adds positional embeddings\n",
    "4. Processes through transformer blocks\n",
    "5. Reconstructs image from patch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Testing DiT Components\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize random number generators\n",
    "rngs = nnx.Rngs(42)\n",
    "\n",
    "# Test 1: Create DiT backbone\n",
    "print(\"\\n1. Testing DiffusionTransformer backbone...\")\n",
    "dit_backbone = DiffusionTransformer(\n",
    "    img_size=32,  # Input image size\n",
    "    patch_size=4,  # Each patch is 4x4 pixels\n",
    "    in_channels=3,  # RGB images\n",
    "    hidden_size=256,  # Embedding dimension\n",
    "    depth=4,  # Number of transformer blocks\n",
    "    num_heads=8,  # Attention heads per block\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "x = jnp.ones((2, 32, 32, 3))  # Batch of 2 images\n",
    "t = jnp.array([100, 500])  # Different timesteps\n",
    "\n",
    "output = dit_backbone(x, t, deterministic=True)\n",
    "\n",
    "assert output.shape == (2, 32, 32, 3), f\"Unexpected output shape: {output.shape}\"\n",
    "print(\"   ‚úì DiffusionTransformer forward pass successful\")\n",
    "print(f\"   Input shape: {x.shape}, Output shape: {output.shape}\")\n",
    "print(f\"   Number of patches: {(32 // 4) ** 2} patches per image\")\n",
    "\n",
    "# Test 2: Create DiT model\n",
    "print(\"\\n2. Testing DiTModel (full diffusion model)...\")\n",
    "\n",
    "# Create noise schedule config (required for DiTConfig)\n",
    "noise_schedule_config = NoiseScheduleConfig(\n",
    "    name=\"dit_test_schedule\",\n",
    "    schedule_type=\"linear\",\n",
    "    num_timesteps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    ")\n",
    "\n",
    "# Create DiTConfig with nested noise_schedule\n",
    "config = DiTConfig(\n",
    "    name=\"dit_test\",\n",
    "    noise_schedule=noise_schedule_config,\n",
    "    input_shape=(3, 32, 32),  # C, H, W format\n",
    "    patch_size=4,\n",
    "    hidden_size=256,\n",
    "    depth=4,\n",
    "    num_heads=8,\n",
    ")\n",
    "\n",
    "model = DiTModel(config, rngs=rngs)\n",
    "output = model(x, t, deterministic=True)\n",
    "\n",
    "assert output.shape == (2, 32, 32, 3), f\"Unexpected output shape: {output.shape}\"\n",
    "print(\"   ‚úì DiTModel forward pass successful\")\n",
    "print(\"   Model processes images through transformer blocks\")\n",
    "\n",
    "print(\"\\n‚úÖ All component tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 3. Test Different DiT Model Sizes\n",
    "\n",
    "DiT comes in different sizes, similar to GPT or BERT models:\n",
    "- **DiT-S (Small):** 384 hidden dim, 12 blocks, 6 heads\n",
    "- **DiT-B (Base):** 768 hidden dim, 12 blocks, 12 heads\n",
    "- **DiT-L (Large):** 1024 hidden dim, 24 blocks, 16 heads\n",
    "- **DiT-XL (Extra Large):** 1152 hidden dim, 28 blocks, 16 heads (not shown here for speed)\n",
    "\n",
    "Larger models have:\n",
    "- More parameters ‚Üí Better quality\n",
    "- Deeper networks ‚Üí Better feature extraction\n",
    "- More attention heads ‚Üí Richer representations\n",
    "- Slower inference ‚Üí Higher computational cost\n",
    "\n",
    "Let's test S, B, and L to see the performance tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Testing DiT Model Sizes\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define model size configurations (from DiT paper)\n",
    "sizes = {\n",
    "    \"S\": {\"hidden_size\": 384, \"depth\": 12, \"num_heads\": 6},\n",
    "    \"B\": {\"hidden_size\": 768, \"depth\": 12, \"num_heads\": 12},\n",
    "    \"L\": {\"hidden_size\": 1024, \"depth\": 24, \"num_heads\": 16},\n",
    "}\n",
    "\n",
    "img_size = 32\n",
    "batch_size = 2\n",
    "\n",
    "# Create shared noise schedule config\n",
    "size_test_schedule = NoiseScheduleConfig(\n",
    "    name=\"size_test_schedule\",\n",
    "    schedule_type=\"linear\",\n",
    "    num_timesteps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    ")\n",
    "\n",
    "for size_name, size_params in sizes.items():\n",
    "    print(f\"\\n{size_name}. Testing DiT-{size_name}...\")\n",
    "\n",
    "    # Create DiTConfig with nested noise_schedule\n",
    "    config = DiTConfig(\n",
    "        name=f\"dit_{size_name.lower()}\",\n",
    "        noise_schedule=size_test_schedule,\n",
    "        input_shape=(3, img_size, img_size),  # C, H, W format\n",
    "        patch_size=4,\n",
    "        hidden_size=size_params[\"hidden_size\"],\n",
    "        depth=size_params[\"depth\"],\n",
    "        num_heads=size_params[\"num_heads\"],\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    rngs = nnx.Rngs(42)\n",
    "    model = DiTModel(config, rngs=rngs)\n",
    "\n",
    "    # Test forward pass with timing\n",
    "    x = jnp.ones((batch_size, img_size, img_size, 3))\n",
    "    t = jnp.array([100, 500])\n",
    "\n",
    "    start = time.time()\n",
    "    output = model(x, t, deterministic=True)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    assert output.shape == (batch_size, img_size, img_size, 3)\n",
    "\n",
    "    print(\n",
    "        f\"   Config: hidden_size={size_params['hidden_size']}, \"\n",
    "        f\"depth={size_params['depth']}, \"\n",
    "        f\"heads={size_params['num_heads']}\"\n",
    "    )\n",
    "    print(f\"   ‚úì Forward pass successful (time: {elapsed:.3f}s)\")\n",
    "    capacity = size_params[\"hidden_size\"] * size_params[\"depth\"] // 1000\n",
    "    print(f\"   Model capacity: ~{capacity}K params (approx)\")\n",
    "\n",
    "print(\"\\n‚úÖ All model sizes tested successfully!\")\n",
    "print(\"\\nüí° Takeaway: Larger models are slower but produce better quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 4. Conditional Generation with Classifier-Free Guidance\n",
    "\n",
    "One of DiT's strengths is native support for conditional generation.\n",
    "\n",
    "### Classifier-Free Guidance (CFG)\n",
    "\n",
    "CFG balances between:\n",
    "- **Unconditional generation:** p(x)\n",
    "- **Conditional generation:** p(x|y) where y is class label\n",
    "\n",
    "**Formula:**\n",
    "$$\\\\epsilon_{\\\\text{pred}} = \\\\epsilon_{\\\\text{uncond}} + w \\\\cdot\n",
    "(\\\\epsilon_{\\\\text{cond}} - \\\\epsilon_{\\\\text{uncond}})$$\n",
    "\n",
    "Where:\n",
    "- $w$ is the guidance scale (typically 1.5-10)\n",
    "- Higher $w$ ‚Üí stronger conditioning (more faithful to class)\n",
    "- Lower $w$ ‚Üí more diversity but less control\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The model runs inference twice:\n",
    "1. With class label ‚Üí conditional prediction\n",
    "2. Without class label (null) ‚Üí unconditional prediction\n",
    "3. Combine predictions with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Testing Conditional Generation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create noise schedule config for conditional model\n",
    "cond_schedule_config = NoiseScheduleConfig(\n",
    "    name=\"conditional_schedule\",\n",
    "    schedule_type=\"linear\",\n",
    "    num_timesteps=10,  # Few steps for demo\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    ")\n",
    "\n",
    "# Create conditional model configuration using DiTConfig\n",
    "config = DiTConfig(\n",
    "    name=\"conditional_dit\",\n",
    "    noise_schedule=cond_schedule_config,\n",
    "    input_shape=(3, 16, 16),  # C, H, W format - smaller for faster demo\n",
    "    patch_size=2,\n",
    "    hidden_size=384,  # DiT-S for speed\n",
    "    depth=12,\n",
    "    num_heads=6,\n",
    "    num_classes=10,  # 10 class labels (like CIFAR-10)\n",
    "    cfg_scale=3.0,  # Guidance scale\n",
    ")\n",
    "\n",
    "rngs = nnx.Rngs(42)\n",
    "model = DiTModel(config, rngs=rngs)\n",
    "\n",
    "print(\"\\n1. Testing conditional forward pass...\")\n",
    "x = jnp.ones((2, 16, 16, 3))\n",
    "t = jnp.array([5, 8])\n",
    "y = jnp.array([2, 7])  # Class labels for conditioning\n",
    "\n",
    "output = model(x, t, y, deterministic=True, cfg_scale=3.0)\n",
    "assert output.shape == (2, 16, 16, 3)\n",
    "print(\"   ‚úì Conditional forward pass successful\")\n",
    "print(f\"   Conditioned on classes: {y}\")\n",
    "print(\"   Guidance scale: 3.0 (moderate conditioning)\")\n",
    "\n",
    "print(\"\\n2. Testing sample generation...\")\n",
    "samples = model.generate(\n",
    "    n_samples=4,\n",
    "    rngs=rngs,\n",
    "    num_steps=10,\n",
    "    y=jnp.array([0, 1, 2, 3]),  # Generate one sample per class\n",
    "    cfg_scale=3.0,\n",
    "    img_size=16,\n",
    ")\n",
    "\n",
    "assert samples.shape == (4, 16, 16, 3)\n",
    "print(f\"   ‚úì Generated {samples.shape[0]} conditional samples\")\n",
    "print(\n",
    "    f\"   Sample statistics: min={samples.min():.3f}, \"\n",
    "    f\"max={samples.max():.3f}, mean={samples.mean():.3f}\"\n",
    ")\n",
    "print(\"\\nüí° Each sample corresponds to a different class label\")\n",
    "\n",
    "print(\"\\n‚úÖ Conditional generation test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 5. Visualize Generated Samples\n",
    "\n",
    "Let's visualize the samples we just generated to see what the model produces.\n",
    "\n",
    "Note: Since this is a demo with an untrained model, the samples will look like\n",
    "structured noise rather than real images. With a trained model, you would see\n",
    "actual class-specific images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Visualizing Generated Samples\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert JAX array to numpy for visualization\n",
    "samples_np = np.array(samples)\n",
    "\n",
    "# Normalize to [0, 1] for display\n",
    "samples_vis = (samples_np - samples_np.min()) / (samples_np.max() - samples_np.min() + 1e-8)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, len(samples_vis), figsize=(12, 3))\n",
    "\n",
    "for i, (ax, sample) in enumerate(zip(axes, samples_vis)):\n",
    "    ax.imshow(sample)\n",
    "    ax.set_title(f\"Class {i}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"DiT Generated Samples (Untrained Model - Noise Visualization)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_path = \"dit_samples.png\"\n",
    "plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\n‚úì Visualization saved to {output_path}\")\n",
    "print(\"üí° With a trained model, you would see class-specific images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 6. Performance Benchmark\n",
    "\n",
    "Let's benchmark DiT performance to understand computational costs.\n",
    "\n",
    "This helps you choose the right model size for your application:\n",
    "- Real-time applications ‚Üí Use DiT-S\n",
    "- High-quality generation ‚Üí Use DiT-L or XL\n",
    "- Balanced use case ‚Üí Use DiT-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Performance Benchmark\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test configuration\n",
    "img_size = 64\n",
    "batch_size = 8\n",
    "num_iterations = 10\n",
    "\n",
    "# Create noise schedule config for benchmark\n",
    "benchmark_schedule_config = NoiseScheduleConfig(\n",
    "    name=\"benchmark_schedule\",\n",
    "    schedule_type=\"linear\",\n",
    "    num_timesteps=1000,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    ")\n",
    "\n",
    "# Create DiTConfig with DiT-B parameters\n",
    "config = DiTConfig(\n",
    "    name=\"benchmark_dit\",\n",
    "    noise_schedule=benchmark_schedule_config,\n",
    "    input_shape=(3, img_size, img_size),  # C, H, W format\n",
    "    patch_size=4,\n",
    "    hidden_size=768,  # DiT-B\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    ")\n",
    "\n",
    "rngs = nnx.Rngs(42)\n",
    "model = DiTModel(config, rngs=rngs)\n",
    "\n",
    "# Prepare inputs\n",
    "x = jnp.ones((batch_size, img_size, img_size, 3))\n",
    "t = jnp.array([100] * batch_size)\n",
    "\n",
    "# Warm-up (JIT compilation)\n",
    "print(\"\\nüî• Warming up (JIT compilation)...\")\n",
    "for _ in range(3):\n",
    "    _ = model(x, t, deterministic=True)\n",
    "\n",
    "# Benchmark\n",
    "print(f\"‚è±Ô∏è  Running {num_iterations} iterations...\")\n",
    "start = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    output = model(x, t, deterministic=True)\n",
    "    output.block_until_ready()  # Ensure computation completes\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Calculate metrics\n",
    "time_per_iteration = elapsed / num_iterations\n",
    "throughput = batch_size / time_per_iteration\n",
    "\n",
    "print(\"\\nüìà Results:\")\n",
    "print(\"  Model: DiT-B\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Time per iteration: {time_per_iteration:.3f}s\")\n",
    "print(f\"  Throughput: {throughput:.1f} samples/s\")\n",
    "print(f\"  Total time: {elapsed:.2f}s\")\n",
    "\n",
    "print(\"\\n‚úÖ Benchmark completed!\")\n",
    "print(\"\\nüí° This is for a single denoising step. Full generation requires ~50-1000 steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 7. Summary and Key Takeaways\n",
    "\n",
    "### üéì What You Learned\n",
    "\n",
    "In this demo, you learned:\n",
    "\n",
    "1. **DiT Architecture:** How transformers can replace U-Nets in diffusion models\n",
    "2. **Model Sizes:** Different scales (S, B, L) and their tradeoffs\n",
    "3. **Conditional Generation:** Using classifier-free guidance for control\n",
    "4. **Patch Processing:** How images are divided into patches for transformers\n",
    "5. **Performance:** Computational costs and throughput metrics\n",
    "\n",
    "### üí° Key Concepts Recap\n",
    "\n",
    "- **Patch Embedding:** Images ‚Üí patches ‚Üí embeddings\n",
    "- **DiT Block:** Transformer with adaptive layer norm\n",
    "- **CFG:** Balances conditional and unconditional generation\n",
    "- **Scalability:** Easy to scale by adding layers/heads\n",
    "- **Quality vs Speed:** Larger models = better quality but slower\n",
    "\n",
    "### üî¨ Experiments to Try\n",
    "\n",
    "Now that you understand DiT, try these modifications:\n",
    "\n",
    "1. **Adjust guidance scale:**\n",
    "   ```python\n",
    "   samples = model.generate(\n",
    "       ...,\n",
    "       cfg_scale=7.0,  # Higher for stronger conditioning\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Try different model sizes:**\n",
    "   ```python\n",
    "   # Experiment with XL configuration\n",
    "   config.parameters[\"hidden_size\"] = 1152\n",
    "   config.parameters[\"depth\"] = 28\n",
    "   config.parameters[\"num_heads\"] = 16\n",
    "   ```\n",
    "\n",
    "3. **Vary the patch size:**\n",
    "   ```python\n",
    "   config.parameters[\"patch_size\"] = 8  # Fewer patches, faster\n",
    "   ```\n",
    "\n",
    "4. **Experiment with image sizes:**\n",
    "   ```python\n",
    "   # Test on higher resolution\n",
    "   config.input_dim = (256, 256, 3)\n",
    "   config.parameters[\"img_size\"] = 256\n",
    "   ```\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "To learn more about DiT and advanced diffusion models:\n",
    "\n",
    "- **Training DiT:** See complete training loop with ImageNet\n",
    "- **Custom Conditioning:** Add text or other modalities\n",
    "- **Distillation:** Reduce sampling steps with knowledge distillation\n",
    "- **Latent DiT:** Combine with VAE for latent space generation\n",
    "\n",
    "### üìñ Additional Resources\n",
    "\n",
    "- **Paper:** [Scalable Diffusion Models with Transformers (Peebles & Xie, 2023)](https://arxiv.org/abs/2212.09748)\n",
    "- **Paper:** [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)\n",
    "- **Paper:** [Classifier-Free Diffusion Guidance (Ho & Salimans, 2021)](https://arxiv.org/abs/2207.12598)\n",
    "- **Documentation:** [Artifex DiT Guide](../../../docs/models/dit.md)\n",
    "- **Related Examples:**\n",
    "  - `simple_diffusion_example.py` - Basic diffusion concepts\n",
    "  - `vae/` - VAE for latent space modeling\n",
    "\n",
    "### üêõ Troubleshooting\n",
    "\n",
    "**Problem:** Out of memory errors\n",
    "- **Solution:** Reduce batch_size, img_size, or use smaller model (DiT-S)\n",
    "\n",
    "**Problem:** Slow generation\n",
    "- **Solution:** Use fewer diffusion steps or smaller model\n",
    "\n",
    "**Problem:** Poor sample quality (after training)\n",
    "- **Solution:** Increase model size, train longer, or increase CFG scale\n",
    "\n",
    "**Problem:** Samples don't match conditioning\n",
    "- **Solution:** Increase cfg_scale (try 5.0-10.0)\n",
    "\n",
    "### üí¨ Feedback\n",
    "\n",
    "Found a bug or have suggestions? Please open an issue on GitHub!\n",
    "\n",
    "---\n",
    "\n",
    "**Demo completed successfully! üéâ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ú® DiT (Diffusion Transformer) Demo Complete! ‚ú®\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüí° Key Takeaways:\")\n",
    "print(\"   1. DiT replaces U-Nets with transformers for diffusion models\")\n",
    "print(\"   2. Patch-based processing enables long-range dependencies\")\n",
    "print(\"   3. Classifier-free guidance provides conditional control\")\n",
    "print(\"   4. Model size determines quality vs speed tradeoff\")\n",
    "print(\"   5. DiT scales better than U-Nets for large models\")\n",
    "print(\"\\nüîó Next Steps:\")\n",
    "print(\"   - Train DiT on your dataset\")\n",
    "print(\"   - Experiment with different model sizes\")\n",
    "print(\"   - Try various guidance scales\")\n",
    "print(\"   - Combine with VAE for latent diffusion\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ DiT implementation is working correctly and ready to use!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
