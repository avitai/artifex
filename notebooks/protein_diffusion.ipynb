{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jaxtyping\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "import pandas as pd\n",
    "import py3Dmol\n",
    "from flax import nnx\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NpExampleType = dict[str, np.ndarray]\n",
    "LossValuesType = list[tuple[int, float]]\n",
    "BatchType = dict[str, jax.Array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Choose model type\n",
    "# @markdown Default is a simple Point Cloud model.\n",
    "# @markdown <font color='red'>**The simple EGNN model needs more work.**</font>\n",
    "model_type = \"Point Cloud Protein Diffusion Model\"\n",
    "# @param [\"Point Cloud Protein Diffusion Model\",\"EGNN Protein Diffusion Model\"]\n",
    "if model_type == \"EGNN Protein Diffusion Model\":\n",
    "    # model = EGNNProteinDiffusionModel(config, rngs=rngs)\n",
    "    raise NotImplementedError(\"EGNN model not working yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on data features**\n",
    "\n",
    "*  'atom_positions':\n",
    "\n",
    "    Shape = [batch_size, number of residues or amino acids, number of atom types, coordinates].\n",
    "\n",
    "    There are 37 possible different atom types in proteins. We want to restrict\n",
    "    ourselves to the backbone atoms ['N', 'CA', 'C', 'O'], which are at index 0, 1,\n",
    "    2 and 4 of axis 2 of the 'atom_positions' array. The positions of all other atom types are set to 0. For each protein in the batch, the positions are centered on the center of mass. In cases where the protein chain is shorter than 'max_seq_length', the sequence dimension (= axis 1) is padded with zeros to 'max_seq_length' length. In cases where the protein chain is larger than 'max_seq_length', the protein chain is cut off beyond 'max_seq_length'.\n",
    "\n",
    "* 'atom_mask':\n",
    "\n",
    "    Denotes which atoms have a known position. Non backbone atoms are masked, i.e. set to 0 in 'atom_mask', see above.\n",
    "\n",
    "    Shape = [batch_size, number of residues or amino acids, number of atom types].\n",
    "\n",
    "* 'residue_index':\n",
    "\n",
    "    Shape = [batch_size, number of residues or amino acids].\n",
    "\n",
    "    Is literally just arange(seq_length) and denotes the linearly increasing position of individual residues, so amino acids, along the protein chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration and\n",
    "\n",
    "# @markdown --- Configuration Todos--- \\\\\n",
    "# @markdown - Use a better way to make configs. Potential needs\n",
    "# @markdown   -   Allowing multiple models with model specific configs\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TrainingConfig:\n",
    "    backbone_atom_indices: list[int] = dataclasses.field(\n",
    "        default_factory=lambda: [0, 1, 2, 4]\n",
    "    )  # N, CA, C, O indices in atom_types\n",
    "    max_seq_length: int = 128  # Shorter sequence length for faster training initially\n",
    "    batch_size: int = 128  # A good compromise for T4, Adjust based on GPU memory\n",
    "    eval_batch_size: int = 32  #\n",
    "    num_epochs: int = 100  # Reduced epochs for quick testing\n",
    "    # Gradient accumulation\n",
    "    gradient_accumulation_steps: int = 4  # Number of steps to accumulate gradients over\n",
    "    # Whether to use effective batch size for scheduler calculations\n",
    "    use_effective_batch_size: bool = True\n",
    "    learning_rate: float = 1e-4  # Adjust later, smaller value for debugging loss\n",
    "    warmup_steps: int = (\n",
    "        500  # Number of warmup steps for learning rate, around 10-20% of total steps\n",
    "    )\n",
    "    # TODO: debug this, sometimes crashes in colab if the value os larger than one\n",
    "    num_workers: int = 0  # For DataLoader,\n",
    "    log_freq: int = 20  # Log training loss more frequently, adjust later\n",
    "    eval_freq: int = 200  # Evaluate and sample more frequently for debugging\n",
    "    save_freq: int = 100  # Save model checkpoint frequently\n",
    "    output_dir: str = \"protein_diffusion_jax_output\"\n",
    "    seed: int = 42  # for reproducibility\n",
    "\n",
    "    # Diffusion specific\n",
    "    timesteps: int = 200  # Number of diffusion steps (reduced for speed and debugging)\n",
    "    beta_schedule: str = \"linear\"  # Noise schedule type ('linear' or 'cosine')\n",
    "\n",
    "    # Model specific\n",
    "    model_dim: int = 128  # Embedding dimension in the Transformer\n",
    "    num_layers: int = 8  # Number of Transformer layers\n",
    "    num_heads: int = 8  # Number of attention heads\n",
    "\n",
    "\n",
    "# --- Data Processing and Loading ---\n",
    "# Constants for protein structure representation\n",
    "atom_types = [\n",
    "    \"N\",\n",
    "    \"CA\",\n",
    "    \"C\",\n",
    "    \"CB\",\n",
    "    \"O\",\n",
    "    \"CG\",\n",
    "    \"CG1\",\n",
    "    \"CG2\",\n",
    "    \"OG\",\n",
    "    \"OG1\",\n",
    "    \"SG\",\n",
    "    \"CD\",\n",
    "    \"CD1\",\n",
    "    \"CD2\",\n",
    "    \"ND1\",\n",
    "    \"ND2\",\n",
    "    \"OD1\",\n",
    "    \"OD2\",\n",
    "    \"SD\",\n",
    "    \"CE\",\n",
    "    \"CE1\",\n",
    "    \"CE2\",\n",
    "    \"CE3\",\n",
    "    \"NE\",\n",
    "    \"NE1\",\n",
    "    \"NE2\",\n",
    "    \"OE1\",\n",
    "    \"OE2\",\n",
    "    \"CH2\",\n",
    "    \"NH1\",\n",
    "    \"NH2\",\n",
    "    \"OH\",\n",
    "    \"CZ\",\n",
    "    \"CZ2\",\n",
    "    \"CZ3\",\n",
    "    \"NZ\",\n",
    "    \"OXT\",\n",
    "]\n",
    "\n",
    "\n",
    "restypes = [\n",
    "    \"A\",\n",
    "    \"R\",\n",
    "    \"N\",\n",
    "    \"D\",\n",
    "    \"C\",\n",
    "    \"Q\",\n",
    "    \"E\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"L\",\n",
    "    \"K\",\n",
    "    \"M\",\n",
    "    \"F\",\n",
    "    \"P\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"W\",\n",
    "    \"Y\",\n",
    "    \"V\",\n",
    "]\n",
    "\n",
    "restype_order = {restype: i for i, restype in enumerate(restypes)}\n",
    "restype_num = len(restypes)  # should be 20.\n",
    "\n",
    "restype_1to3 = {\n",
    "    \"A\": \"ALA\",\n",
    "    \"R\": \"ARG\",\n",
    "    \"N\": \"ASN\",\n",
    "    \"D\": \"ASP\",\n",
    "    \"C\": \"CYS\",\n",
    "    \"Q\": \"GLN\",\n",
    "    \"E\": \"GLU\",\n",
    "    \"G\": \"GLY\",\n",
    "    \"H\": \"HIS\",\n",
    "    \"I\": \"ILE\",\n",
    "    \"L\": \"LEU\",\n",
    "    \"K\": \"LYS\",\n",
    "    \"M\": \"MET\",\n",
    "    \"F\": \"PHE\",\n",
    "    \"P\": \"PRO\",\n",
    "    \"S\": \"SER\",\n",
    "    \"T\": \"THR\",\n",
    "    \"W\": \"TRP\",\n",
    "    \"Y\": \"TYR\",\n",
    "    \"V\": \"VAL\",\n",
    "}\n",
    "\n",
    "bb_atom_types = [\"N\", \"CA\", \"C\", \"O\"]  # indices: 0, 1, 2, 4\n",
    "bb_indices = [i for i, atom_type in enumerate(atom_types) if atom_type in bb_atom_types]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class Protein:\n",
    "    \"\"\"Protein structure representation.\"\"\"\n",
    "\n",
    "    atom_positions: np.ndarray  # [num_res, num_atom_type, 3]\n",
    "    aatype: np.ndarray  # [num_res]\n",
    "    atom_mask: np.ndarray  # [num_res, num_atom_type]\n",
    "    residue_index: np.ndarray  # [num_res]\n",
    "    chain_index: np.ndarray  # [num_res]\n",
    "    b_factors: np.ndarray  # [num_res, num_atom_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Utilities\n",
    "\n",
    "\n",
    "def calculate_radius_of_gyration(coords: jax.Array, mask: jax.Array) -> float | jax.Array:\n",
    "    \"\"\"Calculates Radius of Gyration (Rg). coords=[L, 4, 3], mask=[L, 4]\"\"\"\n",
    "    # Use CA atoms for Rg calculation (index 1)\n",
    "    ca_coords = coords[:, 1, :]  # [L, 3]\n",
    "    ca_mask = mask[:, 1]  # [L]\n",
    "\n",
    "    valid_ca_coords = ca_coords[ca_mask > 0.5]  # [N_valid, 3]\n",
    "    if valid_ca_coords.shape[0] < 2:\n",
    "        return 0.0  # Not enough atoms to calculate Rg\n",
    "\n",
    "    center_of_mass = jnp.mean(valid_ca_coords, axis=0)  # [3]\n",
    "    diff_sq = (valid_ca_coords - center_of_mass) ** 2  # [N_valid, 3]\n",
    "    rg_sq = jnp.mean(jnp.sum(diff_sq, axis=1))  # Scalar\n",
    "    return jnp.sqrt(rg_sq)\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps: int) -> jax.Array:\n",
    "    \"\"\"Linear schedule, proposed in original DDPM paper.\"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return jnp.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps: int, s: float = 0.008) -> jax.Array:\n",
    "    \"\"\"Cosine schedule, proposed in Improved DDPM paper.\"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = jnp.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = jnp.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return jnp.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "def get_diffusion_variables(beta_schedule_type: str, timesteps: int) -> dict[str, jax.Array]:\n",
    "    \"\"\"Create diffusion schedule variables.\"\"\"\n",
    "    if beta_schedule_type == \"linear\":\n",
    "        betas = linear_beta_schedule(timesteps)\n",
    "    elif beta_schedule_type == \"cosine\":\n",
    "        betas = cosine_beta_schedule(timesteps)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown beta schedule: {beta_schedule_type}\")\n",
    "\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = jnp.cumprod(alphas)\n",
    "    alphas_cumprod_prev = jnp.pad(alphas_cumprod[:-1], (1, 0), constant_values=1.0)\n",
    "\n",
    "    # Required calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "    sqrt_alphas_cumprod = jnp.sqrt(alphas_cumprod)\n",
    "    sqrt_one_minus_alphas_cumprod = jnp.sqrt(1.0 - alphas_cumprod)\n",
    "    log_one_minus_alphas_cumprod = jnp.log(1.0 - alphas_cumprod)\n",
    "    sqrt_recip_alphas_cumprod = jnp.sqrt(1.0 / alphas_cumprod)\n",
    "    sqrt_recipm1_alphas_cumprod = jnp.sqrt(1.0 / alphas_cumprod - 1)\n",
    "\n",
    "    sqrt_recip_alphas = jnp.sqrt(1.0 / alphas)\n",
    "\n",
    "    # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "    posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "    posterior_variance = jnp.clip(posterior_variance, 1e-20)  # Ensure non-negative variance\n",
    "    posterior_log_variance_clipped = jnp.log(posterior_variance)\n",
    "    posterior_mean_coef1 = betas * jnp.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "    posterior_mean_coef2 = (1.0 - alphas_cumprod_prev) * jnp.sqrt(alphas) / (1.0 - alphas_cumprod)\n",
    "\n",
    "    return {\n",
    "        \"betas\": betas,\n",
    "        \"alphas_cumprod\": alphas_cumprod,\n",
    "        \"sqrt_alphas_cumprod\": sqrt_alphas_cumprod,\n",
    "        \"sqrt_one_minus_alphas_cumprod\": sqrt_one_minus_alphas_cumprod,\n",
    "        \"log_one_minus_alphas_cumprod\": log_one_minus_alphas_cumprod,\n",
    "        \"sqrt_recip_alphas_cumprod\": sqrt_recip_alphas_cumprod,\n",
    "        \"sqrt_recipm1_alphas_cumprod\": sqrt_recipm1_alphas_cumprod,\n",
    "        \"sqrt_recip_alphas\": sqrt_recip_alphas,\n",
    "        \"posterior_variance\": posterior_variance,\n",
    "        \"posterior_log_variance_clipped\": posterior_log_variance_clipped,\n",
    "        \"posterior_mean_coef1\": posterior_mean_coef1,\n",
    "        \"posterior_mean_coef2\": posterior_mean_coef2,\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract backbone coordinates and mask\n",
    "def select_backbone(batch: BatchType, backbone_atom_indices: list[int]):\n",
    "    \"\"\"Selects backbone atoms and updates mask.\"\"\"\n",
    "    # Shape: [B, L, 37, 3] -> [B, L, 4, 3]\n",
    "    batch[\"positions\"] = batch[\"positions\"][:, :, backbone_atom_indices, :]\n",
    "    # Shape: [B, L, 37] -> [B, L, 4]\n",
    "    batch[\"mask\"] = batch[\"mask\"][:, :, backbone_atom_indices]\n",
    "    return batch\n",
    "\n",
    "\n",
    "def denormalize_coords(coords: jax.Array, scale: jax.Array) -> jax.Array:\n",
    "    \"\"\"Denormalizes coordinates by multiplying by scale.\"\"\"\n",
    "    return coords * scale\n",
    "\n",
    "\n",
    "def calculate_stats(loader, backbone_atom_indices: list[int], max_batches: int = 100) -> jax.Array:\n",
    "    \"\"\"Calculates mean and std dev for normalization.\"\"\"\n",
    "    all_coords = []\n",
    "    count = 0\n",
    "    for i, batch in enumerate(tqdm(loader)):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        batch = select_backbone(batch, backbone_atom_indices)\n",
    "        coords = batch[\"atom_positions\"]  # B, L, 4, 3\n",
    "        mask = batch[\"atom_mask\"]  # B, L, 4\n",
    "\n",
    "        # Mask out non-existent atoms\n",
    "        masked_coords = coords[mask > 0.5]  # Select only valid coordinates\n",
    "        if masked_coords.size > 0:\n",
    "            all_coords.append(masked_coords.reshape(-1, 3))\n",
    "        count += coords.shape[0]\n",
    "\n",
    "    if not all_coords:\n",
    "        raise ValueError(\"No valid coordinates found to calculate statistics.\")\n",
    "\n",
    "    all_coords_concat = jnp.concatenate(all_coords, axis=0)\n",
    "    mean = jnp.mean(all_coords_concat, axis=0)  # Should be close to [0,0,0] due to centering\n",
    "    print(f\"Calculated mean: {mean}\")\n",
    "    std = jnp.std(all_coords_concat, axis=0)\n",
    "    # Use a single std dev for all coordinates for simplicity\n",
    "    coord_scale = jnp.sqrt(jnp.mean(std**2))\n",
    "    print(f\"Calculated coordinate scale (std dev): {coord_scale}\")\n",
    "    return coord_scale\n",
    "\n",
    "\n",
    "def calculate_radius_of_gyration_distribution(\n",
    "    batch: BatchType, coord_scale: jax.Array, config: TrainingConfig\n",
    ") -> list[float]:\n",
    "    rg_distribution = []\n",
    "    batch = select_backbone(batch, config.backbone_atom_indices)\n",
    "    coords_batch = batch[\"positions\"]  # Normalized coords here\n",
    "    mask_batch = batch[\"mask\"]\n",
    "\n",
    "    # Denormalize before Rg calculation\n",
    "    coords_batch_denorm = denormalize_coords(coords_batch, coord_scale)\n",
    "\n",
    "    for i in range(coords_batch_denorm.shape[0]):\n",
    "        rg = calculate_radius_of_gyration(coords_batch_denorm[i], mask_batch[i])\n",
    "        if rg > 0:  # Only store valid Rgs\n",
    "            rg_distribution.append(rg)\n",
    "\n",
    "    return rg_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualization\n",
    "\n",
    "\n",
    "def _chain_end(atom_index, end_resname, chain_name, residue_index) -> str:\n",
    "    chain_end = \"TER\"\n",
    "    return f\"{chain_end:<6}{atom_index:>5}      {end_resname:>3} {chain_name:>1}{residue_index:>4}\"\n",
    "\n",
    "\n",
    "def to_pdb(prot: Protein) -> str:\n",
    "    \"\"\"Converts a `Protein` instance to a PDB string.\"\"\"\n",
    "    pdb_atom_types = [\"N\", \"CA\", \"C\", \"O\"]\n",
    "    restypes_list = [\"GLY\"] * len(prot.aatype)  # Use GLY as placeholder\n",
    "    res_1to3 = lambda r: restype_1to3.get(r, \"UNK\")\n",
    "\n",
    "    # Rest of the PDB conversion code closely following the PyTorch implementation\n",
    "    PDB_CHAIN_IDS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    PDB_MAX_CHAINS = len(PDB_CHAIN_IDS)\n",
    "\n",
    "    pdb_lines = []\n",
    "    atom_mask = prot.atom_mask\n",
    "    atom_positions = prot.atom_positions\n",
    "    residue_index = prot.residue_index.astype(np.int32)\n",
    "    chain_index = prot.chain_index.astype(np.int32)\n",
    "    b_factors = prot.b_factors\n",
    "\n",
    "    # Construct a mapping from chain indices to chain ID strings\n",
    "    chain_ids = {}\n",
    "    unique_chains = np.unique(chain_index) if len(chain_index) > 0 else []\n",
    "    for i in unique_chains:\n",
    "        if i >= PDB_MAX_CHAINS:\n",
    "            chain_ids[i] = PDB_CHAIN_IDS[i % PDB_MAX_CHAINS]\n",
    "        else:\n",
    "            chain_ids[i] = PDB_CHAIN_IDS[i]\n",
    "\n",
    "    pdb_lines.append(\"MODEL        1\")\n",
    "    atom_index = 1\n",
    "    last_chain_index = chain_index[0] if len(chain_index) > 0 else 0\n",
    "\n",
    "    num_residues = len(prot.aatype)\n",
    "    if num_residues == 0:\n",
    "        print(\"Warning: No residues to write to PDB.\")\n",
    "        pdb_lines.append(\"ENDMDL\")\n",
    "        pdb_lines.append(\"END\")\n",
    "        return \"\\n\".join(pdb_lines) + \"\\n\"\n",
    "\n",
    "    for i in range(num_residues):\n",
    "        current_chain_idx = chain_index[i]\n",
    "        # Close previous chain if necessary\n",
    "        if i > 0 and last_chain_index != current_chain_idx:\n",
    "            if (i - 1) < len(restypes_list) and (i - 1) < len(residue_index):\n",
    "                pdb_lines.append(\n",
    "                    _chain_end(\n",
    "                        atom_index,\n",
    "                        res_1to3(restypes_list[i - 1]),\n",
    "                        chain_ids.get(last_chain_index, \"A\"),\n",
    "                        residue_index[i - 1],\n",
    "                    )\n",
    "                )\n",
    "                atom_index += 1\n",
    "\n",
    "        last_chain_index = current_chain_idx\n",
    "        res_name_3 = res_1to3(restypes_list[i])\n",
    "\n",
    "        # Iterate through backbone atom types\n",
    "        for atom_idx_bb, (atom_name, pos, mask, b_factor) in enumerate(\n",
    "            zip(pdb_atom_types, atom_positions[i], atom_mask[i], b_factors[i])\n",
    "        ):\n",
    "            if mask < 0.5:\n",
    "                continue\n",
    "\n",
    "            record_type = \"ATOM\"\n",
    "            name = atom_name.ljust(4) if len(atom_name) >= 4 else f\" {atom_name}\".ljust(4)\n",
    "\n",
    "            alt_loc = \"\"\n",
    "            insertion_code = \"\"\n",
    "            occupancy = 1.00\n",
    "            element = atom_name.strip()[0] if atom_name.strip() else \" \"\n",
    "            charge = \"\"\n",
    "\n",
    "            atom_line = (\n",
    "                f\"{record_type:<6}{atom_index:>5} {name:<4}{alt_loc:>1}\"\n",
    "                f\"{res_name_3:>3} {chain_ids.get(current_chain_idx, 'A'):>1}\"\n",
    "                f\"{residue_index[i]:>4}{insertion_code:>1}   \"\n",
    "                f\"{pos[0]:>8.3f}{pos[1]:>8.3f}{pos[2]:>8.3f}\"\n",
    "                f\"{occupancy:>6.2f}{b_factor:>6.2f}          \"\n",
    "                f\"{element:>2}{charge:>2}\"\n",
    "            )\n",
    "            pdb_lines.append(atom_line)\n",
    "            atom_index += 1\n",
    "\n",
    "    # Close the final chain\n",
    "    if num_residues > 0:\n",
    "        if (num_residues - 1) < len(restypes_list) and (num_residues - 1) < len(residue_index):\n",
    "            pdb_lines.append(\n",
    "                _chain_end(\n",
    "                    atom_index,\n",
    "                    res_1to3(restypes_list[-1]),\n",
    "                    chain_ids.get(last_chain_index, \"A\"),\n",
    "                    residue_index[-1],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    pdb_lines.append(\"ENDMDL\")\n",
    "    pdb_lines.append(\"END\")\n",
    "\n",
    "    # Pad all lines to 80 characters\n",
    "    pdb_lines = [line.ljust(80) for line in pdb_lines]\n",
    "    return \"\\n\".join(pdb_lines) + \"\\n\"\n",
    "\n",
    "\n",
    "def visualize_pdb(pdb_string: str, width=600, height=400):\n",
    "    \"\"\"Visualize PDB string using py3Dmol.\"\"\"\n",
    "    if not pdb_string:\n",
    "        print(\"Cannot visualize empty PDB string.\")\n",
    "        return\n",
    "    view = py3Dmol.view(width=width, height=height)\n",
    "    view.addModel(pdb_string, \"pdb\")\n",
    "    view.setStyle({\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "    view.zoomTo()\n",
    "    view.show()\n",
    "\n",
    "\n",
    "def flat_coords_to_protein(\n",
    "    flat_coords: np.ndarray,\n",
    "    flat_mask: np.ndarray,\n",
    "    target_seq_len: int,\n",
    "    num_bb_atoms: int = 4,\n",
    ") -> Protein:\n",
    "    \"\"\"Converts flattened coordinates back to Protein object format for PDB saving.\"\"\"\n",
    "    # Convert JAX arrays to numpy if needed\n",
    "    if hasattr(flat_coords, \"device_buffer\"):\n",
    "        flat_coords = np.array(flat_coords)\n",
    "    if hasattr(flat_mask, \"device_buffer\"):\n",
    "        flat_mask = np.array(flat_mask)\n",
    "\n",
    "    # Calculate expected number of residues\n",
    "    expected_num_res = target_seq_len\n",
    "    expected_total_points = expected_num_res * num_bb_atoms\n",
    "\n",
    "    # Handle potential size mismatch\n",
    "    num_total_points = flat_coords.shape[0]\n",
    "    if num_total_points != expected_total_points:\n",
    "        print(\n",
    "            f\"Warning: flat_coords/flat_mask length ({num_total_points}) doesn't match\"\n",
    "            f\" target_seq_len * num_bb_atoms ({expected_total_points}). Truncating or padding.\"\n",
    "        )\n",
    "\n",
    "        # Truncate if too long\n",
    "        if num_total_points > expected_total_points:\n",
    "            flat_coords = flat_coords[:expected_total_points]\n",
    "            flat_mask = flat_mask[:expected_total_points]\n",
    "        # Pad if too short (less likely scenario)\n",
    "        elif num_total_points < expected_total_points:\n",
    "            pad_len = expected_total_points - num_total_points\n",
    "            flat_coords = np.pad(flat_coords, ((0, pad_len), (0, 0)), mode=\"constant\")\n",
    "            flat_mask = np.pad(flat_mask, ((0, pad_len),), mode=\"constant\")\n",
    "\n",
    "    # Reshape back to [N_res, num_bb_atoms, 3]\n",
    "    atom_positions_bb = flat_coords.reshape(expected_num_res, num_bb_atoms, 3)\n",
    "    atom_mask_bb = flat_mask.reshape(expected_num_res, num_bb_atoms)\n",
    "\n",
    "    # Determine actual length based on CA mask (index 1 within bb atoms)\n",
    "    ca_mask = atom_mask_bb[:, 1]\n",
    "    actual_len = int(np.sum(ca_mask > 0.5))\n",
    "\n",
    "    if actual_len == 0:\n",
    "        print(\n",
    "            \"Warning: Generated mask suggests zero length within target_seq_len.\"\n",
    "            \" Creating PDB with length 1.\"\n",
    "        )\n",
    "        actual_len = 1\n",
    "        atom_positions_bb = atom_positions_bb[:1]\n",
    "        atom_mask_bb = atom_mask_bb[:1]\n",
    "        atom_mask_bb[:, :] = 1.0\n",
    "    else:\n",
    "        # Truncate to the actual determined length\n",
    "        atom_positions_bb = atom_positions_bb[:actual_len]\n",
    "        atom_mask_bb = atom_mask_bb[:actual_len]\n",
    "\n",
    "    # Create dummy data needed for Protein object\n",
    "    aatype = np.zeros(actual_len, dtype=np.int32)  # Dummy aatype\n",
    "    residue_index = np.arange(1, actual_len + 1)  # PDB standard 1-based indexing\n",
    "    chain_index = np.zeros(actual_len, dtype=np.int32)  # Single chain 'A'\n",
    "    b_factors = np.ones((actual_len, num_bb_atoms)) * 50.0  # Dummy B-factors\n",
    "\n",
    "    # Create the Protein object\n",
    "    protein = Protein(\n",
    "        atom_positions=atom_positions_bb,\n",
    "        aatype=aatype,\n",
    "        atom_mask=atom_mask_bb,\n",
    "        residue_index=residue_index,\n",
    "        chain_index=chain_index,\n",
    "        b_factors=b_factors,\n",
    "    )\n",
    "    return protein\n",
    "\n",
    "\n",
    "def plot_loss_curves(\n",
    "    train_losses: LossValuesType,\n",
    "    val_losses: LossValuesType,\n",
    "    plot_filename: str | None = None,\n",
    "    current_step: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots training and validation loss curves.\n",
    "\n",
    "    Args:\n",
    "        train_losses: list of tuples (step, loss).\n",
    "        val_losses: list of tuples (step, loss).\n",
    "        current_step: The current global step, used for title.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    # Plot training loss\n",
    "    if train_losses:\n",
    "        train_steps, train_vals = zip(*train_losses)\n",
    "        if not current_step:\n",
    "            current_step = train_steps[-1]\n",
    "        plt.plot(train_steps, train_vals, label=\"Training Loss\", alpha=0.8)\n",
    "\n",
    "    # Plot validation loss\n",
    "    if val_losses:\n",
    "        val_steps, val_vals = zip(*val_losses)\n",
    "        # Use markers for validation points as they are less frequent\n",
    "        plt.plot(val_steps, val_vals, label=\"Validation Loss\", marker=\"o\", linestyle=\"--\")\n",
    "\n",
    "    plt.xlabel(\"Global Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss Curves up to Step {current_step}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.yscale(\"log\")  # Log scale often helpful for losses\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot if in interactive environment\n",
    "    try:\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error showing plot: {e}\")\n",
    "        pass\n",
    "\n",
    "    if not plot_filename:\n",
    "        try:\n",
    "            plt.savefig(plot_filename)\n",
    "            print(f\"Saved loss plot to {plot_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving loss plot: {e}\")\n",
    "\n",
    "\n",
    "def plot_radius_of_gyration(rg_train_hist: list[float], rg_samples_hist: list[float]):\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(6, 3))\n",
    "\n",
    "    # Rg Histogram plot\n",
    "    if rg_train_hist and rg_samples_hist:\n",
    "        bins = np.linspace(\n",
    "            min(rg_train_hist + rg_samples_hist),\n",
    "            max(rg_train_hist + rg_samples_hist),\n",
    "            30,\n",
    "        )\n",
    "        axs[0].hist(rg_train_hist, bins=bins, alpha=0.6, label=\"Train Set Rg\", density=True)\n",
    "        axs[0].hist(\n",
    "            rg_samples_hist,\n",
    "            bins=bins,\n",
    "            alpha=0.6,\n",
    "            label=\"Generated Samples Rg\",\n",
    "            density=True,\n",
    "        )\n",
    "        axs[0].set_xlabel(\"Radius of Gyration (Angstrom)\")\n",
    "        axs[0].set_ylabel(\"Density\")\n",
    "        axs[0].set_title(\"Rg Distribution\")\n",
    "        axs[0].legend()\n",
    "        axs[0].grid(True)\n",
    "    else:\n",
    "        axs[0].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Rg data not available yet\",\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "        )\n",
    "        axs[0].set_title(\"Rg Distribution\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot to buffer (optional, useful if running non-interactively)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    plt.show()  # Display the plot\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data\n",
    "# @markdown TODOs: \\\\\n",
    "# @markdown - Migrate to [grain](https://github.com/google/grain) for a full JAX/Flax implementation\n",
    "# @markdown -\n",
    "\n",
    "\n",
    "def make_np_example(coords_dict: dict[str, np.ndarray]) -> NpExampleType:\n",
    "    \"\"\"Make a dictionary of non-batched numpy protein features.\"\"\"\n",
    "    # Handle potential missing keys gracefully\n",
    "    if \"N\" not in coords_dict or coords_dict[\"N\"] is None:\n",
    "        # Return None for skipping this item in the dataset\n",
    "        return None\n",
    "\n",
    "    num_res = np.array(coords_dict[\"N\"]).shape[0]\n",
    "    if num_res == 0:\n",
    "        return None\n",
    "\n",
    "    atom_positions = np.zeros([num_res, len(atom_types), 3], dtype=np.float32)\n",
    "\n",
    "    for i, atom_type in enumerate(atom_types):\n",
    "        if atom_type in bb_atom_types:\n",
    "            # Check if the key exists and is not None before converting to array\n",
    "            if atom_type in coords_dict and coords_dict[atom_type] is not None:\n",
    "                try:\n",
    "                    pos_array = np.array(coords_dict[atom_type])\n",
    "                    # Ensure the array has the expected number of residues\n",
    "                    if pos_array.shape[0] == num_res:\n",
    "                        atom_positions[:, i, :] = pos_array\n",
    "                except Exception:\n",
    "                    pass  # Set to zero implicitly\n",
    "\n",
    "    # Mask nan / None coordinates.\n",
    "    nan_pos = np.isnan(atom_positions)[..., 0]\n",
    "    atom_positions[nan_pos] = 0.0\n",
    "    atom_mask = np.zeros([num_res, len(atom_types)], dtype=np.float32)\n",
    "    atom_mask[..., bb_indices] = 1\n",
    "    atom_mask[nan_pos] = 0  # Ensure NaNs are masked\n",
    "\n",
    "    batch = {\n",
    "        \"atom_positions\": atom_positions,\n",
    "        \"atom_mask\": atom_mask,\n",
    "        \"residue_index\": np.arange(num_res, dtype=np.int32),\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def make_fixed_size(np_example: NpExampleType, max_seq_length: int = 500) -> NpExampleType:\n",
    "    \"\"\"Pad features to fixed sequence length.\"\"\"\n",
    "    # Check if np_example is None before proceeding\n",
    "    if np_example is None:\n",
    "        return None\n",
    "\n",
    "    for k, v in np_example.items():\n",
    "        # Check if v is None or empty before accessing shape\n",
    "        if v is None or v.shape[0] == 0:\n",
    "            return None  # Or handle appropriately\n",
    "\n",
    "        pad = max_seq_length - v.shape[0]\n",
    "        if pad > 0:\n",
    "            pad_shape = [(0, pad)] + [(0, 0)] * (len(v.shape) - 1)\n",
    "            v = np.pad(v, pad_shape, mode=\"constant\")\n",
    "        elif pad < 0:\n",
    "            v = v[:max_seq_length]\n",
    "        np_example[k] = v\n",
    "\n",
    "    return np_example\n",
    "\n",
    "\n",
    "def center_positions(np_example: NpExampleType) -> NpExampleType:\n",
    "    \"\"\"Center 'atom_positions' on CA center of mass.\"\"\"\n",
    "    # Check if np_example is None before proceeding\n",
    "    if np_example is None:\n",
    "        return None\n",
    "\n",
    "    atom_positions = np_example[\"atom_positions\"]\n",
    "    atom_mask = np_example[\"atom_mask\"]\n",
    "\n",
    "    # Check if arrays are valid before processing\n",
    "    if (\n",
    "        atom_positions is None\n",
    "        or atom_mask is None\n",
    "        or atom_positions.shape[0] == 0\n",
    "        or atom_mask.shape[0] == 0\n",
    "    ):\n",
    "        return np_example  # Return unmodified example\n",
    "\n",
    "    ca_positions = atom_positions[:, 1, :]  # CA is at index 1\n",
    "    ca_mask = atom_mask[:, 1]\n",
    "\n",
    "    # Ensure ca_mask sum is not zero before division\n",
    "    ca_mask_sum = np.sum(ca_mask)\n",
    "    if ca_mask_sum < 1e-9:\n",
    "        return np_example\n",
    "\n",
    "    ca_center = np.sum(ca_mask[..., None] * ca_positions, axis=0) / ca_mask_sum\n",
    "\n",
    "    # Center only valid atoms\n",
    "    atom_positions = (atom_positions - ca_center[None, None, :]) * atom_mask[..., None]\n",
    "    np_example[\"atom_positions\"] = atom_positions\n",
    "\n",
    "    return np_example\n",
    "\n",
    "\n",
    "class DatasetFromDataframe:\n",
    "    \"\"\"Dataset class for protein coordinates data from a DataFrame.\"\"\"\n",
    "\n",
    "    def __init__(self, data_frame: pd.DataFrame, max_seq_length: int = 512):\n",
    "        self.data = data_frame\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.valid_indices = self._preprocess_data()\n",
    "        self._cache = {}  # Cache for processed examples\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"Pre-filter data to find valid indices.\"\"\"\n",
    "        valid_indices = []\n",
    "        print(\"Preprocessing dataset to check for valid entries...\")\n",
    "        num_skipped = 0\n",
    "\n",
    "        for idx in tqdm(range(len(self.data)), desc=\"Preprocessing\"):\n",
    "            coords_dict = self.data.iloc[idx].coords\n",
    "            # Basic check: ensure 'coords' is a dict and 'N' exists and has > 0 residues\n",
    "            if (\n",
    "                isinstance(coords_dict, dict)\n",
    "                and \"N\" in coords_dict\n",
    "                and coords_dict[\"N\"] is not None\n",
    "                and len(coords_dict[\"N\"]) > 0\n",
    "            ):\n",
    "                # Further check with make_np_example to catch more issues early\n",
    "                temp_example = make_np_example(coords_dict)\n",
    "                if temp_example is not None:\n",
    "                    valid_indices.append(idx)\n",
    "                else:\n",
    "                    num_skipped += 1\n",
    "            else:\n",
    "                num_skipped += 1\n",
    "\n",
    "        print(\n",
    "            f\"Preprocessing complete. Found {len(valid_indices)} valid entries.\"\n",
    "            f\" Skipped {num_skipped} invalid/problematic entries.\"\n",
    "        )\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of valid preprocessed entries\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Check if example is already in cache\n",
    "        if idx in self._cache:\n",
    "            return self._cache[idx]\n",
    "\n",
    "        # Map the requested index to the valid index list\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        coords_dict = self.data.iloc[actual_idx].coords\n",
    "\n",
    "        # Process the item\n",
    "        np_example = make_np_example(coords_dict)\n",
    "        if np_example is None:\n",
    "            raise RuntimeError(f\"make_np_example failed for pre-validated index {actual_idx}\")\n",
    "\n",
    "        np_example = make_fixed_size(np_example, self.max_seq_length)\n",
    "        if np_example is None:\n",
    "            raise RuntimeError(f\"make_fixed_size failed for pre-validated index {actual_idx}\")\n",
    "\n",
    "        np_example = center_positions(np_example)\n",
    "        if np_example is None:\n",
    "            raise RuntimeError(f\"center_positions failed for pre-validated index {actual_idx}\")\n",
    "\n",
    "        # Select only backbone atoms for positions and mask\n",
    "        positions_bb = np_example[\"atom_positions\"][:, bb_indices, :]\n",
    "        mask_bb = np_example[\"atom_mask\"][:, bb_indices]  # Now [N_res, 4]\n",
    "        res_mask_ca = np_example[\"atom_mask\"][:, 1]  # CA mask [N_res]\n",
    "        residue_idx_orig = np_example[\"residue_index\"]\n",
    "\n",
    "        # Reshape for point cloud: [N_res * 4, 3] for positions, [N_res * 4] for mask\n",
    "        num_res = positions_bb.shape[0]\n",
    "        num_bb_atoms = len(bb_indices)  # Should be 4\n",
    "\n",
    "        positions_flat = positions_bb.reshape(num_res * num_bb_atoms, 3)\n",
    "        mask_flat = mask_bb.reshape(num_res * num_bb_atoms)\n",
    "\n",
    "        # Create residue index mapping for the flattened structure\n",
    "        residue_index_flat = np.repeat(residue_idx_orig, num_bb_atoms)\n",
    "\n",
    "        # Create a JAX-compatible example (convert to jnp arrays)\n",
    "        example = {\n",
    "            \"positions\": jnp.array(positions_flat, dtype=jnp.float32),\n",
    "            \"mask\": jnp.array(mask_flat, dtype=jnp.float32),  # Mask for points\n",
    "            \"residue_index\": jnp.array(residue_index_flat, dtype=jnp.int32),\n",
    "            \"res_mask\": jnp.array(res_mask_ca, dtype=jnp.float32),  # Mask for residues\n",
    "        }\n",
    "\n",
    "        # Cache the processed example\n",
    "        self._cache[idx] = example\n",
    "\n",
    "        return example\n",
    "\n",
    "    def get_batch(self, indices: list[int]) -> BatchType:\n",
    "        \"\"\"Get a batch of examples.\"\"\"\n",
    "        examples = [self[i] for i in indices]\n",
    "\n",
    "        # Stack the examples into a batch\n",
    "        batch = {\n",
    "            \"positions\": jnp.stack([ex[\"positions\"] for ex in examples]),\n",
    "            \"mask\": jnp.stack([ex[\"mask\"] for ex in examples]),\n",
    "            \"residue_index\": jnp.stack([ex[\"residue_index\"] for ex in examples]),\n",
    "            \"res_mask\": jnp.stack([ex[\"res_mask\"] for ex in examples]),\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class JAXDataLoader:\n",
    "    \"\"\"JAX equivalent of PyTorch's DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: DatasetFromDataframe,\n",
    "        batch_size: int,\n",
    "        shuffle: bool = False,\n",
    "        drop_last: bool = False,\n",
    "        rng_key=None,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.rng_key = rng_key if rng_key is not None else jrandom.PRNGKey(0)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Get indices for all data points\n",
    "        indices = list(range(len(self.dataset)))\n",
    "\n",
    "        # Shuffle if needed\n",
    "        if self.shuffle:\n",
    "            self.rng_key, subkey = jrandom.split(self.rng_key)\n",
    "            indices = jrandom.permutation(subkey, jnp.array(indices)).tolist()\n",
    "\n",
    "        # Create batches\n",
    "        batch_indices = []\n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            if i + self.batch_size <= len(indices) or not self.drop_last:\n",
    "                batch_idx = indices[i : i + self.batch_size]\n",
    "                batch_indices.append(batch_idx)\n",
    "\n",
    "        # Return an iterator over batches\n",
    "        for batch_idx in batch_indices:\n",
    "            yield self.dataset.get_batch(batch_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.dataset) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.dataset) + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Abstract Protein Diffusion Model\n",
    "class ProteinDiffusionModel(nnx.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Time step embedding\n",
    "\n",
    "\n",
    "# --- Timestep Embedding ---\n",
    "# Source: https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/nn.py\n",
    "# Modified slightly for Flax NNX\n",
    "class TimestepEmbedding(nnx.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal timestep embeddings, adapted from the original DDPM implementation.\n",
    "    Converted from PyTorch to Flax NNX.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_period=10000, *, rngs=None):\n",
    "        # rngs is included for API compatibility but not used for this module\n",
    "        # since we don't have parameters that need initialization\n",
    "        self.dim = dim\n",
    "        self.max_period = max_period\n",
    "\n",
    "    def __call__(self, t) -> jax.Array:\n",
    "        \"\"\"\n",
    "        Create sinusoidal timestep embeddings.\n",
    "\n",
    "        Args:\n",
    "            t: 1D tensor of timesteps with shape [batch]\n",
    "\n",
    "        Returns:\n",
    "            Timestep embeddings with shape [batch, dim]\n",
    "        \"\"\"\n",
    "        half = self.dim // 2\n",
    "        freqs = jnp.exp(-math.log(self.max_period) * jnp.arange(0, half) / half)\n",
    "        args = t[:, None] * freqs[None, :]\n",
    "        embedding = jnp.concatenate([jnp.cos(args), jnp.sin(args)], axis=-1)\n",
    "\n",
    "        # Handle odd dimensions\n",
    "        if self.dim % 2:\n",
    "            embedding = jnp.concatenate([embedding, jnp.zeros_like(embedding[:, :1])], axis=-1)\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# @title Point Cloud Protein Diffusion Model\n",
    "\n",
    "# --- Model Architecture (using Flax NNX) ---\n",
    "\n",
    "\n",
    "class PointCloudTransformerBlock(ProteinDiffusionModel):\n",
    "    \"\"\"Transformer block adapted for point clouds.\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1, *, rngs: nnx.Rngs):\n",
    "        # Multi-head attention with all required parameters\n",
    "        self.attention = nnx.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            in_features=embed_dim,\n",
    "            qkv_features=embed_dim,\n",
    "            out_features=embed_dim,\n",
    "            dropout_rate=dropout,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
    "        self.norm2 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nnx.Dropout(dropout, rngs=rngs)\n",
    "\n",
    "        # FFN layers - 4x expansion in hidden layer as in original transformer\n",
    "        self.ffn_linear1 = nnx.Linear(embed_dim, embed_dim * 4, rngs=rngs)\n",
    "        self.ffn_linear2 = nnx.Linear(embed_dim * 4, embed_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jax.Array,\n",
    "        time_emb: jax.Array,\n",
    "        mask: jax.Array | None = None,\n",
    "        *,\n",
    "        rngs: nnx.Rngs | None = None,\n",
    "        deterministic: bool = False,\n",
    "    ) -> jax.Array:\n",
    "        # Add time embedding to each point embedding\n",
    "        x_with_time = x + time_emb[:, None, :]  # [batch, num_points, embed_dim]\n",
    "\n",
    "        # Self-attention with residual connection\n",
    "        attn_mask = None\n",
    "        if mask is not None:\n",
    "            # Create attention mask for MultiHeadAttention\n",
    "            # Following Flax NNX API - we need to create it with proper broadcasting\n",
    "            attn_mask = nnx.make_attention_mask(\n",
    "                mask >= 0.5,  # query_input (True for valid positions)\n",
    "                mask >= 0.5,  # key_input (True for valid positions)\n",
    "                dtype=jnp.float32,\n",
    "            )\n",
    "\n",
    "        # Self-attention - use query only form as we're doing self-attention\n",
    "        # Need to provide query, key, value for self-attention\n",
    "        attn_output = self.attention(\n",
    "            x_with_time,\n",
    "            x_with_time,\n",
    "            x_with_time,\n",
    "            deterministic=deterministic,\n",
    "            mask=attn_mask,\n",
    "            decode=False,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        attn_output = self.dropout(attn_output, rngs=rngs, deterministic=deterministic)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # FFN with residual connection\n",
    "        x_res = x  # Store for residual connection\n",
    "        ffn_output = self.ffn_linear1(x)\n",
    "        ffn_output = jax.nn.gelu(ffn_output)\n",
    "        ffn_output = self.dropout(ffn_output, rngs=rngs, deterministic=deterministic)\n",
    "        ffn_output = self.ffn_linear2(ffn_output)\n",
    "        ffn_output = self.dropout(ffn_output, rngs=rngs, deterministic=deterministic)\n",
    "        x = x_res + ffn_output  # Add residual connection\n",
    "        x = self.norm2(x)  # Apply second layer normalization\n",
    "        x = x + ffn_output\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PointCloudProteinDiffusionModel(nnx.Module):\n",
    "    \"\"\"Protein structure diffusion model.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TrainingConfig, *, rngs: nnx.Rngs):\n",
    "        \"\"\"\n",
    "        Initialize the protein diffusion model.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object with model parameters\n",
    "            rngs: Random number generator key wrapper\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "        # Input projection: map 3D coords to model_dim\n",
    "        self.coord_proj = nnx.Linear(3, config.model_dim, rngs=rngs)\n",
    "\n",
    "        # Timestep embedding\n",
    "        self.time_embed = TimestepEmbedding(config.model_dim, rngs=rngs)\n",
    "        self.time_embed_mlp_1 = nnx.Linear(config.model_dim, config.model_dim * 4, rngs=rngs)\n",
    "        self.time_embed_mlp_2 = nnx.Linear(config.model_dim * 4, config.model_dim, rngs=rngs)\n",
    "\n",
    "        # In Flax NNX, modules need to be stored as direct attributes\n",
    "        # Using setattr to dynamically create attributes\n",
    "        for i in range(config.num_layers):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"block_{i}\",\n",
    "                PointCloudTransformerBlock(\n",
    "                    config.model_dim, config.num_heads, dropout=0.1, rngs=rngs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # Output projection\n",
    "        self.final_norm = nnx.LayerNorm(config.model_dim, rngs=rngs)\n",
    "        self.final_proj = nnx.Linear(config.model_dim, 3, rngs=rngs)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x_t: jax.Array,\n",
    "        t: jax.Array,\n",
    "        point_mask: jax.Array,\n",
    "        *,\n",
    "        rngs=None,\n",
    "        deterministic=None,\n",
    "    ) -> jax.Array:\n",
    "        \"\"\"\n",
    "        Forward pass of the model, exactly matching the PyTorch implementation.\n",
    "\n",
    "        Args:\n",
    "            x_t: [batch, num_points, 3] - noisy coordinates\n",
    "            t: [batch] - timesteps\n",
    "            point_mask: [batch, num_points] - 1.0 for valid points, 0.0 for padding\n",
    "            rngs: Optional dictionary of PRNG keys for stochastic operations\n",
    "            deterministic: If True, disables stochastic operations (like dropout)\n",
    "        \"\"\"\n",
    "        # Set training mode based on deterministic flag\n",
    "        _training = not deterministic if deterministic is not None else True\n",
    "        _batch_size, num_points, _ = x_t.shape\n",
    "\n",
    "        # Project coordinates to embedding dimension\n",
    "        x_emb = self.coord_proj(x_t)\n",
    "\n",
    "        # Process timestep embedding\n",
    "        time_emb = self.time_embed(t)\n",
    "        time_emb = self.time_embed_mlp_1(time_emb)\n",
    "        time_emb = jax.nn.gelu(time_emb)\n",
    "        time_emb = self.time_embed_mlp_2(time_emb)\n",
    "\n",
    "        # Apply transformer blocks sequentially\n",
    "        h = x_emb\n",
    "        for i in range(self.config.num_layers):\n",
    "            block = getattr(self, f\"block_{i}\")\n",
    "            h = block(h, time_emb, point_mask, rngs=rngs, deterministic=deterministic)\n",
    "\n",
    "        # Final normalization and projection to 3D coordinates\n",
    "        h = self.final_norm(h)\n",
    "        predicted_noise = self.final_proj(h)\n",
    "\n",
    "        # Apply mask to output (zero out predictions for masked/padded points)\n",
    "        predicted_noise = predicted_noise * point_mask[:, :, None]\n",
    "\n",
    "        return predicted_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title EGNN Protein Diffusion Model\n",
    "\n",
    "# @markdown <font color='red'>**NOT WORKING YET**</font>\n",
    "\n",
    "\n",
    "class EGNNLayer(nnx.Module):\n",
    "    \"\"\"\n",
    "    A conceptual and simplified E(n) Equivariant Graph Neural Network Layer.\n",
    "    Operates on node features (h) and node coordinates (x).\n",
    "    Assumes a fully connected graph for simplicity in this example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, *, rngs: nnx.Rngs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dim: Dimensionality of input/output node features (h).\n",
    "            hidden_dim: Internal dimensionality for MLPs.\n",
    "            rngs: NNX random number generators.\n",
    "        \"\"\"\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # MLP for edge features/messages (phi_e)\n",
    "        # Input: h_i, h_j, ||x_i - x_j||^2\n",
    "        self.phi_e = nnx.Sequential(\n",
    "            nnx.Linear(feature_dim * 2 + 1, hidden_dim, rngs=rngs),\n",
    "            nnx.silu,  # Swish activation\n",
    "            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs),\n",
    "            nnx.silu,\n",
    "        )\n",
    "\n",
    "        # MLP for node feature update (phi_h)\n",
    "        # Input: h_i, aggregated_message_m_i\n",
    "        self.phi_h = nnx.Sequential(\n",
    "            nnx.Linear(feature_dim + hidden_dim, hidden_dim, rngs=rngs),\n",
    "            nnx.silu,\n",
    "            nnx.Linear(hidden_dim, feature_dim, rngs=rngs),\n",
    "        )\n",
    "\n",
    "        # MLP for coordinate update weights (phi_x)\n",
    "        # Input: message_m_ij (output of phi_e)\n",
    "        self.phi_x = nnx.Sequential(\n",
    "            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs),\n",
    "            nnx.silu,\n",
    "            # Output a single scalar weight per message\n",
    "            nnx.Linear(hidden_dim, 1, use_bias=False, rngs=rngs),  # Often bias is omitted here\n",
    "        )\n",
    "\n",
    "        # Optional: Layer Normalization\n",
    "        self.norm_h = nnx.LayerNorm(feature_dim, rngs=rngs)\n",
    "        # Note: Normalizing coordinates directly can break equivariance.\n",
    "        # Coordinate normalization/stabilization often happens outside the layer\n",
    "        # or via techniques like normalizing the coordinate update vectors.\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        h: jax.Array,\n",
    "        x: jax.Array,\n",
    "        mask: jax.Array | None = None,\n",
    "        *,\n",
    "        rngs: nnx.Rngs | None = None,\n",
    "    ) -> tuple[jax.Array, jax.Array]:\n",
    "        \"\"\"\n",
    "        Forward pass of the EGNN layer.\n",
    "\n",
    "        Args:\n",
    "            h: Node features [batch, num_nodes, feature_dim]\n",
    "            x: Node coordinates [batch, num_nodes, 3]\n",
    "            mask: Optional node mask [batch, num_nodes] (1 for real nodes, 0 for padding)\n",
    "\n",
    "        Returns:\n",
    "            tuple containing updated node features and coordinates.\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, _ = h.shape\n",
    "\n",
    "        # --- 1. Calculate Pairwise Differences and Distances ---\n",
    "        # Expand dims for broadcasting: x_i [B, N, 1, 3], x_j [B, 1, N, 3]\n",
    "        delta_x = x[:, :, None, :] - x[:, None, :, :]  # [B, N, N, 3] (vector x_i - x_j)\n",
    "        # Calculate squared distances ||x_i - x_j||^2\n",
    "        sq_dist = jnp.sum(delta_x**2, axis=-1, keepdims=True)  # [B, N, N, 1]\n",
    "\n",
    "        # --- 2. Prepare Edge Features ---\n",
    "        # Expand dims for broadcasting: h_i [B, N, 1, F], h_j [B, 1, N, F]\n",
    "        h_i = h[:, :, None, :].repeat(num_nodes, axis=2)\n",
    "        h_j = h[:, None, :, :].repeat(num_nodes, axis=1)\n",
    "\n",
    "        # Concatenate features for the edge MLP input: [h_i, h_j, ||x_i - x_j||^2]\n",
    "        edge_mlp_input = jnp.concatenate([h_i, h_j, sq_dist], axis=-1)  # [B, N, N, 2*F + 1]\n",
    "\n",
    "        # --- 3. Calculate Edge Messages (m_ij) ---\n",
    "        # Apply the edge MLP phi_e\n",
    "        m_ij = self.phi_e(edge_mlp_input)  # [B, N, N, hidden_dim]\n",
    "\n",
    "        # --- Masking (Important!) ---\n",
    "        node_mask = None\n",
    "        if mask is not None:\n",
    "            # Ensure messages involving padded nodes are zeroed out\n",
    "            # Also apply to delta_x and sq_dist for safety, although sq_dist is invariant\n",
    "            node_mask = mask[:, :, None, None] * mask[:, None, :, None]  # [B, N, N, 1]\n",
    "            m_ij = m_ij * node_mask\n",
    "            delta_x = delta_x * node_mask\n",
    "            # Prevent self-loops from contributing to coordinate updates later\n",
    "            self_mask = (1.0 - jnp.eye(num_nodes, dtype=m_ij.dtype))[\n",
    "                None, :, :, None\n",
    "            ]  # [1, N, N, 1]\n",
    "            delta_x = delta_x * self_mask  # Zero out diagonal delta_x\n",
    "            m_ij = (\n",
    "                m_ij * self_mask\n",
    "            )  # Zero out diagonal messages if desired (can affect feature updates)\n",
    "\n",
    "        else:\n",
    "            # Prevent self-loops if no mask provided\n",
    "            self_mask = (1.0 - jnp.eye(num_nodes, dtype=m_ij.dtype))[None, :, :, None]\n",
    "            delta_x = delta_x * self_mask\n",
    "            m_ij = m_ij * self_mask\n",
    "\n",
    "        # --- 4. Aggregate Messages for Feature Update ---\n",
    "        # Sum messages arriving at each node i: m_i = sum_j(m_ij)\n",
    "        m_i = jnp.sum(m_ij, axis=2)  # [B, N, hidden_dim]\n",
    "\n",
    "        # --- 5. Update Node Features (h') ---\n",
    "        # Input to node MLP phi_h: [h_i, m_i]\n",
    "        h_mlp_input = jnp.concatenate([h, m_i], axis=-1)  # [B, N, F + hidden_dim]\n",
    "        # Residual connection: h' = h + phi_h([h, m_i])\n",
    "        h_update = self.phi_h(h_mlp_input)  # [B, N, F]\n",
    "        h_new = self.norm_h(h + h_update)  # Apply norm after residual\n",
    "\n",
    "        if mask is not None:\n",
    "            h_new = h_new * mask[:, :, None]  # Apply node mask\n",
    "\n",
    "        # --- 6. Calculate Coordinate Update Weights ---\n",
    "        # Apply coordinate MLP phi_x to messages\n",
    "        coord_weights = self.phi_x(m_ij)  # [B, N, N, 1]\n",
    "\n",
    "        # Stabilize weights (optional but common)\n",
    "        # Avoid division by zero if sq_dist is zero (i.e. i=j)\n",
    "        coord_weights = coord_weights / jnp.sqrt(sq_dist + 1e-8)  # Normalize by distance\n",
    "        coord_weights = jax.nn.tanh(coord_weights)  # Apply activation (e.g., tanh)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Ensure weights involving masked nodes or self-loops are zero\n",
    "            coord_weights = coord_weights * node_mask * self_mask\n",
    "\n",
    "        # --- 7. Update Coordinates (x') ---\n",
    "        # Equivariant update: x'_i = x_i + C * sum_{j!=i} (x_i - x_j) * phi_x(m_ij)\n",
    "        # Note: delta_x is (x_i - x_j)\n",
    "        coord_update = jnp.sum(delta_x * coord_weights, axis=2)  # [B, N, 3]\n",
    "\n",
    "        # Constant C (often 1/(num_nodes-1) or learned) - simple average here\n",
    "        # Calculate num_real_nodes per batch item for potentially better normalization\n",
    "        if mask is not None:\n",
    "            num_real_nodes = jnp.sum(mask, axis=1, keepdims=True)  # [B, 1]\n",
    "            C = 1.0 / jnp.maximum(num_real_nodes - 1, 1)  # [B, 1], avoid div by zero\n",
    "            C = C[:, :, None]  # Broadcast to [B, 1, 1]\n",
    "        else:\n",
    "            C = 1.0 / (num_nodes - 1 + 1e-8)  # Avoid division by zero if N=1\n",
    "\n",
    "        x_new = x + C * coord_update  # [B, N, 3]\n",
    "\n",
    "        if mask is not None:\n",
    "            x_new = x_new * mask[:, :, None]  # Apply node mask\n",
    "\n",
    "        return h_new, x_new\n",
    "\n",
    "\n",
    "class EGNNProteinDiffusionModel(ProteinDiffusionModel):\n",
    "    \"\"\"Protein structure diffusion model using EGNN layers.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TrainingConfig, *, rngs: nnx.Rngs):\n",
    "        \"\"\"\n",
    "        Initialize the EGNN-based protein diffusion model.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object with model parameters.\n",
    "            rngs: NNX random number generator key wrapper.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "        # Input projection: map 3D coords to feature_dim (h_0)\n",
    "        # This creates initial node features from the input coordinates\n",
    "        self.coord_embed = nnx.Linear(3, config.model_dim, rngs=rngs)\n",
    "\n",
    "        # Timestep embedding (remains the same as in the Transformer version)\n",
    "        self.time_embed = TimestepEmbedding(config.model_dim, rngs=rngs)\n",
    "        # Use MLPs to process time embedding before adding to node features\n",
    "        self.time_embed_mlp_1 = nnx.Linear(config.model_dim, config.model_dim * 4, rngs=rngs)\n",
    "        self.time_embed_mlp_2 = nnx.Linear(config.model_dim * 4, config.model_dim, rngs=rngs)\n",
    "\n",
    "        # EGNN Layers instead of Transformer Blocks\n",
    "        # Store layers in a list or Modulelist if NNX provides one\n",
    "        self.egnn_layers = [\n",
    "            EGNNLayer(\n",
    "                feature_dim=config.model_dim,\n",
    "                hidden_dim=config.model_dim // 2,  # Example hidden dim, can be tuned\n",
    "                rngs=rngs,\n",
    "            )\n",
    "            for _ in range(config.num_layers)\n",
    "        ]\n",
    "\n",
    "        # Output projection (operates on final node features `h`)\n",
    "        self.final_norm = nnx.LayerNorm(config.model_dim, rngs=rngs)\n",
    "        # Predicts the noise added to the original coordinates based on final features\n",
    "        self.final_proj = nnx.Linear(config.model_dim, 3, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x_t, t, point_mask, *, rngs=None, deterministic=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the EGNN-based model.\n",
    "\n",
    "        Args:\n",
    "            x_t: [batch, num_points, 3] - noisy coordinates at timestep t.\n",
    "            t: [batch] - timesteps.\n",
    "            point_mask: [batch, num_points] - 1.0 for valid points, 0.0 for padding.\n",
    "            rngs: Optional dictionary of PRNG keys (EGNNLayer doesn't use dropout here).\n",
    "            deterministic: If True, disables stochastic ops (not relevant for this EGNNLayer).\n",
    "\n",
    "        Returns:\n",
    "            Predicted noise [batch, num_points, 3].\n",
    "        \"\"\"\n",
    "        batch_size, num_points, _ = x_t.shape\n",
    "\n",
    "        # 1. Initial Feature Embedding\n",
    "        # Create initial node features h_0 from input coordinates x_t\n",
    "        h = self.coord_embed(x_t)  # [B, N, F]\n",
    "\n",
    "        # 2. Process Timestep Embedding\n",
    "        time_emb = self.time_embed(t)  # [B, F]\n",
    "        # Pass through MLPs\n",
    "        time_emb = self.time_embed_mlp_1(time_emb)\n",
    "        time_emb = jax.nn.silu(time_emb)  # Use SiLU activation consistent with EGNN\n",
    "        time_emb = self.time_embed_mlp_2(time_emb)  # [B, F]\n",
    "\n",
    "        # Add time embedding to initial node features (broadcast across nodes)\n",
    "        h = h + time_emb[:, None, :]  # [B, N, F]\n",
    "\n",
    "        # Apply mask after adding time embedding\n",
    "        if point_mask is not None:\n",
    "            h = h * point_mask[:, :, None]\n",
    "\n",
    "        # 3. Apply EGNN Layers Sequentially\n",
    "        # The EGNN layer updates both features (h) and coordinates (x)\n",
    "        current_x = x_t  # Start with the input noisy coordinates\n",
    "        current_h = h  # Start with the embedded features + time\n",
    "        for layer in self.egnn_layers:\n",
    "            # Pass current features (h) and coordinates (x), and the mask\n",
    "            current_h, current_x = layer(current_h, current_x, mask=point_mask)\n",
    "            # The layer outputs updated h' and x'\n",
    "            # Note: current_x is updated equivariantly inside the layer\n",
    "\n",
    "        # 4. Final Projection\n",
    "        # Use the final node *features* `current_h` to predict the noise\n",
    "        h_final = self.final_norm(current_h)\n",
    "        predicted_noise = self.final_proj(h_final)  # [B, N, 3]\n",
    "\n",
    "        # Apply mask to the final output prediction\n",
    "        if point_mask is not None:\n",
    "            predicted_noise = predicted_noise * point_mask[:, :, None]\n",
    "\n",
    "        # Return the predicted noise. The DiffusionProcess loss function\n",
    "        # will compare this prediction to the actual noise added to the original x_0.\n",
    "        return predicted_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Diffusion Process\n",
    "\n",
    "\n",
    "# --- Diffusion Process ---\n",
    "class DiffusionProcess:\n",
    "    \"\"\"Implementation of diffusion processes for protein structures.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.timesteps = config.timesteps\n",
    "        self.vars = get_diffusion_variables(config.beta_schedule, config.timesteps)\n",
    "\n",
    "    def q_sample(\n",
    "        self,\n",
    "        x_start: jax.Array,\n",
    "        t: jax.Array,\n",
    "        noise: jax.Array | None = None,\n",
    "        key: jax.random.PRNGKey | None = None,\n",
    "    ):\n",
    "        \"\"\"Forward diffusion process: q(x_t | x_0).\"\"\"\n",
    "        if noise is None:\n",
    "            if key is None:\n",
    "                key = jrandom.PRNGKey(0)\n",
    "            noise = jrandom.normal(key, shape=x_start.shape)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = self._extract(self.vars[\"sqrt_alphas_cumprod\"], t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(\n",
    "            self.vars[\"sqrt_one_minus_alphas_cumprod\"], t, x_start.shape\n",
    "        )\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def _extract(self, a: jax.Array, t: jax.Array, x_shape: tuple[int]) -> jax.Array:\n",
    "        # Get value at index t for a batch of shape x_shape\n",
    "        batch_size = t.shape[0]\n",
    "\n",
    "        # The axis parameter specifies which axis to gather from (in this case, the last axis, -1)\n",
    "        out = jnp.take_along_axis(a, t, axis=-1)\n",
    "\n",
    "        # Reshape to match the desired output shape\n",
    "        return out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "    def p_losses(\n",
    "        self,\n",
    "        model_fn,\n",
    "        x_start: jax.Array,\n",
    "        t: jax.Array,\n",
    "        point_mask: jax.Array,\n",
    "        deterministic: bool = False,\n",
    "        noise: jax.Array | None = None,\n",
    "        key: jax.random.PRNGKey | None = None,\n",
    "    ) -> jax.Array:\n",
    "        \"\"\"Calculate diffusion loss.\"\"\"\n",
    "        if noise is None:\n",
    "            if key is None:\n",
    "                key = jrandom.PRNGKey(0)\n",
    "            noise = jrandom.normal(key, shape=x_start.shape)\n",
    "\n",
    "        # Apply forward process to get noisy input x_t\n",
    "        x_noisy = self.q_sample(x_start, t, noise, key)\n",
    "\n",
    "        # Predict noise using the model\n",
    "        predicted_noise = model_fn(x_noisy, t, point_mask, deterministic=deterministic)\n",
    "\n",
    "        # Calculate MSE loss between predicted and actual noise\n",
    "        loss = (noise - predicted_noise) ** 2\n",
    "        loss = loss * point_mask[:, :, None]  # Apply mask to only compute loss on valid points\n",
    "\n",
    "        # Compute the SNR-weighted loss\n",
    "        snr = self._extract(self.vars[\"sqrt_alphas_cumprod\"], t, x_start.shape) / self._extract(\n",
    "            self.vars[\"sqrt_one_minus_alphas_cumprod\"], t, x_start.shape\n",
    "        )\n",
    "        loss_weights = jnp.clip(snr / (1.0 + snr), 0.5, 5.0)\n",
    "        loss = loss * loss_weights\n",
    "\n",
    "        # Average loss per valid point across the batch\n",
    "        per_sample_loss = jnp.sum(loss, axis=(1, 2)) / (jnp.sum(point_mask, axis=1) * 3 + 1e-8)\n",
    "        return jnp.mean(per_sample_loss)\n",
    "\n",
    "    def p_sample(\n",
    "        self,\n",
    "        model_fn,\n",
    "        x_t: jax.Array,\n",
    "        t: jax.Array,\n",
    "        point_mask: jax.Array,\n",
    "        key=None,\n",
    "        dropout_key=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Reverse process step: Sample x_{t-1} from p(x_{t-1} | x_t).\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = jrandom.PRNGKey(0)\n",
    "\n",
    "        # Create dropout RNG if not provided\n",
    "        if dropout_key is None:\n",
    "            key, dropout_key = jrandom.split(key)\n",
    "\n",
    "        # Extract required coefficients for timestep t\n",
    "        betas_t = self._extract(self.vars[\"betas\"], t, x_t.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self._extract(\n",
    "            self.vars[\"sqrt_one_minus_alphas_cumprod\"], t, x_t.shape\n",
    "        )\n",
    "        sqrt_recip_alphas_t = self._extract(self.vars[\"sqrt_recip_alphas\"], t, x_t.shape)\n",
    "\n",
    "        # Create a wrapper function that includes the dropout RNG\n",
    "        # Use try/except to handle whether the function accepts deterministic\n",
    "        try:\n",
    "            # First try to call with dropout RNGs but without deterministic param\n",
    "            fn_rngs = nnx.Rngs(dropout=dropout_key)\n",
    "            predicted_noise = model_fn(x_t, t, point_mask, rngs=fn_rngs)\n",
    "        except TypeError:\n",
    "            # If that doesn't work, try with both parameters\n",
    "            try:\n",
    "                fn_rngs = nnx.Rngs(dropout=dropout_key)\n",
    "                predicted_noise = model_fn(x_t, t, point_mask, deterministic=True, rngs=fn_rngs)\n",
    "            except TypeError:\n",
    "                # If both fail, try without any extra params\n",
    "                predicted_noise = model_fn(x_t, t, point_mask)\n",
    "\n",
    "        # Rest of the function remains the same\n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x_t - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "\n",
    "        if jnp.all(t == 0):\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = self._extract(self.vars[\"posterior_variance\"], t, x_t.shape)\n",
    "            noise = jrandom.normal(key, shape=x_t.shape)\n",
    "            noise = noise * point_mask[:, :, None]\n",
    "            return model_mean + jnp.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "    def sample(\n",
    "        self, model_fn, shape: tuple[int], point_mask: jax.Array, *, rng_key=None\n",
    "    ) -> list[jax.Array]:\n",
    "        \"\"\"\n",
    "        Generate samples starting from pure noise (x_T).\n",
    "\n",
    "        Args:\n",
    "            model_fn: Function that takes (x_t, t, point_mask) and returns predicted noise\n",
    "            shape: Shape of the output tensor [batch, num_points, 3]\n",
    "            point_mask: Mask for valid points [batch, num_points]\n",
    "            rng_key: JAX PRNG key\n",
    "\n",
    "        Returns:\n",
    "            list of generated samples at different denoising steps\n",
    "        \"\"\"\n",
    "        if rng_key is None:\n",
    "            rng_key = jrandom.PRNGKey(0)\n",
    "\n",
    "        batch_size = shape[0]\n",
    "        # Start from pure Gaussian noise ~ N(0, I)\n",
    "        rng_key, noise_key = jrandom.split(rng_key)\n",
    "        denoised_p = jrandom.normal(noise_key, shape=shape)\n",
    "\n",
    "        # lists to store generated samples\n",
    "        denoised_ps = []\n",
    "\n",
    "        # Loop from T-1 down to 0 (reverse diffusion process)\n",
    "        for i in tqdm(\n",
    "            reversed(range(0, self.timesteps)),\n",
    "            desc=\"Sampling\",\n",
    "            total=self.timesteps,\n",
    "            leave=False,\n",
    "        ):\n",
    "            # Create PRNG keys for this step - one for sampling noise, one for dropout\n",
    "            rng_key, step_key, dropout_key = jrandom.split(rng_key, 3)\n",
    "\n",
    "            # Create a tensor of the current timestep for the batch\n",
    "            t = jnp.full((batch_size,), i, dtype=jnp.int32)\n",
    "\n",
    "            # Perform one denoising step with both keys\n",
    "            denoised_p = self.p_sample(\n",
    "                model_fn,\n",
    "                denoised_p,\n",
    "                t,\n",
    "                point_mask,\n",
    "                key=step_key,\n",
    "                dropout_key=dropout_key,\n",
    "            )\n",
    "\n",
    "            # Save intermediate steps (at regular intervals)\n",
    "            if i % 50 == 0:\n",
    "                denoised_ps.append(denoised_p)\n",
    "\n",
    "        # Add final sample if not already added\n",
    "        if self.timesteps % 50 != 0:\n",
    "            denoised_ps.append(denoised_p)\n",
    "\n",
    "        return denoised_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Checkpointing utilities\n",
    "\n",
    "\n",
    "def setup_checkpoint_manager(base_dir):\n",
    "    \"\"\"Setup Orbax checkpoint manager.\"\"\"\n",
    "    try:\n",
    "        # Ensure the base directory path is absolute\n",
    "        base_dir_abs = os.path.abspath(base_dir)\n",
    "        print(f\"Setting up checkpoint manager in absolute path: {base_dir_abs}...\")\n",
    "\n",
    "        # Ensure the directory exists before creating the manager\n",
    "        # Use pathlib for robust directory creation\n",
    "        pathlib.Path(base_dir_abs).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        options = ocp.CheckpointManagerOptions(\n",
    "            max_to_keep=5,\n",
    "            create=True,  # create=True is often default, but explicit is fine\n",
    "        )\n",
    "\n",
    "        checkpoint_manager = ocp.CheckpointManager(\n",
    "            directory=base_dir_abs,  # Pass the absolute path string directly\n",
    "            options=options,\n",
    "        )\n",
    "\n",
    "        print(f\"Successfully created checkpoint manager in {base_dir_abs}\")\n",
    "        # Return the absolute path for consistency\n",
    "        return checkpoint_manager, base_dir_abs\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up checkpoint manager: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_manager, model, step):\n",
    "    \"\"\"Save model checkpoint using Orbax.\"\"\"\n",
    "    print(f\"Attempting to save checkpoint for step {step}...\")\n",
    "\n",
    "    # Get the model state using nnx.state\n",
    "    try:\n",
    "        # Ensure the model is an NNX Module or GraphDef before getting state\n",
    "        if not isinstance(model, (nnx.Module, nnx.GraphDef)):\n",
    "            raise TypeError(f\"Expected model to be nnx.Module or nnx.GraphDef, got {type(model)}\")\n",
    "\n",
    "        model_state = nnx.state(model)\n",
    "        print(\"Successfully extracted model state.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting model state: {e}\")\n",
    "        traceback.print_exc()  # traceback is now defined globally\n",
    "        raise\n",
    "\n",
    "    # Define the arguments for saving using Orbax argument classes\n",
    "    try:\n",
    "        # Create save args. StandardSave works well for standard PyTrees like NNX state.\n",
    "        save_args = ocp.args.Composite(\n",
    "            model=ocp.args.StandardSave(model_state)\n",
    "            # You can add other things to save here, e.g., optimizer state:\n",
    "            # optimizer=ocp.args.StandardSave(optimizer_state)\n",
    "        )\n",
    "\n",
    "        checkpoint_manager.save(\n",
    "            step,\n",
    "            args=save_args,\n",
    "            # force=False # Set to True to overwrite if step exists (use with caution)\n",
    "        )\n",
    "\n",
    "        # Wait for checkpointing to finish before proceeding (important for async saves)\n",
    "        checkpoint_manager.wait_until_finished()\n",
    "\n",
    "        # Use the manager's directory property which holds the correct Path object\n",
    "        print(f\"Successfully saved checkpoint for step {step} to {checkpoint_manager.directory}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during checkpoint_manager.save or wait: {e}\")\n",
    "        traceback.print_exc()  # traceback is now defined globally\n",
    "        raise\n",
    "\n",
    "    return checkpoint_manager\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_manager, target_model_template=None, step=None):\n",
    "    \"\"\"\n",
    "    Load model checkpoint using Orbax.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_manager: The Orbax CheckpointManager instance.\n",
    "        target_model_template: An optional NNX Module instance or GraphDef\n",
    "                               with the same structure as the saved model.\n",
    "                               If provided, the loaded state will be applied\n",
    "                               to this template. If None, the raw state dict\n",
    "                               is returned.\n",
    "        step: The specific step to restore. If None, restores the latest step.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The restored model state (either applied to target_model_template\n",
    "          or as a raw dictionary).\n",
    "        - The step number restored from.\n",
    "        Returns (None, None) if no checkpoint is found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if step is None:\n",
    "            step = checkpoint_manager.latest_step()\n",
    "            if step is None:\n",
    "                print(f\"No checkpoints found in {checkpoint_manager.directory} to restore.\")\n",
    "                return None, None\n",
    "\n",
    "        print(\n",
    "            \"Attempting to restore checkpoint from step\"\n",
    "            f\" {step} in {checkpoint_manager.directory}...\"\n",
    "        )\n",
    "\n",
    "        # Define the args for restoring.\n",
    "        # If a template is provided, Orbax can restore directly into it.\n",
    "        # Otherwise, StandardRestore() loads the raw data.\n",
    "        if target_model_template:\n",
    "            if not isinstance(target_model_template, (nnx.Module, nnx.GraphDef)):\n",
    "                raise TypeError(\n",
    "                    \"Expected target_model_template to be nnx.Module\"\n",
    "                    f\" or nnx.GraphDef, got {type(target_model_template)}\"\n",
    "                )\n",
    "            # Use the template's state as the target for restoration\n",
    "            target_state = nnx.state(target_model_template)\n",
    "            restore_args = ocp.args.Composite(\n",
    "                model=ocp.args.StandardRestore(target_state)\n",
    "                # Add args for other items if they were saved, e.g.:\n",
    "                # optimizer=ocp.args.StandardRestore(optimizer_template.state)\n",
    "            )\n",
    "            print(\"Restoring checkpoint into provided model template.\")\n",
    "            # Restore directly into the target state\n",
    "            restored_data = checkpoint_manager.restore(\n",
    "                step,\n",
    "                args=restore_args,  # Pass the args with the target state\n",
    "            )\n",
    "            # Orbax modifies the target state in-place.\n",
    "            # We need to update the original model template with the modified state.\n",
    "            nnx.update(\n",
    "                target_model_template, restored_data[\"model\"]\n",
    "            )  # Apply the restored state back\n",
    "\n",
    "            print(f\"Successfully restored checkpoint from step {step} into the template.\")\n",
    "            return target_model_template, step  # Return the modified template\n",
    "        else:\n",
    "            # Restore raw state dictionary if no template is given\n",
    "            restore_args = ocp.args.Composite(\n",
    "                model=ocp.args.StandardRestore()\n",
    "                # Add args for other items if they were saved\n",
    "                # optimizer=ocp.args.StandardRestore()\n",
    "            )\n",
    "            print(\"Restoring checkpoint as raw state dictionary.\")\n",
    "            restored_data = checkpoint_manager.restore(step, args=restore_args)\n",
    "            print(f\"Successfully restored raw checkpoint data from step {step}\")\n",
    "            # Return the specific part of the restored data corresponding to the model\n",
    "            if \"model\" not in restored_data:\n",
    "                print(\"Warning: 'model' key not found in restored data dictionary.\")\n",
    "                return None, step  # Or handle as appropriate\n",
    "            return restored_data[\"model\"], step\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        traceback.print_exc()  # traceback is now defined globally\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Optimization utilities\n",
    "\n",
    "\n",
    "class GradAccumulationState(nnx.Module):\n",
    "    \"\"\"Module to track gradient accumulation state.\"\"\"\n",
    "\n",
    "    grads: Any | None = None\n",
    "    count: int = 0\n",
    "\n",
    "    def update(self, new_grads: jax.Array) -> None:\n",
    "        \"\"\"Update accumulated gradients.\"\"\"\n",
    "        if self.grads is None:\n",
    "            self.grads = jax.tree.map(lambda g: g.copy(), new_grads)\n",
    "        else:\n",
    "            self.grads = jax.tree.map(lambda g1, g2: g1 + g2, self.grads, new_grads)\n",
    "        self.count += 1\n",
    "\n",
    "    def get_averaged_grads(self) -> jax.Array:\n",
    "        \"\"\"Get averaged gradients and reset state.\"\"\"\n",
    "        averaged_grads = jax.tree.map(lambda g: g / self.count, self.grads)\n",
    "        return averaged_grads\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset accumulation state.\"\"\"\n",
    "        self.grads = None\n",
    "        self.count = 0\n",
    "\n",
    "\n",
    "def accumulate_gradients(\n",
    "    grad_state: GradAccumulationState, grads: jax.Array\n",
    ") -> GradAccumulationState:\n",
    "    \"\"\"Update gradient accumulation state with new gradients.\"\"\"\n",
    "    grad_state.update(grads)\n",
    "    return grad_state\n",
    "\n",
    "\n",
    "def apply_accumulated_gradients(\n",
    "    optimizer: optax.Optimizer, grad_state: GradAccumulationState\n",
    ") -> optax.Optimizer:\n",
    "    \"\"\"Apply accumulated gradients and reset state.\"\"\"\n",
    "    if grad_state.count > 0:\n",
    "        averaged_grads = grad_state.get_averaged_grads()\n",
    "        optimizer.update(averaged_grads)\n",
    "        grad_state.reset()\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training functions\n",
    "\n",
    "# @markdown ToDo: \\\\\n",
    "# @markdown   - Start from a checkpoint for training\n",
    "\n",
    "\n",
    "def create_linear_warmup_cosine_decay_schedule(\n",
    "    init_value, peak_value, warmup_steps, decay_steps, end_value\n",
    "):\n",
    "    \"\"\"Create a learning rate schedule with linear warmup and cosine decay.\"\"\"\n",
    "\n",
    "    # Create linear warmup schedule\n",
    "    warmup_schedule = optax.linear_schedule(\n",
    "        init_value=init_value, end_value=peak_value, transition_steps=warmup_steps\n",
    "    )\n",
    "\n",
    "    # Create cosine decay schedule\n",
    "    decay_schedule = optax.cosine_decay_schedule(\n",
    "        init_value=peak_value,\n",
    "        decay_steps=decay_steps,\n",
    "        alpha=end_value / peak_value,  # alpha = final_value / initial_value\n",
    "    )\n",
    "\n",
    "    # Join the two schedules\n",
    "    return optax.join_schedules(\n",
    "        schedules=[warmup_schedule, decay_schedule], boundaries=[warmup_steps]\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_batch_size_info(config: TrainingConfig, dataset_size: int):\n",
    "    \"\"\"Calculate and print batch size information.\"\"\"\n",
    "    effective_batch_size = config.batch_size * config.gradient_accumulation_steps\n",
    "\n",
    "    # Calculate steps with either nominal or effective batch size\n",
    "    divisor = effective_batch_size if config.use_effective_batch_size else config.batch_size\n",
    "    total_steps = (dataset_size * config.num_epochs) // divisor\n",
    "\n",
    "    print(\"Batch size configuration:\")\n",
    "    print(f\"   Nominal batch size: {config.batch_size}\")\n",
    "    print(f\"   Gradient accumulation steps: {config.gradient_accumulation_steps}\")\n",
    "    print(f\"   Effective batch size: {effective_batch_size}\")\n",
    "    print(f\"   Total training update steps: {total_steps}\")\n",
    "\n",
    "    return effective_batch_size, total_steps\n",
    "\n",
    "\n",
    "@nnx.jit(static_argnames=[\"diffusion\"])\n",
    "def compute_grads_step(\n",
    "    model: ProteinDiffusionModel, diffusion: DiffusionProcess, batch: BatchType, rng_key\n",
    "):\n",
    "    \"\"\"Compute gradients without applying them.\"\"\"\n",
    "\n",
    "    x_start = batch[\"positions\"]\n",
    "    point_mask = batch[\"mask\"]\n",
    "\n",
    "    # Split RNG key for timestep sampling, noise, and dropout\n",
    "    rng_key, timestep_key, noise_key, dropout_key = jrandom.split(rng_key, 4)\n",
    "\n",
    "    # Sample random timesteps (uniform from 0 to T-1)\n",
    "    batch_size = x_start.shape[0]\n",
    "    t = jrandom.randint(timestep_key, (batch_size,), 0, diffusion.timesteps, dtype=jnp.int32)\n",
    "\n",
    "    # Create noise\n",
    "    noise = jrandom.normal(noise_key, shape=x_start.shape)\n",
    "\n",
    "    def loss_fn(model):\n",
    "        # Create a forward function for diffusion process\n",
    "        def forward_fn(x_t, t, mask, deterministic=False):\n",
    "            fn_rngs = nnx.Rngs(dropout=dropout_key)\n",
    "            return model(x_t, t, mask, deterministic=deterministic, rngs=fn_rngs)\n",
    "\n",
    "        # Apply diffusion loss calculation\n",
    "        return diffusion.p_losses(\n",
    "            forward_fn, x_start, t, point_mask, deterministic=False, noise=noise\n",
    "        )\n",
    "\n",
    "    # Calculate gradients using nnx value_and_grad\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "\n",
    "    return loss, grads, model, rng_key\n",
    "\n",
    "\n",
    "# --- Training Functions ---\n",
    "@nnx.jit(static_argnames=[\"diffusion\"])\n",
    "def train_step(\n",
    "    model: ProteinDiffusionModel,\n",
    "    optimizer: nnx.Optimizer,\n",
    "    diffusion: DiffusionProcess,\n",
    "    batch: BatchType,\n",
    "    rng_key,\n",
    "):\n",
    "    \"\"\"\n",
    "    Single training step, JIT-compiled for speed with nnx.jit.\n",
    "    Matches the PyTorch implementation's training step.\n",
    "\n",
    "    Args:\n",
    "        model: ProteinDiffusionModel\n",
    "        optimizer: Flax NNX optimizer wrapping the model\n",
    "        diffusion: DiffusionProcess object\n",
    "        batch: dictionary containing 'positions' and 'mask'\n",
    "        rng_key: JAX PRNG key\n",
    "\n",
    "    Returns:\n",
    "        loss: Training loss for this batch\n",
    "        model: Updated model\n",
    "        optimizer: Updated optimizer\n",
    "        rng_key: Updated PRNG key\n",
    "    \"\"\"\n",
    "\n",
    "    x_start = batch[\"positions\"]\n",
    "    point_mask = batch[\"mask\"]\n",
    "\n",
    "    # Split RNG key for timestep sampling, noise, and dropout\n",
    "    rng_key, timestep_key, noise_key, dropout_key = jrandom.split(rng_key, 4)\n",
    "\n",
    "    # Sample random timesteps (uniform from 0 to T-1)\n",
    "    batch_size = x_start.shape[0]\n",
    "    t = jrandom.randint(timestep_key, (batch_size,), 0, diffusion.timesteps, dtype=jnp.int32)\n",
    "\n",
    "    # Create noise\n",
    "    noise = jrandom.normal(noise_key, shape=x_start.shape)\n",
    "\n",
    "    # In Flax NNX, state is kept inside an nnx.Module and is mutable\n",
    "    def loss_fn(model):\n",
    "        # Create a forward function for diffusion process (training mode)\n",
    "        def forward_fn(x_t, t, mask, deterministic=False):\n",
    "            # Create a dropout RNG for this call\n",
    "            fn_rngs = nnx.Rngs(dropout=dropout_key)\n",
    "            return model(x_t, t, mask, deterministic=deterministic, rngs=fn_rngs)\n",
    "\n",
    "        # Apply diffusion loss calculation\n",
    "        return diffusion.p_losses(\n",
    "            forward_fn, x_start, t, point_mask, deterministic=False, noise=noise\n",
    "        )\n",
    "\n",
    "    # Calculate gradients using nnx value_and_grad\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "\n",
    "    # Update the optimizer and model in-place (NNX supports in-place mutation)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "    return loss, model, optimizer, rng_key\n",
    "\n",
    "\n",
    "@nnx.jit(static_argnames=[\"diffusion\"])\n",
    "def eval_step(\n",
    "    model: ProteinDiffusionModel,\n",
    "    diffusion: DiffusionProcess,\n",
    "    batch: BatchType,\n",
    "    *,\n",
    "    rng_key,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluation step, JIT-compiled for speed using nnx.jit.\n",
    "    Matches the PyTorch implementation's evaluation logic.\n",
    "\n",
    "    Args:\n",
    "        model: The ProteinDiffusionModel\n",
    "        diffusion: DiffusionProcess object\n",
    "        batch: dictionary containing 'positions' and 'mask'\n",
    "        rng_key: JAX PRNG key\n",
    "\n",
    "    Returns:\n",
    "        loss: Validation loss for this batch\n",
    "        rng_key: Updated PRNG key\n",
    "    \"\"\"\n",
    "\n",
    "    x_start = batch[\"positions\"]\n",
    "    point_mask = batch[\"mask\"]\n",
    "\n",
    "    # Split RNG key for timestep sampling, noise, and dropout\n",
    "    rng_key, timestep_key, noise_key, dropout_key = jrandom.split(rng_key, 4)\n",
    "\n",
    "    # Sample random timesteps\n",
    "    batch_size = x_start.shape[0]\n",
    "    t = jrandom.randint(timestep_key, (batch_size,), 0, diffusion.timesteps, dtype=jnp.int32)\n",
    "\n",
    "    # Create noise (same as in train_step)\n",
    "    noise = jrandom.normal(noise_key, shape=x_start.shape)\n",
    "\n",
    "    # Calculate loss without gradients\n",
    "    def loss_fn(model):\n",
    "        # Forward function for diffusion process (evaluation mode)\n",
    "        def forward_fn(x_t, t, mask, deterministic=True):\n",
    "            # Create a dropout RNG for this call\n",
    "            fn_rngs = nnx.Rngs(dropout=dropout_key)\n",
    "            return model(x_t, t, mask, deterministic=deterministic, rngs=fn_rngs)\n",
    "\n",
    "        # Apply diffusion loss calculation\n",
    "        return diffusion.p_losses(\n",
    "            forward_fn, x_start, t, point_mask, deterministic=True, noise=noise\n",
    "        )\n",
    "\n",
    "    # Calculate loss (no gradients needed for evaluation)\n",
    "    loss = loss_fn(model)\n",
    "\n",
    "    return loss, rng_key\n",
    "\n",
    "\n",
    "def train(\n",
    "    config: TrainingConfig,\n",
    "    model: ProteinDiffusionModel,\n",
    "    diffusion: DiffusionProcess,\n",
    "    train_loader: JAXDataLoader,\n",
    "    val_loader: JAXDataLoader,\n",
    "    *,\n",
    "    rng_key,\n",
    "    plot_rgs=False,\n",
    "):\n",
    "    \"\"\"Training loop for the protein diffusion model.\"\"\"\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    checkpoint_manager, checkpoint_dir = setup_checkpoint_manager(config.output_dir)\n",
    "\n",
    "    effective_batch_size, total_steps = calculate_batch_size_info(config, len(train_loader.dataset))\n",
    "    decay_steps = total_steps - config.warmup_steps\n",
    "\n",
    "    # Create learning rate schedule based on effective or nominal batch size\n",
    "    schedule_fn = create_linear_warmup_cosine_decay_schedule(\n",
    "        init_value=config.learning_rate / 3,\n",
    "        peak_value=config.learning_rate,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        decay_steps=decay_steps,\n",
    "        end_value=config.learning_rate / 1e3,\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer with Flax NNX Optimizer\n",
    "    # This creates an optimizer that holds a reference to the model\n",
    "    optimizer = nnx.Optimizer(\n",
    "        model,\n",
    "        optax.chain(\n",
    "            optax.clip_by_global_norm(1.0),  # Gradient clipping.\n",
    "            optax.adamw(\n",
    "                learning_rate=schedule_fn,\n",
    "                weight_decay=2e-5,\n",
    "                b1=0.9,\n",
    "                b2=0.99,\n",
    "                eps=1e-8,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    metrics = nnx.MultiMetric(\n",
    "        loss=nnx.metrics.Average(\"loss\"),\n",
    "    )\n",
    "\n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # rg_distribution_train = []  # Store Rg values from training set once\n",
    "    # rg_distribution_samples = []  # Store Rg values from generated samples per evaluation\n",
    "\n",
    "    if plot_rgs:\n",
    "        raise NotImplementedError(\"Plotting Rg distributions not implemented yet.\")\n",
    "\n",
    "    print(f\"Starting training for {config.num_epochs} epochs...\")\n",
    "\n",
    "    # Initialize state tracking\n",
    "    global_step = 0  # Total batches processed\n",
    "    update_step = 0  # Parameter updates performed\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize gradient accumulation state\n",
    "    grad_state = GradAccumulationState()\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        # Training loop\n",
    "        epoch_loss = 0.0\n",
    "        train_batch_count = 0\n",
    "\n",
    "        # Get dataloader length if available\n",
    "        try:\n",
    "            num_batches = len(train_loader)\n",
    "            has_known_length = True\n",
    "        except (TypeError, AttributeError):\n",
    "            has_known_length = False\n",
    "            num_batches = \"unknown\"\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{config.num_epochs}: {num_batches} batches\")\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "\n",
    "        # Track current batch in epoch\n",
    "        batch_idx = 0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            batch_idx += 1\n",
    "\n",
    "            # Generate a new RNG key for this step\n",
    "            rng_key, step_key = jrandom.split(rng_key)\n",
    "\n",
    "            # Compute gradients without applying them\n",
    "            loss, grads, model, rng_key = compute_grads_step(model, diffusion, batch, step_key)\n",
    "\n",
    "            # Accumulate gradients\n",
    "            grad_state = accumulate_gradients(grad_state, grads)\n",
    "\n",
    "            # Update metrics\n",
    "            metrics.update(loss=loss)\n",
    "            epoch_loss += loss\n",
    "            train_batch_count += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                loss=f\"{loss:.4f}\",\n",
    "                acc=f\"{grad_state.count}/{config.gradient_accumulation_steps}\",\n",
    "            )\n",
    "\n",
    "            # Log training progress\n",
    "            if global_step % config.log_freq == 0:\n",
    "                print(f\"Step: {global_step}, Train Loss: {loss:.4f}\")\n",
    "                train_losses.append((global_step, loss))\n",
    "\n",
    "            # Check if it's time to apply gradients\n",
    "            # 1. We've reached the specified number of accumulation steps, OR\n",
    "            # 2. We're at the end of an epoch (if we can determine this)\n",
    "            is_last_batch = has_known_length and batch_idx == num_batches\n",
    "            should_apply_grads = (grad_state.count >= config.gradient_accumulation_steps) or (\n",
    "                is_last_batch and grad_state.count > 0\n",
    "            )\n",
    "            if should_apply_grads:\n",
    "                # Apply accumulated gradients\n",
    "                optimizer = apply_accumulated_gradients(optimizer, grad_state)\n",
    "                update_step += 1\n",
    "\n",
    "                # Reset accumulation state\n",
    "                grad_state.reset()\n",
    "\n",
    "            # Evaluation and sampling\n",
    "            if global_step > 0 and global_step % config.eval_freq == 0 and val_loader:\n",
    "                print(\n",
    "                    f\"\\n--- Evaluating at update step {update_step} (batch step {global_step}) ---\"\n",
    "                )\n",
    "\n",
    "                # Validation\n",
    "                total_val_loss = 0.0\n",
    "                val_batch_count = 0\n",
    "\n",
    "                for val_batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "                    # Generate a new RNG key for evaluation\n",
    "                    rng_key, eval_key = jrandom.split(rng_key)\n",
    "\n",
    "                    # Perform evaluation step\n",
    "                    val_loss, rng_key = eval_step(model, diffusion, val_batch, rng_key=eval_key)\n",
    "\n",
    "                    # Update validation metrics\n",
    "                    total_val_loss += val_loss\n",
    "                    val_batch_count += 1\n",
    "\n",
    "                # Calculate average validation loss\n",
    "                avg_val_loss = (\n",
    "                    total_val_loss / val_batch_count if val_batch_count > 0 else float(\"inf\")\n",
    "                )\n",
    "                val_losses.append((global_step, avg_val_loss))\n",
    "                print(f\"Step: {global_step}, Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "                # Generate a sample structure\n",
    "                print(\"Generating a sample structure...\")\n",
    "\n",
    "                # Define sample parameters (e.g., protein of length 100 residues)\n",
    "                sample_len_res = config.max_seq_length\n",
    "                num_sample_points = sample_len_res * 4\n",
    "                sample_shape = 1, num_sample_points, 3\n",
    "\n",
    "                # Create a mask for this length (all valid points)\n",
    "                sample_point_mask = jnp.ones(sample_shape[:-1])\n",
    "\n",
    "                # Split RNG key for sampling\n",
    "                rng_key, sample_key = jrandom.split(rng_key)\n",
    "\n",
    "                # Generate sample with deterministic=True for inference\n",
    "                def sample_forward_fn(x_t, t, mask, deterministic=True, rngs=None):\n",
    "                    # Use the rngs if provided, otherwise function without it\n",
    "                    if rngs is not None:\n",
    "                        return model(x_t, t, mask, deterministic=deterministic, rngs=rngs)\n",
    "                    else:\n",
    "                        return model(x_t, t, mask, deterministic=deterministic)\n",
    "\n",
    "                # Generate samples\n",
    "                generated_samples = diffusion.sample(\n",
    "                    sample_forward_fn,\n",
    "                    sample_shape,\n",
    "                    sample_point_mask,\n",
    "                    rng_key=sample_key,\n",
    "                )\n",
    "\n",
    "                # Extract the final denoised sample (at step t=0)\n",
    "                final_sample_flat = np.array(generated_samples[-1][0])  # Convert to numpy array\n",
    "                final_mask_flat = np.array(sample_point_mask[0])\n",
    "\n",
    "                try:\n",
    "                    # Convert flat coordinates back to Protein object\n",
    "                    protein_sample = flat_coords_to_protein(\n",
    "                        final_sample_flat, final_mask_flat, sample_len_res\n",
    "                    )\n",
    "\n",
    "                    # Convert to PDB string format\n",
    "                    pdb_string = to_pdb(protein_sample)\n",
    "\n",
    "                    if pdb_string:  # Check if PDB string was generated successfully\n",
    "                        print(\"--- Generated Sample PDB (first 15 lines) ---\")\n",
    "                        print(\"\\n\".join(pdb_string.splitlines()[:15]))\n",
    "                        print(\"---------------------------------------------\")\n",
    "\n",
    "                        # Save PDB file\n",
    "                        pdb_filename = os.path.join(\n",
    "                            config.output_dir, f\"sample_step_{global_step}.pdb\"\n",
    "                        )\n",
    "\n",
    "                        with open(pdb_filename, \"w\") as f:\n",
    "                            f.write(pdb_string)\n",
    "                        print(f\"Saved sample PDB to {pdb_filename}\")\n",
    "\n",
    "                        # Try to visualize if in interactive environment\n",
    "                        try:\n",
    "                            if COLAB_ENV:\n",
    "                                print(\"Plotting loss curves...\")\n",
    "                                plot_loss_curves(train_losses, val_losses, current_step=global_step)\n",
    "                                visualize_pdb(pdb_string)\n",
    "\n",
    "                        except (ImportError, ModuleNotFoundError):\n",
    "                            print(\"py3Dmol not available for visualization\")\n",
    "                    else:\n",
    "                        print(\"PDB string generation failed for the sample.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating, saving, or visualizing sample: {e}\")\n",
    "                    import traceback\n",
    "\n",
    "                    traceback.print_exc()\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if update_step > 0 and update_step % config.save_freq == 0:\n",
    "                # Save using Orbax checkpoint manager\n",
    "                save_checkpoint(checkpoint_manager, model, update_step)\n",
    "                print(f\"Saved checkpoint at update step {update_step}\")\n",
    "\n",
    "        # End of epoch logging\n",
    "        avg_epoch_loss = epoch_loss / train_batch_count if train_batch_count > 0 else float(\"inf\")\n",
    "        print(f\"--- Epoch {epoch + 1} Finished ---\")\n",
    "        print(f\"Average Training Loss for Epoch: {avg_epoch_loss:.4f}\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "    # Calculate total training time\n",
    "    end_time = time.time()\n",
    "    print(f\"Total training time: {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "    # Final sample generation (similar to the evaluation sample generation)\n",
    "    print(\"\\n--- Generating final sample ---\")\n",
    "    sample_len_res = 150  # Generate a longer protein for final sample\n",
    "    num_sample_points = sample_len_res * 4\n",
    "    sample_shape = (1, num_sample_points, 3)\n",
    "    sample_point_mask = jnp.ones(sample_shape[:-1])\n",
    "\n",
    "    rng_key, final_sample_key = jrandom.split(rng_key)\n",
    "\n",
    "    def final_sample_fn(x_t, t, mask):\n",
    "        return model(x_t, t, mask, deterministic=True)\n",
    "\n",
    "    generated_samples = diffusion.sample(\n",
    "        final_sample_fn, sample_shape, sample_point_mask, rng_key=final_sample_key\n",
    "    )\n",
    "\n",
    "    final_sample_flat = np.array(generated_samples[-1][0])\n",
    "    final_mask_flat = np.array(sample_point_mask[0])\n",
    "\n",
    "    try:\n",
    "        protein_sample = flat_coords_to_protein(final_sample_flat, final_mask_flat, sample_len_res)\n",
    "\n",
    "        pdb_string = to_pdb(protein_sample)\n",
    "\n",
    "        if pdb_string:\n",
    "            pdb_filename = os.path.join(config.output_dir, f\"final_sample_{sample_len_res}res.pdb\")\n",
    "\n",
    "            with open(pdb_filename, \"w\") as f:\n",
    "                f.write(pdb_string)\n",
    "            print(f\"Saved final sample PDB ({sample_len_res} residues) to {pdb_filename}\")\n",
    "\n",
    "            visualize_pdb(pdb_string)\n",
    "        else:\n",
    "            print(\"Final PDB string generation failed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating final sample: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CATH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load CATH data\n",
    "\n",
    "# Check if running in Colab to determine data loading approach\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    COLAB_ENV = False\n",
    "    drive = None\n",
    "\n",
    "\n",
    "def read_cath_data() -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df, cath_splits = pd.DataFrame(), pd.DataFrame()\n",
    "    try:\n",
    "        drive.mount(\"/content/drive\", force_remount=True)\n",
    "        data_path_prefix = \"/content/drive/MyDrive/CATH_ml_takehome/\"\n",
    "\n",
    "        # Check if path exists\n",
    "        if not os.path.exists(data_path_prefix):\n",
    "            raise FileNotFoundError(f\"Data path not found: {data_path_prefix}\")\n",
    "        print(\"Google Drive mounted successfully.\")\n",
    "\n",
    "        print(\"Reading chain_set.jsonl...\")\n",
    "        df = pd.read_json(os.path.join(data_path_prefix, \"chain_set.jsonl\"), lines=True)\n",
    "        cath_splits = pd.read_json(\n",
    "            os.path.join(data_path_prefix, \"chain_set_splits.json\"), lines=True\n",
    "        )\n",
    "\n",
    "        print(\"Read data.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Please ensure the CATH_ml_takehome folder exists in 'My Drive'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data loading: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return df, cath_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize model and data\n",
    "\n",
    "\n",
    "def initialize_model(\n",
    "    config: TrainingConfig,\n",
    ") -> tuple[ProteinDiffusionModel, DiffusionProcess, jaxtyping.PRNGKeyArray]:\n",
    "    seed = config.seed\n",
    "    rng_key = jrandom.PRNGKey(seed)\n",
    "    rng_key, params_key, dropout_key = jrandom.split(rng_key, 3)\n",
    "    rngs = nnx.Rngs(params=params_key, dropout=dropout_key)\n",
    "\n",
    "    # Initialize model and diffusion process\n",
    "    rng_key, model_key = jrandom.split(rng_key)\n",
    "    rngs = nnx.Rngs(params=model_key)\n",
    "\n",
    "    # Initialize model\n",
    "    if model_type == \"Point Cloud Protein Diffusion Model\":\n",
    "        model = PointCloudProteinDiffusionModel(config, rngs=rngs)\n",
    "    elif model_type == \"EGNN Protein Diffusion Model\":\n",
    "        # model = EGNNProteinDiffusionModel(config, rngs=rngs)\n",
    "        raise NotImplementedError(\"EGNN model not implemented yet.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "    # Initialize diffusion process\n",
    "    diffusion = DiffusionProcess(config)\n",
    "\n",
    "    return model, diffusion, rng_key\n",
    "\n",
    "\n",
    "def read_data_from_colab() -> pd.DataFrame:\n",
    "    df, cath_splits = read_cath_data()\n",
    "\n",
    "    # Function to determine split for each PDB\n",
    "    def get_split(pdb_name):\n",
    "        if (\n",
    "            \"train\" in cath_splits\n",
    "            and cath_splits.train.iloc[0]\n",
    "            and pdb_name in cath_splits.train.iloc[0]\n",
    "        ):\n",
    "            return \"train\"\n",
    "\n",
    "        if (\n",
    "            \"validation\" in cath_splits\n",
    "            and cath_splits.validation.iloc[0]\n",
    "            and pdb_name in cath_splits.validation.iloc[0]\n",
    "        ):\n",
    "            return \"validation\"\n",
    "\n",
    "        if (\n",
    "            \"test\" in cath_splits\n",
    "            and cath_splits.test.iloc[0]\n",
    "            and pdb_name in cath_splits.test.iloc[0]\n",
    "        ):\n",
    "            return \"test\"\n",
    "\n",
    "        return \"None\"\n",
    "\n",
    "    # Add split and sequence length information\n",
    "    df[\"split\"] = df.name.apply(get_split)\n",
    "    df[\"seq_len\"] = df.seq.apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Main Execution\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main(config):\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(config.seed)\n",
    "    np.random.seed(config.seed)\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "    model, diffusion, rng_key = initialize_model(config)\n",
    "\n",
    "    # Count model parameters\n",
    "    params_state = nnx.state(model, nnx.Param)\n",
    "    param_count = sum(p.size for p in jax.tree_util.tree_leaves(params_state))\n",
    "    print(f\"Total trainable parameters: {param_count:,}\")\n",
    "\n",
    "    train_loader, val_loader = None, None  # Initialize to None\n",
    "\n",
    "    # Load data if in Colab environment\n",
    "    if COLAB_ENV and drive:\n",
    "        df = read_data_from_colab()\n",
    "        # Filter based on split\n",
    "        df_train = df[df.split == \"train\"].reset_index(drop=True)\n",
    "        df_val = df[df.split == \"validation\"].reset_index(drop=True)\n",
    "\n",
    "        print(f\"Total train PDBs found: {len(df_train)}\")\n",
    "        print(f\"Total validation PDBs found: {len(df_val)}\")\n",
    "\n",
    "        # Create datasets using our custom DatasetFromDataframe class\n",
    "        train_set = DatasetFromDataframe(df_train, max_seq_length=config.max_seq_length)\n",
    "        val_set = DatasetFromDataframe(df_val, max_seq_length=config.max_seq_length)\n",
    "\n",
    "        # Get new RNG keys for data loaders\n",
    "        rng_key, train_key, val_key = jrandom.split(rng_key, 3)\n",
    "\n",
    "        # Create JAX DataLoaders only if datasets are not empty\n",
    "        if len(train_set) > 0:\n",
    "            train_loader = JAXDataLoader(\n",
    "                train_set,\n",
    "                batch_size=config.batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=False,\n",
    "                rng_key=train_key,\n",
    "            )\n",
    "            print(f\"Train DataLoader created with {len(train_set)} valid samples.\")\n",
    "\n",
    "            # Check a sample batch to validate the data pipeline\n",
    "            try:\n",
    "                sample_batch = next(iter(train_loader))\n",
    "                print(\"Sample train batch keys:\", list(sample_batch.keys()))\n",
    "                print(\"Sample positions shape:\", sample_batch[\"positions\"].shape)  # [B, N_res*4, 3]\n",
    "                print(\"Sample mask shape:\", sample_batch[\"mask\"].shape)  # [B, N_res*4]\n",
    "                print(\n",
    "                    \"Sample residue index shape:\", sample_batch[\"residue_index\"].shape\n",
    "                )  # [B, N_res*4]\n",
    "                print(\"Sample residue mask shape:\", sample_batch[\"res_mask\"].shape)  # [B, N_res]\n",
    "            except StopIteration:\n",
    "                print(\"Warning: Train DataLoader is empty after filtering.\")\n",
    "                train_loader = None\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching sample batch from train_loader: {e}\")\n",
    "                train_loader = None  # Disable loader if fetching fails\n",
    "        else:\n",
    "            print(\"Warning: Training dataset is empty after preprocessing.\")\n",
    "            train_loader = None\n",
    "\n",
    "        if len(val_set) > 0:\n",
    "            val_loader = JAXDataLoader(\n",
    "                val_set,\n",
    "                batch_size=config.eval_batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                rng_key=val_key,\n",
    "            )\n",
    "            print(f\"Validation DataLoader created with {len(val_set)} valid samples.\")\n",
    "        else:\n",
    "            print(\"Warning: Validation dataset is empty after preprocessing.\")\n",
    "            val_loader = None\n",
    "\n",
    "    else:\n",
    "        print(\"Not running in Colab or Google Drive not available. Data loading skipped.\")\n",
    "        print(\"Please adapt data loading if running locally.\")\n",
    "\n",
    "        # For local environment, you could implement alternative data loading logic here\n",
    "        # Example:\n",
    "        # local_data_path = \"path/to/local/data\"\n",
    "        # if os.path.exists(local_data_path):\n",
    "        #     # Load and process data similarly to the Colab approach\n",
    "\n",
    "        raise NotImplementedError(\"Data loading not implemented for local environment.\")\n",
    "\n",
    "    # Perform training if datasets are available\n",
    "    if train_loader and val_loader:\n",
    "        model, train_losses, val_losses = train(\n",
    "            config, model, diffusion, train_loader, val_loader, rng_key=rng_key\n",
    "        )\n",
    "\n",
    "        # Plot training and validation losses if data exists\n",
    "        if train_losses:\n",
    "            plot_filename = os.path.join(config.output_dir, \"loss_plot.png\")\n",
    "            plot_loss_curves(train_losses, val_losses, plot_filename=plot_filename)\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            \"Could not start training because the training data loader or validation data loader\"\n",
    "            \"is empty or failed to initialize.\"\n",
    "        )\n",
    "\n",
    "    return model, diffusion\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = TrainingConfig()\n",
    "    model, diffusion = main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
