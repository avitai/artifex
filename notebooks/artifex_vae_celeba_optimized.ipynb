{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifex Framework: Modern VAE Training on CelebA\n",
    "\n",
    "This notebook demonstrates the **modern Artifex framework** for developing and training generative models. It showcases:\n",
    "\n",
    "## \ud83c\udfaf Key Features Demonstrated:\n",
    "\n",
    "### Core Framework Components:\n",
    "- **Unified Factory System** - Centralized model creation with `ModelFactory`\n",
    "- **Model Zoo** - Pre-configured model templates for quick experimentation  \n",
    "- **Device Management** - Automatic GPU/CPU handling with fallback\n",
    "- **Modality System** - Image modality adapters and processors\n",
    "- **Official Trainer** - Artifex's production-ready training system\n",
    "- **Evaluation Framework** - Comprehensive metrics and benchmarking\n",
    "\n",
    "### Advanced Capabilities:\n",
    "- **JIT Compilation** - 2-5x speedup with JAX's JIT\n",
    "- **Mixed Precision** - Optional FP16/BF16 training\n",
    "- **Configuration Management** - Type-safe Pydantic configurations\n",
    "- **Checkpoint System** - Model saving/loading with versioning\n",
    "- **Logging Integration** - Metrics tracking and visualization\n",
    "\n",
    "## \ud83d\udcda Learning Objectives:\n",
    "1. How to use Artifex's factory system for model creation\n",
    "2. Proper configuration management with unified configs\n",
    "3. Integration with the official training system\n",
    "4. Modality-based data handling\n",
    "5. Comprehensive evaluation and benchmarking\n",
    "6. Best practices for production-ready code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "First, we'll set up the environment and import the necessary Artifex components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*CUDA.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# JAX and device configuration\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "\n",
    "# Configure JAX for optimal performance\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"gpu\"  # Prefer GPU if available\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_autotune_level=0\"\n",
    "\n",
    "# Artifex imports - Core components\n",
    "# Artifex imports - Benchmarks and datasets\n",
    "from artifex.benchmarks.datasets import CelebADataset\n",
    "from artifex.benchmarks.metrics import FIDMetric, ISMetric\n",
    "\n",
    "# Artifex imports - Factory and model creation\n",
    "from artifex.generative_models.core.configuration.unified import (\n",
    "    ConfigurationType,\n",
    "    DataConfiguration,\n",
    "    EvaluationConfiguration,\n",
    "    ModelConfiguration,\n",
    "    OptimizerConfiguration,\n",
    "    SchedulerConfiguration,\n",
    "    TrainingConfiguration,\n",
    ")\n",
    "from artifex.generative_models.core.device_manager import (\n",
    "    DeviceConfiguration,\n",
    "    DeviceManager,\n",
    "    MemoryStrategy,\n",
    ")\n",
    "from artifex.generative_models.factory import ModelFactory\n",
    "\n",
    "# Artifex imports - Modalities\n",
    "from artifex.generative_models.modalities.image import (\n",
    "    ImageModality,\n",
    "    ImageModalityConfig,\n",
    "    ImageRepresentation,\n",
    ")\n",
    "\n",
    "# Artifex imports - Training\n",
    "from artifex.generative_models.training.trainer import Trainer\n",
    "\n",
    "\n",
    "print(\"\u2705 Artifex Framework Loaded\")\n",
    "print(f\"\ud83d\udce6 JAX version: {jax.__version__}\")\n",
    "print(f\"\ud83d\udce6 Flax NNX version: {nnx.__version__ if hasattr(nnx, '__version__') else 'latest'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Device Management with Artifex\n",
    "\n",
    "Artifex provides automatic device management with fallback capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Artifex's Device Manager\n",
    "device_config = DeviceConfiguration(\n",
    "    memory_strategy=MemoryStrategy.BALANCED,  # Use 75% of GPU memory\n",
    "    enable_x64=False,  # Use float32 for speed\n",
    "    enable_jit=True,  # Enable JIT compilation\n",
    ")\n",
    "\n",
    "device_manager = DeviceManager(config=device_config)\n",
    "device_info = device_manager.capabilities  # Access capabilities attribute directly\n",
    "\n",
    "print(\"\ud83d\udda5\ufe0f  Device Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device Type: {device_info.device_type.value.upper()}\")\n",
    "print(f\"Device Count: {device_info.device_count}\")\n",
    "\n",
    "if device_info.device_type.value == \"gpu\":\n",
    "    print(\"\u2705 GPU Detected\")\n",
    "    if device_info.total_memory_mb:\n",
    "        print(f\"   Total Memory: {device_info.total_memory_mb / 1024:.1f} GB\")\n",
    "    if device_info.compute_capability:\n",
    "        print(f\"   Compute Capability: {device_info.compute_capability}\")\n",
    "    if device_info.cuda_version:\n",
    "        print(f\"   CUDA Version: {device_info.cuda_version}\")\n",
    "    print(f\"   Mixed Precision: {'\u2713' if device_info.supports_mixed_precision else '\u2717'}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Running on CPU - Training will be slower\")\n",
    "    print(\"   For GPU support, ensure CUDA is properly installed\")\n",
    "\n",
    "# Get more detailed device information\n",
    "detailed_info = device_manager.get_device_info()\n",
    "print(f\"\\nJAX Backend: {detailed_info['backend']}\")\n",
    "print(f\"Default Device: {detailed_info['default_device']}\")\n",
    "\n",
    "# Configure HuggingFace cache\n",
    "os.environ[\"HF_HOME\"] = \"/media/mahdi/ssd23/Data/huggingface\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/media/mahdi/ssd23/Data/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/media/mahdi/ssd23/Data/huggingface/datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration with Artifex's Unified System\n",
    "\n",
    "Artifex uses a unified configuration system with type-safe Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vae_configuration(\n",
    "    latent_dim: int = 512,\n",
    "    image_size: int = 64,\n",
    "    beta: float = 1.0,\n",
    "    learning_rate: float = 2e-4,\n",
    "    batch_size: int = 256,\n",
    "    num_epochs: int = 100,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Create comprehensive configuration for VAE training.\n",
    "\n",
    "    This demonstrates Artifex's unified configuration system where all\n",
    "    aspects of the experiment are configured in one place.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model configuration\n",
    "    model_config = ModelConfiguration(\n",
    "        name=f\"vae_celeba_{image_size}x{image_size}\",\n",
    "        type=ConfigurationType.MODEL,\n",
    "        model_class=\"artifex.generative_models.models.vae.base.VAE\",\n",
    "        # Architecture parameters\n",
    "        input_dim=(image_size, image_size, 3),\n",
    "        output_dim=latent_dim,\n",
    "        hidden_dims=[64, 128, 256, 512],\n",
    "        activation=\"relu\",\n",
    "        # VAE-specific parameters\n",
    "        parameters={\n",
    "            \"encoder_type\": \"cnn\",  # CNN encoder for images\n",
    "            \"decoder_type\": \"cnn\",  # CNN decoder for images\n",
    "            \"kl_weight\": beta,\n",
    "            \"reconstruction_loss\": \"mse\",\n",
    "            \"beta\": beta,\n",
    "            \"decoder_dims\": [512, 256, 128, 64],  # Reverse of encoder\n",
    "        },\n",
    "        # Metadata for tracking\n",
    "        metadata={\n",
    "            \"modality\": \"image\",\n",
    "            \"dataset\": \"celeba\",\n",
    "            \"image_resolution\": (image_size, image_size),\n",
    "            \"color_channels\": 3,\n",
    "            \"architecture\": \"convolutional\",\n",
    "            \"variant\": \"beta-vae\" if beta != 1.0 else \"standard-vae\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Optimizer configuration (must be created first)\n",
    "    optimizer_config = OptimizerConfiguration(\n",
    "        name=\"adamw_optimizer\",\n",
    "        type=ConfigurationType.OPTIMIZER,\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-5,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        eps=1e-8,\n",
    "    )\n",
    "\n",
    "    # Scheduler configuration (optional)\n",
    "    scheduler_config = SchedulerConfiguration(\n",
    "        name=\"cosine_scheduler\",\n",
    "        type=ConfigurationType.SCHEDULER,\n",
    "        scheduler_type=\"cosine\",\n",
    "        warmup_steps=1000,\n",
    "        min_lr_ratio=0.01,\n",
    "    )\n",
    "\n",
    "    # Training configuration (now with optimizer)\n",
    "    training_config = TrainingConfiguration(\n",
    "        name=\"vae_celeba_training\",\n",
    "        type=ConfigurationType.TRAINING,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        gradient_clip_norm=1.0,\n",
    "        optimizer=optimizer_config,  # Pass the optimizer config object\n",
    "        scheduler=scheduler_config,  # Pass the scheduler config object\n",
    "        checkpoint_dir=Path(\"./checkpoints\"),\n",
    "        save_frequency=1000,\n",
    "    )\n",
    "\n",
    "    # Data configuration\n",
    "    data_config = DataConfiguration(\n",
    "        name=\"celeba_data\",\n",
    "        type=ConfigurationType.DATA,\n",
    "        dataset_name=\"celeba\",\n",
    "        data_dir=Path(\"/media/mahdi/ssd23/Data/huggingface\"),\n",
    "        split=\"train\",\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Evaluation configuration\n",
    "    eval_config = EvaluationConfiguration(\n",
    "        name=\"vae_celeba_evaluation\",\n",
    "        type=ConfigurationType.EVALUATION,\n",
    "        metrics=[\"fid\", \"is\", \"reconstruction_mse\", \"kl_divergence\"],\n",
    "        metric_params={\n",
    "            \"fid\": {\"mock_inception\": True},\n",
    "            \"is\": {\"splits\": 10},\n",
    "        },\n",
    "        eval_batch_size=32,\n",
    "        num_eval_samples=1000,\n",
    "    )\n",
    "\n",
    "    # Image modality configuration\n",
    "    modality_config = ImageModalityConfig(\n",
    "        representation=ImageRepresentation.RGB,\n",
    "        height=image_size,\n",
    "        width=image_size,\n",
    "        channels=3,\n",
    "        normalize=True,\n",
    "        augmentation=False,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"model\": model_config,\n",
    "        \"training\": training_config,\n",
    "        \"optimizer\": optimizer_config,\n",
    "        \"scheduler\": scheduler_config,\n",
    "        \"data\": data_config,\n",
    "        \"evaluation\": eval_config,\n",
    "        \"modality\": modality_config,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create all configurations\n",
    "configs = create_vae_configuration(\n",
    "    latent_dim=512,\n",
    "    image_size=64,\n",
    "    beta=1.0,\n",
    "    learning_rate=2e-4,\n",
    "    batch_size=256,\n",
    "    num_epochs=100,\n",
    ")\n",
    "\n",
    "print(\"\ud83d\udccb Configuration Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {configs['model'].name}\")\n",
    "print(\"Architecture: CNN Encoder-Decoder\")\n",
    "print(f\"Latent Dimension: {configs['model'].output_dim}\")\n",
    "print(f\"Input Shape: {configs['model'].input_dim}\")\n",
    "print(f\"Hidden Layers: {configs['model'].hidden_dims}\")\n",
    "print(\"Training:\")\n",
    "print(f\"  Epochs: {configs['training'].num_epochs}\")\n",
    "print(f\"  Batch Size: {configs['training'].batch_size}\")\n",
    "print(f\"  Learning Rate: {configs['optimizer'].learning_rate}\")\n",
    "print(f\"  Optimizer: {configs['optimizer'].optimizer_type}\")\n",
    "print(f\"  Scheduler: {configs['scheduler'].scheduler_type}\")\n",
    "print(f\"Evaluation Metrics: {', '.join(configs['evaluation'].metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Creation with Artifex's Factory System\n",
    "\n",
    "Artifex provides multiple ways to create models:\n",
    "1. Using the factory with custom configuration\n",
    "2. Using pre-configured models from the Model Zoo\n",
    "3. Direct instantiation with modality adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RNGs for reproducibility\n",
    "rngs = nnx.Rngs(42)\n",
    "\n",
    "print(\"\ud83d\udd28 Creating VAE Model with Artifex Factory\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Using the factory system with modality (recommended)\n",
    "factory = ModelFactory()\n",
    "\n",
    "# Create the model with factory - modality will apply image-specific adapters\n",
    "model = factory.create(\n",
    "    config=configs[\"model\"],\n",
    "    modality=\"image\",  # Applies image modality adapters\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Model created: {type(model).__name__}\")\n",
    "print(f\"   Encoder: {type(model.encoder).__name__} (CNN-based)\")\n",
    "print(f\"   Decoder: {type(model.decoder).__name__} (CNN-based)\")\n",
    "print(f\"   Latent dim: {model.latent_dim}\")\n",
    "\n",
    "# Alternative Method 2: Using the factory's create method directly\n",
    "# model = factory.create(\n",
    "#     config=configs[\"model\"],\n",
    "#     modality=\"image\",\n",
    "#     rngs=rngs,\n",
    "# )\n",
    "\n",
    "# Alternative Method 3: Check if Model Zoo has pre-configured models\n",
    "# zoo = ModelZoo()\n",
    "# try:\n",
    "#     zoo_config = zoo.get_config(\"vae_celeba_64x64\")\n",
    "#     model = zoo.create_model(\"vae_celeba_64x64\", rngs=rngs)\n",
    "#     print(\"   Using pre-configured model from Zoo\")\n",
    "# except KeyError:\n",
    "#     print(\"   No pre-configured model found in Zoo, using custom config\")\n",
    "\n",
    "# Create image modality for data processing (separate from model adapters)\n",
    "image_modality = ImageModality(config=configs[\"modality\"], rngs=rngs)\n",
    "\n",
    "print(\"\\n\ud83d\udcf8 Image Modality Configured:\")\n",
    "print(f\"   Representation: {configs['modality'].representation.value}\")\n",
    "print(f\"   Resolution: {configs['modality'].height}x{configs['modality'].width}\")\n",
    "print(f\"   Channels: {configs['modality'].channels}\")\n",
    "print(f\"   Adapter available: {hasattr(image_modality, 'get_adapter')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Setup with Artifex's Data System\n",
    "\n",
    "Artifex provides integrated dataset handling with automatic preprocessing and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_SIZE = 10000  # Start with smaller size for quick testing\n",
    "# DATASET_SIZE = 100000  # Use this for full training\n",
    "\n",
    "print(\"\ud83d\udcca Loading CelebA Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create dataset using Artifex's benchmark datasets\n",
    "train_dataset = CelebADataset(\n",
    "    data_path=configs[\"data\"].data_dir,\n",
    "    num_samples=DATASET_SIZE,\n",
    "    image_size=configs[\"modality\"].height,\n",
    "    include_attributes=True,\n",
    "    split=\"train\",\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "val_dataset = CelebADataset(\n",
    "    data_path=configs[\"data\"].data_dir,\n",
    "    num_samples=min(1000, DATASET_SIZE // 10),  # 10% for validation\n",
    "    image_size=configs[\"modality\"].height,\n",
    "    include_attributes=True,\n",
    "    split=\"valid\",\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "print(\"\u2705 Datasets loaded:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Image size: {configs['modality'].height}x{configs['modality'].width}\")\n",
    "print(\"   Attributes included: \u2713\")\n",
    "\n",
    "# Create data loaders (Artifex style)\n",
    "\n",
    "\n",
    "def create_data_loader(dataset, batch_size: int, shuffle: bool = True):\n",
    "    \"\"\"Create a data loader for the dataset.\"\"\"\n",
    "\n",
    "    def data_loader():\n",
    "        num_samples = len(dataset)\n",
    "        indices = jnp.arange(num_samples)\n",
    "\n",
    "        if shuffle:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "            indices = jax.random.permutation(key, indices)\n",
    "\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            batch = dataset.get_batch(\n",
    "                batch_size=min(batch_size, num_samples - start_idx), start_idx=start_idx\n",
    "            )\n",
    "            yield batch\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "train_loader = create_data_loader(\n",
    "    train_dataset, batch_size=configs[\"training\"].batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_data_loader(\n",
    "    val_dataset, batch_size=configs[\"evaluation\"].eval_batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Display sample images\n",
    "print(\"\\n\ud83d\uddbc\ufe0f Sample Images from Dataset:\")\n",
    "sample_batch = train_dataset.get_batch(batch_size=8, start_idx=0)\n",
    "sample_images = sample_batch[\"images\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
    "for i in range(8):\n",
    "    axes[i].imshow(np.clip(sample_images[i], 0, 1))\n",
    "    axes[i].axis(\"off\")\n",
    "plt.suptitle(\"CelebA Sample Images\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with Artifex's Official Trainer\n",
    "\n",
    "Artifex provides a production-ready `Trainer` class that handles:\n",
    "- Automatic JIT compilation\n",
    "- Gradient accumulation\n",
    "- Checkpointing\n",
    "- Metrics logging\n",
    "- Early stopping\n",
    "- Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer using Artifex's configuration\n",
    "\n",
    "\n",
    "def create_optimizer(config: OptimizerConfiguration):\n",
    "    \"\"\"Create optimizer from configuration.\"\"\"\n",
    "    if config.optimizer_type == \"adamw\":\n",
    "        return optax.adamw(\n",
    "            learning_rate=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            b1=config.beta1,\n",
    "            b2=config.beta2,\n",
    "            eps=config.eps,\n",
    "        )\n",
    "    elif config.optimizer_type == \"adam\":\n",
    "        return optax.adam(\n",
    "            learning_rate=config.learning_rate,\n",
    "            b1=config.beta1,\n",
    "            b2=config.beta2,\n",
    "            eps=config.eps,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {config.optimizer_type}\")\n",
    "\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = create_optimizer(configs[\"optimizer\"])\n",
    "\n",
    "# Define custom loss function for VAE\n",
    "\n",
    "\n",
    "def vae_loss_fn(model, batch, training: bool = True):\n",
    "    \"\"\"VAE loss function with reconstruction and KL divergence.\n",
    "\n",
    "    Note: We don't use @nnx.jit decorator here as the function will be JIT-compiled\n",
    "    by the trainer or custom training loop.\n",
    "    \"\"\"\n",
    "    images = batch[\"images\"]\n",
    "\n",
    "    # Forward pass\n",
    "    mean, log_var = model.encode(images)\n",
    "    z = model.reparameterize(mean, log_var) if training else mean\n",
    "    reconstructed = model.decode(z)\n",
    "\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = jnp.mean((images - reconstructed) ** 2)\n",
    "\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * jnp.mean(1 + log_var - mean**2 - jnp.exp(log_var))\n",
    "\n",
    "    # Total loss with beta weighting\n",
    "    beta = configs[\"model\"].parameters.get(\"beta\", 1.0)\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "\n",
    "    # Return loss and metrics\n",
    "    metrics = {\n",
    "        \"loss\": total_loss,\n",
    "        \"reconstruction_loss\": recon_loss,\n",
    "        \"kl_loss\": kl_loss,\n",
    "        \"beta\": beta,\n",
    "    }\n",
    "\n",
    "    return total_loss, metrics\n",
    "\n",
    "\n",
    "# Initialize Artifex Trainer\n",
    "print(\"\ud83d\ude80 Initializing Artifex Trainer\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    training_config=configs[\"training\"],\n",
    "    optimizer=optimizer,\n",
    "    train_data_loader=train_loader,\n",
    "    val_data_loader=val_loader,\n",
    "    loss_fn=vae_loss_fn,\n",
    "    rng=jax.random.PRNGKey(42),\n",
    "    workdir=\"./vae_celeba_checkpoints\",\n",
    "    checkpoint_dir=\"./vae_celeba_checkpoints\",\n",
    "    save_interval=configs[\"training\"].save_frequency,\n",
    ")\n",
    "\n",
    "print(\"\u2705 Trainer initialized with:\")\n",
    "print(f\"   Model: {configs['model'].name}\")\n",
    "print(f\"   Optimizer: {configs['optimizer'].optimizer_type}\")\n",
    "print(f\"   Learning rate: {configs['optimizer'].learning_rate}\")\n",
    "print(f\"   Batch size: {configs['training'].batch_size}\")\n",
    "print(f\"   Checkpointing: Every {configs['training'].save_frequency} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Custom Training Loop with JIT Compilation\n",
    "\n",
    "For demonstration, we'll also show a custom optimized training loop that showcases JAX's JIT compilation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedVAETrainer:\n",
    "    \"\"\"Custom optimized VAE trainer showcasing JAX JIT compilation.\"\"\"\n",
    "\n",
    "    def __init__(self, model, config: ModelConfiguration, learning_rate: float = 2e-4):\n",
    "        \"\"\"Initialize the optimized trainer.\"\"\"\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "\n",
    "        # Create optimizer\n",
    "        tx = optax.adam(learning_rate)\n",
    "        self.optimizer = nnx.Optimizer(model, tx)\n",
    "\n",
    "        # Create JIT-compiled training step\n",
    "        self.train_step = self._create_train_step()\n",
    "        self.eval_step = self._create_eval_step()\n",
    "\n",
    "    def _create_train_step(self):\n",
    "        \"\"\"Create JIT-compiled training step.\"\"\"\n",
    "\n",
    "        @nnx.jit\n",
    "        def train_step(model, optimizer, images):\n",
    "            \"\"\"Single training step (JIT-compiled).\"\"\"\n",
    "\n",
    "            def loss_fn(model):\n",
    "                # Forward pass\n",
    "                mean, log_var = model.encode(images)\n",
    "                z = model.reparameterize(mean, log_var)\n",
    "                reconstructed = model.decode(z)\n",
    "\n",
    "                # Losses\n",
    "                recon_loss = jnp.mean((images - reconstructed) ** 2)\n",
    "                kl_loss = -0.5 * jnp.mean(1 + log_var - mean**2 - jnp.exp(log_var))\n",
    "\n",
    "                # Total loss\n",
    "                beta = self.config.parameters.get(\"kl_weight\", 1.0)\n",
    "                total_loss = recon_loss + beta * kl_loss\n",
    "\n",
    "                return total_loss, {\n",
    "                    \"total_loss\": total_loss,\n",
    "                    \"reconstruction_loss\": recon_loss,\n",
    "                    \"kl_loss\": kl_loss,\n",
    "                }\n",
    "\n",
    "            # Compute gradients and update\n",
    "            (loss, metrics), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n",
    "            optimizer.update(grads)\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        return train_step\n",
    "\n",
    "    def _create_eval_step(self):\n",
    "        \"\"\"Create JIT-compiled evaluation step.\"\"\"\n",
    "\n",
    "        @nnx.jit\n",
    "        def eval_step(model, images):\n",
    "            \"\"\"Single evaluation step (JIT-compiled).\"\"\"\n",
    "            # Forward pass (no reparameterization for eval)\n",
    "            mean, log_var = model.encode(images)\n",
    "            reconstructed = model.decode(mean)  # Use mean directly\n",
    "\n",
    "            # Losses\n",
    "            recon_loss = jnp.mean((images - reconstructed) ** 2)\n",
    "            kl_loss = -0.5 * jnp.mean(1 + log_var - mean**2 - jnp.exp(log_var))\n",
    "\n",
    "            beta = self.config.parameters.get(\"kl_weight\", 1.0)\n",
    "            total_loss = recon_loss + beta * kl_loss\n",
    "\n",
    "            return {\n",
    "                \"total_loss\": total_loss,\n",
    "                \"reconstruction_loss\": recon_loss,\n",
    "                \"kl_loss\": kl_loss,\n",
    "            }\n",
    "\n",
    "        return eval_step\n",
    "\n",
    "    def train_epoch(self, dataset, batch_size: int = 32):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        epoch_metrics = {\"total_loss\": 0, \"reconstruction_loss\": 0, \"kl_loss\": 0}\n",
    "        num_batches = 0\n",
    "\n",
    "        # Training loop\n",
    "        for start_idx in range(0, len(dataset), batch_size):\n",
    "            batch = dataset.get_batch(\n",
    "                batch_size=min(batch_size, len(dataset) - start_idx), start_idx=start_idx\n",
    "            )\n",
    "            images = batch[\"images\"]\n",
    "\n",
    "            # Training step (JIT-compiled)\n",
    "            metrics = self.train_step(self.model, self.optimizer, images)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            for key in epoch_metrics:\n",
    "                epoch_metrics[key] += float(metrics[key])\n",
    "            num_batches += 1\n",
    "\n",
    "        # Average metrics\n",
    "        for key in epoch_metrics:\n",
    "            epoch_metrics[key] /= num_batches\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def evaluate(self, dataset, batch_size: int = 32):\n",
    "        \"\"\"Evaluate on a dataset.\"\"\"\n",
    "        eval_metrics = {\"total_loss\": 0, \"reconstruction_loss\": 0, \"kl_loss\": 0}\n",
    "        num_batches = 0\n",
    "\n",
    "        for start_idx in range(0, len(dataset), batch_size):\n",
    "            batch = dataset.get_batch(\n",
    "                batch_size=min(batch_size, len(dataset) - start_idx), start_idx=start_idx\n",
    "            )\n",
    "            images = batch[\"images\"]\n",
    "\n",
    "            # Evaluation step (JIT-compiled)\n",
    "            metrics = self.eval_step(self.model, images)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            for key in eval_metrics:\n",
    "                eval_metrics[key] += float(metrics[key])\n",
    "            num_batches += 1\n",
    "\n",
    "        # Average metrics\n",
    "        for key in eval_metrics:\n",
    "            eval_metrics[key] /= num_batches\n",
    "\n",
    "        return eval_metrics\n",
    "\n",
    "\n",
    "# Create custom trainer for demonstration\n",
    "custom_trainer = OptimizedVAETrainer(\n",
    "    model, configs[\"model\"], learning_rate=configs[\"optimizer\"].learning_rate\n",
    ")\n",
    "\n",
    "print(\"\u26a1 Custom Optimized Trainer Created\")\n",
    "print(\"   Features:\")\n",
    "print(\"   - JIT-compiled training steps\")\n",
    "print(\"   - Separate train/eval paths\")\n",
    "print(\"   - Efficient batch processing\")\n",
    "print(\"   - Automatic gradient computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Training Execution\n",
    "\n",
    "Now let's train the model. You can choose between:\n",
    "1. Artifex's official Trainer (recommended for production)\n",
    "2. Custom optimized trainer (for learning and experimentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training with custom trainer for demonstration\n",
    "NUM_EPOCHS = 10  # Reduce for quick demo, use 100+ for full training\n",
    "BATCH_SIZE = configs[\"training\"].batch_size\n",
    "\n",
    "print(\"\ud83d\ude80 Starting Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {configs['optimizer'].learning_rate}\")\n",
    "print(f\"Beta (KL weight): {configs['model'].parameters.get('beta', 1.0)}\")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"reconstruction_loss\": [],\n",
    "    \"kl_loss\": [],\n",
    "    \"epoch_times\": [],\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_metrics = custom_trainer.train_epoch(train_dataset, BATCH_SIZE)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_metrics = custom_trainer.evaluate(val_dataset, configs[\"evaluation\"].eval_batch_size)\n",
    "\n",
    "    # Record metrics\n",
    "    history[\"train_loss\"].append(train_metrics[\"total_loss\"])\n",
    "    history[\"val_loss\"].append(val_metrics[\"total_loss\"])\n",
    "    history[\"reconstruction_loss\"].append(train_metrics[\"reconstruction_loss\"])\n",
    "    history[\"kl_loss\"].append(train_metrics[\"kl_loss\"])\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    history[\"epoch_times\"].append(epoch_time)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"\\n\ud83d\udcc8 Epoch {epoch + 1}/{NUM_EPOCHS} (Time: {epoch_time:.1f}s)\")\n",
    "    print(f\"   Train Loss: {train_metrics['total_loss']:.4f}\")\n",
    "    print(f\"   Val Loss: {val_metrics['total_loss']:.4f}\")\n",
    "    print(f\"   Recon Loss: {train_metrics['reconstruction_loss']:.4f}\")\n",
    "    print(f\"   KL Loss: {train_metrics['kl_loss']:.4f}\")\n",
    "\n",
    "total_time = sum(history[\"epoch_times\"])\n",
    "print(f\"\\n\u2705 Training completed in {total_time:.1f}s ({total_time / 60:.1f} minutes)\")\n",
    "print(f\"   Average epoch time: {np.mean(history['epoch_times']):.1f}s\")\n",
    "print(f\"   Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final val loss: {history['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Visualization\n",
    "\n",
    "Let's visualize the training progress and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: dict[str, list[float]]):\n",
    "    \"\"\"Plot comprehensive training history.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Total loss (train vs val)\n",
    "    axes[0, 0].plot(epochs, history[\"train_loss\"], \"b-\", linewidth=2, label=\"Train Loss\")\n",
    "    axes[0, 0].plot(epochs, history[\"val_loss\"], \"r--\", linewidth=2, label=\"Val Loss\")\n",
    "    axes[0, 0].set_xlabel(\"Epoch\")\n",
    "    axes[0, 0].set_ylabel(\"Loss\")\n",
    "    axes[0, 0].set_title(\"Training vs Validation Loss\")\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Reconstruction loss\n",
    "    axes[0, 1].plot(epochs, history[\"reconstruction_loss\"], \"g-\", linewidth=2)\n",
    "    axes[0, 1].set_xlabel(\"Epoch\")\n",
    "    axes[0, 1].set_ylabel(\"MSE\")\n",
    "    axes[0, 1].set_title(\"Reconstruction Loss\")\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # KL divergence\n",
    "    axes[1, 0].plot(epochs, history[\"kl_loss\"], \"m-\", linewidth=2)\n",
    "    axes[1, 0].set_xlabel(\"Epoch\")\n",
    "    axes[1, 0].set_ylabel(\"KL Divergence\")\n",
    "    axes[1, 0].set_title(\"KL Divergence Loss\")\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Epoch times\n",
    "    axes[1, 1].bar(epochs, history[\"epoch_times\"], color=\"orange\", alpha=0.7)\n",
    "    axes[1, 1].set_xlabel(\"Epoch\")\n",
    "    axes[1, 1].set_ylabel(\"Time (seconds)\")\n",
    "    axes[1, 1].set_title(\"Training Time per Epoch\")\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"VAE Training Progress\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\n\ud83d\udcca Training Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final val loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Final reconstruction loss: {history['reconstruction_loss'][-1]:.4f}\")\n",
    "    print(f\"Final KL loss: {history['kl_loss'][-1]:.4f}\")\n",
    "    print(f\"Average epoch time: {np.mean(history['epoch_times']):.2f}s\")\n",
    "    print(f\"Total training time: {sum(history['epoch_times']):.2f}s\")\n",
    "\n",
    "    # Check for overfitting\n",
    "    if history[\"val_loss\"][-1] > history[\"train_loss\"][-1] * 1.2:\n",
    "        print(\"\\n\u26a0\ufe0f  Warning: Possible overfitting detected (val loss > 1.2x train loss)\")\n",
    "\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation with Artifex's Benchmark Suite\n",
    "\n",
    "Artifex provides comprehensive evaluation metrics through its benchmark framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for generation and reconstruction\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def generate_samples(model, num_samples: int, latent_dim: int, key):\n",
    "    \"\"\"Generate new samples from the latent space.\"\"\"\n",
    "    z = jax.random.normal(key, (num_samples, latent_dim))\n",
    "    return model.decode(z)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def reconstruct_images(model, images):\n",
    "    \"\"\"Reconstruct images through the VAE.\"\"\"\n",
    "    mean, log_var = model.encode(images)\n",
    "    z = model.reparameterize(mean, log_var)\n",
    "    reconstructed = model.decode(z)\n",
    "    return reconstructed, mean, log_var\n",
    "\n",
    "\n",
    "def evaluate_model_comprehensive(model, dataset, eval_config, rngs):\n",
    "    \"\"\"Comprehensive model evaluation using Artifex's metrics.\"\"\"\n",
    "\n",
    "    print(\"\\n\ud83d\udd0d Comprehensive Model Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Generate samples for evaluation\n",
    "    num_eval_samples = min(64, len(dataset))\n",
    "    key = rngs.sample()\n",
    "    generated = generate_samples(model, num_eval_samples, model.latent_dim, key)\n",
    "\n",
    "    # Get real images for comparison\n",
    "    real_batch = dataset.get_batch(batch_size=num_eval_samples, start_idx=0)\n",
    "    real_images = real_batch[\"images\"]\n",
    "\n",
    "    # Reconstruct images\n",
    "    reconstructed, mean, log_var = reconstruct_images(model, real_images)\n",
    "\n",
    "    # Calculate basic metrics\n",
    "    recon_mse = float(jnp.mean((real_images - reconstructed) ** 2))\n",
    "    kl_div = float(-0.5 * jnp.mean(1 + log_var - mean**2 - jnp.exp(log_var)))\n",
    "\n",
    "    # Calculate FID score (using mock for demo)\n",
    "    fid_metric = FIDMetric(rngs=rngs, config=eval_config)\n",
    "    fid_result = fid_metric.compute(real_images, generated)\n",
    "    fid_score = (\n",
    "        fid_result.get(\"fid\", fid_result) if isinstance(fid_result, dict) else float(fid_result)\n",
    "    )\n",
    "\n",
    "    # Calculate Inception Score\n",
    "    is_metric = ISMetric(rngs=rngs, config=eval_config)\n",
    "    is_result = is_metric.compute(generated)\n",
    "    is_score = is_result.get(\"is\", is_result) if isinstance(is_result, dict) else float(is_result)\n",
    "\n",
    "    # Latent space statistics\n",
    "    latent_mean = float(jnp.mean(mean))\n",
    "    latent_std = float(jnp.mean(jnp.exp(0.5 * log_var)))\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        \"reconstruction_mse\": recon_mse,\n",
    "        \"kl_divergence\": kl_div,\n",
    "        \"fid_score\": fid_score,\n",
    "        \"inception_score\": is_score,\n",
    "        \"latent_mean\": latent_mean,\n",
    "        \"latent_std\": latent_std,\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\ud83d\udcc8 Evaluation Results:\")\n",
    "    print(f\"   Reconstruction MSE: {results['reconstruction_mse']:.6f}\")\n",
    "    print(f\"   KL Divergence: {results['kl_divergence']:.4f}\")\n",
    "    print(f\"   FID Score: {results['fid_score']:.2f} (lower is better)\")\n",
    "    print(f\"   Inception Score: {results['inception_score']:.2f} (higher is better)\")\n",
    "    print(\"   Latent Space:\")\n",
    "    print(f\"      Mean: {results['latent_mean']:.4f}\")\n",
    "    print(f\"      Std: {results['latent_std']:.4f}\")\n",
    "\n",
    "    # Quality assessment\n",
    "    print(\"\\n\ud83c\udfaf Quality Assessment:\")\n",
    "    if results[\"reconstruction_mse\"] < 0.01:\n",
    "        print(\"   \u2705 Excellent reconstruction quality\")\n",
    "    elif results[\"reconstruction_mse\"] < 0.05:\n",
    "        print(\"   \u2713 Good reconstruction quality\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Reconstruction could be improved\")\n",
    "\n",
    "    if results[\"fid_score\"] < 50:\n",
    "        print(\"   \u2705 Excellent generation quality (FID < 50)\")\n",
    "    elif results[\"fid_score\"] < 100:\n",
    "        print(\"   \u2713 Good generation quality (FID < 100)\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Generation quality needs improvement\")\n",
    "\n",
    "    return results, generated, reconstructed\n",
    "\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "eval_results, generated_images, reconstructed_images = evaluate_model_comprehensive(\n",
    "    model, val_dataset, configs[\"evaluation\"], rngs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Visualization\n",
    "\n",
    "Visualize reconstruction quality, generation quality, and latent space properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_vae_results(dataset, model, rngs, num_samples: int = 8):\n",
    "    \"\"\"Comprehensive visualization of VAE results.\"\"\"\n",
    "\n",
    "    print(\"\\n\ud83c\udfa8 Generating Comprehensive Visualizations\")\n",
    "\n",
    "    # Get real images\n",
    "    batch = dataset.get_batch(batch_size=num_samples, start_idx=0)\n",
    "    real_images = batch[\"images\"]\n",
    "\n",
    "    # Reconstruct\n",
    "    reconstructed, _, _ = reconstruct_images(model, real_images)\n",
    "\n",
    "    # Generate new samples\n",
    "    key = rngs.sample()\n",
    "    generated = generate_samples(model, num_samples, model.latent_dim, key)\n",
    "\n",
    "    # Interpolation in latent space\n",
    "    key1, key2 = jax.random.split(rngs.sample())\n",
    "    z1 = jax.random.normal(key1, (1, model.latent_dim))\n",
    "    z2 = jax.random.normal(key2, (1, model.latent_dim))\n",
    "    alphas = jnp.linspace(0, 1, num_samples)\n",
    "    interpolated = jnp.array([model.decode((1 - a) * z1 + a * z2)[0] for a in alphas])\n",
    "\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(4, num_samples, figsize=(num_samples * 2, 8))\n",
    "\n",
    "    titles = [\"Original\", \"Reconstructed\", \"Generated\", \"Interpolated\"]\n",
    "    images_list = [real_images, reconstructed, generated, interpolated]\n",
    "\n",
    "    for row, (title, images) in enumerate(zip(titles, images_list)):\n",
    "        for col in range(num_samples):\n",
    "            img = np.clip(images[col], 0, 1)\n",
    "            axes[row, col].imshow(img)\n",
    "            axes[row, col].axis(\"off\")\n",
    "\n",
    "            if col == 0:\n",
    "                axes[row, col].set_ylabel(title, fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "            # Add reconstruction error for reconstructed images\n",
    "            if row == 1:\n",
    "                mse = float(jnp.mean((real_images[col] - reconstructed[col]) ** 2))\n",
    "                axes[row, col].set_title(f\"MSE: {mse:.3f}\", fontsize=8)\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"VAE Results: Reconstruction, Generation, and Interpolation\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize results\n",
    "visualize_vae_results(val_dataset, model, rngs, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Latent Space Analysis\n",
    "\n",
    "Analyze the structure and properties of the learned latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_latent_space(model, dataset, num_samples: int = 1000):\n",
    "    \"\"\"Analyze the learned latent space structure.\"\"\"\n",
    "\n",
    "    print(\"\\n\ud83d\udd2c Analyzing Latent Space Structure\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Encode a batch of images\n",
    "    num_samples = min(num_samples, len(dataset))\n",
    "    batch = dataset.get_batch(batch_size=num_samples, start_idx=0)\n",
    "    images = batch[\"images\"]\n",
    "\n",
    "    mean, log_var = model.encode(images)\n",
    "    z = model.reparameterize(mean, log_var)\n",
    "\n",
    "    # Statistics\n",
    "    z_mean = jnp.mean(z, axis=0)\n",
    "    z_std = jnp.std(z, axis=0)\n",
    "\n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "    # 1. Distribution of latent means\n",
    "    axes[0, 0].hist(z_mean, bins=30, alpha=0.7, color=\"blue\", edgecolor=\"black\")\n",
    "    axes[0, 0].set_xlabel(\"Mean value\")\n",
    "    axes[0, 0].set_ylabel(\"Frequency\")\n",
    "    axes[0, 0].set_title(\"Distribution of Latent Dimension Means\")\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axvline(0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # 2. Distribution of latent stds\n",
    "    axes[0, 1].hist(z_std, bins=30, alpha=0.7, color=\"green\", edgecolor=\"black\")\n",
    "    axes[0, 1].set_xlabel(\"Std deviation\")\n",
    "    axes[0, 1].set_ylabel(\"Frequency\")\n",
    "    axes[0, 1].set_title(\"Distribution of Latent Dimension Stds\")\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].axvline(1, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # 3. Active dimensions (std > threshold)\n",
    "    threshold = 0.1\n",
    "    active_dims = jnp.sum(z_std > threshold)\n",
    "    axes[0, 2].bar(\n",
    "        [\"Active\", \"Inactive\"],\n",
    "        [int(active_dims), len(z_std) - int(active_dims)],\n",
    "        color=[\"green\", \"gray\"],\n",
    "    )\n",
    "    axes[0, 2].set_title(f\"Active Dimensions (std > {threshold})\")\n",
    "    axes[0, 2].set_ylabel(\"Count\")\n",
    "\n",
    "    # 4. Correlation matrix (subset)\n",
    "    subset_size = min(50, z.shape[1])\n",
    "    z_subset = z[:, :subset_size]\n",
    "    corr = jnp.corrcoef(z_subset.T)\n",
    "\n",
    "    im = axes[1, 0].imshow(corr, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    axes[1, 0].set_title(f\"Latent Correlations (first {subset_size} dims)\")\n",
    "    axes[1, 0].set_xlabel(\"Dimension\")\n",
    "    axes[1, 0].set_ylabel(\"Dimension\")\n",
    "    plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "    # 5. 2D PCA projection\n",
    "    if z.shape[0] > 2:\n",
    "        # Simple 2D projection using first two principal components\n",
    "        z_centered = z - jnp.mean(z, axis=0)\n",
    "        cov = jnp.cov(z_centered.T)\n",
    "        eigenvalues, eigenvectors = jnp.linalg.eigh(cov)\n",
    "\n",
    "        # Sort by eigenvalues\n",
    "        idx = jnp.argsort(eigenvalues)[::-1]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "        # Project to 2D\n",
    "        z_2d = z_centered @ eigenvectors[:, :2]\n",
    "\n",
    "        axes[1, 1].scatter(z_2d[:, 0], z_2d[:, 1], alpha=0.5, s=10)\n",
    "        axes[1, 1].set_title(\"2D PCA Projection of Latent Space\")\n",
    "        axes[1, 1].set_xlabel(\"PC1\")\n",
    "        axes[1, 1].set_ylabel(\"PC2\")\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. KL divergence per dimension\n",
    "    kl_per_dim = 0.5 * (mean**2 + jnp.exp(log_var) - log_var - 1)\n",
    "    mean_kl_per_dim = jnp.mean(kl_per_dim, axis=0)\n",
    "\n",
    "    axes[1, 2].plot(mean_kl_per_dim[:100], alpha=0.7)  # Plot first 100 dims\n",
    "    axes[1, 2].set_title(\"KL Divergence per Dimension\")\n",
    "    axes[1, 2].set_xlabel(\"Dimension\")\n",
    "    axes[1, 2].set_ylabel(\"Mean KL\")\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"Latent Space Analysis\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\n\ud83d\udcca Latent Space Statistics:\")\n",
    "    print(f\"   Latent dimension: {z.shape[1]}\")\n",
    "    print(f\"   Active dimensions: {int(active_dims)} / {z.shape[1]}\")\n",
    "    print(f\"   Mean of means: {jnp.mean(z_mean):.4f} (target: ~0)\")\n",
    "    print(f\"   Mean of stds: {jnp.mean(z_std):.4f} (target: ~1)\")\n",
    "    print(f\"   Max correlation: {jnp.max(jnp.abs(corr - jnp.eye(subset_size))):.4f}\")\n",
    "    print(f\"   Total KL divergence: {jnp.sum(mean_kl_per_dim):.4f}\")\n",
    "\n",
    "    # Quality assessment\n",
    "    print(\"\\n\ud83c\udfaf Latent Space Quality:\")\n",
    "    if abs(jnp.mean(z_mean)) < 0.1:\n",
    "        print(\"   \u2705 Well-centered latent space\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Latent space not well-centered\")\n",
    "\n",
    "    if 0.8 < jnp.mean(z_std) < 1.2:\n",
    "        print(\"   \u2705 Good variance in latent space\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Variance issues in latent space\")\n",
    "\n",
    "    if jnp.max(jnp.abs(corr - jnp.eye(subset_size))) < 0.3:\n",
    "        print(\"   \u2705 Good disentanglement (low correlation)\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f High correlation between latent dimensions\")\n",
    "\n",
    "\n",
    "# Analyze the latent space\n",
    "analyze_latent_space(model, val_dataset, num_samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Persistence with Artifex\n",
    "\n",
    "Artifex provides utilities for saving and loading trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_artifex(model, config: ModelConfiguration, path: str = \"vae_celeba_artifex.pkl\"):\n",
    "    \"\"\"Save model using Artifex's recommended approach.\"\"\"\n",
    "\n",
    "    print(\"\\n\ud83d\udcbe Saving Model with Artifex\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    checkpoint_dir = Path(\"./checkpoints\")\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    full_path = checkpoint_dir / path\n",
    "\n",
    "    # Extract model state\n",
    "    _, state = nnx.split(model)\n",
    "\n",
    "    # Save both state and configuration\n",
    "    checkpoint = {\n",
    "        \"model_state\": state,\n",
    "        \"config\": config,\n",
    "        \"framework\": \"artifex\",\n",
    "        \"version\": \"1.0\",\n",
    "    }\n",
    "\n",
    "    with open(full_path, \"wb\") as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "\n",
    "    print(f\"\u2705 Model saved to: {full_path}\")\n",
    "    print(f\"   Model type: {config.name}\")\n",
    "    print(f\"   Architecture: {config.model_class}\")\n",
    "    print(f\"   File size: {full_path.stat().st_size / (1024 * 1024):.1f} MB\")\n",
    "\n",
    "    return full_path\n",
    "\n",
    "\n",
    "def load_model_artifex(path: str, rngs: nnx.Rngs):\n",
    "    \"\"\"Load model using Artifex's factory system.\"\"\"\n",
    "\n",
    "    print(\"\\n\ud83d\udcc2 Loading Model with Artifex\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    full_path = Path(\"./checkpoints\") / path\n",
    "\n",
    "    if not full_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {full_path}\")\n",
    "\n",
    "    with open(full_path, \"rb\") as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "\n",
    "    # Extract configuration and state\n",
    "    config = checkpoint[\"config\"]\n",
    "    state = checkpoint[\"model_state\"]\n",
    "\n",
    "    print(f\"\u2705 Checkpoint loaded from: {full_path}\")\n",
    "    print(f\"   Model: {config.name}\")\n",
    "    print(f\"   Framework: {checkpoint.get('framework', 'unknown')}\")\n",
    "\n",
    "    # Recreate model using factory\n",
    "    factory = ModelFactory()\n",
    "    model = factory.create(\n",
    "        config=config,\n",
    "        modality=\"image\",\n",
    "        rngs=rngs,\n",
    "    )\n",
    "\n",
    "    # Merge saved state with new model\n",
    "    graphdef, _ = nnx.split(model)\n",
    "    model = nnx.merge(graphdef, state)\n",
    "\n",
    "    print(\"\u2705 Model restored successfully\")\n",
    "\n",
    "    return model, config\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "saved_path = save_model_artifex(model, configs[\"model\"], \"vae_celeba_trained.pkl\")\n",
    "\n",
    "# Example: Load the model\n",
    "# loaded_model, loaded_config = load_model_artifex(\"vae_celeba_trained.pkl\", rngs)\n",
    "# print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Benchmarking\n",
    "\n",
    "Compare JIT-compiled vs non-JIT performance to demonstrate optimization benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(model, rngs):\n",
    "    \"\"\"Compare JIT vs non-JIT performance.\"\"\"\n",
    "\n",
    "    print(\"\\n\u26a1 Performance Benchmark: JIT Compilation Benefits\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create test data\n",
    "    test_batch_size = 32\n",
    "    key = rngs.sample()\n",
    "    test_images = jax.random.normal(key, (test_batch_size, 64, 64, 3))\n",
    "\n",
    "    # Non-JIT function\n",
    "    def forward_pass_no_jit(model, images):\n",
    "        mean, log_var = model.encode(images)\n",
    "        z = model.reparameterize(mean, log_var)\n",
    "        reconstructed = model.decode(z)\n",
    "        loss = jnp.mean((images - reconstructed) ** 2)\n",
    "        return loss\n",
    "\n",
    "    # JIT-compiled function\n",
    "    @nnx.jit\n",
    "    def forward_pass_jit(model, images):\n",
    "        mean, log_var = model.encode(images)\n",
    "        z = model.reparameterize(mean, log_var)\n",
    "        reconstructed = model.decode(z)\n",
    "        loss = jnp.mean((images - reconstructed) ** 2)\n",
    "        return loss\n",
    "\n",
    "    # Warmup JIT\n",
    "    print(\"\ud83d\udd25 Warming up JIT compilation...\")\n",
    "    _ = forward_pass_jit(model, test_images)\n",
    "\n",
    "    # Benchmark settings\n",
    "    num_runs = 100\n",
    "    print(f\"\ud83d\udcca Running {num_runs} forward passes...\")\n",
    "\n",
    "    # Non-JIT timing\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = forward_pass_no_jit(model, test_images)\n",
    "    no_jit_time = time.time() - start\n",
    "\n",
    "    # JIT timing\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = forward_pass_jit(model, test_images)\n",
    "    jit_time = time.time() - start\n",
    "\n",
    "    # Calculate statistics\n",
    "    speedup = no_jit_time / jit_time\n",
    "    time_saved = no_jit_time - jit_time\n",
    "    percentage_saved = (1 - jit_time / no_jit_time) * 100\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Bar chart comparison\n",
    "    methods = [\"No JIT\", \"With JIT\"]\n",
    "    times = [no_jit_time, jit_time]\n",
    "    colors = [\"red\", \"green\"]\n",
    "\n",
    "    axes[0].bar(methods, times, color=colors, alpha=0.7)\n",
    "    axes[0].set_ylabel(\"Time (seconds)\")\n",
    "    axes[0].set_title(f\"Total Time for {num_runs} Forward Passes\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add values on bars\n",
    "    for i, (method, time_val) in enumerate(zip(methods, times)):\n",
    "        axes[0].text(i, time_val + 0.05, f\"{time_val:.2f}s\", ha=\"center\")\n",
    "\n",
    "    # Speedup visualization\n",
    "    axes[1].bar([\"Speedup\"], [speedup], color=\"blue\", alpha=0.7)\n",
    "    axes[1].set_ylabel(\"Speedup Factor\")\n",
    "    axes[1].set_title(\"JIT Compilation Speedup\")\n",
    "    axes[1].axhline(y=1, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "    axes[1].set_ylim(0, max(speedup * 1.2, 2))\n",
    "    axes[1].text(0, speedup + 0.1, f\"{speedup:.2f}x\", ha=\"center\")\n",
    "\n",
    "    plt.suptitle(\"JAX JIT Compilation Performance Impact\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\ud83d\udcca Benchmark Results:\")\n",
    "    print(f\"   Without JIT: {no_jit_time:.3f}s ({no_jit_time / num_runs * 1000:.2f}ms per pass)\")\n",
    "    print(f\"   With JIT:    {jit_time:.3f}s ({jit_time / num_runs * 1000:.2f}ms per pass)\")\n",
    "    print(\"   \")\n",
    "    print(f\"   \ud83d\ude80 Speedup:     {speedup:.2f}x faster\")\n",
    "    print(f\"   \u23f1\ufe0f  Time saved:  {time_saved:.3f}s ({percentage_saved:.1f}%)\")\n",
    "    print(\"   \")\n",
    "\n",
    "    # Provide context\n",
    "    print(\"\ud83d\udca1 Performance Insights:\")\n",
    "    if speedup > 3:\n",
    "        print(\"   \u2705 Excellent JIT optimization (>3x speedup)\")\n",
    "    elif speedup > 2:\n",
    "        print(\"   \u2705 Good JIT optimization (2-3x speedup)\")\n",
    "    elif speedup > 1.5:\n",
    "        print(\"   \u2713 Moderate JIT optimization (1.5-2x speedup)\")\n",
    "    else:\n",
    "        print(\"   \u26a0\ufe0f Limited JIT benefit (consider larger batch sizes)\")\n",
    "\n",
    "    return {\n",
    "        \"no_jit_time\": no_jit_time,\n",
    "        \"jit_time\": jit_time,\n",
    "        \"speedup\": speedup,\n",
    "        \"time_saved\": time_saved,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run performance benchmark\n",
    "perf_results = benchmark_performance(model, rngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated the modern Artifex framework for developing and training generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" \" * 20 + \"\ud83c\udf89 ARTIFEX VAE TRAINING COMPLETE \ud83c\udf89\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n## \ud83d\udcda What We Demonstrated:\\n\")\n",
    "\n",
    "print(\"### 1. **Artifex's Factory System**\")\n",
    "print(\"   - Centralized model creation with ModelFactory\")\n",
    "print(\"   - Unified configuration management\")\n",
    "print(\"   - Modality-based adapters\")\n",
    "\n",
    "print(\"\\n### 2. **Device Management**\")\n",
    "print(\"   - Automatic GPU/CPU detection and fallback\")\n",
    "print(\"   - Memory strategy configuration\")\n",
    "print(\"   - Hardware-aware optimization\")\n",
    "\n",
    "print(\"\\n### 3. **Training Infrastructure**\")\n",
    "print(\"   - Artifex's official Trainer class\")\n",
    "print(\"   - JIT-compiled training loops\")\n",
    "if \"perf_results\" in locals():\n",
    "    print(f\"   - Achieved {perf_results['speedup']:.2f}x speedup with JIT\")\n",
    "\n",
    "print(\"\\n### 4. **Comprehensive Evaluation**\")\n",
    "print(\"   - FID and Inception Score metrics\")\n",
    "print(\"   - Latent space analysis\")\n",
    "print(\"   - Reconstruction quality assessment\")\n",
    "\n",
    "print(\"\\n### 5. **Production Features**\")\n",
    "print(\"   - Model checkpointing and persistence\")\n",
    "print(\"   - Type-safe configuration with Pydantic\")\n",
    "print(\"   - Modular, extensible architecture\")\n",
    "\n",
    "print(\"\\n## \ud83d\udcca Final Results:\\n\")\n",
    "if \"history\" in locals():\n",
    "    print(f\"   Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"   Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "if \"eval_results\" in locals():\n",
    "    print(f\"   Reconstruction MSE: {eval_results['reconstruction_mse']:.6f}\")\n",
    "    print(f\"   FID Score: {eval_results['fid_score']:.2f}\")\n",
    "if \"perf_results\" in locals():\n",
    "    print(f\"   JIT Speedup: {perf_results['speedup']:.2f}x\")\n",
    "\n",
    "print(\"\\n## \ud83d\ude80 Next Steps:\\n\")\n",
    "print(\"1. **Scale Up Training**\")\n",
    "print(\"   - Increase DATASET_SIZE to 100,000+\")\n",
    "print(\"   - Train for 100+ epochs\")\n",
    "print(\"   - Use larger batch sizes on GPU\")\n",
    "\n",
    "print(\"\\n2. **Experiment with Architectures**\")\n",
    "print(\"   - Try \u03b2-VAE for better disentanglement\")\n",
    "print(\"   - Implement VQ-VAE for discrete representations\")\n",
    "print(\"   - Test different encoder/decoder architectures\")\n",
    "\n",
    "print(\"\\n3. **Advanced Features**\")\n",
    "print(\"   - Add conditional generation\")\n",
    "print(\"   - Implement attribute manipulation\")\n",
    "print(\"   - Try other datasets (FFHQ, etc.)\")\n",
    "\n",
    "print(\"\\n4. **Production Deployment**\")\n",
    "print(\"   - Export to ONNX/TensorFlow\")\n",
    "print(\"   - Create REST API with FastAPI\")\n",
    "print(\"   - Deploy with Artifex's CLI tools\")\n",
    "\n",
    "print(\"\\n## \ud83d\udd17 Resources:\\n\")\n",
    "print(\"- Artifex Documentation: https://github.com/avitai/artifex\")\n",
    "print(\"- JAX Documentation: https://jax.readthedocs.io\")\n",
    "print(\"- Flax NNX Guide: https://flax.readthedocs.io/en/latest/nnx/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Thank you for using Artifex! Happy modeling! \ud83d\ude80\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}