{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bb5b1d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Quickstart: Train Your First VAE\n",
    "\n",
    "This quickstart demonstrates how to train a Variational Autoencoder (VAE) on MNIST\n",
    "using Artifex's high-performance training infrastructure.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Load data with `TFDSEagerSource` (pure JAX, no TensorFlow during training)\n",
    "- Configure a CNN-based VAE with `VAEConfig`\n",
    "- Train using JIT-compiled training loops for maximum performance\n",
    "- Generate and visualize samples\n",
    "\n",
    "**Expected runtime:** ~30 seconds on GPU, ~2 minutes on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39ea57",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "from datarax.sources import TFDSEagerSource\n",
    "from datarax.sources.tfds_source import TFDSEagerConfig\n",
    "from flax import nnx\n",
    "\n",
    "from artifex.generative_models.core.configuration import (\n",
    "    DecoderConfig,\n",
    "    EncoderConfig,\n",
    "    VAEConfig,\n",
    ")\n",
    "from artifex.generative_models.models.vae import VAE\n",
    "from artifex.generative_models.training import train_epoch_staged\n",
    "from artifex.generative_models.training.trainers import VAETrainer, VAETrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759bfe7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Load Data with TFDSEagerSource\n",
    "\n",
    "`TFDSEagerSource` loads the entire dataset into JAX arrays at initialization.\n",
    "This eliminates TensorFlow overhead during training - pure JAX from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b29712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST with TFDSEagerSource\n",
    "print(\"Loading MNIST...\")\n",
    "tfds_config = TFDSEagerConfig(name=\"mnist\", split=\"train\", shuffle=True, seed=42)\n",
    "mnist_source = TFDSEagerSource(tfds_config, rngs=nnx.Rngs(0))\n",
    "\n",
    "# Get images as JAX array and normalize to [0, 1]\n",
    "images = mnist_source.data[\"image\"].astype(jnp.float32) / 255.0\n",
    "num_samples = len(mnist_source)\n",
    "print(f\"Loaded {num_samples} images, shape: {images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11963d1d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Configure the VAE Model\n",
    "\n",
    "We use a CNN architecture for better image quality:\n",
    "- **Encoder**: 3-layer CNN (32 -> 64 -> 128 channels) mapping images to 20-dim latent space\n",
    "- **Decoder**: Symmetric CNN reconstructing images from latent codes\n",
    "- **KL weight**: 1.0 (standard VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure encoder\n",
    "encoder = EncoderConfig(\n",
    "    name=\"mnist_cnn_encoder\",\n",
    "    input_shape=(28, 28, 1),\n",
    "    latent_dim=20,\n",
    "    hidden_dims=(32, 64, 128),\n",
    "    activation=\"relu\",\n",
    "    use_batch_norm=False,\n",
    ")\n",
    "\n",
    "# Configure decoder (symmetric to encoder)\n",
    "decoder = DecoderConfig(\n",
    "    name=\"mnist_cnn_decoder\",\n",
    "    latent_dim=20,\n",
    "    output_shape=(28, 28, 1),\n",
    "    hidden_dims=(32, 64, 128),\n",
    "    activation=\"relu\",\n",
    "    batch_norm=False,\n",
    ")\n",
    "\n",
    "# Combine into VAE config\n",
    "model_config = VAEConfig(\n",
    "    name=\"mnist_cnn_vae\",\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    encoder_type=\"cnn\",\n",
    "    kl_weight=1.0,\n",
    ")\n",
    "\n",
    "print(\"Model configured:\")\n",
    "print(f\"  Latent dimension: {encoder.latent_dim}\")\n",
    "print(f\"  Encoder type: CNN with dims {encoder.hidden_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5050dba",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Create Model, Optimizer, and Trainer\n",
    "\n",
    "- **Model**: VAE with CNN encoder/decoder\n",
    "- **Optimizer**: Adam with learning rate 2e-3\n",
    "- **Trainer**: VAETrainer with linear KL annealing (gradual warmup of KL term)\n",
    "\n",
    "KL annealing helps training stability by letting the model learn good reconstructions\n",
    "first before the KL regularization kicks in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and optimizer\n",
    "model = VAE(model_config, rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.adam(2e-3), wrt=nnx.Param)\n",
    "\n",
    "# Create trainer with KL annealing\n",
    "trainer = VAETrainer(\n",
    "    VAETrainingConfig(\n",
    "        kl_annealing=\"linear\",\n",
    "        kl_warmup_steps=2000,  # ~4 epochs of warmup\n",
    "        beta=1.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "state_leaves = jax.tree.leaves(nnx.state(model))\n",
    "param_count = sum(p.size for p in state_leaves if hasattr(p, \"size\"))\n",
    "print(f\"Model created with ~{param_count / 1e3:.1f}K parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a97105",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Train with JIT-Compiled Training Loop\n",
    "\n",
    "We use `train_epoch_staged` which:\n",
    "1. Pre-stages data on GPU with `jax.device_put()`\n",
    "2. Uses JIT-compiled training steps\n",
    "3. Achieves 100-500x speedup over naive Python loops\n",
    "\n",
    "The first epoch includes JIT compilation overhead; subsequent epochs are much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdfdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage data on GPU for maximum performance\n",
    "print()\n",
    "print(\"Staging data on GPU...\")\n",
    "staged_data = jax.device_put(images)\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Warmup JIT compilation (don't count this in training time)\n",
    "print(\"Warming up JIT compilation...\")\n",
    "warmup_rng = jax.random.key(999)\n",
    "# Create loss function - step is passed dynamically inside train_epoch_staged\n",
    "loss_fn = trainer.create_loss_fn(loss_type=\"bce\")\n",
    "_ = train_epoch_staged(\n",
    "    model,\n",
    "    optimizer,\n",
    "    staged_data[:256],\n",
    "    batch_size=128,\n",
    "    rng=warmup_rng,\n",
    "    loss_fn=loss_fn,\n",
    ")\n",
    "print(\"JIT warmup complete.\")\n",
    "print()\n",
    "\n",
    "# Training loop\n",
    "print(f\"Training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# IMPORTANT: Reuse the same loss_fn across epochs for JIT cache hits\n",
    "# The warmup already created and cached the epoch runner for this loss_fn\n",
    "step = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    rng = jax.random.key(epoch)\n",
    "\n",
    "    # Train one epoch (reuses JIT-compiled function from warmup)\n",
    "    step, metrics = train_epoch_staged(\n",
    "        model,\n",
    "        optimizer,\n",
    "        staged_data,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        rng=rng,\n",
    "        loss_fn=loss_fn,  # Reuse same loss_fn for JIT caching\n",
    "        base_step=step,\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch + 1:2d}/{NUM_EPOCHS} | Loss: {metrics['loss']:7.2f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a420ec9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 5: Generate and Reconstruct Images\n",
    "\n",
    "Now let's test the trained model:\n",
    "- **Generation**: Sample from the prior p(z) = N(0, I) and decode\n",
    "- **Reconstruction**: Encode test images to latent space, then decode back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914617d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new samples\n",
    "print()\n",
    "print(\"Generating samples...\")\n",
    "samples = model.sample(n_samples=16)\n",
    "print(f\"Generated {samples.shape[0]} samples\")\n",
    "\n",
    "# Reconstruct test images\n",
    "print(\"Testing reconstruction...\")\n",
    "test_images = jnp.array(images[:8])\n",
    "reconstructed = model.reconstruct(test_images, deterministic=True)\n",
    "print(f\"Reconstructed {reconstructed.shape[0]} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ac889",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 6: Visualize Results\n",
    "\n",
    "Let's visualize the generated samples and reconstructions to verify\n",
    "the model learned meaningful representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generated samples (4x4 grid)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(samples[i].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax.axis(\"off\")\n",
    "fig.suptitle(\"Generated Samples from VAE\", fontsize=14, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"vae_samples.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "print(\"Saved samples to vae_samples.png\")\n",
    "\n",
    "# Plot reconstructions (original vs reconstructed)\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "fig.text(0.02, 0.75, \"Original\", fontsize=12, fontweight=\"bold\", va=\"center\")\n",
    "fig.text(0.02, 0.25, \"Reconstructed\", fontsize=12, fontweight=\"bold\", va=\"center\")\n",
    "\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(test_images[i].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[1, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"VAE Reconstruction Quality\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.08)\n",
    "plt.savefig(\"vae_reconstruction.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "print(\"Saved reconstruction to vae_reconstruction.png\")\n",
    "\n",
    "print()\n",
    "print(\"Success! You've trained your first VAE with Artifex!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6adf535",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## What You Just Did\n",
    "\n",
    "1. **Loaded data efficiently** with `TFDSEagerSource` - pure JAX, no TF overhead\n",
    "2. **Configured a CNN VAE** using Artifex's modular config system\n",
    "3. **Used VAETrainer** with KL annealing for stable training\n",
    "4. **Trained with JIT-compiled loops** for maximum performance\n",
    "5. **Generated new samples** from the learned latent space\n",
    "6. **Reconstructed images** to verify encoder-decoder quality\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Core Concepts**: Learn about Artifex's architecture and design principles\n",
    "- **VAE Guide**: Advanced techniques like beta-VAE, conditional VAE, VQ-VAE\n",
    "- **Other Models**: Try Diffusion models, GANs, Flow models\n",
    "- **Custom Data**: Load your own datasets with datarax"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
