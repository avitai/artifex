# Sequence-to-Sequence Models

!!! info "Coming Soon"
    This example is planned for a future release. Check back for updates on seq2seq implementations.

## Overview

This example will demonstrate:

- Encoder-decoder architectures
- Attention mechanisms
- Machine translation
- Text summarization

## Planned Features

- Transformer encoder-decoder
- Cross-attention layers
- Beam search decoding
- Teacher forcing training

## Related Documentation

- [Transformer Text](../basic/transformer-text.md)
- [Text Compression](text-compression.md)
- [Autoregressive Models](../../api/models/autoregressive.md)

## References

- Sutskever et al., "Sequence to Sequence Learning with Neural Networks" (2014)
- Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate" (2015)
