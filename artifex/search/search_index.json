{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Artifex: Generative Modeling Research Library","text":"<p>A research-focused modular generative modeling library built on JAX/Flax NNX, providing implementations of state-of-the-art generative models with multi-modal support and scientific computing focus.</p>"},{"location":"#why-artifex","title":"Why Artifex?","text":"<ul> <li> <p> State-of-the-Art Models</p> <p>VAEs, GANs, Diffusion, Flows, EBMs, Autoregressive, and Geometric models with 2025 research compliance</p> <p> Model Gallery</p> </li> <li> <p> Research-Focused</p> <p>Hardware-aware optimization, distributed training, mixed precision validated through 2150+ tests</p> <p> Getting Started</p> </li> <li> <p> Multi-Modal Support</p> <p>Native support for images, text, audio, proteins, and multi-modal data</p> <p> Modalities Guide</p> </li> <li> <p> Scalable Architecture</p> <p>From single GPU to multi-node distributed training with FSDP and tensor parallelism</p> <p> Scaling Guide</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># CPU-only version\npip install artifex\n\n# With GPU support (CUDA 12.0+)\npip install artifex[cuda]\n</code></pre> <p>See the Installation Guide for detailed setup instructions including Docker and source installation.</p>"},{"location":"#train-a-diffusion-model-on-fashion-mnist","title":"Train a Diffusion Model on Fashion-MNIST","text":"<pre><code>import jax\nimport jax.numpy as jnp\nimport optax\nfrom datarax import from_source\nfrom datarax.core.config import ElementOperatorConfig\nfrom datarax.dag.nodes import OperatorNode\nfrom datarax.operators import ElementOperator\nfrom datarax.sources import TfdsDataSourceConfig, TFDSSource\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration import (\n    DDPMConfig, NoiseScheduleConfig, UNetBackboneConfig,\n)\nfrom artifex.generative_models.models.diffusion import DDPMModel\nfrom artifex.generative_models.training.trainers import (\n    DiffusionTrainer, DiffusionTrainingConfig,\n)\n\n# 1. Load Fashion-MNIST with datarax\ndef normalize(element, _key):\n    image = element.data[\"image\"].astype(jnp.float32) / 127.5 - 1.0\n    return element.replace(data={**element.data, \"image\": image})\n\nsource = TFDSSource(\n    TfdsDataSourceConfig(name=\"fashion_mnist\", split=\"train\", shuffle=True),\n    rngs=nnx.Rngs(0),\n)\nnormalize_op = ElementOperator(ElementOperatorConfig(stochastic=False), fn=normalize, rngs=nnx.Rngs(1))\npipeline = from_source(source, batch_size=64) &gt;&gt; OperatorNode(normalize_op)\n\n# 2. Create DDPM model\nconfig = DDPMConfig(\n    name=\"fashion_ddpm\",\n    input_shape=(28, 28, 1),\n    backbone=UNetBackboneConfig(\n        name=\"unet\", in_channels=1, out_channels=1,\n        hidden_dims=(32, 64, 128), channel_mult=(1, 2, 4), activation=\"silu\",\n    ),\n    noise_schedule=NoiseScheduleConfig(\n        name=\"cosine\", schedule_type=\"cosine\", num_timesteps=1000,\n    ),\n)\nmodel = DDPMModel(config, rngs=nnx.Rngs(42))\noptimizer = nnx.Optimizer(model, optax.adamw(1e-4), wrt=nnx.Param)\n\n# 3. Configure trainer with SOTA techniques\ntrainer = DiffusionTrainer(\n    noise_schedule=model.noise_schedule,\n    config=DiffusionTrainingConfig(\n        loss_weighting=\"min_snr\", snr_gamma=5.0, ema_decay=0.9999,\n    ),\n)\njit_train_step = nnx.jit(trainer.train_step)\n\n# 4. Training loop\nrng = jax.random.PRNGKey(0)\nfor batch in pipeline:\n    rng, step_rng = jax.random.split(rng)\n    _, metrics = jit_train_step(model, optimizer, {\"image\": batch[\"image\"]}, step_rng)\n    trainer.update_ema(model)\n\n# 5. Generate samples\nsamples = model.sample(n_samples_or_shape=16, steps=100)\n</code></pre> <p>See the Quickstart Guide for complete examples including VAE training, visualization, and more.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Core Concepts</p> <p>Understand generative modeling concepts and Artifex architecture</p> <p> Learn</p> </li> <li> <p> Quickstart Guide</p> <p>Train your first generative model with Artifex</p> <p> Quickstart</p> </li> <li> <p> Model Guides</p> <p>Deep dive into VAEs, GANs, Diffusion, Flows, and more</p> <p> Guides</p> </li> <li> <p> Examples</p> <p>Working examples for all model types and use cases</p> <p> Examples</p> </li> </ul>"},{"location":"#model-types","title":"Model Types","text":"Model Type Best For Guide VAE Latent space exploration, representation learning VAE Guide GAN High-quality image synthesis, style transfer GAN Guide Diffusion State-of-the-art generation, inpainting Diffusion Guide Flow Exact likelihood, density estimation Flow Guide EBM Compositional generation, constraints EBM Guide Autoregressive Text, sequential data AR Guide Geometric Proteins, molecules, 3D structures Examples"},{"location":"#architecture","title":"Architecture","text":"<p>See Architecture Overview for detailed system design, component structure, and extension points.</p>"},{"location":"#citation","title":"Citation","text":"<pre><code>@software{artifex_2025,\n  title = {Artifex: Generative Modeling Research Library},\n  author = {Shafiei, Mahdi and contributors},\n  year = {2025},\n  url = {https://github.com/avitai/artifex},\n  version = {0.1.0}\n}\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! See the Contributing Guide for guidelines.</p> <p> GitHub |   Issues |   Discussions </p>"},{"location":"api/configuration/","title":"Configuration API","text":"<p>Coming Soon</p> <p>This page is under development. Check back for configuration API documentation.</p>"},{"location":"api/configuration/#overview","title":"Overview","text":"<p>API reference for the configuration system.</p>"},{"location":"api/configuration/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Guide</li> <li>Core Configuration</li> </ul>"},{"location":"api/extensions/","title":"Extensions API","text":"<p>Coming Soon</p> <p>This page is under development. Check back for comprehensive extensions API documentation.</p>"},{"location":"api/extensions/#overview","title":"Overview","text":"<p>API reference for the extensions system.</p>"},{"location":"api/extensions/#available-extensions","title":"Available Extensions","text":"<ul> <li>Protein modeling extensions</li> <li>Geometric extensions</li> <li>Custom domain extensions</li> </ul>"},{"location":"api/extensions/#related-documentation","title":"Related Documentation","text":"<ul> <li>Extensions Guide</li> <li>Protein Extensions</li> </ul>"},{"location":"api/factory/","title":"Factory API","text":"<p>Coming Soon</p> <p>This page is under development. Check back for factory API documentation.</p>"},{"location":"api/factory/#overview","title":"Overview","text":"<p>API reference for the model factory system.</p>"},{"location":"api/factory/#related-documentation","title":"Related Documentation","text":"<ul> <li>Factory Guide</li> <li>Model Factory</li> </ul>"},{"location":"api/sampling/","title":"Sampling API Reference","text":"<p>API reference for sampling methods and utilities in Artifex.</p>"},{"location":"api/sampling/#overview","title":"Overview","text":"<p>This module provides sampling utilities for generating outputs from trained generative models.</p>"},{"location":"api/sampling/#core-sampling-functions","title":"Core Sampling Functions","text":""},{"location":"api/sampling/#basic-sampling","title":"Basic Sampling","text":"<pre><code>from artifex.generative_models.core.sampling import sample_from_model\n\nsamples = sample_from_model(\n    model=model,\n    num_samples=64,\n    rng=jax.random.key(0),\n)\n</code></pre>"},{"location":"api/sampling/#temperature-scaling","title":"Temperature Scaling","text":"<pre><code>from artifex.generative_models.core.sampling import temperature_sample\n\nsamples = temperature_sample(\n    model=model,\n    num_samples=64,\n    temperature=0.8,\n    rng=jax.random.key(0),\n)\n</code></pre>"},{"location":"api/sampling/#model-specific-sampling","title":"Model-Specific Sampling","text":"<p>For detailed sampling methods for each model type, see:</p> <ul> <li>VAE Sampling</li> <li>GAN Sampling</li> <li>Diffusion Sampling</li> <li>Flow Sampling</li> </ul>"},{"location":"api/sampling/#related-documentation","title":"Related Documentation","text":"<ul> <li>Sampling User Guide - Comprehensive sampling guide</li> <li>Inference Optimization - Performance optimization</li> <li>Core API - Core module reference</li> </ul>"},{"location":"api/configs/protein-extension/","title":"Protein Extension Configuration","text":"<p>Coming Soon</p> <p>This page is under development. Check back for protein extension configuration documentation.</p>"},{"location":"api/configs/protein-extension/#overview","title":"Overview","text":"<p>Configuration options for protein modeling extensions.</p>"},{"location":"api/configs/protein-extension/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling Guide</li> <li>Configuration Guide</li> </ul>"},{"location":"api/core/base/","title":"Core Base Classes","text":"<p>The base module provides the foundational classes and utilities for all generative models in Artifex. These classes define common interfaces, helper utilities, and design patterns that ensure consistency across different model types.</p>"},{"location":"api/core/base/#overview","title":"Overview","text":"<ul> <li> <p> Modular Architecture</p> <p>Clean separation between base abstractions and concrete implementations</p> </li> <li> <p> Type-Safe Interfaces</p> <p>Protocol-based design with full type checking support</p> </li> <li> <p> Flax NNX Integration</p> <p>Built on Flax NNX for modern neural network patterns</p> </li> <li> <p> Extensible Design</p> <p>Easy to extend for custom model types and architectures</p> </li> </ul>"},{"location":"api/core/base/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>graph TD\n    A[nnx.Module] --&gt; B[GenerativeModule]\n    B --&gt; C[GenerativeModel]\n    C --&gt; D[BaseVAE]\n    C --&gt; E[BaseDiffusion]\n    C --&gt; F[BaseGAN]\n    C --&gt; G[BaseFlow]\n\n    B --&gt; H[MLP]\n    B --&gt; I[CNN]\n\n    style A fill:#e1f5ff\n    style B fill:#b3e5fc\n    style C fill:#81d4fa\n    style D fill:#4fc3f7\n    style E fill:#4fc3f7\n    style F fill:#4fc3f7\n    style G fill:#4fc3f7\n    style H fill:#b3e5fc\n    style I fill:#b3e5fc</code></pre>"},{"location":"api/core/base/#core-classes","title":"Core Classes","text":""},{"location":"api/core/base/#generativemodule","title":"GenerativeModule","text":"<p><code>GenerativeModule</code> is the base class for all generative model components. It extends Flax NNX's <code>Module</code> with Artifex-specific functionality.</p> <p>Location: <code>src/artifex/generative_models/core/base.py:107</code></p>"},{"location":"api/core/base/#features","title":"Features","text":"<ul> <li>Proper RNG handling through <code>nnx.Rngs</code></li> <li>Precision control for numerical computations</li> <li>Default activation function management</li> <li>State management through NNX's Variable system</li> </ul>"},{"location":"api/core/base/#basic-usage","title":"Basic Usage","text":"<pre><code>from flax import nnx\nfrom artifex.generative_models.core.base import GenerativeModule\n\nclass CustomLayer(GenerativeModule):\n    def __init__(\n        self,\n        features: int,\n        *,\n        rngs: nnx.Rngs,\n        precision: jax.lax.Precision | None = None,\n    ):\n        # ALWAYS call super().__init__()\n        super().__init__(rngs=rngs, precision=precision)\n\n        # Initialize your components\n        self.dense = nnx.Linear(\n            in_features=64,\n            out_features=features,\n            rngs=rngs\n        )\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        return self.dense(x)\n</code></pre>"},{"location":"api/core/base/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Description <code>rngs</code> <code>nnx.Rngs</code> Random number generators (keyword-only, required) <code>precision</code> <code>jax.lax.Precision \\| None</code> Numerical precision for computations (optional)"},{"location":"api/core/base/#generativemodel","title":"GenerativeModel","text":"<p><code>GenerativeModel</code> is the unified base class for all generative models in Artifex. It provides a consistent interface that works across VAEs, GANs, Diffusion Models, Flow Models, and more.</p> <p>Location: <code>src/artifex/generative_models/core/base.py:142</code></p>"},{"location":"api/core/base/#core-interface","title":"Core Interface","text":"<p>Every generative model must implement three key methods:</p> <ol> <li><code>__call__</code>: Forward pass through the model</li> <li><code>generate</code>: Sample generation from the model</li> <li><code>loss_fn</code>: Loss computation for training</li> </ol> <pre><code>graph LR\n    A[Input Data] --&gt; B[__call__]\n    B --&gt; C[Model Outputs]\n    C --&gt; D[loss_fn]\n    D --&gt; E[Loss + Metrics]\n\n    F[Random Noise] --&gt; G[generate]\n    G --&gt; H[Generated Samples]\n\n    style B fill:#81d4fa\n    style D fill:#4fc3f7\n    style G fill:#66bb6a</code></pre>"},{"location":"api/core/base/#method-__call__","title":"Method: <code>__call__</code>","text":"<p>The forward pass method processes input data and returns model-specific outputs.</p> <p>Signature:</p> <pre><code>def __call__(\n    self,\n    x: PyTree,\n    *args,\n    rngs: nnx.Rngs | None = None,\n    training: bool = False,\n    **kwargs\n) -&gt; dict[str, Any]:\n    ...\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>x</code> <code>PyTree</code> Input data (arrays, dicts, etc.) <code>*args</code> <code>Any</code> Model-specific positional args (e.g., timesteps for diffusion) <code>rngs</code> <code>nnx.Rngs \\| None</code> Random number generators for stochastic operations <code>training</code> <code>bool</code> Whether the model is in training mode <code>**kwargs</code> <code>Any</code> Model-specific keyword arguments <p>Returns:</p> <p>A dictionary containing model outputs. Common keys include:</p> <ul> <li>Diffusion models: <code>{\"predicted_noise\": ..., \"loss\": ...}</code></li> <li>Flow models: <code>{\"z\": ..., \"logdet\": ..., \"log_prob\": ...}</code></li> <li>VAEs: <code>{\"reconstruction\": ..., \"z\": ..., \"kl_loss\": ...}</code></li> <li>GANs: <code>{\"generated\": ..., \"discriminator_logits\": ...}</code></li> </ul> <p>Example:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# Create a simple diffusion model\nfrom artifex.generative_models.models.diffusion import DDPM\n\n# Initialize RNGs\nrngs = nnx.Rngs(0)\n\n# Create model\nmodel = DDPM(\n    input_shape=(28, 28, 1),\n    timesteps=1000,\n    rngs=rngs\n)\n\n# Forward pass\nbatch = jnp.ones((4, 28, 28, 1))\ntimesteps = jnp.array([100, 200, 300, 400])\n\noutputs = model(batch, timesteps, training=True, rngs=rngs)\nprint(f\"Predicted noise shape: {outputs['predicted_noise'].shape}\")\nprint(f\"Loss: {outputs['loss']}\")\n</code></pre>"},{"location":"api/core/base/#method-generate","title":"Method: <code>generate</code>","text":"<p>Generate samples from the model distribution.</p> <p>Signature:</p> <pre><code>def generate(\n    self,\n    n_samples: int = 1,\n    *,\n    rngs: nnx.Rngs | None = None,\n    **kwargs\n) -&gt; PyTree:\n    ...\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>n_samples</code> <code>int</code> Number of samples to generate <code>rngs</code> <code>nnx.Rngs \\| None</code> Random number generators <code>**kwargs</code> <code>Any</code> Model-specific generation parameters <p>Common kwargs:</p> <ul> <li><code>condition</code>: Conditioning information for conditional models</li> <li><code>shape</code>: Target shape for generated samples</li> <li><code>steps</code>: Number of generation steps (for diffusion models)</li> <li><code>temperature</code>: Sampling temperature</li> </ul> <p>Example:</p> <pre><code># Generate 16 samples\nsamples = model.generate(n_samples=16, rngs=rngs)\nprint(f\"Generated samples shape: {samples.shape}\")  # (16, 28, 28, 1)\n\n# Conditional generation (if model supports it)\nconditions = jnp.array([0, 1, 2, 3])  # Class labels\nconditional_samples = model.generate(\n    n_samples=4,\n    condition=conditions,\n    rngs=rngs\n)\n</code></pre>"},{"location":"api/core/base/#method-loss_fn","title":"Method: <code>loss_fn</code>","text":"<p>Compute the loss for model training.</p> <p>Signature:</p> <pre><code>def loss_fn(\n    self,\n    batch: PyTree,\n    model_outputs: dict[str, Any],\n    *,\n    rngs: nnx.Rngs | None = None,\n    **kwargs\n) -&gt; dict[str, Any]:\n    ...\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>batch</code> <code>PyTree</code> Batch of training data <code>model_outputs</code> <code>dict[str, Any]</code> Outputs from forward pass <code>rngs</code> <code>nnx.Rngs \\| None</code> Random number generators <code>**kwargs</code> <code>Any</code> Additional loss computation parameters <p>Returns:</p> <p>A dictionary containing at minimum:</p> <ul> <li><code>\"loss\"</code>: Primary loss value for optimization</li> </ul> <p>Additional keys may include component losses and metrics.</p> <p>Example:</p> <pre><code># Compute loss for training\noutputs = model(batch, timesteps, training=True, rngs=rngs)\nloss_dict = model.loss_fn(batch, outputs, rngs=rngs)\n\nprint(f\"Total loss: {loss_dict['loss']}\")\nprint(f\"Component losses: {loss_dict.keys()}\")\n</code></pre>"},{"location":"api/core/base/#optional-methods","title":"Optional Methods","text":"<p>These methods provide additional functionality for specific model types:</p> <p><code>encode(x)</code> - Encode input to latent representation (VAEs, Flow models)</p> <pre><code># VAE encoding\nlatent = model.encode(batch, rngs=rngs)\nprint(f\"Latent shape: {latent.shape}\")\n</code></pre> <p><code>decode(z)</code> - Decode latent representation to data space (VAEs, Flow models)</p> <pre><code># VAE decoding\nreconstruction = model.decode(latent, rngs=rngs)\nprint(f\"Reconstruction shape: {reconstruction.shape}\")\n</code></pre> <p><code>log_prob(x)</code> - Compute log probability of data (Flow models, VAEs)</p> <pre><code># Compute log probability\nlog_p = model.log_prob(batch, rngs=rngs)\nprint(f\"Log probability shape: {log_p.shape}\")\n</code></pre> <p><code>reconstruct(x)</code> - Reconstruct input through encode-decode cycle (VAEs, Autoencoders)</p> <pre><code># Reconstruct data\nreconstruction = model.reconstruct(batch, rngs=rngs)\nloss = jnp.mean((batch - reconstruction) ** 2)\n</code></pre> <p><code>interpolate(x1, x2, alpha)</code> - Interpolate between two data points</p> <pre><code># Linear interpolation in latent space\nx1, x2 = batch[0], batch[1]\ninterpolated = model.interpolate(x1, x2, alpha=0.5, rngs=rngs)\n</code></pre>"},{"location":"api/core/base/#helper-classes","title":"Helper Classes","text":""},{"location":"api/core/base/#mlp-multi-layer-perceptron","title":"MLP (Multi-Layer Perceptron)","text":"<p>A memory-efficient, configurable MLP module with advanced features.</p> <p>Location: <code>src/artifex/generative_models/core/base.py:372</code></p>"},{"location":"api/core/base/#features_1","title":"Features","text":"<ul> <li> <p>Flexible Architecture</p> <p>Arbitrary hidden dimensions with customizable activations</p> </li> <li> <p>Memory Efficient</p> <p>Lazy initialization and gradient checkpointing support</p> </li> <li> <p>Batch Normalization</p> <p>Optional batch norm between layers</p> </li> <li> <p>Dropout</p> <p>Configurable dropout with proper NNX integration</p> </li> </ul>"},{"location":"api/core/base/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.core.base import MLP\nfrom flax import nnx\n\n# Create RNGs\nrngs = nnx.Rngs(0)\n\n# Simple MLP\nmlp = MLP(\n    hidden_dims=[256, 128, 64],\n    in_features=784,\n    activation=\"gelu\",\n    rngs=rngs\n)\n\n# Forward pass\nx = jnp.ones((32, 784))\noutput = mlp(x)\nprint(f\"Output shape: {output.shape}\")  # (32, 64)\n</code></pre>"},{"location":"api/core/base/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># MLP with all features\nadvanced_mlp = MLP(\n    hidden_dims=[512, 256, 128],\n    in_features=784,\n    activation=\"gelu\",\n    dropout_rate=0.1,\n    use_bias=True,\n    output_activation=\"tanh\",\n    use_batch_norm=True,\n    use_gradient_checkpointing=False,\n    rngs=rngs\n)\n\n# Forward with options\noutput = advanced_mlp(\n    x,\n    return_intermediates=False,\n    use_scan=False,\n    use_checkpointing=False\n)\n</code></pre>"},{"location":"api/core/base/#constructor-parameters_1","title":"Constructor Parameters","text":"Parameter Type Default Description <code>hidden_dims</code> <code>list[int]</code> Required Output dimensions for each layer <code>in_features</code> <code>int</code> Required Input feature dimension <code>activation</code> <code>str \\| Callable</code> <code>\"gelu\"</code> Activation function <code>dropout_rate</code> <code>float</code> <code>0.0</code> Dropout probability <code>use_bias</code> <code>bool</code> <code>True</code> Whether to use bias in linear layers <code>output_activation</code> <code>str \\| Callable \\| None</code> <code>None</code> Activation for final layer <code>use_batch_norm</code> <code>bool</code> <code>False</code> Use batch normalization <code>use_gradient_checkpointing</code> <code>bool</code> <code>False</code> Enable gradient checkpointing <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/core/base/#supported-activations","title":"Supported Activations","text":"<pre><code># Available activation functions\nactivations = [\n    \"relu\", \"gelu\", \"elu\", \"leaky_relu\",\n    \"silu\", \"swish\", \"tanh\", \"sigmoid\",\n    \"softmax\", \"log_softmax\"\n]\n\n# Using custom activation\nfrom flax import nnx\n\ndef custom_activation(x):\n    return nnx.gelu(x) * 0.5\n\nmlp = MLP(\n    hidden_dims=[128, 64],\n    in_features=256,\n    activation=custom_activation,\n    rngs=rngs\n)\n</code></pre>"},{"location":"api/core/base/#memory-efficient-deep-networks","title":"Memory-Efficient Deep Networks","text":"<p>For very deep networks (\u22658 layers), MLP automatically uses scan-based computation:</p> <pre><code># Deep network (20 layers)\ndeep_mlp = MLP(\n    hidden_dims=[256] * 20,\n    in_features=512,\n    activation=\"gelu\",\n    rngs=rngs\n)\n\n# Automatically uses memory-efficient scan\noutput = deep_mlp(x)  # Memory-efficient computation\n</code></pre>"},{"location":"api/core/base/#utility-methods","title":"Utility Methods","text":"<pre><code># Get parameter count\nnum_params = mlp.get_num_params()\nprint(f\"Total parameters: {num_params:,}\")\n\n# Reset dropout RNGs for reproducibility\nnew_rngs = nnx.Rngs(42)\nmlp.reset_dropout(new_rngs)\n</code></pre>"},{"location":"api/core/base/#cnn-convolutional-neural-network","title":"CNN (Convolutional Neural Network)","text":"<p>An enhanced CNN module with support for various convolution types.</p> <p>Location: <code>src/artifex/generative_models/core/base.py:612</code></p>"},{"location":"api/core/base/#features_2","title":"Features","text":"<ul> <li> <p>Multiple Convolution Types</p> <p>Regular, transpose, depthwise separable, grouped convolutions</p> </li> <li> <p>Flexible Configuration</p> <p>Customizable kernel sizes, strides, and padding</p> </li> <li> <p>Batch Normalization</p> <p>Optional batch norm for training stability</p> </li> <li> <p>Encoder/Decoder Support</p> <p>Use transpose convolutions for decoder networks</p> </li> </ul>"},{"location":"api/core/base/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.core.base import CNN\nfrom flax import nnx\n\n# Create RNGs\nrngs = nnx.Rngs(0)\n\n# Simple CNN encoder\nencoder = CNN(\n    hidden_dims=[32, 64, 128],\n    in_features=3,  # RGB channels\n    activation=\"relu\",\n    kernel_size=(3, 3),\n    strides=(2, 2),\n    padding=\"SAME\",\n    rngs=rngs\n)\n\n# Forward pass\nx = jnp.ones((16, 64, 64, 3))\nfeatures = encoder(x)\nprint(f\"Features shape: {features.shape}\")\n</code></pre>"},{"location":"api/core/base/#decoder-with-transpose-convolutions","title":"Decoder with Transpose Convolutions","text":"<pre><code># CNN decoder\ndecoder = CNN(\n    hidden_dims=[64, 32, 3],\n    in_features=128,\n    activation=\"relu\",\n    kernel_size=(3, 3),\n    strides=(2, 2),\n    padding=\"SAME\",\n    use_transpose=True,  # Use transpose convolutions\n    rngs=rngs\n)\n\n# Decode features\nlatent = jnp.ones((16, 8, 8, 128))\nreconstructed = decoder(latent)\nprint(f\"Reconstructed shape: {reconstructed.shape}\")\n</code></pre>"},{"location":"api/core/base/#advanced-features","title":"Advanced Features","text":"<p>Depthwise Separable Convolutions:</p> <pre><code># Memory-efficient separable convolutions\nefficient_cnn = CNN(\n    hidden_dims=[64, 128, 256],\n    in_features=3,\n    activation=\"relu\",\n    use_depthwise_separable=True,  # Depthwise separable\n    rngs=rngs\n)\n</code></pre> <p>Grouped Convolutions:</p> <pre><code># Grouped convolutions (like ResNeXt)\ngrouped_cnn = CNN(\n    hidden_dims=[64, 128, 256],\n    in_features=64,\n    activation=\"relu\",\n    groups=4,  # 4 groups\n    rngs=rngs\n)\n</code></pre> <p>With Batch Normalization and Dropout:</p> <pre><code># Full-featured CNN\nfull_cnn = CNN(\n    hidden_dims=[64, 128, 256],\n    in_features=3,\n    activation=\"relu\",\n    use_batch_norm=True,\n    dropout_rate=0.1,\n    rngs=rngs\n)\n</code></pre>"},{"location":"api/core/base/#constructor-parameters_2","title":"Constructor Parameters","text":"Parameter Type Default Description <code>hidden_dims</code> <code>list[int]</code> Required Output channels for each layer <code>in_features</code> <code>int</code> Required Input channels <code>activation</code> <code>str \\| Callable</code> <code>\"relu\"</code> Activation function <code>kernel_size</code> <code>int \\| tuple[int, int]</code> <code>(3, 3)</code> Convolution kernel size <code>strides</code> <code>int \\| tuple[int, int]</code> <code>(2, 2)</code> Convolution strides <code>padding</code> <code>str \\| int \\| tuple[int, int]</code> <code>\"SAME\"</code> Padding strategy <code>use_transpose</code> <code>bool</code> <code>False</code> Use transpose convolutions <code>use_batch_norm</code> <code>bool</code> <code>False</code> Use batch normalization <code>dropout_rate</code> <code>float</code> <code>0.0</code> Dropout probability <code>use_depthwise_separable</code> <code>bool</code> <code>False</code> Use depthwise separable convolutions <code>groups</code> <code>int</code> <code>1</code> Number of groups for grouped convolutions <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/core/base/#utility-functions","title":"Utility Functions","text":""},{"location":"api/core/base/#get_activation_function","title":"get_activation_function","text":"<p>Retrieve an activation function by name or pass through a callable.</p> <p>Location: <code>src/artifex/generative_models/core/base.py:18</code></p> <pre><code>from artifex.generative_models.core.base import get_activation_function\n\n# Get activation by name\nrelu = get_activation_function(\"relu\")\ngelu = get_activation_function(\"gelu\")\n\n# Pass through callable\ncustom = get_activation_function(lambda x: x ** 2)\n\n# Use in model\nx = jnp.array([1.0, -1.0, 2.0])\noutput = relu(x)\n</code></pre> <p>Available Activations:</p> <ul> <li><code>\"relu\"</code>, <code>\"gelu\"</code>, <code>\"elu\"</code>, <code>\"leaky_relu\"</code></li> <li><code>\"silu\"</code>, <code>\"swish\"</code>, <code>\"tanh\"</code>, <code>\"sigmoid\"</code></li> <li><code>\"softmax\"</code>, <code>\"log_softmax\"</code></li> </ul>"},{"location":"api/core/base/#get_default_backbone","title":"get_default_backbone","text":"<p>Create a default UNet backbone for diffusion models.</p> <p>Location: <code>src/artifex/generative_models/core/base.py:56</code></p> <pre><code>from artifex.generative_models.core.base import get_default_backbone\nfrom artifex.generative_models.core.configuration import ModelConfig\n\n# Create configuration\nconfig = ModelConfig(\n    name=\"my_diffusion\",\n    model_class=\"artifex.generative_models.models.diffusion.DDPM\",\n    input_dim=(64, 64, 3),\n    hidden_dims=[64, 128, 256],\n)\n\n# Get default backbone\nbackbone = get_default_backbone(config, rngs=rngs)\n</code></pre>"},{"location":"api/core/base/#common-patterns","title":"Common Patterns","text":""},{"location":"api/core/base/#pattern-1-creating-custom-models","title":"Pattern 1: Creating Custom Models","text":"<pre><code>from artifex.generative_models.core.base import GenerativeModel\nfrom flax import nnx\nimport jax.numpy as jnp\n\nclass MyCustomModel(GenerativeModel):\n    \"\"\"A custom generative model.\"\"\"\n\n    def __init__(\n        self,\n        latent_dim: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(rngs=rngs)\n\n        # Store configuration\n        self.latent_dim = latent_dim\n\n        # Initialize components\n        self.encoder = nnx.Linear(784, latent_dim, rngs=rngs)\n        self.decoder = nnx.Linear(latent_dim, 784, rngs=rngs)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        *args,\n        rngs: nnx.Rngs | None = None,\n        training: bool = False,\n        **kwargs\n    ) -&gt; dict[str, Any]:\n        # Encode\n        z = self.encoder(x)\n\n        # Decode\n        reconstruction = self.decoder(z)\n\n        return {\n            \"reconstruction\": reconstruction,\n            \"z\": z,\n        }\n\n    def generate(\n        self,\n        n_samples: int = 1,\n        *,\n        rngs: nnx.Rngs | None = None,\n        **kwargs\n    ) -&gt; jax.Array:\n        # Sample from latent space\n        if rngs is not None and \"sample\" in rngs:\n            key = rngs.sample()\n        else:\n            key = jax.random.key(0)\n\n        z = jax.random.normal(key, (n_samples, self.latent_dim))\n\n        # Decode to data space\n        return self.decoder(z)\n\n    def loss_fn(\n        self,\n        batch: jax.Array,\n        model_outputs: dict[str, Any],\n        *,\n        rngs: nnx.Rngs | None = None,\n        **kwargs\n    ) -&gt; dict[str, Any]:\n        reconstruction = model_outputs[\"reconstruction\"]\n\n        # Compute reconstruction loss\n        recon_loss = jnp.mean((batch - reconstruction) ** 2)\n\n        return {\n            \"loss\": recon_loss,\n            \"reconstruction_loss\": recon_loss,\n        }\n</code></pre>"},{"location":"api/core/base/#pattern-2-using-helper-classes-in-models","title":"Pattern 2: Using Helper Classes in Models","text":"<pre><code>from artifex.generative_models.core.base import GenerativeModel, MLP, CNN\nfrom flax import nnx\n\nclass AdvancedVAE(GenerativeModel):\n    \"\"\"VAE with CNN encoder and MLP decoder.\"\"\"\n\n    def __init__(\n        self,\n        latent_dim: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(rngs=rngs)\n\n        # CNN encoder\n        self.encoder_conv = CNN(\n            hidden_dims=[32, 64, 128],\n            in_features=3,\n            activation=\"relu\",\n            rngs=rngs\n        )\n\n        # MLP encoder head\n        self.encoder_head = MLP(\n            hidden_dims=[256, latent_dim * 2],\n            in_features=128 * 8 * 8,  # Flattened conv output\n            activation=\"gelu\",\n            rngs=rngs\n        )\n\n        # MLP decoder\n        self.decoder = MLP(\n            hidden_dims=[256, 512, 32 * 32 * 3],\n            in_features=latent_dim,\n            activation=\"gelu\",\n            output_activation=\"sigmoid\",\n            rngs=rngs\n        )\n\n    def encode(self, x: jax.Array, *, rngs: nnx.Rngs | None = None) -&gt; tuple[jax.Array, jax.Array]:\n        # Convolutional features\n        features = self.encoder_conv(x)\n\n        # Flatten\n        features = features.reshape(features.shape[0], -1)\n\n        # Get mean and logvar\n        params = self.encoder_head(features)\n        mean, logvar = jnp.split(params, 2, axis=-1)\n\n        return mean, logvar\n\n    def decode(self, z: jax.Array, *, rngs: nnx.Rngs | None = None) -&gt; jax.Array:\n        # Decode through MLP\n        output = self.decoder(z)\n\n        # Reshape to image\n        return output.reshape(-1, 32, 32, 3)\n</code></pre>"},{"location":"api/core/base/#pattern-3-proper-rng-handling","title":"Pattern 3: Proper RNG Handling","text":"<pre><code>from flax import nnx\nimport jax\n\nclass StochasticModel(GenerativeModel):\n    \"\"\"Model with stochastic layers.\"\"\"\n\n    def __call__(\n        self,\n        x: jax.Array,\n        *args,\n        rngs: nnx.Rngs | None = None,\n        training: bool = False,\n        **kwargs\n    ) -&gt; dict[str, Any]:\n        # Check if specific RNG key exists\n        if rngs is not None and \"sample\" in rngs:\n            sample_key = rngs.sample()\n        else:\n            # Fallback for deterministic mode\n            sample_key = jax.random.key(0)\n\n        # Use the key for stochastic operations\n        noise = jax.random.normal(sample_key, x.shape)\n\n        # Process\n        output = x + noise * 0.1\n\n        return {\"output\": output}\n</code></pre>"},{"location":"api/core/base/#best-practices","title":"Best Practices","text":""},{"location":"api/core/base/#do","title":"DO","text":"<ul> <li>\u2705 Always call <code>super().__init__()</code> in constructors</li> <li>\u2705 Use keyword-only arguments for <code>rngs</code> parameter</li> <li>\u2705 Check if RNG keys exist before using them</li> <li>\u2705 Provide fallback RNG keys for deterministic mode</li> <li>\u2705 Use Flax NNX activations (<code>nnx.gelu</code>, <code>nnx.relu</code>)</li> <li>\u2705 Return dictionaries from <code>__call__</code> and <code>loss_fn</code></li> <li>\u2705 Use type hints for all method signatures</li> </ul>"},{"location":"api/core/base/#dont","title":"DON'T","text":"<ul> <li>\u274c Use <code>rngs.get(\"key_name\")</code> - check with <code>\"key_name\" in rngs</code></li> <li>\u274c Use functional dropout <code>nnx.dropout()</code> - use <code>nnx.Dropout</code> class</li> <li>\u274c Forget to initialize dropout with <code>rngs</code> parameter</li> <li>\u274c Use numpy-based packages inside <code>nnx.Module</code> classes</li> <li>\u274c Use JAX activations (<code>jax.nn.gelu</code>) - use NNX versions</li> </ul>"},{"location":"api/core/base/#performance-tips","title":"Performance Tips","text":""},{"location":"api/core/base/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code># For deep MLPs (\u22658 layers), enable gradient checkpointing\ndeep_mlp = MLP(\n    hidden_dims=[512] * 20,\n    in_features=1024,\n    use_gradient_checkpointing=True,\n    rngs=rngs\n)\n\n# Or use scan for automatic memory efficiency\noutput = deep_mlp(x, use_scan=True)\n</code></pre>"},{"location":"api/core/base/#precision-control","title":"Precision Control","text":"<pre><code># Use mixed precision for faster training\nmodel = GenerativeModel(\n    rngs=rngs,\n    precision=jax.lax.Precision.HIGH  # or DEFAULT, HIGHEST\n)\n</code></pre>"},{"location":"api/core/base/#depthwise-separable-convolutions","title":"Depthwise Separable Convolutions","text":"<pre><code># Reduce parameters and computation\nefficient_cnn = CNN(\n    hidden_dims=[64, 128, 256],\n    in_features=3,\n    use_depthwise_separable=True,  # Much more efficient\n    rngs=rngs\n)\n</code></pre>"},{"location":"api/core/base/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/core/base/#issue-attributeerror-rngs-object-has-no-attribute-get","title":"Issue: \"AttributeError: 'Rngs' object has no attribute 'get'\"","text":"<p>Solution: Use <code>\"key_name\" in rngs</code> to check for key existence:</p> <pre><code># WRONG\nif rngs is not None:\n    key = rngs.get(\"sample\")  # This will fail\n\n# CORRECT\nif rngs is not None and \"sample\" in rngs:\n    key = rngs.sample()\nelse:\n    key = jax.random.key(0)  # Fallback\n</code></pre>"},{"location":"api/core/base/#issue-typeerror-missing-required-argument-rngs","title":"Issue: \"TypeError: Missing required argument 'rngs'\"","text":"<p>Solution: Always provide <code>rngs</code> as a keyword argument:</p> <pre><code># WRONG\nmodel = MLP([256, 128], 784, rngs)\n\n# CORRECT\nmodel = MLP([256, 128], in_features=784, rngs=rngs)\n</code></pre>"},{"location":"api/core/base/#issue-forgot-to-call-superinit","title":"Issue: \"Forgot to call super().init()\"","text":"<p>Solution: Always call <code>super().__init__()</code> in your constructor:</p> <pre><code>class MyModel(GenerativeModule):\n    def __init__(self, *, rngs: nnx.Rngs):\n        super().__init__(rngs=rngs)  # ALWAYS include this\n        # ... rest of initialization\n</code></pre>"},{"location":"api/core/base/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Configuration</p> <p>Learn about the unified configuration system</p> <p> Configuration Guide</p> </li> <li> <p> Losses</p> <p>Explore the loss function catalog</p> <p> Loss Functions</p> </li> <li> <p> Device Management</p> <p>Manage GPU/CPU/TPU devices effectively</p> <p> Device Manager</p> </li> <li> <p> Model Implementations</p> <p>See concrete model implementations</p> <p> Models</p> </li> </ul>"},{"location":"api/core/base/#references","title":"References","text":"<ul> <li>Source Code: <code>src/artifex/generative_models/core/base.py</code></li> <li>Tests: <code>tests/artifex/generative_models/core/test_base.py</code></li> <li>Flax NNX Documentation: https://flax.readthedocs.io/en/latest/</li> </ul>"},{"location":"api/core/configuration/","title":"Configuration System","text":"<p>Artifex provides a unified, type-safe configuration system built on Pydantic. This system replaces fragmented configuration approaches with a centralized, validated, and extensible framework.</p>"},{"location":"api/core/configuration/#overview","title":"Overview","text":"<ul> <li> <p> Type-Safe</p> <p>Full Pydantic validation with automatic type checking</p> </li> <li> <p> Hierarchical</p> <p>Configuration inheritance and composition support</p> </li> <li> <p>:material-registry: Centralized Registry</p> <p>Global registry for configuration management</p> </li> <li> <p> YAML/JSON Support</p> <p>Easy serialization and deserialization</p> </li> </ul>"},{"location":"api/core/configuration/#configuration-architecture","title":"Configuration Architecture","text":"<pre><code>graph TD\n    A[BaseConfiguration] --&gt; B[ModelConfig]\n    A --&gt; C[TrainingConfig]\n    A --&gt; D[DataConfig]\n    A --&gt; E[EvaluationConfig]\n    A --&gt; F[ExperimentConfig]\n    A --&gt; G[OptimizerConfig]\n    A --&gt; H[SchedulerConfig]\n\n    C --&gt; G\n    C --&gt; H\n\n    F --&gt; B\n    F --&gt; C\n    F --&gt; D\n    F --&gt; E\n\n    I[ConfigurationRegistry] -.-&gt; B\n    I -.-&gt; C\n    I -.-&gt; D\n    I -.-&gt; E\n\n    style A fill:#e1f5ff\n    style B fill:#81d4fa\n    style C fill:#81d4fa\n    style D fill:#81d4fa\n    style E fill:#81d4fa\n    style F fill:#4fc3f7\n    style G fill:#b3e5fc\n    style H fill:#b3e5fc\n    style I fill:#66bb6a</code></pre>"},{"location":"api/core/configuration/#baseconfiguration","title":"BaseConfiguration","text":"<p>The root class for all configurations, providing common functionality.</p> <p>Location: <code>src/artifex/generative_models/core/configuration/unified.py:42</code></p>"},{"location":"api/core/configuration/#core-fields","title":"Core Fields","text":"Field Type Description <code>name</code> <code>str</code> Unique configuration name (required) <code>type</code> <code>ConfigurationType</code> Configuration type (required) <code>description</code> <code>str \\| None</code> Human-readable description <code>version</code> <code>str</code> Configuration version (default: \"1.0.0\") <code>tags</code> <code>list[str]</code> Tags for categorization <code>metadata</code> <code>dict[str, Any]</code> Non-functional metadata"},{"location":"api/core/configuration/#basic-usage","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    ModelConfig,\n    ConfigurationType\n)\n\n# Create a basic configuration\nconfig = ModelConfig(\n    name=\"my_vae\",\n    model_class=\"artifex.generative_models.models.vae.VAE\",\n    input_dim=(28, 28, 1),\n    description=\"VAE for MNIST\",\n    tags=[\"vae\", \"mnist\", \"baseline\"]\n)\n</code></pre>"},{"location":"api/core/configuration/#yaml-serialization","title":"YAML Serialization","text":"<pre><code>from pathlib import Path\n\n# Save to YAML\nconfig.to_yaml(\"configs/my_vae.yaml\")\n\n# Load from YAML\nloaded_config = ModelConfig.from_yaml(\"configs/my_vae.yaml\")\n</code></pre> <p>Example YAML:</p> <pre><code>name: my_vae\ntype: model\ndescription: VAE for MNIST\nversion: 1.0.0\ntags:\n  - vae\n  - mnist\n  - baseline\nmodel_class: artifex.generative_models.models.vae.VAE\ninput_dim:\n  - 28\n  - 28\n  - 1\nhidden_dims:\n  - 256\n  - 128\noutput_dim: 32\nactivation: gelu\ndropout_rate: 0.1\nparameters:\n  beta: 1.0\n  kl_weight: 0.5\n</code></pre>"},{"location":"api/core/configuration/#configuration-merging","title":"Configuration Merging","text":"<pre><code># Base configuration\nbase_config = ModelConfig(\n    name=\"base_vae\",\n    model_class=\"artifex.generative_models.models.vae.VAE\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 128],\n)\n\n# Override specific fields\noverride_dict = {\n    \"hidden_dims\": [512, 256, 128],\n    \"dropout_rate\": 0.2,\n}\n\n# Merge configurations\nmerged_config = base_config.merge(override_dict)\nprint(merged_config.hidden_dims)  # [512, 256, 128]\nprint(merged_config.dropout_rate)  # 0.2\n</code></pre>"},{"location":"api/core/configuration/#modelconfig","title":"ModelConfig","text":"<p>The primary configuration class for all generative models.</p> <p>Location: <code>src/artifex/generative_models.core/configuration/unified.py:126</code></p>"},{"location":"api/core/configuration/#architecture-fields","title":"Architecture Fields","text":"Field Type Default Description <code>model_class</code> <code>str</code> Required Fully qualified model class name <code>input_dim</code> <code>int \\| tuple[int, ...]</code> Required Input dimensions <code>hidden_dims</code> <code>list[int]</code> <code>[128, 256, 512]</code> Hidden layer dimensions <code>output_dim</code> <code>int \\| tuple[int, ...] \\| None</code> <code>None</code> Output dimensions <code>activation</code> <code>str</code> <code>\"gelu\"</code> Activation function <code>dropout_rate</code> <code>float</code> <code>0.1</code> Dropout rate <code>use_batch_norm</code> <code>bool</code> <code>True</code> Use batch normalization"},{"location":"api/core/configuration/#nnx-specific-fields","title":"NNX-Specific Fields","text":"Field Type Default Description <code>rngs_seeds</code> <code>dict[str, int]</code> <code>{\"params\": 0, \"dropout\": 1}</code> RNG seeds for NNX"},{"location":"api/core/configuration/#parameter-handling","title":"Parameter Handling","text":"<p>Important: parameters vs metadata</p> <p>Use <code>parameters</code> for functional configuration that affects model behavior. Use <code>metadata</code> for non-functional information like experiment tracking.</p>"},{"location":"api/core/configuration/#the-parameters-field","title":"The <code>parameters</code> Field","text":"<p>For functional model-specific parameters:</p> <pre><code>config = ModelConfig(\n    name=\"beta_vae\",\n    model_class=\"artifex.generative_models.models.vae.BetaVAE\",\n    input_dim=(64, 64, 3),\n    hidden_dims=[256, 512],\n    output_dim=128,\n    # Functional parameters that affect model behavior\n    parameters={\n        \"beta\": 4.0,                    # VAE \u03b2-parameter\n        \"kl_weight\": 0.5,               # KL divergence weight\n        \"reconstruction_loss\": \"mse\",   # Loss function type\n        \"use_free_bits\": True,          # Free bits constraint\n        \"free_bits_lambda\": 2.0,        # Free bits threshold\n    }\n)\n</code></pre>"},{"location":"api/core/configuration/#the-metadata-field","title":"The <code>metadata</code> Field","text":"<p>For non-functional tracking and documentation:</p> <pre><code>config = ModelConfig(\n    name=\"experiment_vae\",\n    model_class=\"artifex.generative_models.models.vae.VAE\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 128],\n    # Non-functional metadata\n    metadata={\n        \"experiment_id\": \"exp_2024_001\",\n        \"dataset_version\": \"v2.1\",\n        \"author\": \"research_team\",\n        \"notes\": \"Testing higher beta values\",\n        \"hyperparameter_search_space\": {\n            \"beta\": [1.0, 2.0, 4.0, 8.0],\n            \"kl_weight\": [0.1, 0.5, 1.0]\n        }\n    }\n)\n</code></pre>"},{"location":"api/core/configuration/#complete-example","title":"Complete Example","text":"<pre><code>from artifex.generative_models.core.configuration import ModelConfig\n\nconfig = ModelConfig(\n    # Basic identification\n    name=\"celeba_vae\",\n    description=\"VAE trained on CelebA dataset\",\n    version=\"1.0.0\",\n    tags=[\"vae\", \"celeba\", \"production\"],\n\n    # Model architecture\n    model_class=\"artifex.generative_models.models.vae.VAE\",\n    input_dim=(64, 64, 3),\n    hidden_dims=[64, 128, 256, 512],\n    output_dim=256,\n\n    # Training configuration\n    activation=\"gelu\",\n    dropout_rate=0.1,\n    use_batch_norm=True,\n\n    # RNG seeds for reproducibility\n    rngs_seeds={\n        \"params\": 42,\n        \"dropout\": 43,\n        \"sample\": 44,\n    },\n\n    # Functional model parameters\n    parameters={\n        \"beta\": 1.0,\n        \"kl_weight\": 0.5,\n        \"reconstruction_loss\": \"mse\",\n        \"latent_distribution\": \"gaussian\",\n    },\n\n    # Non-functional metadata\n    metadata={\n        \"experiment_id\": \"celeba_baseline_001\",\n        \"dataset_version\": \"v3.0\",\n        \"created_at\": \"2024-01-15\",\n    }\n)\n\n# Validate and use\nprint(config.model_dump())\n</code></pre>"},{"location":"api/core/configuration/#validation","title":"Validation","text":"<p>ModelConfig includes automatic validation:</p> <pre><code># Valid activation\nconfig = ModelConfig(\n    name=\"valid_model\",\n    model_class=\"artifex.generative_models.models.vae.VAE\",\n    input_dim=(28, 28, 1),\n    activation=\"gelu\"  # \u2713 Valid\n)\n\n# Invalid activation - raises ValidationError\ntry:\n    config = ModelConfig(\n        name=\"invalid_model\",\n        model_class=\"artifex.generative_models.models.vae.VAE\",\n        input_dim=(28, 28, 1),\n        activation=\"invalid_activation\"  # \u2717 Invalid\n    )\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n</code></pre> <p>Valid activations:</p> <ul> <li><code>relu</code>, <code>gelu</code>, <code>swish</code>, <code>silu</code></li> <li><code>tanh</code>, <code>sigmoid</code>, <code>elu</code>, <code>leaky_relu</code></li> </ul>"},{"location":"api/core/configuration/#trainingconfig","title":"TrainingConfig","text":"<p>Configuration for model training.</p> <p>Location: <code>src/artifex/generative_models/core/configuration/unified.py:323</code></p>"},{"location":"api/core/configuration/#fields","title":"Fields","text":"Field Type Default Description <code>batch_size</code> <code>int</code> <code>32</code> Training batch size <code>num_epochs</code> <code>int</code> <code>100</code> Number of training epochs <code>gradient_clip_norm</code> <code>float \\| None</code> <code>1.0</code> Gradient clipping norm <code>optimizer</code> <code>OptimizerConfig</code> Required Optimizer configuration <code>scheduler</code> <code>SchedulerConfig \\| None</code> <code>None</code> LR scheduler configuration"},{"location":"api/core/configuration/#checkpointing","title":"Checkpointing","text":"Field Type Default Description <code>checkpoint_dir</code> <code>Path</code> <code>\"./checkpoints\"</code> Checkpoint directory <code>save_frequency</code> <code>int</code> <code>1000</code> Save every N steps <code>max_checkpoints</code> <code>int</code> <code>5</code> Maximum checkpoints to keep"},{"location":"api/core/configuration/#logging","title":"Logging","text":"Field Type Default Description <code>log_frequency</code> <code>int</code> <code>100</code> Log every N steps <code>use_wandb</code> <code>bool</code> <code>False</code> Use Weights &amp; Biases <code>wandb_project</code> <code>str \\| None</code> <code>None</code> W&amp;B project name"},{"location":"api/core/configuration/#complete-example_1","title":"Complete Example","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    TrainingConfig,\n    OptimizerConfig,\n    SchedulerConfig,\n)\nfrom pathlib import Path\n\n# Create optimizer configuration\noptimizer = OptimizerConfig(\n    name=\"adamw_optimizer\",\n    optimizer_type=\"adamw\",\n    learning_rate=1e-4,\n    weight_decay=0.01,\n    beta1=0.9,\n    beta2=0.999,\n    gradient_clip_norm=1.0,\n)\n\n# Create scheduler configuration\nscheduler = SchedulerConfig(\n    name=\"cosine_scheduler\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    min_lr_ratio=0.1,\n)\n\n# Create training configuration\ntraining_config = TrainingConfig(\n    name=\"vae_training\",\n    batch_size=64,\n    num_epochs=100,\n    gradient_clip_norm=1.0,\n\n    # Optimizer and scheduler\n    optimizer=optimizer,\n    scheduler=scheduler,\n\n    # Checkpointing\n    checkpoint_dir=Path(\"./checkpoints/vae_exp\"),\n    save_frequency=1000,\n    max_checkpoints=5,\n\n    # Logging\n    log_frequency=100,\n    use_wandb=True,\n    wandb_project=\"vae-experiments\",\n)\n</code></pre>"},{"location":"api/core/configuration/#optimizerconfig","title":"OptimizerConfig","text":"<p>Configuration for optimizers.</p> <p>Location: <code>src/artifex/generative_models/core/configuration/unified.py:224</code></p>"},{"location":"api/core/configuration/#core-fields_1","title":"Core Fields","text":"Field Type Description <code>optimizer_type</code> <code>str</code> Type of optimizer <code>learning_rate</code> <code>float</code> Learning rate (required, &gt; 0) <code>weight_decay</code> <code>float</code> Weight decay / L2 penalty"},{"location":"api/core/configuration/#adamadamw-parameters","title":"Adam/AdamW Parameters","text":"Field Type Default Description <code>beta1</code> <code>float</code> <code>0.9</code> First moment decay <code>beta2</code> <code>float</code> <code>0.999</code> Second moment decay <code>eps</code> <code>float</code> <code>1e-8</code> Epsilon for stability"},{"location":"api/core/configuration/#gradient-clipping","title":"Gradient Clipping","text":"Field Type Default Description <code>gradient_clip_norm</code> <code>float \\| None</code> <code>None</code> Clip gradients by norm <code>gradient_clip_value</code> <code>float \\| None</code> <code>None</code> Clip gradients by value"},{"location":"api/core/configuration/#examples","title":"Examples","text":"<p>AdamW:</p> <pre><code>optimizer = OptimizerConfig(\n    name=\"adamw\",\n    optimizer_type=\"adamw\",\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    beta1=0.9,\n    beta2=0.999,\n    eps=1e-8,\n    gradient_clip_norm=1.0,\n)\n</code></pre> <p>SGD with Momentum:</p> <pre><code>optimizer = OptimizerConfig(\n    name=\"sgd_momentum\",\n    optimizer_type=\"sgd\",\n    learning_rate=0.1,\n    momentum=0.9,\n    nesterov=True,\n    gradient_clip_norm=5.0,\n)\n</code></pre> <p>Valid optimizer types:</p> <ul> <li><code>adam</code>, <code>adamw</code>, <code>sgd</code>, <code>rmsprop</code></li> <li><code>adagrad</code>, <code>lamb</code>, <code>radam</code>, <code>nadam</code></li> </ul>"},{"location":"api/core/configuration/#schedulerconfig","title":"SchedulerConfig","text":"<p>Configuration for learning rate schedulers.</p> <p>Location: <code>src/artifex/generative_models/core/configuration/unified.py:270</code></p>"},{"location":"api/core/configuration/#core-fields_2","title":"Core Fields","text":"Field Type Default Description <code>scheduler_type</code> <code>str</code> Required Type of scheduler <code>warmup_steps</code> <code>int</code> <code>0</code> Number of warmup steps <code>min_lr_ratio</code> <code>float</code> <code>0.0</code> Minimum LR as ratio of initial LR"},{"location":"api/core/configuration/#scheduler-specific-parameters","title":"Scheduler-Specific Parameters","text":"<p>Cosine Scheduler:</p> <pre><code>scheduler = SchedulerConfig(\n    name=\"cosine\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    min_lr_ratio=0.1,\n    cycle_length=10000,  # Optional: restart cycle length\n)\n</code></pre> <p>Exponential Decay:</p> <pre><code>scheduler = SchedulerConfig(\n    name=\"exponential\",\n    scheduler_type=\"exponential\",\n    decay_rate=0.95,\n    decay_steps=1000,\n)\n</code></pre> <p>Linear Schedule:</p> <pre><code>scheduler = SchedulerConfig(\n    name=\"linear\",\n    scheduler_type=\"linear\",\n    warmup_steps=500,\n    total_steps=10000,\n)\n</code></pre> <p>Step Scheduler:</p> <pre><code>scheduler = SchedulerConfig(\n    name=\"step\",\n    scheduler_type=\"step\",\n    step_size=1000,\n    gamma=0.1,\n)\n</code></pre> <p>Valid scheduler types:</p> <ul> <li><code>constant</code>, <code>linear</code>, <code>cosine</code>, <code>exponential</code></li> <li><code>polynomial</code>, <code>step</code>, <code>multistep</code>, <code>cyclic</code>, <code>one_cycle</code></li> </ul>"},{"location":"api/core/configuration/#dataconfig","title":"DataConfig","text":"<p>Configuration for data loading and preprocessing.</p> <p>Location: <code>src/artifex/generative_models/core/configuration/unified.py:352</code></p>"},{"location":"api/core/configuration/#dataset-fields","title":"Dataset Fields","text":"Field Type Default Description <code>dataset_name</code> <code>str</code> Required Name of the dataset <code>data_dir</code> <code>Path</code> <code>\"./data\"</code> Data directory <code>split</code> <code>str</code> <code>\"train\"</code> Data split to use"},{"location":"api/core/configuration/#processing-fields","title":"Processing Fields","text":"Field Type Default Description <code>num_workers</code> <code>int</code> <code>4</code> Number of data loading workers <code>prefetch_factor</code> <code>int</code> <code>2</code> Prefetch factor <code>pin_memory</code> <code>bool</code> <code>True</code> Pin memory for GPU transfer"},{"location":"api/core/configuration/#augmentation","title":"Augmentation","text":"Field Type Default Description <code>augmentation</code> <code>bool</code> <code>False</code> Use data augmentation <code>augmentation_params</code> <code>dict[str, Any]</code> <code>{}</code> Augmentation parameters"},{"location":"api/core/configuration/#example","title":"Example","text":"<pre><code>from pathlib import Path\nfrom artifex.generative_models.core.configuration import DataConfig\n\ndata_config = DataConfig(\n    name=\"celeba_data\",\n    dataset_name=\"celeba\",\n    data_dir=Path(\"./data/celeba\"),\n    split=\"train\",\n\n    # Data loading\n    num_workers=8,\n    prefetch_factor=2,\n    pin_memory=True,\n\n    # Augmentation\n    augmentation=True,\n    augmentation_params={\n        \"horizontal_flip\": True,\n        \"random_crop\": (64, 64),\n        \"color_jitter\": {\n            \"brightness\": 0.2,\n            \"contrast\": 0.2,\n        }\n    },\n\n    # Validation split\n    validation_split=0.1,\n    test_split=0.1,\n)\n</code></pre>"},{"location":"api/core/configuration/#evaluationconfig","title":"EvaluationConfig","text":"<p>Configuration for evaluation and metrics.</p> <p>Location: <code>src/artifex/generative_models/core/configuration/unified.py:399</code></p>"},{"location":"api/core/configuration/#metrics","title":"Metrics","text":"Field Type Description <code>metrics</code> <code>list[str]</code> Metrics to compute (required) <code>metric_params</code> <code>dict[str, dict[str, Any]]</code> Per-metric parameters"},{"location":"api/core/configuration/#settings","title":"Settings","text":"Field Type Default Description <code>eval_batch_size</code> <code>int</code> <code>32</code> Evaluation batch size <code>num_eval_samples</code> <code>int \\| None</code> <code>None</code> Number of samples to evaluate"},{"location":"api/core/configuration/#output","title":"Output","text":"Field Type Default Description <code>save_predictions</code> <code>bool</code> <code>False</code> Save model predictions <code>save_metrics</code> <code>bool</code> <code>True</code> Save computed metrics <code>output_dir</code> <code>Path</code> <code>\"./evaluation\"</code> Output directory"},{"location":"api/core/configuration/#example_1","title":"Example","text":"<pre><code>from artifex.generative_models.core.configuration import EvaluationConfig\n\neval_config = EvaluationConfig(\n    name=\"vae_evaluation\",\n\n    # Metrics to compute\n    metrics=[\"fid\", \"inception_score\", \"reconstruction_error\"],\n\n    # Per-metric parameters\n    metric_params={\n        \"fid\": {\n            \"num_samples\": 10000,\n            \"batch_size\": 50,\n        },\n        \"inception_score\": {\n            \"num_samples\": 50000,\n            \"splits\": 10,\n        }\n    },\n\n    # Evaluation settings\n    eval_batch_size=64,\n    num_eval_samples=10000,\n\n    # Output\n    save_predictions=True,\n    save_metrics=True,\n    output_dir=Path(\"./results/evaluation\"),\n)\n</code></pre>"},{"location":"api/core/configuration/#experimentconfig","title":"ExperimentConfig","text":"<p>Complete experiment configuration that composes all other configurations.</p> <p>Location: <code>src/artifex/generative_models/core/configuration/unified.py:420</code></p>"},{"location":"api/core/configuration/#component-configurations","title":"Component Configurations","text":"Field Type Description <code>model_cfg</code> <code>ModelConfig \\| str</code> Model configuration <code>training_cfg</code> <code>TrainingConfig \\| str</code> Training configuration <code>data_cfg</code> <code>DataConfig \\| str</code> Data configuration <code>eval_cfg</code> <code>EvaluationConfig \\| str \\| None</code> Evaluation configuration"},{"location":"api/core/configuration/#experiment-settings","title":"Experiment Settings","text":"Field Type Default Description <code>seed</code> <code>int</code> <code>42</code> Random seed <code>deterministic</code> <code>bool</code> <code>True</code> Use deterministic algorithms <code>output_dir</code> <code>Path</code> <code>\"./experiments\"</code> Output directory"},{"location":"api/core/configuration/#tracking","title":"Tracking","text":"Field Type Default Description <code>track_carbon</code> <code>bool</code> <code>False</code> Track carbon emissions <code>track_memory</code> <code>bool</code> <code>False</code> Track memory usage"},{"location":"api/core/configuration/#complete-example_2","title":"Complete Example","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    ExperimentConfig,\n    ModelConfig,\n    TrainingConfig,\n    OptimizerConfig,\n    DataConfig,\n)\n\n# Define all component configurations\nmodel_cfg = ModelConfig(\n    name=\"vae_model\",\n    model_class=\"artifex.generative_models.models.vae.VAE\",\n    input_dim=(64, 64, 3),\n    hidden_dims=[64, 128, 256],\n    output_dim=128,\n)\n\noptimizer_cfg = OptimizerConfig(\n    name=\"adamw\",\n    optimizer_type=\"adamw\",\n    learning_rate=1e-4,\n    weight_decay=0.01,\n)\n\ntraining_cfg = TrainingConfig(\n    name=\"training\",\n    batch_size=64,\n    num_epochs=100,\n    optimizer=optimizer_cfg,\n)\n\ndata_cfg = DataConfig(\n    name=\"celeba\",\n    dataset_name=\"celeba\",\n    data_dir=Path(\"./data/celeba\"),\n)\n\n# Create experiment configuration\nexperiment = ExperimentConfig(\n    name=\"celeba_vae_experiment\",\n    description=\"VAE training on CelebA dataset\",\n\n    # Component configurations\n    model_cfg=model_cfg,\n    training_cfg=training_cfg,\n    data_cfg=data_cfg,\n\n    # Experiment settings\n    seed=42,\n    deterministic=True,\n    output_dir=Path(\"./experiments/celeba_vae\"),\n\n    # Tracking\n    track_carbon=True,\n    track_memory=True,\n)\n\n# Save complete experiment configuration\nexperiment.to_yaml(\"experiments/celeba_vae/config.yaml\")\n</code></pre>"},{"location":"api/core/configuration/#configurationregistry","title":"ConfigurationRegistry","text":"<p>Global registry for managing configurations.</p> <p>Location: <code>src/artifex/generative_models/core/configuration/unified.py:454</code></p>"},{"location":"api/core/configuration/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    ConfigurationRegistry,\n    ModelConfig,\n    ConfigurationType,\n)\n\n# Create registry\nregistry = ConfigurationRegistry()\n\n# Register a configuration\nconfig = ModelConfig(\n    name=\"vae_v1\",\n    model_class=\"artifex.generative_models.models.vae.VAE\",\n    input_dim=(28, 28, 1),\n)\nregistry.register(config)\n\n# Retrieve configuration\nretrieved = registry.get(\"vae_v1\", ConfigurationType.MODEL)\n\n# List all configurations\nall_configs = registry.list_configs()\nprint(all_configs)  # ['model/vae_v1', ...]\n</code></pre>"},{"location":"api/core/configuration/#templates","title":"Templates","text":"<p>Create reusable configuration templates:</p> <pre><code># Register a template\ntemplate = {\n    \"model_class\": \"artifex.generative_models.models.vae.VAE\",\n    \"hidden_dims\": [256, 128],\n    \"activation\": \"gelu\",\n    \"dropout_rate\": 0.1,\n}\nregistry.register_template(\"base_vae\", template)\n\n# Create configuration from template\nconfig = registry.create_from_template(\n    \"base_vae\",\n    ModelConfig,\n    name=\"my_vae\",\n    input_dim=(28, 28, 1),\n    # Override template values\n    hidden_dims=[512, 256, 128],\n)\n</code></pre>"},{"location":"api/core/configuration/#load-from-directory","title":"Load from Directory","text":"<pre><code># Load all configurations from a directory\nregistry.load_from_directory(\"./configs\")\n\n# Automatically loads:\n# - model_*.yaml \u2192 ModelConfig\n# - training_*.yaml \u2192 TrainingConfig\n# - data_*.yaml \u2192 DataConfig\n# - eval_*.yaml \u2192 EvaluationConfig\n# - experiment_*.yaml \u2192 ExperimentConfig\n</code></pre>"},{"location":"api/core/configuration/#global-registry-functions","title":"Global Registry Functions","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    register_config,\n    get_config,\n    list_configs,\n)\n\n# Register configuration globally\nregister_config(my_config)\n\n# Get configuration from global registry\nconfig = get_config(\"vae_v1\", ConfigurationType.MODEL)\n\n# List all configs\nconfigs = list_configs(ConfigurationType.MODEL)\n</code></pre>"},{"location":"api/core/configuration/#common-patterns","title":"Common Patterns","text":""},{"location":"api/core/configuration/#pattern-1-configuration-from-yaml","title":"Pattern 1: Configuration from YAML","text":"<pre><code># Create YAML file: configs/my_model.yaml\n```yaml\nname: my_vae\ntype: model\nmodel_class: artifex.generative_models.models.vae.VAE\ninput_dim: [64, 64, 3]\nhidden_dims: [64, 128, 256]\noutput_dim: 128\nparameters:\n  beta: 1.0\n  kl_weight: 0.5\n</code></pre> <pre><code># Load configuration\nconfig = ModelConfig.from_yaml(\"configs/my_model.yaml\")\n\n# Use in code\nprint(config.parameters[\"beta\"])  # 1.0\n</code></pre>"},{"location":"api/core/configuration/#pattern-2-configuration-inheritance","title":"Pattern 2: Configuration Inheritance","text":"<pre><code># Base configuration for all VAE experiments\nbase_vae = ModelConfig(\n    name=\"base_vae\",\n    model_class=\"artifex.generative_models.models.vae.VAE\",\n    input_dim=(64, 64, 3),\n    hidden_dims=[64, 128, 256],\n    activation=\"gelu\",\n    dropout_rate=0.1,\n)\n\n# Experiment 1: Higher capacity\nhigh_capacity = base_vae.merge({\n    \"name\": \"high_capacity_vae\",\n    \"hidden_dims\": [128, 256, 512, 1024],\n    \"output_dim\": 512,\n})\n\n# Experiment 2: Different beta\nbeta_vae = base_vae.merge({\n    \"name\": \"beta_vae_4\",\n    \"parameters\": {\"beta\": 4.0},\n})\n</code></pre>"},{"location":"api/core/configuration/#pattern-3-complete-experiment-setup","title":"Pattern 3: Complete Experiment Setup","text":"<pre><code>def create_experiment_config(\n    name: str,\n    model_params: dict,\n    training_params: dict,\n) -&gt; ExperimentConfig:\n    \"\"\"Create a complete experiment configuration.\"\"\"\n\n    # Model configuration\n    model_cfg = ModelConfig(\n        name=f\"{name}_model\",\n        **model_params\n    )\n\n    # Optimizer\n    optimizer_cfg = OptimizerConfig(\n        name=\"adamw\",\n        optimizer_type=\"adamw\",\n        **training_params.get(\"optimizer\", {})\n    )\n\n    # Training configuration\n    training_cfg = TrainingConfig(\n        name=f\"{name}_training\",\n        optimizer=optimizer_cfg,\n        **{k: v for k, v in training_params.items() if k != \"optimizer\"}\n    )\n\n    # Data configuration\n    data_cfg = DataConfig(\n        name=f\"{name}_data\",\n        dataset_name=training_params.get(\"dataset\", \"mnist\"),\n    )\n\n    # Experiment\n    return ExperimentConfig(\n        name=name,\n        model_cfg=model_cfg,\n        training_cfg=training_cfg,\n        data_cfg=data_cfg,\n        seed=42,\n    )\n\n# Use the factory\nexperiment = create_experiment_config(\n    name=\"vae_exp_001\",\n    model_params={\n        \"model_class\": \"artifex.generative_models.models.vae.VAE\",\n        \"input_dim\": (28, 28, 1),\n        \"hidden_dims\": [256, 128],\n    },\n    training_params={\n        \"batch_size\": 64,\n        \"num_epochs\": 100,\n        \"optimizer\": {\n            \"learning_rate\": 1e-4,\n            \"weight_decay\": 0.01,\n        },\n        \"dataset\": \"mnist\",\n    }\n)\n</code></pre>"},{"location":"api/core/configuration/#best-practices","title":"Best Practices","text":""},{"location":"api/core/configuration/#do","title":"DO","text":"<ul> <li>\u2705 Use <code>parameters</code> for functional model configuration</li> <li>\u2705 Use <code>metadata</code> for non-functional tracking information</li> <li>\u2705 Validate configurations with Pydantic before training</li> <li>\u2705 Save configurations with experiments for reproducibility</li> <li>\u2705 Use YAML files for human-readable configurations</li> <li>\u2705 Use configuration registry for managing multiple configs</li> <li>\u2705 Create configuration templates for common patterns</li> </ul>"},{"location":"api/core/configuration/#dont","title":"DON'T","text":"<ul> <li>\u274c Store model parameters in <code>metadata</code> field</li> <li>\u274c Use nested dictionaries for model parameters (use <code>parameters</code> field)</li> <li>\u274c Skip validation - let Pydantic catch errors early</li> <li>\u274c Hard-code configuration values in training scripts</li> <li>\u274c Forget to version your configurations</li> </ul>"},{"location":"api/core/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/core/configuration/#issue-validationerror-invalid-activation","title":"Issue: \"ValidationError: Invalid activation\"","text":"<p>Solution: Use one of the supported activation functions:</p> <pre><code># Valid\nconfig = ModelConfig(\n    ...,\n    activation=\"gelu\"  # \u2713\n)\n\n# Invalid\nconfig = ModelConfig(\n    ...,\n    activation=\"custom_act\"  # \u2717 - not in valid list\n)\n</code></pre>"},{"location":"api/core/configuration/#issue-configuration-merge-not-working-as-expected","title":"Issue: \"Configuration merge not working as expected\"","text":"<p>Solution: Understand deep merge behavior:</p> <pre><code>config1 = ModelConfig(\n    name=\"base\",\n    ...,\n    parameters={\"beta\": 1.0, \"kl_weight\": 0.5}\n)\n\n# This replaces the entire parameters dict\nmerged = config1.merge({\"parameters\": {\"beta\": 2.0}})\nprint(merged.parameters)  # {\"beta\": 2.0} - kl_weight is gone!\n\n# To preserve keys, merge at parameter level\nmerged = config1.merge({\n    \"parameters\": {\n        **config1.parameters,\n        \"beta\": 2.0\n    }\n})\nprint(merged.parameters)  # {\"beta\": 2.0, \"kl_weight\": 0.5} \u2713\n</code></pre>"},{"location":"api/core/configuration/#issue-extra-fields-not-allowed","title":"Issue: \"Extra fields not allowed\"","text":"<p>Solution: Pydantic forbids extra fields by default. Use the correct fields:</p> <pre><code># Wrong - unknown_field is not defined\nconfig = ModelConfig(\n    name=\"test\",\n    ...,\n    unknown_field=\"value\"  # \u2717 ValidationError\n)\n\n# Correct - use metadata for custom fields\nconfig = ModelConfig(\n    name=\"test\",\n    ...,\n    metadata={\"unknown_field\": \"value\"}  # \u2713\n)\n</code></pre>"},{"location":"api/core/configuration/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Base Classes</p> <p>Learn about the base model architecture</p> <p> Base Classes</p> </li> <li> <p> Losses</p> <p>Explore the loss function catalog</p> <p> Loss Functions</p> </li> <li> <p> Training</p> <p>See how to use configurations in training</p> <p> Training Guide</p> </li> </ul>"},{"location":"api/core/configuration/#references","title":"References","text":"<ul> <li>Source Code: <code>src/artifex/generative_models/core/configuration/unified.py</code></li> <li>Tests: <code>tests/artifex/generative_models/core/configuration/test_unified.py</code></li> <li>Pydantic Documentation: https://docs.pydantic.dev/</li> </ul>"},{"location":"api/core/device-manager/","title":"Device Management","text":"<p>Artifex provides a comprehensive device management system for efficiently utilizing GPUs, TPUs, and CPUs. The system automatically detects hardware capabilities, configures JAX appropriately, and provides utilities for distributed computing.</p>"},{"location":"api/core/device-manager/#overview","title":"Overview","text":"<ul> <li> <p> Auto-Detection</p> <p>Automatic detection of available compute devices</p> </li> <li> <p> Memory Management</p> <p>Configurable memory allocation strategies</p> </li> <li> <p> Performance Tuning</p> <p>Optimized configurations for different model sizes</p> </li> <li> <p> Multi-Device Support</p> <p>Seamless scaling across multiple devices</p> </li> </ul>"},{"location":"api/core/device-manager/#architecture","title":"Architecture","text":"<pre><code>graph TD\n    A[DeviceManager] --&gt; B[CUDADetector]\n    A --&gt; C[JAXDeviceManager]\n    A --&gt; D[DeviceConfiguration]\n\n    B --&gt; E[Device Capabilities]\n    C --&gt; F[Device Placement]\n    C --&gt; G[Data Distribution]\n\n    D --&gt; H[Memory Strategy]\n    D --&gt; I[Environment Variables]\n\n    style A fill:#81d4fa\n    style B fill:#b3e5fc\n    style C fill:#b3e5fc\n    style D fill:#b3e5fc\n    style E fill:#e1f5ff\n    style F fill:#e1f5ff\n    style G fill:#e1f5ff</code></pre> <p>Location: <code>src/artifex/generative_models/core/device_manager.py</code></p>"},{"location":"api/core/device-manager/#quick-start","title":"Quick Start","text":""},{"location":"api/core/device-manager/#basic-usage","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.core.device_manager import DeviceManager\n\n# Create device manager with default configuration\ndevice_manager = DeviceManager()\n\n# Check available devices\nprint(f\"Has GPU: {device_manager.has_gpu}\")\nprint(f\"Device count: {device_manager.device_count}\")\nprint(f\"GPU count: {device_manager.gpu_count}\")\n\n# Get device information\ninfo = device_manager.get_device_info()\nprint(f\"Backend: {info['backend']}\")\nprint(f\"Default device: {info['default_device']}\")\n</code></pre>"},{"location":"api/core/device-manager/#getting-default-device","title":"Getting Default Device","text":"<pre><code>from artifex.generative_models.core.device_manager import get_default_device\n\n# Get default compute device\ndevice = get_default_device()\nprint(f\"Using device: {device}\")\n\n# Place data on device\nimport jax\nx = jax.device_put(jnp.ones((100, 100)), device)\n</code></pre>"},{"location":"api/core/device-manager/#deviceconfiguration","title":"DeviceConfiguration","text":"<p>Configure device behavior with memory strategies and environment variables.</p>"},{"location":"api/core/device-manager/#memory-strategies","title":"Memory Strategies","text":"Strategy Memory Fraction Use Case <code>CONSERVATIVE</code> 60% Small models, shared GPUs <code>BALANCED</code> 75% Default, most use cases <code>AGGRESSIVE</code> 90% Large models, dedicated GPUs <code>CUSTOM</code> User-defined Special requirements"},{"location":"api/core/device-manager/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from artifex.generative_models.core.device_manager import (\n    DeviceManager,\n    DeviceConfiguration,\n    MemoryStrategy\n)\n\n# Conservative memory usage (60%)\nconfig = DeviceConfiguration(\n    memory_strategy=MemoryStrategy.CONSERVATIVE,\n    enable_x64=False,\n    enable_jit=True\n)\n\ndevice_manager = DeviceManager(config)\n</code></pre>"},{"location":"api/core/device-manager/#custom-memory-fraction","title":"Custom Memory Fraction","text":"<pre><code># Specify exact memory fraction\nconfig = DeviceConfiguration(\n    memory_strategy=MemoryStrategy.CUSTOM,\n    memory_fraction=0.8,  # Use 80% of GPU memory\n    enable_x64=False\n)\n\ndevice_manager = DeviceManager(config)\n</code></pre>"},{"location":"api/core/device-manager/#platform-priority","title":"Platform Priority","text":"<pre><code># Specify device priority\nconfig = DeviceConfiguration(\n    platform_priority=[\"cuda\", \"cpu\"],  # Prefer CUDA over CPU\n    memory_strategy=MemoryStrategy.BALANCED\n)\n\ndevice_manager = DeviceManager(config)\n</code></pre>"},{"location":"api/core/device-manager/#environment-variables","title":"Environment Variables","text":"<pre><code># Custom JAX environment configuration\nconfig = DeviceConfiguration(\n    environment_variables={\n        \"XLA_PYTHON_CLIENT_PREALLOCATE\": \"false\",\n        \"XLA_FLAGS\": \"--xla_gpu_cuda_data_dir=/usr/local/cuda\",\n    }\n)\n\ndevice_manager = DeviceManager(config)\n</code></pre>"},{"location":"api/core/device-manager/#devicecapabilities","title":"DeviceCapabilities","text":"<p>Information about detected hardware capabilities.</p> <pre><code># Get device capabilities\ncapabilities = device_manager.capabilities\n\nprint(f\"Device type: {capabilities.device_type}\")\nprint(f\"Device count: {capabilities.device_count}\")\nprint(f\"Total memory: {capabilities.total_memory_mb} MB\")\nprint(f\"CUDA version: {capabilities.cuda_version}\")\nprint(f\"Compute capability: {capabilities.compute_capability}\")\nprint(f\"Mixed precision support: {capabilities.supports_mixed_precision}\")\nprint(f\"Distributed support: {capabilities.supports_distributed}\")\n</code></pre>"},{"location":"api/core/device-manager/#fields","title":"Fields","text":"Field Type Description <code>device_type</code> <code>DeviceType</code> CPU, GPU, or TPU <code>device_count</code> <code>int</code> Number of available devices <code>total_memory_mb</code> <code>int \\| None</code> Total memory in MB (GPU only) <code>compute_capability</code> <code>str \\| None</code> CUDA compute capability <code>cuda_version</code> <code>str \\| None</code> CUDA version <code>driver_version</code> <code>str \\| None</code> GPU driver version <code>supports_mixed_precision</code> <code>bool</code> Mixed precision support <code>supports_distributed</code> <code>bool</code> Multi-device support"},{"location":"api/core/device-manager/#jaxdevicemanager","title":"JAXDeviceManager","text":"<p>Low-level JAX device management and data distribution.</p>"},{"location":"api/core/device-manager/#getting-devices","title":"Getting Devices","text":"<pre><code># Access JAX device manager\njax_manager = device_manager.jax_manager\n\n# Get all devices\nall_devices = jax_manager.devices\nprint(f\"All devices: {all_devices}\")\n\n# Get GPU devices only\ngpu_devices = jax_manager.gpu_devices\nprint(f\"GPU devices: {gpu_devices}\")\n\n# Get CPU devices\ncpu_devices = jax_manager.cpu_devices\nprint(f\"CPU devices: {cpu_devices}\")\n\n# Get default device\ndefault_device = jax_manager.get_default_device()\nprint(f\"Default: {default_device}\")\n</code></pre>"},{"location":"api/core/device-manager/#data-distribution","title":"Data Distribution","text":"<p>Distribute data across multiple devices for parallel processing.</p> <pre><code>import jax.numpy as jnp\n\n# Create data\ndata = jnp.ones((128, 224, 224, 3))\n\n# Distribute across all GPU devices\ndistributed_data = jax_manager.distribute_data(data)\n\nprint(f\"Original shape: {data.shape}\")\nprint(f\"Shards: {len(distributed_data)}\")\nfor i, shard in enumerate(distributed_data):\n    print(f\"  Shard {i}: {shard.shape} on {shard.device()}\")\n</code></pre>"},{"location":"api/core/device-manager/#custom-device-selection","title":"Custom Device Selection","text":"<pre><code># Distribute to specific devices\ntarget_devices = jax_manager.gpu_devices[:2]  # First 2 GPUs\ndistributed = jax_manager.distribute_data(data, target_devices=target_devices)\n</code></pre>"},{"location":"api/core/device-manager/#convenience-functions","title":"Convenience Functions","text":""},{"location":"api/core/device-manager/#global-device-manager","title":"Global Device Manager","text":"<p>Get the global singleton device manager instance.</p> <pre><code>from artifex.generative_models.core.device_manager import get_device_manager\n\n# Get global instance\ndm = get_device_manager()\n\n# Force re-initialization with new config\nconfig = DeviceConfiguration(memory_strategy=MemoryStrategy.AGGRESSIVE)\ndm = get_device_manager(config, force_reinit=True)\n</code></pre>"},{"location":"api/core/device-manager/#check-gpu-availability","title":"Check GPU Availability","text":"<pre><code>from artifex.generative_models.core.device_manager import has_gpu\n\nif has_gpu():\n    print(\"GPU is available!\")\nelse:\n    print(\"Running on CPU\")\n</code></pre>"},{"location":"api/core/device-manager/#get-default-device","title":"Get Default Device","text":"<pre><code>from artifex.generative_models.core.device_manager import get_default_device\nimport jax\n\ndevice = get_default_device()\n\n# Use device for computations\nx = jax.device_put(jnp.ones((100, 100)), device)\n</code></pre>"},{"location":"api/core/device-manager/#print-device-information","title":"Print Device Information","text":"<pre><code>from artifex.generative_models.core.device_manager import print_device_info\n\n# Print comprehensive device information\nprint_device_info()\n</code></pre> <p>Example output:</p> <pre><code>\ud83d\udd0d Artifex Device Manager\n========================================\nBackend: gpu\nDevice Type: gpu\nDevice Count: 2\nGPU Count: 2\nMemory Strategy: balanced\nMemory Fraction: 0.75\nDefault Device: gpu:0\nCUDA Version: 12.1\nCompute Capability: 8.6\n</code></pre>"},{"location":"api/core/device-manager/#optimized-configurations","title":"Optimized Configurations","text":""},{"location":"api/core/device-manager/#for-generative-models","title":"For Generative Models","text":"<p>Pre-configured settings optimized for generative modeling.</p> <pre><code>from artifex.generative_models.core.device_manager import configure_for_generative_models\n\n# Optimized for generative models\ndm = configure_for_generative_models(\n    memory_strategy=MemoryStrategy.BALANCED,\n    enable_mixed_precision=True\n)\n</code></pre>"},{"location":"api/core/device-manager/#for-model-size","title":"For Model Size","text":"<p>Automatically choose optimal configuration based on parameter count.</p> <pre><code># Get optimized config for model size\nmodel_params = 150_000_000  # 150M parameters\n\noptimal_config = device_manager.optimize_for_model_size(model_params)\nprint(f\"Recommended strategy: {optimal_config.memory_strategy}\")\n</code></pre> <p>Guidelines:</p> <ul> <li>&lt; 1M parameters \u2192 CONSERVATIVE</li> <li>&lt; 100M parameters \u2192 BALANCED</li> <li>\u2265 100M parameters \u2192 AGGRESSIVE</li> </ul>"},{"location":"api/core/device-manager/#common-patterns","title":"Common Patterns","text":""},{"location":"api/core/device-manager/#pattern-1-training-setup","title":"Pattern 1: Training Setup","text":"<pre><code>from artifex.generative_models.core.device_manager import (\n    DeviceManager,\n    DeviceConfiguration,\n    MemoryStrategy\n)\n\ndef setup_training_environment(model_size: int):\n    \"\"\"Setup optimal training environment.\"\"\"\n    # Create device manager\n    config = DeviceConfiguration(\n        memory_strategy=MemoryStrategy.BALANCED,\n        enable_jit=True,\n        platform_priority=[\"cuda\", \"cpu\"]\n    )\n\n    dm = DeviceManager(config)\n\n    # Optimize for model size\n    optimal_config = dm.optimize_for_model_size(model_size)\n\n    # Reinitialize with optimal config\n    dm = DeviceManager(optimal_config)\n\n    print(f\"Training on: {dm.jax_manager.get_default_device()}\")\n    print(f\"GPU count: {dm.gpu_count}\")\n    print(f\"Memory strategy: {dm.config.memory_strategy.value}\")\n\n    return dm\n\n# Use in training\ndm = setup_training_environment(model_size=50_000_000)\n</code></pre>"},{"location":"api/core/device-manager/#pattern-2-multi-gpu-data-parallel","title":"Pattern 2: Multi-GPU Data Parallel","text":"<pre><code>import jax\nfrom flax import nnx\n\ndef setup_data_parallel(model, batch_size: int):\n    \"\"\"Setup data parallel training.\"\"\"\n    dm = DeviceManager()\n\n    if dm.gpu_count &gt; 1:\n        print(f\"Using {dm.gpu_count} GPUs for data parallel training\")\n\n        # Distribute batch across GPUs\n        per_device_batch_size = batch_size // dm.gpu_count\n\n        def replicate_model(model):\n            \"\"\"Replicate model on all devices.\"\"\"\n            replicated = []\n            for device in dm.jax_manager.gpu_devices:\n                model_copy = jax.device_put(model, device)\n                replicated.append(model_copy)\n            return replicated\n\n        return replicate_model(model), per_device_batch_size\n    else:\n        print(\"Single GPU training\")\n        return [model], batch_size\n</code></pre>"},{"location":"api/core/device-manager/#pattern-3-mixed-precision-training","title":"Pattern 3: Mixed Precision Training","text":"<pre><code>def setup_mixed_precision():\n    \"\"\"Setup mixed precision training if supported.\"\"\"\n    dm = DeviceManager()\n\n    if dm.capabilities.supports_mixed_precision:\n        print(\"Mixed precision training enabled\")\n\n        # Configure for mixed precision\n        config = DeviceConfiguration(\n            memory_strategy=MemoryStrategy.AGGRESSIVE,\n            enable_x64=False,  # Use float32/float16\n        )\n\n        return DeviceManager(config)\n    else:\n        print(\"Mixed precision not supported\")\n        return dm\n</code></pre>"},{"location":"api/core/device-manager/#pattern-4-graceful-cpu-fallback","title":"Pattern 4: Graceful CPU Fallback","text":"<pre><code>def create_model_with_fallback(model_cls, *args, **kwargs):\n    \"\"\"Create model with automatic device selection.\"\"\"\n    dm = DeviceManager()\n\n    if dm.has_gpu:\n        print(\"Creating model on GPU\")\n        device = dm.jax_manager.gpu_devices[0]\n    else:\n        print(\"No GPU found, falling back to CPU\")\n        device = dm.jax_manager.cpu_devices[0]\n\n    # Create model on selected device\n    with jax.default_device(device):\n        model = model_cls(*args, **kwargs)\n\n    return model, device\n</code></pre>"},{"location":"api/core/device-manager/#performance-tips","title":"Performance Tips","text":""},{"location":"api/core/device-manager/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Start with conservative, increase if needed\nconfig = DeviceConfiguration(\n    memory_strategy=MemoryStrategy.CONSERVATIVE\n)\n\n# Monitor memory usage\ndm = DeviceManager(config)\n\n# If successful, try balanced\nif training_successful:\n    config.memory_strategy = MemoryStrategy.BALANCED\n    dm = DeviceManager(config)\n</code></pre>"},{"location":"api/core/device-manager/#jit-compilation","title":"JIT Compilation","text":"<pre><code># Enable JIT for faster computation\nconfig = DeviceConfiguration(\n    enable_jit=True,\n    memory_strategy=MemoryStrategy.BALANCED\n)\n\ndm = DeviceManager(config)\n\n# Disable for debugging\nconfig.enable_jit = False\n</code></pre>"},{"location":"api/core/device-manager/#environment-tuning","title":"Environment Tuning","text":"<pre><code># Fine-tune XLA compilation\nconfig = DeviceConfiguration(\n    environment_variables={\n        \"XLA_FLAGS\": \" \".join([\n            \"--xla_gpu_enable_fast_min_max=true\",\n            \"--xla_gpu_enable_triton_softmax_fusion=true\",\n            \"--xla_gpu_triton_gemm_any=True\",\n        ])\n    }\n)\n</code></pre>"},{"location":"api/core/device-manager/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/core/device-manager/#issue-out-of-memory-oom-errors","title":"Issue: \"Out of Memory (OOM) Errors\"","text":"<p>Solutions:</p> <ol> <li>Reduce memory fraction:</li> </ol> <pre><code>config = DeviceConfiguration(\n    memory_strategy=MemoryStrategy.CONSERVATIVE,\n    memory_fraction=0.5  # Use only 50% of GPU memory\n)\n</code></pre> <ol> <li>Enable gradient checkpointing:</li> </ol> <pre><code># In your model\nfrom artifex.generative_models.core.base import MLP\n\nmlp = MLP(\n    hidden_dims=[512] * 20,\n    in_features=1024,\n    use_gradient_checkpointing=True,  # Trade compute for memory\n    rngs=rngs\n)\n</code></pre> <ol> <li>Reduce batch size:</li> </ol> <pre><code># Use smaller batches per device\nbatch_size = 32 // dm.gpu_count\n</code></pre>"},{"location":"api/core/device-manager/#issue-cuda-out-of-memory-despite-low-reported-usage","title":"Issue: \"CUDA Out of Memory Despite Low Reported Usage\"","text":"<p>Solution: JAX preallocates memory. Disable preallocation:</p> <pre><code>config = DeviceConfiguration(\n    environment_variables={\n        \"XLA_PYTHON_CLIENT_PREALLOCATE\": \"false\",\n    }\n)\n</code></pre>"},{"location":"api/core/device-manager/#issue-no-gpu-detected","title":"Issue: \"No GPU Detected\"","text":"<p>Diagnostic:</p> <pre><code>dm = DeviceManager()\n\nif not dm.has_gpu:\n    print(\"GPU not detected. Possible issues:\")\n    print(\"1. CUDA not installed\")\n    print(\"2. JAX not built with CUDA support\")\n    print(\"3. GPU drivers not installed\")\n    print(\"4. CUDA version mismatch\")\n\n# Check CUDA installation\nimport subprocess\ntry:\n    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n    print(result.stdout)\nexcept FileNotFoundError:\n    print(\"nvidia-smi not found - CUDA may not be installed\")\n</code></pre>"},{"location":"api/core/device-manager/#issue-slow-first-iteration","title":"Issue: \"Slow First Iteration\"","text":"<p>Explanation: JAX compiles on first run (JIT compilation).</p> <p>Solution: Warmup with dummy data:</p> <pre><code># Warmup model\n@nnx.jit\ndef warmup(model, x):\n    return model(x)\n\n# Run once to compile\ndummy_input = jnp.ones((1, 28, 28, 1))\n_ = warmup(model, dummy_input)\n\n# Now training will be fast\nfor batch in training_data:\n    loss = train_step(model, batch)\n</code></pre>"},{"location":"api/core/device-manager/#best-practices","title":"Best Practices","text":""},{"location":"api/core/device-manager/#do","title":"DO","text":"<ul> <li>\u2705 Use <code>DeviceManager</code> for automatic device detection</li> <li>\u2705 Start with <code>BALANCED</code> memory strategy</li> <li>\u2705 Enable JIT compilation for production</li> <li>\u2705 Test on CPU first, then move to GPU</li> <li>\u2705 Use <code>print_device_info()</code> to verify setup</li> <li>\u2705 Monitor memory usage during training</li> <li>\u2705 Use conservative settings for shared GPUs</li> </ul>"},{"location":"api/core/device-manager/#dont","title":"DON'T","text":"<ul> <li>\u274c Hard-code device strings (<code>\"gpu:0\"</code>)</li> <li>\u274c Assume GPU is always available</li> <li>\u274c Use <code>AGGRESSIVE</code> strategy on shared systems</li> <li>\u274c Disable JIT compilation in production</li> <li>\u274c Forget to handle CPU fallback</li> <li>\u274c Preallocate memory unnecessarily</li> </ul>"},{"location":"api/core/device-manager/#advanced-custom-detector","title":"Advanced: Custom Detector","text":"<p>Implement custom device detection logic.</p> <pre><code>from artifex.generative_models.core.device_manager import (\n    DeviceDetector,\n    DeviceCapabilities,\n    DeviceType\n)\n\nclass CustomDetector:\n    \"\"\"Custom device detector.\"\"\"\n\n    def detect_capabilities(self) -&gt; DeviceCapabilities:\n        \"\"\"Detect custom hardware.\"\"\"\n        # Custom detection logic\n        return DeviceCapabilities(\n            device_type=DeviceType.GPU,\n            device_count=4,\n            total_memory_mb=80000,\n            compute_capability=\"8.0\",\n            supports_mixed_precision=True,\n            supports_distributed=True\n        )\n\n# Use custom detector\ndetector = CustomDetector()\ndm = DeviceManager(detector=detector)\n</code></pre>"},{"location":"api/core/device-manager/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Base Classes</p> <p>Learn about model architecture</p> <p> Base Classes</p> </li> <li> <p> Configuration</p> <p>Configure models and training</p> <p> Configuration</p> </li> <li> <p> Training</p> <p>Use device manager in training</p> <p> Training Guide</p> </li> <li> <p> Distributed Training</p> <p>Scale across multiple devices</p> <p> Distributed Guide</p> </li> </ul>"},{"location":"api/core/device-manager/#references","title":"References","text":"<ul> <li>Source Code: <code>src/artifex/generative_models/core/device_manager.py</code></li> <li>Tests: <code>tests/artifex/generative_models/core/test_device_manager.py</code></li> <li>JAX Device Documentation: https://jax.readthedocs.io/en/latest/</li> <li>CUDA Toolkit: https://developer.nvidia.com/cuda-toolkit</li> </ul>"},{"location":"api/core/losses/","title":"Loss Functions","text":"<p>Artifex provides a comprehensive catalog of loss functions for training generative models. These losses are organized into categories based on their purpose and mathematical foundation.</p>"},{"location":"api/core/losses/#overview","title":"Overview","text":"<ul> <li> <p> 40+ Loss Functions</p> <p>Extensive catalog covering all generative model types</p> </li> <li> <p> Composable Framework</p> <p>Easily combine multiple losses with weights and scheduling</p> </li> <li> <p> JAX-Optimized</p> <p>Fully JIT-compatible and vectorized implementations</p> </li> <li> <p> Numerically Stable</p> <p>Careful handling of edge cases and numerical precision</p> </li> </ul>"},{"location":"api/core/losses/#loss-categories","title":"Loss Categories","text":"<pre><code>graph TD\n    A[Loss Functions] --&gt; B[Reconstruction Losses]\n    A --&gt; C[Adversarial Losses]\n    A --&gt; D[Divergence Losses]\n    A --&gt; E[Perceptual Losses]\n    A --&gt; F[Composable Framework]\n\n    B --&gt; B1[MSE]\n    B --&gt; B2[MAE]\n    B --&gt; B3[Huber]\n    B --&gt; B4[Charbonnier]\n\n    C --&gt; C1[Vanilla GAN]\n    C --&gt; C2[LSGAN]\n    C --&gt; C3[WGAN]\n    C --&gt; C4[Hinge]\n\n    D --&gt; D1[KL Divergence]\n    D --&gt; D2[JS Divergence]\n    D --&gt; D3[Wasserstein]\n    D --&gt; D4[MMD]\n\n    E --&gt; E1[Feature Reconstruction]\n    E --&gt; E2[Style Loss]\n    E --&gt; E3[Contextual Loss]\n\n    F --&gt; F1[CompositeLoss]\n    F --&gt; F2[WeightedLoss]\n    F --&gt; F3[ScheduledLoss]\n\n    style A fill:#e1f5ff\n    style B fill:#b3e5fc\n    style C fill:#b3e5fc\n    style D fill:#b3e5fc\n    style E fill:#b3e5fc\n    style F fill:#81d4fa</code></pre>"},{"location":"api/core/losses/#reconstruction-losses","title":"Reconstruction Losses","text":"<p>Reconstruction losses compare predictions directly with targets, typically used in autoencoders and regression tasks.</p> <p>Location: <code>src/artifex/generative_models/core/losses/reconstruction.py</code></p>"},{"location":"api/core/losses/#mse_loss-l2-loss","title":"mse_loss (L2 Loss)","text":"<p>Mean Squared Error - penalizes squared differences between predictions and targets.</p> <pre><code>from artifex.generative_models.core.losses.reconstruction import mse_loss\nimport jax.numpy as jnp\n\npredictions = jnp.array([1.0, 2.0, 3.0])\ntargets = jnp.array([1.1, 1.9, 3.2])\n\nloss = mse_loss(predictions, targets, reduction=\"mean\")\nprint(f\"MSE Loss: {loss}\")  # 0.0233...\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>predictions</code> <code>jax.Array</code> Required Model predictions <code>targets</code> <code>jax.Array</code> Required Ground truth values <code>reduction</code> <code>str</code> <code>\"mean\"</code> Reduction: \"none\", \"mean\", \"sum\" <code>weights</code> <code>jax.Array \\| None</code> <code>None</code> Optional per-element weights <code>axis</code> <code>int \\| tuple \\| None</code> <code>None</code> Axis for reduction <p>Use Cases:</p> <ul> <li>VAE reconstruction loss</li> <li>Image regression</li> <li>General reconstruction tasks</li> </ul>"},{"location":"api/core/losses/#mae_loss-l1-loss","title":"mae_loss (L1 Loss)","text":"<p>Mean Absolute Error - more robust to outliers than MSE.</p> <pre><code>from artifex.generative_models.core.losses.reconstruction import mae_loss\n\nloss = mae_loss(predictions, targets, reduction=\"mean\")\nprint(f\"MAE Loss: {loss}\")  # 0.133...\n</code></pre> <p>Use Cases:</p> <ul> <li>Robust regression</li> <li>Outlier-heavy datasets</li> <li>Image reconstruction with noise</li> </ul>"},{"location":"api/core/losses/#huber_loss","title":"huber_loss","text":"<p>Smooth combination of L1 and L2 losses - quadratic for small errors, linear for large errors.</p> <pre><code>from artifex.generative_models.core.losses.reconstruction import huber_loss\n\n# Delta controls the transition point\nloss = huber_loss(predictions, targets, delta=1.0, reduction=\"mean\")\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>delta</code> <code>float</code> <code>1.0</code> Threshold between quadratic and linear regions <p>Use Cases:</p> <ul> <li>Robust regression with outliers</li> <li>Reinforcement learning (value functions)</li> <li>Object detection</li> </ul>"},{"location":"api/core/losses/#charbonnier_loss","title":"charbonnier_loss","text":"<p>Differentiable variant of L1 loss with smoother gradients.</p> <pre><code>from artifex.generative_models.core.losses.reconstruction import charbonnier_loss\n\nloss = charbonnier_loss(\n    predictions,\n    targets,\n    epsilon=1e-3,  # Smoothing constant\n    alpha=1.0,     # Exponent\n    reduction=\"mean\"\n)\n</code></pre> <p>Use Cases:</p> <ul> <li>Optical flow estimation</li> <li>Image super-resolution</li> <li>Smooth optimization landscapes</li> </ul>"},{"location":"api/core/losses/#psnr_loss","title":"psnr_loss","text":"<p>Peak Signal-to-Noise Ratio expressed as a loss (negative PSNR).</p> <pre><code>from artifex.generative_models.core.losses.reconstruction import psnr_loss\n\n# For normalized images (0-1)\nloss = psnr_loss(pred_images, target_images, max_value=1.0)\n\n# For images in range [0, 255]\nloss = psnr_loss(pred_images, target_images, max_value=255.0)\n</code></pre> <p>Use Cases:</p> <ul> <li>Image quality assessment</li> <li>Super-resolution evaluation</li> <li>Compression evaluation</li> </ul>"},{"location":"api/core/losses/#adversarial-losses","title":"Adversarial Losses","text":"<p>Adversarial losses for training GANs and adversarial networks.</p> <p>Location: <code>src/artifex/generative_models/core/losses/adversarial.py</code></p>"},{"location":"api/core/losses/#vanilla-gan-losses","title":"Vanilla GAN Losses","text":"<p>Original GAN formulation with binary cross-entropy.</p> <pre><code>from artifex.generative_models.core.losses.adversarial import (\n    vanilla_generator_loss,\n    vanilla_discriminator_loss\n)\n\n# Generator loss: -log(D(G(z)))\ng_loss = vanilla_generator_loss(fake_scores)\n\n# Discriminator loss: -log(D(x)) - log(1 - D(G(z)))\nd_loss = vanilla_discriminator_loss(real_scores, fake_scores)\n</code></pre> <p>Pros: Simple, well-studied Cons: Vanishing gradients, mode collapse</p>"},{"location":"api/core/losses/#lsgan-losses","title":"LSGAN Losses","text":"<p>Least Squares GAN - uses mean squared error instead of cross-entropy.</p> <pre><code>from artifex.generative_models.core.losses.adversarial import (\n    least_squares_generator_loss,\n    least_squares_discriminator_loss\n)\n\n# Generator: minimize (D(G(z)) - 1)^2\ng_loss = least_squares_generator_loss(\n    fake_scores,\n    target_real=1.0\n)\n\n# Discriminator: minimize (D(x) - 1)^2 + (D(G(z)) - 0)^2\nd_loss = least_squares_discriminator_loss(\n    real_scores,\n    fake_scores,\n    target_real=1.0,\n    target_fake=0.0\n)\n</code></pre> <p>Pros: More stable training, better gradients Cons: May require more careful tuning</p>"},{"location":"api/core/losses/#wasserstein-gan-losses","title":"Wasserstein GAN Losses","text":"<p>Wasserstein distance-based losses with better convergence properties.</p> <pre><code>from artifex.generative_models.core.losses.adversarial import (\n    wasserstein_generator_loss,\n    wasserstein_discriminator_loss\n)\n\n# Generator: minimize -D(G(z))\ng_loss = wasserstein_generator_loss(fake_scores)\n\n# Critic: minimize D(G(z)) - D(x)\nd_loss = wasserstein_discriminator_loss(real_scores, fake_scores)\n\n# Note: Requires gradient penalty or weight clipping\n</code></pre> <p>Pros: Stable training, meaningful loss curves Cons: Requires gradient penalty or weight clipping</p>"},{"location":"api/core/losses/#hinge-losses","title":"Hinge Losses","text":"<p>Hinge loss formulation used in spectral normalization GANs.</p> <pre><code>from artifex.generative_models.core.losses.adversarial import (\n    hinge_generator_loss,\n    hinge_discriminator_loss\n)\n\n# Generator: -D(G(z))\ng_loss = hinge_generator_loss(fake_scores)\n\n# Discriminator: max(0, 1 - D(x)) + max(0, 1 + D(G(z)))\nd_loss = hinge_discriminator_loss(real_scores, fake_scores)\n</code></pre> <p>Pros: Stable, works well with spectral normalization Cons: May need careful architecture design</p>"},{"location":"api/core/losses/#divergence-losses","title":"Divergence Losses","text":"<p>Statistical divergence measures between probability distributions.</p> <p>Location: <code>src/artifex/generative_models/core/losses/divergence.py</code></p>"},{"location":"api/core/losses/#kl_divergence","title":"kl_divergence","text":"<p>Kullback-Leibler divergence - measures information loss.</p> <pre><code>from artifex.generative_models.core.losses.divergence import kl_divergence\nimport distrax\n\n# With distribution objects\np = distrax.Normal(loc=0.0, scale=1.0)\nq = distrax.Normal(loc=0.5, scale=1.5)\nkl = kl_divergence(p, q)\n\n# With probability arrays\np_probs = jnp.array([0.2, 0.5, 0.3])\nq_probs = jnp.array([0.3, 0.4, 0.3])\nkl = kl_divergence(p_probs, q_probs, reduction=\"sum\")\n</code></pre> <p>Formula: KL(P||Q) = \u03a3 P(x) log(P(x) / Q(x))</p> <p>Use Cases:</p> <ul> <li>VAE latent regularization</li> <li>Distribution matching</li> <li>Information theory applications</li> </ul>"},{"location":"api/core/losses/#js_divergence","title":"js_divergence","text":"<p>Jensen-Shannon divergence - symmetric variant of KL divergence.</p> <pre><code>from artifex.generative_models.core.losses.divergence import js_divergence\n\np = jnp.array([0.2, 0.5, 0.3])\nq = jnp.array([0.1, 0.7, 0.2])\njs = js_divergence(p, q)\n</code></pre> <p>Formula: JS(P||Q) = 0.5 (KL(P||M) + KL(Q||M)) where M = 0.5 (P + Q)</p> <p>Properties:</p> <ul> <li>Symmetric: JS(P||Q) = JS(Q||P)</li> <li>Bounded: 0 \u2264 JS \u2264 log(2)</li> <li>Metric (satisfies triangle inequality)</li> </ul>"},{"location":"api/core/losses/#wasserstein_distance","title":"wasserstein_distance","text":"<p>Earth Mover's Distance - optimal transport-based metric.</p> <pre><code>from artifex.generative_models.core.losses.divergence import wasserstein_distance\n\n# 1D samples\np_samples = jnp.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nq_samples = jnp.array([[1.5, 2.5, 3.5], [4.5, 5.5, 6.5]])\n\n# W1 distance\nw1 = wasserstein_distance(p_samples, q_samples, p=1, axis=1)\n\n# W2 distance\nw2 = wasserstein_distance(p_samples, q_samples, p=2, axis=1)\n</code></pre> <p>Use Cases:</p> <ul> <li>GAN training (WGAN)</li> <li>Distribution comparison</li> <li>Robust statistics</li> </ul>"},{"location":"api/core/losses/#maximum_mean_discrepancy","title":"maximum_mean_discrepancy","text":"<p>Kernel-based distribution distance measure.</p> <pre><code>from artifex.generative_models.core.losses.divergence import maximum_mean_discrepancy\nimport jax\n\n# Generate samples\nkey = jax.random.key(0)\npred_samples = jax.random.normal(key, (2, 100, 5))\ntarget_samples = jax.random.normal(key, (2, 100, 5))\n\n# Compute MMD with RBF kernel\nmmd = maximum_mean_discrepancy(\n    pred_samples,\n    target_samples,\n    kernel_type=\"rbf\",\n    kernel_bandwidth=1.0\n)\n</code></pre> <p>Kernel Types:</p> <ul> <li><code>\"rbf\"</code>: Radial Basis Function (Gaussian)</li> <li><code>\"linear\"</code>: Linear kernel</li> <li><code>\"polynomial\"</code>: Polynomial kernel</li> </ul> <p>Use Cases:</p> <ul> <li>Two-sample testing</li> <li>Domain adaptation</li> <li>Generative model evaluation</li> </ul>"},{"location":"api/core/losses/#energy_distance","title":"energy_distance","text":"<p>Metric between probability distributions based on Euclidean distance.</p> <pre><code>from artifex.generative_models.core.losses.divergence import energy_distance\n\nenergy = energy_distance(\n    pred_samples,\n    target_samples,\n    beta=1.0  # Power parameter (0 &lt; beta &lt;= 2)\n)\n</code></pre> <p>Properties:</p> <ul> <li>Metric (satisfies triangle inequality)</li> <li>Generalizes Euclidean distance</li> <li>Computationally efficient</li> </ul>"},{"location":"api/core/losses/#perceptual-losses","title":"Perceptual Losses","text":"<p>Feature-based losses using pre-trained networks.</p> <p>Location: <code>src/artifex/generative_models/core/losses/perceptual.py</code></p>"},{"location":"api/core/losses/#feature_reconstruction_loss","title":"feature_reconstruction_loss","text":"<p>Compares features extracted from intermediate layers.</p> <pre><code>from artifex.generative_models.core.losses.perceptual import feature_reconstruction_loss\n\n# Dictionary of features (e.g., from VGG)\nfeatures_real = {\n    \"conv1\": jnp.ones((2, 64, 64, 64)),\n    \"conv2\": jnp.ones((2, 32, 32, 128)),\n    \"conv3\": jnp.ones((2, 16, 16, 256)),\n}\n\nfeatures_fake = {\n    \"conv1\": jnp.zeros((2, 64, 64, 64)),\n    \"conv2\": jnp.zeros((2, 32, 32, 128)),\n    \"conv3\": jnp.zeros((2, 16, 16, 256)),\n}\n\n# Weighted feature loss\nloss = feature_reconstruction_loss(\n    features_real,\n    features_fake,\n    weights={\"conv1\": 1.0, \"conv2\": 0.5, \"conv3\": 0.25}\n)\n</code></pre> <p>Use Cases:</p> <ul> <li>Image-to-image translation</li> <li>Style transfer (content loss)</li> <li>Super-resolution</li> </ul>"},{"location":"api/core/losses/#style_loss","title":"style_loss","text":"<p>Gram matrix-based style matching.</p> <pre><code>from artifex.generative_models.core.losses.perceptual import style_loss\n\n# Captures texture/style information\nloss = style_loss(\n    features_real,\n    features_fake,\n    weights={\"conv1\": 1.0, \"conv2\": 1.0, \"conv3\": 1.0}\n)\n</code></pre> <p>How it works:</p> <ol> <li>Computes Gram matrices of features</li> <li>Measures distance between Gram matrices</li> <li>Captures correlations between feature channels</li> </ol> <p>Use Cases:</p> <ul> <li>Style transfer</li> <li>Texture synthesis</li> <li>Artistic image generation</li> </ul>"},{"location":"api/core/losses/#contextual_loss","title":"contextual_loss","text":"<p>Robust to spatial misalignments, measures distributional similarity.</p> <pre><code>from artifex.generative_models.core.losses.perceptual import contextual_loss\n\n# Single-layer features\nfeat_real = jnp.ones((2, 32, 32, 128))\nfeat_fake = jnp.zeros((2, 32, 32, 128))\n\nloss = contextual_loss(\n    feat_real,\n    feat_fake,\n    band_width=0.1,\n    max_samples=512  # Memory-efficient\n)\n</code></pre> <p>Use Cases:</p> <ul> <li>Non-aligned image matching</li> <li>Texture transfer</li> <li>Semantic image editing</li> </ul>"},{"location":"api/core/losses/#perceptualloss-module","title":"PerceptualLoss Module","text":"<p>Composable NNX module combining multiple perceptual losses.</p> <pre><code>from artifex.generative_models.core.losses.perceptual import PerceptualLoss\nfrom flax import nnx\n\n# Create perceptual loss module\nperceptual = PerceptualLoss(\n    feature_extractor=vgg_model,  # Pre-trained VGG\n    layer_weights={\n        \"conv1_2\": 1.0,\n        \"conv2_2\": 1.0,\n        \"conv3_3\": 1.0,\n        \"conv4_3\": 1.0,\n    },\n    content_weight=1.0,\n    style_weight=10.0,\n    contextual_weight=0.1,\n)\n\n# Compute combined loss\nloss = perceptual(pred_images, target_images)\n</code></pre>"},{"location":"api/core/losses/#composable-loss-framework","title":"Composable Loss Framework","text":"<p>NNX-based framework for combining multiple losses.</p> <p>Location: <code>src/artifex/generative_models/core/losses/composable.py</code></p>"},{"location":"api/core/losses/#weightedloss","title":"WeightedLoss","text":"<p>Apply a weight to any loss function.</p> <pre><code>from artifex.generative_models.core.losses.composable import WeightedLoss\nfrom artifex.generative_models.core.losses.reconstruction import mse_loss\n\n# Wrap loss with weight\nweighted_mse = WeightedLoss(\n    loss_fn=mse_loss,\n    weight=0.5,\n    name=\"weighted_mse\"\n)\n\nloss = weighted_mse(predictions, targets)  # 0.5 * MSE\n</code></pre>"},{"location":"api/core/losses/#compositeloss","title":"CompositeLoss","text":"<p>Combine multiple losses into a single module.</p> <pre><code>from artifex.generative_models.core.losses.composable import CompositeLoss, WeightedLoss\nfrom artifex.generative_models.core.losses.reconstruction import mse_loss, mae_loss\n\n# Create weighted losses\nloss1 = WeightedLoss(mse_loss, weight=1.0, name=\"mse\")\nloss2 = WeightedLoss(mae_loss, weight=0.5, name=\"mae\")\n\n# Combine losses\ncomposite = CompositeLoss(\n    losses=[loss1, loss2],\n    return_components=True\n)\n\n# Returns total loss and individual components\ntotal_loss, loss_dict = composite(predictions, targets)\nprint(f\"Total: {total_loss}\")\nprint(f\"Components: {loss_dict}\")  # {\"mse\": ..., \"mae\": ...}\n</code></pre>"},{"location":"api/core/losses/#scheduledloss","title":"ScheduledLoss","text":"<p>Dynamic loss weight scheduling for curriculum learning.</p> <pre><code>from artifex.generative_models.core.losses.composable import ScheduledLoss\n\ndef warmup_schedule(step):\n    \"\"\"Linear warmup over 1000 steps.\"\"\"\n    return min(1.0, step / 1000.0)\n\nscheduled_loss = ScheduledLoss(\n    loss_fn=mse_loss,\n    schedule_fn=warmup_schedule,\n    name=\"scheduled_mse\"\n)\n\n# Weight increases over time\nloss_step_0 = scheduled_loss(pred, target, step=0)     # weight=0.0\nloss_step_500 = scheduled_loss(pred, target, step=500) # weight=0.5\nloss_step_1000 = scheduled_loss(pred, target, step=1000) # weight=1.0\n</code></pre>"},{"location":"api/core/losses/#base-utilities","title":"Base Utilities","text":"<p>Helper classes and functions for loss management.</p> <p>Location: <code>src/artifex/generative_models/core/losses/base.py</code></p>"},{"location":"api/core/losses/#losscollection","title":"LossCollection","text":"<p>Functional collection of multiple losses (non-NNX).</p> <pre><code>from artifex.generative_models.core.losses.base import LossCollection\nfrom artifex.generative_models.core.losses.reconstruction import mse_loss, mae_loss\n\n# Create collection\ncollection = LossCollection()\ncollection.add(mse_loss, weight=1.0, name=\"mse\")\ncollection.add(mae_loss, weight=0.5, name=\"mae\")\n\n# Compute total loss\ntotal_loss, loss_dict = collection(predictions, targets)\n</code></pre>"},{"location":"api/core/losses/#lossscheduler","title":"LossScheduler","text":"<p>Dynamically adjust loss weights during training.</p> <pre><code>from artifex.generative_models.core.losses.base import LossScheduler\n\n# Initialize with initial weights\nscheduler = LossScheduler({\n    \"reconstruction\": 1.0,\n    \"kl_divergence\": 0.1,\n    \"perceptual\": 0.5,\n})\n\n# Add linear warmup for KL divergence\nscheduler.add_schedule(\n    \"kl_divergence\",\n    LossScheduler.linear_warmup(warmup_steps=1000, max_weight=1.0)\n)\n\n# Add cosine annealing for perceptual loss\nscheduler.add_schedule(\n    \"perceptual\",\n    LossScheduler.cosine_annealing(period=5000, min_weight=0.1, max_weight=1.0)\n)\n\n# Update weights at each step\nweights = scheduler.update(step=500)\nprint(weights)  # Updated weights based on schedules\n</code></pre>"},{"location":"api/core/losses/#lossmetrics","title":"LossMetrics","text":"<p>Track loss values during training with exponential moving average.</p> <pre><code>from artifex.generative_models.core.losses.base import LossMetrics\n\n# Create metrics tracker\nmetrics = LossMetrics(momentum=0.99)\n\n# Update with loss values\nfor step in range(100):\n    loss_dict = {\n        \"total\": jnp.array(1.5),\n        \"reconstruction\": jnp.array(1.0),\n        \"kl\": jnp.array(0.5),\n    }\n    metrics.update(loss_dict)\n\n# Get smoothed metrics\ncurrent_metrics = metrics.get_metrics()\nprint(current_metrics)  # Smoothed loss values\n</code></pre>"},{"location":"api/core/losses/#common-patterns","title":"Common Patterns","text":""},{"location":"api/core/losses/#pattern-1-vae-loss","title":"Pattern 1: VAE Loss","text":"<pre><code>from artifex.generative_models.core.losses.reconstruction import mse_loss\nfrom artifex.generative_models.core.losses.divergence import kl_divergence\nimport distrax\n\ndef vae_loss(x, reconstruction, mean, logvar):\n    \"\"\"Complete VAE loss.\"\"\"\n    # Reconstruction loss\n    recon_loss = mse_loss(reconstruction, x, reduction=\"mean\")\n\n    # KL divergence\n    posterior = distrax.MultivariateNormalDiag(mean, jnp.exp(0.5 * logvar))\n    prior = distrax.MultivariateNormalDiag(\n        jnp.zeros_like(mean),\n        jnp.ones_like(logvar)\n    )\n    kl_loss = kl_divergence(posterior, prior, reduction=\"mean\")\n\n    # Total loss\n    total_loss = recon_loss + 0.5 * kl_loss\n\n    return {\n        \"loss\": total_loss,\n        \"reconstruction_loss\": recon_loss,\n        \"kl_loss\": kl_loss,\n    }\n</code></pre>"},{"location":"api/core/losses/#pattern-2-gan-with-multiple-losses","title":"Pattern 2: GAN with Multiple Losses","text":"<pre><code>from artifex.generative_models.core.losses.adversarial import (\n    hinge_generator_loss,\n    hinge_discriminator_loss\n)\nfrom artifex.generative_models.core.losses.perceptual import PerceptualLoss\n\n# Perceptual loss for generator\nperceptual_loss = PerceptualLoss(\n    feature_extractor=vgg,\n    content_weight=1.0,\n    style_weight=10.0,\n)\n\ndef generator_loss(fake_images, real_images, fake_scores):\n    \"\"\"Complete generator loss.\"\"\"\n    # Adversarial loss\n    adv_loss = hinge_generator_loss(fake_scores)\n\n    # Perceptual loss\n    perc_loss = perceptual_loss(fake_images, real_images)\n\n    # Total loss\n    total = adv_loss + 0.1 * perc_loss\n\n    return {\n        \"loss\": total,\n        \"adversarial\": adv_loss,\n        \"perceptual\": perc_loss,\n    }\n</code></pre>"},{"location":"api/core/losses/#pattern-3-composable-multi-loss-system","title":"Pattern 3: Composable Multi-Loss System","text":"<pre><code>from artifex.generative_models.core.losses.composable import (\n    CompositeLoss,\n    WeightedLoss,\n    ScheduledLoss\n)\n\n# Create individual weighted losses\nrecon_loss = WeightedLoss(mse_loss, weight=1.0, name=\"reconstruction\")\nperc_loss = WeightedLoss(perceptual_fn, weight=0.1, name=\"perceptual\")\n\n# Scheduled KL loss with warmup\nkl_loss = ScheduledLoss(\n    loss_fn=kl_divergence_fn,\n    schedule_fn=lambda step: min(1.0, step / 5000),\n    name=\"kl\"\n)\n\n# Combine all losses\ncomposite = CompositeLoss(\n    losses=[recon_loss, perc_loss, kl_loss],\n    return_components=True\n)\n\n# Use in training\ntotal_loss, components = composite(batch, outputs, step=current_step)\n</code></pre>"},{"location":"api/core/losses/#best-practices","title":"Best Practices","text":""},{"location":"api/core/losses/#do","title":"DO","text":"<ul> <li>\u2705 Use <code>reduction=\"mean\"</code> for stable gradients</li> <li>\u2705 Scale losses to similar magnitudes when combining</li> <li>\u2705 Use perceptual losses for visual quality</li> <li>\u2705 Monitor individual loss components during training</li> <li>\u2705 Use numerically stable variants (safe_log, safe_divide)</li> <li>\u2705 Normalize inputs when using distance-based losses</li> <li>\u2705 Use gradient clipping with adversarial losses</li> </ul>"},{"location":"api/core/losses/#dont","title":"DON'T","text":"<ul> <li>\u274c Mix different reduction types without careful consideration</li> <li>\u274c Use very different loss magnitudes without weighting</li> <li>\u274c Forget to normalize features for perceptual losses</li> <li>\u274c Use high-dimensional MMD without subsampling</li> <li>\u274c Apply losses on unnormalized data</li> <li>\u274c Ignore numerical stability (use eps parameters)</li> </ul>"},{"location":"api/core/losses/#performance-tips","title":"Performance Tips","text":""},{"location":"api/core/losses/#memory-efficient-perceptual-loss","title":"Memory-Efficient Perceptual Loss","text":"<pre><code># Use lower resolution or fewer samples\nperceptual = PerceptualLoss(\n    max_contextual_samples=256,  # Reduce memory usage\n    ...\n)\n</code></pre>"},{"location":"api/core/losses/#vectorized-loss-computation","title":"Vectorized Loss Computation","text":"<pre><code># Use JAX's vmap for batched loss computation\nfrom flax import nnx\n\n@nnx.jit\ndef batch_loss(predictions, targets):\n    return nnx.vmap(mse_loss)(predictions, targets)\n</code></pre>"},{"location":"api/core/losses/#gradient-accumulation-for-large-losses","title":"Gradient Accumulation for Large Losses","text":"<pre><code># For very large composite losses\n@nnx.jit\n@nnx.grad\ndef loss_with_accumulation(params, batch):\n    # Compute losses in chunks\n    ...\n</code></pre>"},{"location":"api/core/losses/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/core/losses/#issue-nan-in-loss-computation","title":"Issue: \"NaN in loss computation\"","text":"<p>Solutions:</p> <ul> <li>Use epsilon parameters for numerical stability</li> <li>Check input normalization</li> <li>Use <code>safe_log</code> and <code>safe_divide</code> utilities</li> <li>Clip gradients</li> </ul> <pre><code>from artifex.generative_models.core.losses.base import safe_log, safe_divide\n\n# Instead of jnp.log(x)\nlog_x = safe_log(x, eps=1e-8)\n\n# Instead of x / y\nratio = safe_divide(x, y, eps=1e-8)\n</code></pre>"},{"location":"api/core/losses/#issue-loss-values-differ-greatly-in-magnitude","title":"Issue: \"Loss values differ greatly in magnitude\"","text":"<p>Solution: Scale losses to similar ranges:</p> <pre><code># Bad: losses differ by orders of magnitude\ntotal = recon_loss + kl_loss  # e.g., 100.0 + 0.01\n\n# Good: scale to similar magnitudes\ntotal = recon_loss + 10.0 * kl_loss  # e.g., 100.0 + 0.1\n</code></pre>"},{"location":"api/core/losses/#issue-perceptual-loss-uses-too-much-memory","title":"Issue: \"Perceptual loss uses too much memory\"","text":"<p>Solution: Reduce sample count and feature resolution:</p> <pre><code>contextual_loss(\n    features_real,\n    features_fake,\n    max_samples=256,  # Reduce from default 1024\n)\n</code></pre>"},{"location":"api/core/losses/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Base Classes</p> <p>Learn about model base classes</p> <p> Base Classes</p> </li> <li> <p> Device Management</p> <p>Optimize loss computation on GPU/TPU</p> <p> Device Manager</p> </li> <li> <p> Training</p> <p>Use losses in training loops</p> <p> Training Guide</p> </li> </ul>"},{"location":"api/core/losses/#references","title":"References","text":"<ul> <li>Source Code: <code>src/artifex/generative_models/core/losses/</code></li> <li>Tests: <code>tests/artifex/generative_models/core/losses/</code></li> <li>Research Papers:</li> <li>GAN Paper</li> <li>LSGAN Paper</li> <li>WGAN Paper</li> <li>Perceptual Losses Paper</li> </ul>"},{"location":"api/data/loaders/","title":"Data API Reference","text":"<p>Complete API reference for Artifex's data loading system, including datasets, modalities, and utility functions.</p>"},{"location":"api/data/loaders/#core-protocols","title":"Core Protocols","text":""},{"location":"api/data/loaders/#modality","title":"<code>Modality</code>","text":"<p>Protocol defining the interface for data modalities.</p> <pre><code>@runtime_checkable\nclass Modality(Protocol):\n    \"\"\"Protocol defining interface for data modalities.\"\"\"\n\n    name: str\n\n    def get_extensions(\n        self,\n        config: Any,\n        *,\n        rngs: nnx.Rngs\n    ) -&gt; dict[str, ModelExtension]:\n        \"\"\"Get modality-specific extensions.\n\n        Args:\n            config: Extension configuration\n            rngs: Random number generators\n\n        Returns:\n            Dictionary mapping extension names to extension instances\n        \"\"\"\n        ...\n\n    def get_adapter(\n        self,\n        model_cls: type[GenerativeModel]\n    ) -&gt; ModelAdapter:\n        \"\"\"Get an adapter for the specified model class.\n\n        Args:\n            model_cls: The model class to adapt\n\n        Returns:\n            A model adapter for the specified model class\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#modeladapter","title":"<code>ModelAdapter</code>","text":"<p>Protocol for model adapters that adapt generic models to specific modalities.</p> <pre><code>@runtime_checkable\nclass ModelAdapter(Protocol):\n    \"\"\"Protocol defining interface for model adapters.\"\"\"\n\n    def create(\n        self,\n        config: Any,\n        *,\n        rngs: nnx.Rngs,\n        **kwargs: Any\n    ) -&gt; GenerativeModel:\n        \"\"\"Create a model with modality-specific adaptations.\n\n        Args:\n            config: Model configuration (must be ModelConfig)\n            rngs: Random number generator keys\n            **kwargs: Additional keyword arguments for model creation\n\n        Returns:\n            An initialized model instance\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#basedataset","title":"<code>BaseDataset</code>","text":"<p>Abstract base class for all datasets.</p> <pre><code>class BaseDataset(nnx.Module, ABC):\n    \"\"\"Abstract base class for modality datasets.\"\"\"\n\n    def __init__(\n        self,\n        config: BaseModalityConfig,\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize dataset.\n\n        Args:\n            config: Modality configuration\n            split: Dataset split ('train', 'val', 'test')\n            rngs: Random number generators\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Return dataset size.\"\"\"\n        ...\n\n    @abstractmethod\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        \"\"\"Iterate over dataset samples.\"\"\"\n        ...\n\n    @abstractmethod\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get a batch of samples.\n\n        Args:\n            batch_size: Number of samples in batch\n\n        Returns:\n            Batch dictionary with modality-specific data\n        \"\"\"\n        ...\n\n    def get_sample(self, index: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get a single sample by index.\n\n        Args:\n            index: Sample index\n\n        Returns:\n            Sample data\n\n        Raises:\n            IndexError: If index is out of range\n        \"\"\"\n        ...\n\n    def get_data_statistics(self) -&gt; dict[str, Any]:\n        \"\"\"Get dataset statistics.\n\n        Returns:\n            Dictionary with dataset statistics\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#image-modality","title":"Image Modality","text":""},{"location":"api/data/loaders/#imagemodalityconfig","title":"<code>ImageModalityConfig</code>","text":"<p>Configuration for image modality processing.</p> <pre><code>@dataclass\nclass ImageModalityConfig:\n    \"\"\"Configuration for image modality processing.\"\"\"\n\n    representation: ImageRepresentation = ImageRepresentation.RGB\n    height: int = 64\n    width: int | None = None\n    channels: int | None = None\n    normalize: bool = True\n    augmentation: bool = False\n    resize_method: str = \"bilinear\"\n\n    def __post_init__(self):\n        \"\"\"Set defaults and validate configuration.\"\"\"\n        ...\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>representation</code> <code>ImageRepresentation</code> <code>RGB</code> Image representation format (RGB, RGBA, GRAYSCALE) <code>height</code> <code>int</code> <code>64</code> Image height in pixels <code>width</code> <code>int \\| None</code> <code>None</code> Image width in pixels (defaults to height for square images) <code>channels</code> <code>int \\| None</code> <code>None</code> Number of channels (auto-determined if None) <code>normalize</code> <code>bool</code> <code>True</code> Whether to normalize pixel values to [0, 1] <code>augmentation</code> <code>bool</code> <code>False</code> Whether to enable data augmentation <code>resize_method</code> <code>str</code> <code>\"bilinear\"</code> Method for resizing ('bilinear', 'nearest')"},{"location":"api/data/loaders/#imagemodality","title":"<code>ImageModality</code>","text":"<p>Base image modality class providing unified interface for image generation.</p> <pre><code>class ImageModality(GenerativeModel):\n    \"\"\"Base image modality class.\"\"\"\n\n    name = \"image\"\n\n    def __init__(\n        self,\n        config: ImageModalityConfig | None = None,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize image modality.\n\n        Args:\n            config: Image modality configuration\n            rngs: Random number generators\n        \"\"\"\n        ...\n\n    @property\n    def image_shape(self) -&gt; tuple[int, int, int]:\n        \"\"\"Image shape (height, width, channels).\"\"\"\n        ...\n\n    @property\n    def output_shape(self) -&gt; tuple[int, int, int]:\n        \"\"\"Output shape for generated images.\"\"\"\n        ...\n\n    def generate(\n        self,\n        n_samples: int = 1,\n        height: int | None = None,\n        width: int | None = None,\n        *,\n        rngs: nnx.Rngs | None = None,\n        **kwargs,\n    ) -&gt; jax.Array:\n        \"\"\"Generate image samples.\n\n        Args:\n            n_samples: Number of image samples to generate\n            height: Height override (uses config default if None)\n            width: Width override (uses config default if None)\n            rngs: Random number generators\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Generated image array of shape (n_samples, height, width, channels)\n        \"\"\"\n        ...\n\n    def process(self, data: jax.Array, **kwargs) -&gt; jax.Array:\n        \"\"\"Process image data for multi-modal fusion.\n\n        Args:\n            data: Image data with shape (height, width, channels) or\n                  (batch, height, width, channels)\n            **kwargs: Additional processing arguments\n\n        Returns:\n            Processed image features as flattened array\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#syntheticimagedataset","title":"<code>SyntheticImageDataset</code>","text":"<p>Synthetic image dataset for testing and development.</p> <pre><code>class SyntheticImageDataset(ImageDataset):\n    \"\"\"Synthetic image dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: ImageModalityConfig,\n        dataset_size: int = 1000,\n        pattern_type: str = \"random\",\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize synthetic image dataset.\n\n        Args:\n            config: Image modality configuration\n            dataset_size: Number of synthetic samples\n            pattern_type: Type of pattern to generate\n                ('random', 'gradient', 'checkerboard', 'circles')\n            split: Dataset split\n            rngs: Random number generators\n        \"\"\"\n        ...\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Generate a batch of synthetic images.\n\n        Args:\n            batch_size: Number of images to generate\n\n        Returns:\n            Batch dictionary with 'images' key\n        \"\"\"\n        ...\n</code></pre> <p>Pattern Types:</p> <ul> <li><code>\"random\"</code>: Random noise patterns</li> <li><code>\"gradient\"</code>: Linear gradients with varying directions</li> <li><code>\"checkerboard\"</code>: Checkerboard patterns with random sizes</li> <li><code>\"circles\"</code>: Circular patterns with random positions/radii</li> </ul>"},{"location":"api/data/loaders/#mnistlikedataset","title":"<code>MNISTLikeDataset</code>","text":"<p>MNIST-like synthetic dataset for digit-like patterns.</p> <pre><code>class MNISTLikeDataset(ImageDataset):\n    \"\"\"MNIST-like synthetic dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: ImageModalityConfig,\n        dataset_size: int = 1000,\n        num_classes: int = 10,\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize MNIST-like dataset.\n\n        Args:\n            config: Image modality configuration (should be grayscale, 28x28)\n            dataset_size: Number of synthetic samples\n            num_classes: Number of classes to generate\n            split: Dataset split\n            rngs: Random number generators\n        \"\"\"\n        ...\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Generate a batch of digit-like images with labels.\n\n        Args:\n            batch_size: Number of images to generate\n\n        Returns:\n            Batch dictionary with 'images' and 'labels' keys\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#create_image_dataset","title":"<code>create_image_dataset</code>","text":"<p>Factory function to create image datasets.</p> <pre><code>def create_image_dataset(\n    dataset_type: str = \"synthetic\",\n    config: ImageModalityConfig | None = None,\n    *,\n    rngs: nnx.Rngs,\n    **kwargs,\n) -&gt; ImageDataset:\n    \"\"\"Factory function to create image datasets.\n\n    Args:\n        dataset_type: Type of dataset ('synthetic', 'mnist_like')\n        config: Image modality configuration\n        rngs: Random number generators\n        **kwargs: Additional dataset parameters\n\n    Returns:\n        Created dataset instance\n\n    Raises:\n        ValueError: If dataset_type is unknown\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#text-modality","title":"Text Modality","text":""},{"location":"api/data/loaders/#textdataset","title":"<code>TextDataset</code>","text":"<p>Base class for text datasets.</p> <pre><code>class TextDataset(nnx.Module):\n    \"\"\"Base class for text datasets.\"\"\"\n\n    def __init__(\n        self,\n        config: ModalityConfiguration,\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize text dataset.\n\n        Args:\n            config: Text modality configuration (ModalityConfiguration)\n            split: Dataset split ('train', 'val', 'test')\n            rngs: Random number generators\n        \"\"\"\n        ...\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return dataset size.\"\"\"\n        ...\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        \"\"\"Iterate over dataset samples.\"\"\"\n        ...\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get a batch of samples.\n\n        Args:\n            batch_size: Number of samples in batch\n\n        Returns:\n            Batch dictionary with 'text_tokens' and potentially 'labels'\n        \"\"\"\n        ...\n</code></pre> <p>Text Parameters (in config.metadata[\"text_params\"]):</p> Parameter Type Default Description <code>vocab_size</code> <code>int</code> <code>10000</code> Vocabulary size <code>max_length</code> <code>int</code> <code>512</code> Maximum sequence length <code>pad_token_id</code> <code>int</code> <code>0</code> Padding token ID <code>unk_token_id</code> <code>int</code> <code>1</code> Unknown token ID <code>bos_token_id</code> <code>int</code> <code>2</code> Beginning-of-sequence token ID <code>eos_token_id</code> <code>int</code> <code>3</code> End-of-sequence token ID <code>case_sensitive</code> <code>bool</code> <code>False</code> Whether to preserve case"},{"location":"api/data/loaders/#synthetictextdataset","title":"<code>SyntheticTextDataset</code>","text":"<p>Synthetic text dataset for testing and development.</p> <pre><code>class SyntheticTextDataset(TextDataset):\n    \"\"\"Synthetic text dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: ModalityConfiguration,\n        dataset_size: int = 1000,\n        pattern_type: str = \"random_sentences\",\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize synthetic text dataset.\n\n        Args:\n            config: Text modality configuration (ModalityConfiguration)\n            dataset_size: Number of synthetic samples\n            pattern_type: Type of pattern to generate\n                ('random_sentences', 'repeated_phrases', 'sequences', 'palindromes')\n            split: Dataset split\n            rngs: Random number generators\n        \"\"\"\n        ...\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get a batch of samples.\n\n        Args:\n            batch_size: Number of samples in batch\n\n        Returns:\n            Batch dictionary with text data\n        \"\"\"\n        ...\n\n    def get_vocab_stats(self) -&gt; dict[str, int]:\n        \"\"\"Get vocabulary statistics.\n\n        Returns:\n            Dictionary with vocabulary statistics\n        \"\"\"\n        ...\n</code></pre> <p>Pattern Types:</p> <ul> <li><code>\"random_sentences\"</code>: Simple subject-verb-adverb sentences</li> <li><code>\"repeated_phrases\"</code>: Repeated phrases for pattern testing</li> <li><code>\"sequences\"</code>: Numerical sequences</li> <li><code>\"palindromes\"</code>: Palindromic text patterns</li> </ul>"},{"location":"api/data/loaders/#simpletextdataset","title":"<code>SimpleTextDataset</code>","text":"<p>Simple text dataset from list of strings.</p> <pre><code>class SimpleTextDataset(TextDataset):\n    \"\"\"Simple text dataset from list of strings.\"\"\"\n\n    def __init__(\n        self,\n        config: ModalityConfiguration,\n        texts: list[str],\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize simple text dataset.\n\n        Args:\n            config: Text modality configuration (ModalityConfiguration)\n            texts: List of text strings\n            split: Dataset split\n            rngs: Random number generators\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#create_text_dataset","title":"<code>create_text_dataset</code>","text":"<p>Factory function to create text datasets.</p> <pre><code>def create_text_dataset(\n    config: ModalityConfiguration,\n    dataset_type: str = \"synthetic\",\n    split: str = \"train\",\n    *,\n    rngs: nnx.Rngs,\n    **kwargs,\n) -&gt; TextDataset:\n    \"\"\"Factory function to create text datasets.\n\n    Args:\n        config: Text modality configuration\n        dataset_type: Type of dataset ('synthetic', 'simple')\n        split: Dataset split\n        rngs: Random number generators\n        **kwargs: Additional arguments for specific dataset types\n\n    Returns:\n        Text dataset instance\n\n    Raises:\n        ValueError: If dataset_type is unknown\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#audio-modality","title":"Audio Modality","text":""},{"location":"api/data/loaders/#audiomodalityconfig","title":"<code>AudioModalityConfig</code>","text":"<p>Configuration for audio modality processing.</p> <pre><code>@dataclass\nclass AudioModalityConfig:\n    \"\"\"Configuration for audio modality processing.\"\"\"\n\n    representation: AudioRepresentation = AudioRepresentation.RAW_WAVEFORM\n    sample_rate: int = 16000\n    n_mel_channels: int = 80\n    hop_length: int = 256\n    n_fft: int = 1024\n    duration: float = 2.0\n    normalize: bool = True\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>representation</code> <code>AudioRepresentation</code> <code>RAW_WAVEFORM</code> Audio representation format <code>sample_rate</code> <code>int</code> <code>16000</code> Audio sample rate in Hz <code>n_mel_channels</code> <code>int</code> <code>80</code> Number of mel-spectrogram channels <code>hop_length</code> <code>int</code> <code>256</code> Hop length for STFT/mel-spectrogram <code>n_fft</code> <code>int</code> <code>1024</code> FFT size for spectral representations <code>duration</code> <code>float</code> <code>2.0</code> Default audio duration in seconds <code>normalize</code> <code>bool</code> <code>True</code> Whether to normalize audio values"},{"location":"api/data/loaders/#audiomodality","title":"<code>AudioModality</code>","text":"<p>Base audio modality class providing unified interface for audio generation.</p> <pre><code>class AudioModality(GenerativeModel):\n    \"\"\"Base audio modality class.\"\"\"\n\n    def __init__(\n        self,\n        config: AudioModalityConfig | None = None,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize audio modality.\n\n        Args:\n            config: Audio modality configuration\n            rngs: Random number generators\n        \"\"\"\n        ...\n\n    @property\n    def n_time_steps(self) -&gt; int:\n        \"\"\"Number of time steps for raw waveform.\"\"\"\n        ...\n\n    @property\n    def n_time_frames(self) -&gt; int:\n        \"\"\"Number of time frames for spectral representations.\"\"\"\n        ...\n\n    @property\n    def output_shape(self) -&gt; tuple[int, ...]:\n        \"\"\"Output shape for generated audio.\"\"\"\n        ...\n\n    def generate(\n        self,\n        n_samples: int = 1,\n        duration: float | None = None,\n        *,\n        rngs: nnx.Rngs | None = None,\n        **kwargs,\n    ) -&gt; jnp.ndarray:\n        \"\"\"Generate audio samples.\n\n        Args:\n            n_samples: Number of audio samples to generate\n            duration: Duration override (uses config default if None)\n            rngs: Random number generators\n            **kwargs: Additional generation parameters\n\n        Returns:\n            Generated audio array\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#syntheticaudiodataset","title":"<code>SyntheticAudioDataset</code>","text":"<p>Synthetic audio dataset for testing and benchmarking.</p> <pre><code>class SyntheticAudioDataset(AudioDataset):\n    \"\"\"Synthetic audio dataset.\"\"\"\n\n    def __init__(\n        self,\n        config: AudioModalityConfig,\n        n_samples: int = 1000,\n        audio_types: list | None = None,\n        name: str = \"SyntheticAudioDataset\",\n    ):\n        \"\"\"Initialize synthetic audio dataset.\n\n        Args:\n            config: Audio modality configuration\n            n_samples: Number of synthetic samples to generate\n            audio_types: Types of audio to generate [\"sine\", \"noise\", \"chirp\"]\n            name: Dataset name\n        \"\"\"\n        ...\n\n    def __getitem__(self, idx: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get dataset item.\n\n        Args:\n            idx: Item index\n\n        Returns:\n            Dictionary containing 'audio' and optional metadata\n        \"\"\"\n        ...\n\n    def collate_fn(\n        self,\n        batch: list[dict[str, jax.Array]]\n    ) -&gt; dict[str, jax.Array]:\n        \"\"\"Collate function for batching.\n\n        Args:\n            batch: List of dataset items\n\n        Returns:\n            Batched data dictionary\n        \"\"\"\n        ...\n</code></pre> <p>Audio Types:</p> <ul> <li><code>\"sine\"</code>: Sine waves with random frequencies (200-800 Hz)</li> <li><code>\"noise\"</code>: White Gaussian noise</li> <li><code>\"chirp\"</code>: Linear frequency sweeps</li> </ul>"},{"location":"api/data/loaders/#create_audio_dataset","title":"<code>create_audio_dataset</code>","text":"<p>Factory function to create audio datasets.</p> <pre><code>def create_audio_dataset(\n    dataset_type: str = \"synthetic\",\n    config: AudioModalityConfig | None = None,\n    **kwargs\n) -&gt; AudioDataset:\n    \"\"\"Factory function to create audio datasets.\n\n    Args:\n        dataset_type: Type of dataset to create (\"synthetic\")\n        config: Audio modality configuration\n        **kwargs: Additional dataset-specific parameters\n\n    Returns:\n        Audio dataset instance\n\n    Raises:\n        ValueError: If dataset_type is unknown\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#multi-modal","title":"Multi-Modal","text":""},{"location":"api/data/loaders/#multimodaldataset","title":"<code>MultiModalDataset</code>","text":"<p>Dataset containing multiple aligned modalities.</p> <pre><code>class MultiModalDataset(BaseDataset):\n    \"\"\"Dataset containing multiple aligned modalities.\"\"\"\n\n    def __init__(\n        self,\n        modalities: list[str],\n        num_samples: int,\n        image_shape: tuple[int, int, int] = (32, 32, 3),\n        text_vocab_size: int = 1000,\n        text_sequence_length: int = 50,\n        audio_sample_rate: int = 16000,\n        audio_duration: float = 1.0,\n        alignment_strength: float = 0.8,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize multi-modal dataset.\n\n        Args:\n            modalities: List of modality names to include\n            num_samples: Number of samples in the dataset\n            image_shape: Shape of image data\n            text_vocab_size: Vocabulary size for text\n            text_sequence_length: Length of text sequences\n            audio_sample_rate: Audio sampling rate\n            audio_duration: Audio clip duration in seconds\n            alignment_strength: How strongly modalities are aligned (0-1)\n            rngs: Random number generators\n        \"\"\"\n        ...\n\n    def __getitem__(self, idx: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get a sample from the dataset.\n\n        Args:\n            idx: Sample index\n\n        Returns:\n            Dictionary containing data for each modality\n\n        Raises:\n            IndexError: If index is out of range\n        \"\"\"\n        ...\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get a batch of samples.\n\n        Args:\n            batch_size: Batch size\n\n        Returns:\n            Batch of multi-modal data\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#multimodalpaireddataset","title":"<code>MultiModalPairedDataset</code>","text":"<p>Dataset with explicitly paired multi-modal data.</p> <pre><code>class MultiModalPairedDataset(BaseDataset):\n    \"\"\"Dataset with explicitly paired multi-modal data.\"\"\"\n\n    def __init__(\n        self,\n        pairs: list[tuple[str, str]],\n        data: dict[str, jax.Array],\n        alignments: jax.Array | None = None,\n    ):\n        \"\"\"Initialize paired multi-modal dataset.\n\n        Args:\n            pairs: List of modality pairs\n            data: Dictionary of modality data\n            alignments: Optional alignment scores for pairs\n        \"\"\"\n        ...\n\n    def __getitem__(self, idx: int) -&gt; dict[str, jax.Array | float]:\n        \"\"\"Get a paired sample.\n\n        Args:\n            idx: Sample index\n\n        Returns:\n            Dictionary with paired data\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/data/loaders/#create_synthetic_multi_modal_dataset","title":"<code>create_synthetic_multi_modal_dataset</code>","text":"<p>Create a synthetic multi-modal dataset.</p> <pre><code>def create_synthetic_multi_modal_dataset(\n    modalities: list[str],\n    num_samples: int = 1000,\n    alignment_strength: float = 0.8,\n    *,\n    rngs: nnx.Rngs,\n    **kwargs,\n) -&gt; MultiModalDataset:\n    \"\"\"Create a synthetic multi-modal dataset.\n\n    Args:\n        modalities: List of modality names\n        num_samples: Number of samples\n        alignment_strength: How strongly modalities are aligned\n        rngs: Random number generators\n        **kwargs: Additional arguments for dataset\n\n    Returns:\n        Multi-modal dataset\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#utility-functions","title":"Utility Functions","text":""},{"location":"api/data/loaders/#validate_modality_interface","title":"<code>validate_modality_interface</code>","text":"<p>Validate that an instance implements the Modality protocol.</p> <pre><code>def validate_modality_interface(modality_instance: Any) -&gt; bool:\n    \"\"\"Validate that an instance implements the Modality protocol.\n\n    Args:\n        modality_instance: Instance to validate\n\n    Returns:\n        True if instance implements Modality protocol\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#create_modality_factory","title":"<code>create_modality_factory</code>","text":"<p>Create a factory function for a modality.</p> <pre><code>def create_modality_factory(\n    modality_class: type,\n    default_config: BaseModalityConfig,\n):\n    \"\"\"Create a factory function for a modality.\n\n    Args:\n        modality_class: The modality class to instantiate\n        default_config: Default configuration\n\n    Returns:\n        Factory function\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#registry-functions","title":"Registry Functions","text":""},{"location":"api/data/loaders/#register_modality","title":"<code>register_modality</code>","text":"<p>Register a modality in the global registry.</p> <pre><code>def register_modality(name: str, modality_class: type):\n    \"\"\"Register a modality.\n\n    Args:\n        name: Modality name\n        modality_class: Modality class\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#get_modality","title":"<code>get_modality</code>","text":"<p>Get a modality class by name.</p> <pre><code>def get_modality(name: str) -&gt; type:\n    \"\"\"Get a modality by name.\n\n    Args:\n        name: Modality name\n\n    Returns:\n        Modality class\n\n    Raises:\n        KeyError: If modality not found\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#list_modalities","title":"<code>list_modalities</code>","text":"<p>List all registered modalities.</p> <pre><code>def list_modalities() -&gt; list[str]:\n    \"\"\"List all registered modalities.\n\n    Returns:\n        List of modality names\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/loaders/#type-aliases","title":"Type Aliases","text":"<p>Common type aliases used throughout the data API:</p> <pre><code># Modality data types\nModalityData = jax.Array\nModalityBatch = dict[str, jax.Array]\nModalityConfig = BaseModalityConfig\nEvaluationMetrics = dict[str, float]\n</code></pre>"},{"location":"api/data/loaders/#examples","title":"Examples","text":""},{"location":"api/data/loaders/#creating-a-custom-dataset","title":"Creating a Custom Dataset","text":"<pre><code>from typing import Iterator\nimport jax.numpy as jnp\nfrom artifex.generative_models.modalities.base import BaseDataset\n\nclass MyCustomDataset(BaseDataset):\n    \"\"\"Custom dataset implementation.\"\"\"\n\n    def __init__(\n        self,\n        config: BaseModalityConfig,\n        data_paths: list[str],\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(config, split, rngs=rngs)\n        self.data_paths = data_paths\n        self.data = self._load_data()\n\n    def _load_data(self):\n        # Implement data loading logic\n        pass\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        for sample in self.data:\n            yield sample\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        # Implement batch sampling logic\n        pass\n</code></pre>"},{"location":"api/data/loaders/#using-factory-functions","title":"Using Factory Functions","text":"<pre><code>from flax import nnx\n\n# Create datasets using factories\nrngs = nnx.Rngs(0)\n\n# Image dataset\nimage_dataset = create_image_dataset(\n    dataset_type=\"synthetic\",\n    config=image_config,\n    pattern_type=\"gradient\",\n    dataset_size=1000,\n    rngs=rngs\n)\n\n# Text dataset\ntext_dataset = create_text_dataset(\n    config=text_config,\n    dataset_type=\"synthetic\",\n    pattern_type=\"random_sentences\",\n    dataset_size=1000,\n    rngs=rngs\n)\n\n# Audio dataset\naudio_dataset = create_audio_dataset(\n    dataset_type=\"synthetic\",\n    config=audio_config,\n    n_samples=1000,\n    audio_types=[\"sine\", \"noise\"]\n)\n</code></pre>"},{"location":"api/data/loaders/#see-also","title":"See Also","text":"<ul> <li>Data Loading Overview - System overview</li> <li>Data Loading Guide - Practical usage guide</li> <li>Image Modality Guide - Image-specific guide</li> <li>Text Modality Guide - Text-specific guide</li> <li>Audio Modality Guide - Audio-specific guide</li> <li>Multi-modal Guide - Multi-modal guide</li> </ul>"},{"location":"api/data/protein-dataset/","title":"Protein Dataset API","text":"<p>Coming Soon</p> <p>This page is under development. Check back for protein dataset API documentation.</p>"},{"location":"api/data/protein-dataset/#overview","title":"Overview","text":"<p>API reference for protein structure datasets.</p>"},{"location":"api/data/protein-dataset/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling Guide</li> <li>Data Loaders</li> </ul>"},{"location":"api/datasets/crossdocked/","title":"CrossDocked Dataset API","text":"<p>Coming Soon</p> <p>This page is under development. Check back for CrossDocked dataset API documentation.</p>"},{"location":"api/datasets/crossdocked/#overview","title":"Overview","text":"<p>API reference for the CrossDocked protein-ligand dataset.</p>"},{"location":"api/datasets/crossdocked/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein-Ligand Benchmark</li> <li>Data Loaders</li> </ul>"},{"location":"api/extensions/protein/","title":"Protein Extensions API","text":"<p>Coming Soon</p> <p>This page is under development. Check back for protein extensions API documentation.</p>"},{"location":"api/extensions/protein/#overview","title":"Overview","text":"<p>API reference for protein modeling extensions.</p>"},{"location":"api/extensions/protein/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling Guide</li> <li>Protein Extensions Guide</li> </ul>"},{"location":"api/models/autoregressive/","title":"Autoregressive Models API Reference","text":"<p>Complete API documentation for autoregressive models in Artifex.</p> <p>Coming Soon</p> <p>Autoregressive model implementations are planned for a future release. This documentation will be updated when the feature is available.</p>"},{"location":"api/models/autoregressive/#overview","title":"Overview","text":"<p>Autoregressive models will include:</p> <ul> <li>PixelCNN: Pixel-by-pixel image generation</li> <li>WaveNet: Audio waveform generation</li> <li>Transformer-based: GPT-style text generation</li> <li>MAR (Masked Autoregressive): Masked autoregressive models</li> </ul>"},{"location":"api/models/autoregressive/#planned-api","title":"Planned API","text":""},{"location":"api/models/autoregressive/#base-class","title":"Base Class","text":"<pre><code>from artifex.generative_models.models.autoregressive.base import AutoregressiveModel\n\nmodel = AutoregressiveModel(\n    config: AutoregressiveConfig,\n    *,\n    rngs: nnx.Rngs,\n)\n</code></pre>"},{"location":"api/models/autoregressive/#configuration","title":"Configuration","text":"<pre><code>from artifex.generative_models.core.configuration import AutoregressiveConfig\n\nconfig = AutoregressiveConfig(\n    name=\"autoregressive_model\",\n    sequence_length=256,\n    vocab_size=10000,\n    hidden_dim=512,\n    num_layers=6,\n)\n</code></pre>"},{"location":"api/models/autoregressive/#related-documentation","title":"Related Documentation","text":"<ul> <li>Autoregressive Models Guide - Conceptual overview</li> <li>Autoregressive Concepts - Understanding autoregressive modeling</li> </ul>"},{"location":"api/models/autoregressive/#references","title":"References","text":"<ul> <li>van den Oord et al., \"Pixel Recurrent Neural Networks\" (2016)</li> <li>van den Oord et al., \"WaveNet: A Generative Model for Raw Audio\" (2016)</li> <li>Radford et al., \"Language Models are Unsupervised Multitask Learners\" (2019)</li> </ul>"},{"location":"api/models/diffusion/","title":"Diffusion Models API Reference","text":"<p>Complete API documentation for all diffusion model classes in Artifex.</p>"},{"location":"api/models/diffusion/#base-classes","title":"Base Classes","text":""},{"location":"api/models/diffusion/#diffusionmodel","title":"DiffusionModel","text":"<p>Base class for all diffusion models, implementing the core diffusion process.</p> <p>Purpose: Provides the foundational diffusion framework including forward diffusion (adding noise), reverse diffusion (denoising), and noise scheduling.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel","title":"artifex.generative_models.models.diffusion.base.DiffusionModel","text":"<pre><code>DiffusionModel(config: DiffusionConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>GenerativeModel</code></p> <p>Base class for diffusion models.</p> <p>This implements a general diffusion model that can support various diffusion processes like DDPM (Denoising Diffusion Probabilistic Models) and DDIM (Denoising Diffusion Implicit Models).</p> <p>Uses the nested DiffusionConfig architecture with: - backbone: BackboneConfig (polymorphic) for the denoising network - noise_schedule: NoiseScheduleConfig for the diffusion schedule</p> <p>Backbone type is determined by config.backbone.backbone_type discriminator. Supported backbones: UNet, DiT, U-ViT, UNet2DCondition.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>DiffusionConfig for the model</p> <code>backbone</code> <p>The denoising network (created from config.backbone)</p> <code>noise_schedule</code> <code>NoiseSchedule</code> <p>NoiseSchedule instance for diffusion process</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DiffusionConfig</code> <p>DiffusionConfig with nested backbone and noise_schedule configs.     The backbone field accepts any BackboneConfig type (UNetBackboneConfig,     DiTBackboneConfig, etc.) and the appropriate backbone is created     based on the backbone_type discriminator.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone = create_backbone(backbone, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.noise_schedule","title":"noise_schedule  <code>instance-attribute</code>","text":"<pre><code>noise_schedule: NoiseSchedule = create_noise_schedule(\n    noise_schedule\n)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.betas","title":"betas  <code>instance-attribute</code>","text":"<pre><code>betas = betas\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.alphas","title":"alphas  <code>instance-attribute</code>","text":"<pre><code>alphas = alphas\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.alphas_cumprod","title":"alphas_cumprod  <code>instance-attribute</code>","text":"<pre><code>alphas_cumprod = alphas_cumprod\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.alphas_cumprod_prev","title":"alphas_cumprod_prev  <code>instance-attribute</code>","text":"<pre><code>alphas_cumprod_prev = alphas_cumprod_prev\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.sqrt_alphas_cumprod","title":"sqrt_alphas_cumprod  <code>instance-attribute</code>","text":"<pre><code>sqrt_alphas_cumprod = sqrt_alphas_cumprod\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.sqrt_one_minus_alphas_cumprod","title":"sqrt_one_minus_alphas_cumprod  <code>instance-attribute</code>","text":"<pre><code>sqrt_one_minus_alphas_cumprod = (\n    sqrt_one_minus_alphas_cumprod\n)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.log_one_minus_alphas_cumprod","title":"log_one_minus_alphas_cumprod  <code>instance-attribute</code>","text":"<pre><code>log_one_minus_alphas_cumprod = log_one_minus_alphas_cumprod\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.sqrt_recip_alphas_cumprod","title":"sqrt_recip_alphas_cumprod  <code>instance-attribute</code>","text":"<pre><code>sqrt_recip_alphas_cumprod = sqrt_recip_alphas_cumprod\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.sqrt_recipm1_alphas_cumprod","title":"sqrt_recipm1_alphas_cumprod  <code>instance-attribute</code>","text":"<pre><code>sqrt_recipm1_alphas_cumprod = sqrt_recipm1_alphas_cumprod\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.posterior_variance","title":"posterior_variance  <code>instance-attribute</code>","text":"<pre><code>posterior_variance = posterior_variance\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.posterior_log_variance_clipped","title":"posterior_log_variance_clipped  <code>instance-attribute</code>","text":"<pre><code>posterior_log_variance_clipped = (\n    posterior_log_variance_clipped\n)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.posterior_mean_coef1","title":"posterior_mean_coef1  <code>instance-attribute</code>","text":"<pre><code>posterior_mean_coef1 = posterior_mean_coef1\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.posterior_mean_coef2","title":"posterior_mean_coef2  <code>instance-attribute</code>","text":"<pre><code>posterior_mean_coef2 = posterior_mean_coef2\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.q_sample","title":"q_sample","text":"<pre><code>q_sample(\n    x_start: Array, t: Array, noise: Array | None = None\n) -&gt; Array\n</code></pre> <p>Sample from the forward diffusion process q(x_t | x_0).</p> <p>Parameters:</p> Name Type Description Default <code>x_start</code> <code>Array</code> <p>Starting clean data (x_0)</p> required <code>t</code> <code>Array</code> <p>Timesteps</p> required <code>noise</code> <code>Array | None</code> <p>Optional pre-generated noise</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Noisy samples x_t</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.predict_start_from_noise","title":"predict_start_from_noise","text":"<pre><code>predict_start_from_noise(\n    x_t: Array, t: Array, noise: Array\n) -&gt; Array\n</code></pre> <p>Predict x_0 from noise model output.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Array</code> <p>Noisy input at timestep t</p> required <code>t</code> <code>Array</code> <p>Timesteps</p> required <code>noise</code> <code>Array</code> <p>Predicted noise</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Predicted x_0</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.q_posterior_mean_variance","title":"q_posterior_mean_variance","text":"<pre><code>q_posterior_mean_variance(\n    x_start: Array, x_t: Array, t: Array\n) -&gt; tuple[Array, Array, Array]\n</code></pre> <p>Compute the mean and variance of the diffusion posterior q(x_{t-1} | x_t, x_0).</p> <p>Parameters:</p> Name Type Description Default <code>x_start</code> <code>Array</code> <p>Clean data (x_0)</p> required <code>x_t</code> <code>Array</code> <p>Noisy data (x_t)</p> required <code>t</code> <code>Array</code> <p>Timesteps</p> required <p>Returns:</p> Type Description <code>tuple[Array, Array, Array]</code> <p>Tuple of (posterior_mean, posterior_variance, posterior_log_variance_clipped)</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.p_mean_variance","title":"p_mean_variance","text":"<pre><code>p_mean_variance(\n    model_output: Array,\n    x_t: Array,\n    t: Array,\n    clip_denoised: bool = True,\n) -&gt; dict[str, Array]\n</code></pre> <p>Compute the model's predicted mean and variance for x_{t-1}.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Array</code> <p>Predicted noise or x_0</p> required <code>x_t</code> <code>Array</code> <p>Noisy input at timestep t</p> required <code>t</code> <code>Array</code> <p>Timesteps</p> required <code>clip_denoised</code> <code>bool</code> <p>Whether to clip the denoised signal to [-1, 1]</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Array]</code> <p>dictionary with predicted mean and variance</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.p_sample","title":"p_sample","text":"<pre><code>p_sample(\n    model_output: Array,\n    x_t: Array,\n    t: Array,\n    clip_denoised: bool = True,\n) -&gt; Array\n</code></pre> <p>Sample from the denoising process p(x_{t-1} | x_t).</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>Array</code> <p>Predicted noise</p> required <code>x_t</code> <code>Array</code> <p>Noisy input at timestep t</p> required <code>t</code> <code>Array</code> <p>Timesteps</p> required <code>clip_denoised</code> <code>bool</code> <p>Whether to clip the denoised signal to [-1, 1]</p> <code>True</code> <p>Returns:</p> Type Description <code>Array</code> <p>Denoised x_{t-1}</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    shape: tuple[int, ...] | None = None,\n    clip_denoised: bool = True,\n) -&gt; Array\n</code></pre> <p>Generate samples from random noise.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>shape</code> <code>tuple[int, ...] | None</code> <p>Shape of samples to generate (excluding batch dimension)</p> <code>None</code> <code>clip_denoised</code> <code>bool</code> <p>Whether to clip the denoised signal to [-1, 1]</p> <code>True</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.base.DiffusionModel.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn(\n    batch: Any, model_outputs: dict[str, Any]\n) -&gt; dict[str, Any]\n</code></pre> <p>Compute loss for training.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>Input batch (should contain 'x' key with data)</p> required <code>model_outputs</code> <code>dict[str, Any]</code> <p>Model outputs from forward pass</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing loss and metrics</p>"},{"location":"api/models/diffusion/#initialization","title":"Initialization","text":"<pre><code>DiffusionModel(\n    config: ModelConfig,\n    backbone_fn: Callable,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>config</code> <code>ModelConfig</code> Model configuration with input dimensions and parameters <code>backbone_fn</code> <code>Callable</code> Function that creates the backbone network (e.g., U-Net) <code>rngs</code> <code>nnx.Rngs</code> Random number generators for initialization <p>Configuration Parameters:</p> Parameter Type Default Description <code>noise_steps</code> <code>int</code> 1000 Number of diffusion timesteps <code>beta_start</code> <code>float</code> 1e-4 Initial noise variance <code>beta_end</code> <code>float</code> 0.02 Final noise variance"},{"location":"api/models/diffusion/#methods","title":"Methods","text":""},{"location":"api/models/diffusion/#__call__x-timesteps-rngsnone-trainingfalse-kwargs","title":"<code>__call__(x, timesteps, *, rngs=None, training=False, **kwargs)</code>","text":"<p>Forward pass through the diffusion model.</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input data <code>(batch, *input_dim)</code></li> <li><code>timesteps</code> (<code>jax.Array</code>): Timestep indices <code>(batch,)</code></li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> <li><code>**kwargs</code>: Additional arguments passed to backbone</li> </ul> <p>Returns:</p> <ul> <li><code>dict[str, Any]</code>: Dictionary containing <code>\"predicted_noise\"</code> and potentially other outputs</li> </ul> <p>Example:</p> <pre><code># Create model\nmodel = DiffusionModel(config, backbone_fn, rngs=rngs)\n\n# Forward pass\nx = jax.random.normal(rngs.sample(), (4, 32, 32, 3))\nt = jnp.array([100, 200, 300, 400])\noutputs = model(x, t, rngs=rngs, training=True)\n\nprint(outputs[\"predicted_noise\"].shape)  # (4, 32, 32, 3)\n</code></pre>"},{"location":"api/models/diffusion/#setup_noise_schedule","title":"<code>setup_noise_schedule()</code>","text":"<p>Set up the noise schedule for the diffusion process.</p> <p>Description:</p> <p>Computes the noise schedule (betas) and derived quantities (alphas, alpha_cumprod, etc.) used throughout the diffusion process. Default implementation uses a linear schedule.</p> <p>Computed Attributes:</p> <ul> <li><code>betas</code>: Noise variances at each timestep</li> <li><code>alphas</code>: \\(\\alpha_t = 1 - \\beta_t\\)</li> <li><code>alphas_cumprod</code>: \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\)</li> <li><code>sqrt_alphas_cumprod</code>: \\(\\sqrt{\\bar{\\alpha}_t}\\)</li> <li><code>sqrt_one_minus_alphas_cumprod</code>: \\(\\sqrt{1 - \\bar{\\alpha}_t}\\)</li> <li><code>posterior_variance</code>: Variance of \\(q(x_{t-1} | x_t, x_0)\\)</li> </ul>"},{"location":"api/models/diffusion/#q_samplex_start-t-noisenone-rngsnone","title":"<code>q_sample(x_start, t, noise=None, *, rngs=None)</code>","text":"<p>Sample from the forward diffusion process \\(q(x_t | x_0)\\).</p> <p>Parameters:</p> <ul> <li><code>x_start</code> (<code>jax.Array</code>): Clean data \\(x_0\\)</li> <li><code>t</code> (<code>jax.Array</code>): Timesteps <code>(batch,)</code></li> <li><code>noise</code> (<code>jax.Array | None</code>): Optional pre-generated noise</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Noisy samples \\(x_t\\)</li> </ul> <p>Mathematical Formula:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>Example:</p> <pre><code># Add noise to clean images\nx_clean = jax.random.normal(rngs.sample(), (8, 32, 32, 3))\nt = jnp.array([100] * 8)\n\nx_noisy = model.q_sample(x_clean, t, rngs=rngs)\nprint(f\"Clean: {x_clean.shape}, Noisy: {x_noisy.shape}\")\n</code></pre>"},{"location":"api/models/diffusion/#p_samplemodel_output-x_t-t-rngsnone-clip_denoisedtrue","title":"<code>p_sample(model_output, x_t, t, *, rngs=None, clip_denoised=True)</code>","text":"<p>Sample from the denoising process \\(p(x_{t-1} | x_t)\\).</p> <p>Parameters:</p> <ul> <li><code>model_output</code> (<code>jax.Array</code>): Predicted noise from model</li> <li><code>x_t</code> (<code>jax.Array</code>): Noisy input at timestep \\(t\\)</li> <li><code>t</code> (<code>jax.Array</code>): Current timesteps</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>clip_denoised</code> (<code>bool</code>): Whether to clip to [-1, 1]</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Denoised sample \\(x_{t-1}\\)</li> </ul> <p>Example:</p> <pre><code># Single denoising step\nx_t = noisy_sample\nt = jnp.array([500])\n\n# Get model prediction\noutputs = model(x_t, t, rngs=rngs)\npredicted_noise = outputs[\"predicted_noise\"]\n\n# Denoise one step\nx_t_minus_1 = model.p_sample(predicted_noise, x_t, t, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#generaten_samples1-rngsnone-shapenone-clip_denoisedtrue-kwargs","title":"<code>generate(n_samples=1, *, rngs=None, shape=None, clip_denoised=True, **kwargs)</code>","text":"<p>Generate samples from random noise.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code> (<code>int</code>): Number of samples to generate</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>shape</code> (<code>tuple[int, ...] | None</code>): Sample shape (excluding batch)</li> <li><code>clip_denoised</code> (<code>bool</code>): Whether to clip to [-1, 1]</li> <li><code>**kwargs</code>: Additional model arguments</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated samples <code>(n_samples, *shape)</code></li> </ul> <p>Example:</p> <pre><code># Generate 16 samples\nsamples = model.generate(n_samples=16, rngs=rngs)\nprint(f\"Generated: {samples.shape}\")  # (16, 32, 32, 3)\n</code></pre>"},{"location":"api/models/diffusion/#predict_start_from_noisex_t-t-noise","title":"<code>predict_start_from_noise(x_t, t, noise)</code>","text":"<p>Predict \\(x_0\\) from \\(x_t\\) and predicted noise.</p> <p>Parameters:</p> <ul> <li><code>x_t</code> (<code>jax.Array</code>): Noisy sample at timestep \\(t\\)</li> <li><code>t</code> (<code>jax.Array</code>): Timesteps</li> <li><code>noise</code> (<code>jax.Array</code>): Predicted noise</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Predicted \\(x_0\\)</li> </ul> <p>Mathematical Formula:</p> \\[ x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} x_t - \\frac{\\sqrt{1 - \\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}} \\epsilon \\]"},{"location":"api/models/diffusion/#loss_fnbatch-model_outputs-rngsnone-kwargs","title":"<code>loss_fn(batch, model_outputs, *, rngs=None, **kwargs)</code>","text":"<p>Compute the diffusion loss.</p> <p>Parameters:</p> <ul> <li><code>batch</code> (<code>Any</code>): Input batch (dict with <code>'x'</code> key or array)</li> <li><code>model_outputs</code> (<code>dict[str, Any]</code>): Model predictions</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>**kwargs</code>: Additional arguments</li> </ul> <p>Returns:</p> <ul> <li><code>dict[str, Any]</code>: Dictionary with <code>'loss'</code> and metrics</li> </ul> <p>Example:</p> <pre><code># Training loop\n@nnx.jit\ndef train_step(model, optimizer, batch, rngs):\n    def loss_fn_wrapper(model):\n        # Add noise\n        t = jax.random.randint(rngs.timestep(), (batch.shape[0],), 0, 1000)\n        noise = jax.random.normal(rngs.noise(), batch.shape)\n        x_noisy = model.q_sample(batch, t, noise, rngs=rngs)\n\n        # Predict\n        outputs = model(x_noisy, t, training=True, rngs=rngs)\n\n        # Compute loss\n        loss_dict = model.loss_fn({\"x\": batch}, outputs, rngs=rngs)\n        return loss_dict[\"loss\"]\n\n    loss, grads = nnx.value_and_grad(loss_fn_wrapper)(model)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n    return {\"loss\": loss}\n</code></pre>"},{"location":"api/models/diffusion/#ddpm-denoising-diffusion-probabilistic-models","title":"DDPM (Denoising Diffusion Probabilistic Models)","text":""},{"location":"api/models/diffusion/#ddpmmodel","title":"DDPMModel","text":"<p>Standard DDPM implementation with support for both DDPM and DDIM sampling.</p> <p>Purpose: Implements the foundational denoising diffusion probabilistic model with standard training and sampling procedures.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel","title":"artifex.generative_models.models.diffusion.ddpm.DDPMModel","text":"<pre><code>DDPMModel(config: DDPMConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>DiffusionModel</code></p> <p>DDPM (Denoising Diffusion Probabilistic Models) implementation.</p> <p>This model implements the denoising diffusion probabilistic model as described in the DDPM paper by Ho et al.</p> <p>Uses nested DDPMConfig with: - backbone: BackboneConfig (polymorphic) for the denoising network - noise_schedule: NoiseScheduleConfig for the diffusion schedule - loss_type: Loss function type (mse, l1, huber) - clip_denoised: Whether to clip denoised samples</p> <p>Backbone type is determined by config.backbone.backbone_type discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DDPMConfig</code> <p>DDPMConfig with nested backbone and noise_schedule configs.     The backbone field accepts any BackboneConfig type and the     appropriate backbone is created based on backbone_type.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.loss_type","title":"loss_type  <code>instance-attribute</code>","text":"<pre><code>loss_type = loss_type\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.clip_denoised","title":"clip_denoised  <code>instance-attribute</code>","text":"<pre><code>clip_denoised = clip_denoised\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_shape\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.in_channels","title":"in_channels  <code>instance-attribute</code>","text":"<pre><code>in_channels = input_shape[0]\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.noise_steps","title":"noise_steps  <code>instance-attribute</code>","text":"<pre><code>noise_steps = num_timesteps\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.beta_start","title":"beta_start  <code>instance-attribute</code>","text":"<pre><code>beta_start = beta_start\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.beta_end","title":"beta_end  <code>instance-attribute</code>","text":"<pre><code>beta_end = beta_end\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.beta_schedule","title":"beta_schedule  <code>instance-attribute</code>","text":"<pre><code>beta_schedule = schedule_type\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.forward_diffusion","title":"forward_diffusion","text":"<pre><code>forward_diffusion(\n    x: Array, t: Array\n) -&gt; tuple[Array, Array]\n</code></pre> <p>Forward diffusion process q(x_t | x_0).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input data tensor</p> required <code>t</code> <code>Array</code> <p>Timestep indices</p> required <p>Returns:</p> Type Description <code>tuple[Array, Array]</code> <p>Tuple of (noisy_x, noise)</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.denoise_step","title":"denoise_step","text":"<pre><code>denoise_step(\n    x_t: Array,\n    t: Array,\n    predicted_noise: Array,\n    clip_denoised: bool = True,\n) -&gt; Array\n</code></pre> <p>Perform a single denoising step: x_{t-1} = f(x_t, t, noise).</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Array</code> <p>Noisy input at timestep t</p> required <code>t</code> <code>Array</code> <p>Current timestep indices</p> required <code>predicted_noise</code> <code>Array</code> <p>Predicted noise from the model</p> required <code>clip_denoised</code> <code>bool</code> <p>Whether to clip values to [-1, 1]</p> <code>True</code> <p>Returns:</p> Type Description <code>Array</code> <p>Denoised x_{t-1}</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddpm.DDPMModel.sample","title":"sample","text":"<pre><code>sample(\n    n_samples_or_shape: int | tuple[int, ...],\n    scheduler: str = \"ddpm\",\n    steps: int | None = None,\n) -&gt; Array\n</code></pre> <p>Sample from the diffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples_or_shape</code> <code>int | tuple[int, ...]</code> <p>Number of samples or full shape including batch dimension</p> required <code>scheduler</code> <code>str</code> <p>Sampling scheduler to use ('ddpm', 'ddim')</p> <code>'ddpm'</code> <code>steps</code> <code>int | None</code> <p>Number of sampling steps (if None, use default)</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/diffusion/#initialization_1","title":"Initialization","text":"<pre><code>DDPMModel(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs,\n    **kwargs\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>config</code> <code>ModelConfig</code> Model configuration <code>rngs</code> <code>nnx.Rngs</code> Random number generators <code>**kwargs</code> <code>dict</code> Additional arguments (e.g., <code>backbone_fn</code>) <p>Configuration Parameters:</p> Parameter Type Default Description <code>noise_steps</code> <code>int</code> 1000 Number of diffusion timesteps <code>beta_start</code> <code>float</code> 1e-4 Initial noise variance <code>beta_end</code> <code>float</code> 0.02 Final noise variance <code>beta_schedule</code> <code>str</code> \"linear\" Schedule type (\"linear\" or \"cosine\")"},{"location":"api/models/diffusion/#methods_1","title":"Methods","text":""},{"location":"api/models/diffusion/#forward_diffusionx-t-rngsnone","title":"<code>forward_diffusion(x, t, *, rngs=None)</code>","text":"<p>Forward diffusion process \\(q(x_t | x_0)\\).</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Clean input data</li> <li><code>t</code> (<code>jax.Array</code>): Timestep indices</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>tuple[jax.Array, jax.Array]</code>: <code>(noisy_x, noise)</code> tuple</li> </ul> <p>Example:</p> <pre><code>model = DDPMModel(config, rngs=rngs)\n\nx_clean = jnp.ones((4, 32, 32, 3))\nt = jnp.array([100, 200, 300, 400])\n\nx_noisy, noise = model.forward_diffusion(x_clean, t, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#denoise_stepx_t-t-predicted_noise-clip_denoisedtrue","title":"<code>denoise_step(x_t, t, predicted_noise, clip_denoised=True)</code>","text":"<p>Single denoising step.</p> <p>Parameters:</p> <ul> <li><code>x_t</code> (<code>jax.Array</code>): Noisy input at timestep \\(t\\)</li> <li><code>t</code> (<code>jax.Array</code>): Current timesteps</li> <li><code>predicted_noise</code> (<code>jax.Array</code>): Predicted noise</li> <li><code>clip_denoised</code> (<code>bool</code>): Whether to clip to [-1, 1]</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Denoised \\(x_{t-1}\\)</li> </ul>"},{"location":"api/models/diffusion/#samplen_samples_or_shape-schedulerddpm-stepsnone-rngsnone-kwargs","title":"<code>sample(n_samples_or_shape, scheduler=\"ddpm\", steps=None, *, rngs=None, **kwargs)</code>","text":"<p>Sample from the diffusion model.</p> <p>Parameters:</p> <ul> <li><code>n_samples_or_shape</code> (<code>int | tuple</code>): Number of samples or full shape</li> <li><code>scheduler</code> (<code>str</code>): Sampling scheduler (<code>\"ddpm\"</code> or <code>\"ddim\"</code>)</li> <li><code>steps</code> (<code>int | None</code>): Number of sampling steps</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>**kwargs</code>: Additional arguments</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated samples</li> </ul> <p>Example:</p> <pre><code># DDPM sampling (slow but high quality)\nsamples_ddpm = model.sample(16, scheduler=\"ddpm\", rngs=rngs)\n\n# DDIM sampling (fast)\nsamples_ddim = model.sample(16, scheduler=\"ddim\", steps=50, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#ddim-denoising-diffusion-implicit-models","title":"DDIM (Denoising Diffusion Implicit Models)","text":""},{"location":"api/models/diffusion/#ddimmodel","title":"DDIMModel","text":"<p>DDIM implementation with deterministic sampling and fast inference.</p> <p>Purpose: Enables much faster sampling (10-20x) than DDPM while maintaining quality, and supports deterministic generation for image editing.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel","title":"artifex.generative_models.models.diffusion.ddim.DDIMModel","text":"<pre><code>DDIMModel(config: DDIMConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>DDPMModel</code></p> <p>DDIM (Denoising Diffusion Implicit Models) implementation.</p> <p>This model implements deterministic sampling from diffusion models as described in the DDIM paper by Song et al. DDIM enables faster sampling with fewer steps while maintaining high quality.</p> <p>Uses nested DDIMConfig with: - backbone: BackboneConfig (polymorphic) for the denoising network - noise_schedule: NoiseScheduleConfig for the diffusion schedule - eta: Stochasticity parameter (0=deterministic, 1=DDPM) - num_inference_steps: Number of sampling steps - skip_type: Timestep skip strategy</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DDIMConfig</code> <p>DDIMConfig with nested backbone and noise_schedule configs.     The backbone field accepts any BackboneConfig type and the     appropriate backbone is created based on backbone_type.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel.eta","title":"eta  <code>instance-attribute</code>","text":"<pre><code>eta = eta\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel.ddim_steps","title":"ddim_steps  <code>instance-attribute</code>","text":"<pre><code>ddim_steps = num_inference_steps\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel.skip_type","title":"skip_type  <code>instance-attribute</code>","text":"<pre><code>skip_type = skip_type\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel.get_ddim_timesteps","title":"get_ddim_timesteps","text":"<pre><code>get_ddim_timesteps(ddim_steps: int) -&gt; Array\n</code></pre> <p>Get timesteps for DDIM sampling.</p> <p>Parameters:</p> Name Type Description Default <code>ddim_steps</code> <code>int</code> <p>Number of DDIM sampling steps</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Array of timesteps for DDIM sampling</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel.ddim_step","title":"ddim_step","text":"<pre><code>ddim_step(\n    x_t: Array,\n    t: Array,\n    t_prev: Array,\n    predicted_noise: Array,\n    eta: float | None = None,\n) -&gt; Array\n</code></pre> <p>Perform a single DDIM sampling step.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Array</code> <p>Current sample at timestep t</p> required <code>t</code> <code>Array</code> <p>Current timestep</p> required <code>t_prev</code> <code>Array</code> <p>Previous timestep</p> required <code>predicted_noise</code> <code>Array</code> <p>Predicted noise from the model</p> required <code>eta</code> <code>float | None</code> <p>DDIM interpolation parameter (0=deterministic, 1=DDPM)</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Sample at timestep t_prev</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel.ddim_sample","title":"ddim_sample","text":"<pre><code>ddim_sample(\n    n_samples: int,\n    steps: int | None = None,\n    eta: float | None = None,\n) -&gt; Array\n</code></pre> <p>Generate samples using DDIM.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>steps</code> <code>int | None</code> <p>Number of DDIM steps (default: self.ddim_steps)</p> <code>None</code> <code>eta</code> <code>float | None</code> <p>DDIM interpolation parameter</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel.sample","title":"sample","text":"<pre><code>sample(\n    n_samples: int,\n    scheduler: str = \"ddim\",\n    steps: int | None = None,\n) -&gt; Array\n</code></pre> <p>Sample from the model.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>scheduler</code> <code>str</code> <p>Sampling scheduler (\"ddim\" or \"ddpm\")</p> <code>'ddim'</code> <code>steps</code> <code>int | None</code> <p>Number of sampling steps</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.ddim.DDIMModel.ddim_reverse","title":"ddim_reverse","text":"<pre><code>ddim_reverse(x0: Array, ddim_steps: int) -&gt; Array\n</code></pre> <p>DDIM reverse process (encoding) from x_0 to noise.</p> <p>This is useful for image editing applications where you want to encode a real image into the noise space and then decode it.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Array</code> <p>Clean image to encode</p> required <code>ddim_steps</code> <code>int</code> <p>Number of DDIM steps</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Encoded noise</p>"},{"location":"api/models/diffusion/#initialization_2","title":"Initialization","text":"<pre><code>DDIMModel(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Configuration Parameters:</p> Parameter Type Default Description <code>eta</code> <code>float</code> 0.0 Stochasticity (0=deterministic, 1=DDPM) <code>ddim_steps</code> <code>int</code> 50 Number of sampling steps <code>skip_type</code> <code>str</code> \"uniform\" Timestep selection (\"uniform\" or \"quadratic\") <code>noise_steps</code> <code>int</code> 1000 Training timesteps"},{"location":"api/models/diffusion/#methods_2","title":"Methods","text":""},{"location":"api/models/diffusion/#get_ddim_timestepsddim_steps","title":"<code>get_ddim_timesteps(ddim_steps)</code>","text":"<p>Get timesteps for DDIM sampling.</p> <p>Parameters:</p> <ul> <li><code>ddim_steps</code> (<code>int</code>): Number of sampling steps</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Timestep indices for DDIM</li> </ul> <p>Example:</p> <pre><code>model = DDIMModel(config, rngs=rngs)\n\n# Get 50 uniformly spaced timesteps\ntimesteps = model.get_ddim_timesteps(50)\nprint(timesteps)  # [999, 979, 959, ..., 19, 0]\n</code></pre>"},{"location":"api/models/diffusion/#ddim_stepx_t-t-t_prev-predicted_noise-etanone-rngsnone","title":"<code>ddim_step(x_t, t, t_prev, predicted_noise, eta=None, *, rngs=None)</code>","text":"<p>Single DDIM sampling step.</p> <p>Parameters:</p> <ul> <li><code>x_t</code> (<code>jax.Array</code>): Current sample at timestep \\(t\\)</li> <li><code>t</code> (<code>jax.Array</code>): Current timestep</li> <li><code>t_prev</code> (<code>jax.Array</code>): Previous timestep</li> <li><code>predicted_noise</code> (<code>jax.Array</code>): Predicted noise</li> <li><code>eta</code> (<code>float | None</code>): DDIM parameter (0=deterministic)</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Sample at timestep \\(t_{prev}\\)</li> </ul> <p>Mathematical Formula:</p> \\[ x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\hat{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\epsilon_\\theta(x_t, t) + \\sigma_t \\epsilon \\] <p>Where \\(\\hat{x}_0\\) is the predicted clean sample and \\(\\sigma_t = \\eta \\sqrt{(1-\\bar{\\alpha}_{t-1})/(1-\\bar{\\alpha}_t)}\\sqrt{1-\\bar{\\alpha}_t/\\bar{\\alpha}_{t-1}}\\)</p>"},{"location":"api/models/diffusion/#ddim_samplen_samples-stepsnone-etanone-rngsnone-kwargs","title":"<code>ddim_sample(n_samples, steps=None, eta=None, *, rngs=None, **kwargs)</code>","text":"<p>Generate samples using DDIM.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code> (<code>int</code>): Number of samples</li> <li><code>steps</code> (<code>int | None</code>): Number of DDIM steps</li> <li><code>eta</code> (<code>float | None</code>): Stochasticity parameter</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>**kwargs</code>: Additional arguments</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated samples</li> </ul> <p>Example:</p> <pre><code># Deterministic generation (50 steps)\nsamples = model.ddim_sample(16, steps=50, eta=0.0, rngs=rngs)\n\n# Stochastic generation (more diversity)\nsamples = model.ddim_sample(16, steps=50, eta=0.5, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#ddim_reversex0-ddim_steps-rngsnone-kwargs","title":"<code>ddim_reverse(x0, ddim_steps, *, rngs=None, **kwargs)</code>","text":"<p>DDIM reverse process (encoding) from \\(x_0\\) to noise.</p> <p>Purpose: Encode a real image into the noise space for image editing.</p> <p>Parameters:</p> <ul> <li><code>x0</code> (<code>jax.Array</code>): Clean image to encode</li> <li><code>ddim_steps</code> (<code>int</code>): Number of reverse steps</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>**kwargs</code>: Additional arguments</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Encoded noise</li> </ul> <p>Example:</p> <pre><code># Encode real image to noise\nreal_image = load_image(\"photo.png\")\nnoise_code = model.ddim_reverse(real_image, ddim_steps=50, rngs=rngs)\n\n# Edit in noise space\nedited_noise = noise_code + modification\n\n# Decode back to image\nedited_image = model.ddim_sample(1, steps=50, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#score-based-models","title":"Score-Based Models","text":""},{"location":"api/models/diffusion/#scorediffusionmodel","title":"ScoreDiffusionModel","text":"<p>Score-based diffusion model using score matching.</p> <p>Purpose: Implements score-based generative modeling where the model predicts the score function (gradient of log-likelihood) instead of noise directly.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel","title":"artifex.generative_models.models.diffusion.score.ScoreDiffusionModel","text":"<pre><code>ScoreDiffusionModel(\n    config: ScoreDiffusionConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>DiffusionModel</code></p> <p>Score-based diffusion model.</p> <p>This model is based on score matching principles where the model predicts the score (gradient of log-likelihood) instead of noise directly.</p> <p>Uses nested ScoreDiffusionConfig with: - backbone: BackboneConfig (polymorphic) for the denoising network - noise_schedule: NoiseScheduleConfig for the diffusion schedule - sigma_min: Minimum noise level - sigma_max: Maximum noise level - score_scaling: Score function scaling factor</p> <p>Backbone type is determined by config.backbone.backbone_type discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ScoreDiffusionConfig</code> <p>ScoreDiffusionConfig with nested backbone and noise_schedule configs.     The backbone field accepts any BackboneConfig type and the     appropriate backbone is created based on backbone_type.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for initialization.</p> required"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel.sigma_min","title":"sigma_min  <code>instance-attribute</code>","text":"<pre><code>sigma_min = sigma_min\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel.sigma_max","title":"sigma_max  <code>instance-attribute</code>","text":"<pre><code>sigma_max = sigma_max\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel.score_scaling","title":"score_scaling  <code>instance-attribute</code>","text":"<pre><code>score_scaling = score_scaling\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_shape\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel.score","title":"score","text":"<pre><code>score(x: Array, t: Array) -&gt; Array\n</code></pre> <p>Compute the score function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input samples</p> required <code>t</code> <code>Array</code> <p>Time steps</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Score values</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel.loss","title":"loss","text":"<pre><code>loss(x: Array, *, rngs: Rngs | None = None) -&gt; Array\n</code></pre> <p>Compute the score matching loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input samples</p> required <code>rngs</code> <code>Rngs | None</code> <p>Random number generators</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Loss value</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel.sample","title":"sample","text":"<pre><code>sample(\n    num_samples: int,\n    *,\n    rngs: Rngs | None = None,\n    num_steps: int = 1000,\n    return_trajectory: bool = False,\n) -&gt; Array | list[Array]\n</code></pre> <p>Generate samples using the reverse SDE.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>rngs</code> <code>Rngs | None</code> <p>Random number generators</p> <code>None</code> <code>num_steps</code> <code>int</code> <p>Number of integration steps</p> <code>1000</code> <code>return_trajectory</code> <code>bool</code> <p>If True, return full trajectory</p> <code>False</code> <p>Returns:</p> Type Description <code>Array | list[Array]</code> <p>Generated samples or trajectory</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.score.ScoreDiffusionModel.denoise","title":"denoise","text":"<pre><code>denoise(x: Array, t: Array) -&gt; Array\n</code></pre> <p>Predict denoised output.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Noisy input</p> required <code>t</code> <code>Array</code> <p>Time steps</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Denoised output</p>"},{"location":"api/models/diffusion/#initialization_3","title":"Initialization","text":"<pre><code>ScoreDiffusionModel(\n    *,\n    config: ModelConfig,\n    rngs: nnx.Rngs,\n    **kwargs\n)\n</code></pre> <p>Configuration Parameters:</p> Parameter Type Default Description <code>sigma_min</code> <code>float</code> 0.01 Minimum noise level <code>sigma_max</code> <code>float</code> 1.0 Maximum noise level <code>score_scaling</code> <code>float</code> 1.0 Score scaling factor"},{"location":"api/models/diffusion/#methods_3","title":"Methods","text":""},{"location":"api/models/diffusion/#scorex-t","title":"<code>score(x, t)</code>","text":"<p>Compute the score function \\(\\nabla_x \\log p_t(x)\\).</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input samples</li> <li><code>t</code> (<code>jax.Array</code>): Time steps in [0, 1]</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Score values</li> </ul> <p>Mathematical Formula:</p> \\[ \\nabla_x \\log p_t(x) = -\\frac{\\epsilon}{\\sigma_t} \\]"},{"location":"api/models/diffusion/#samplenum_samples-rngsnone-num_steps1000-return_trajectoryfalse","title":"<code>sample(num_samples, *, rngs=None, num_steps=1000, return_trajectory=False)</code>","text":"<p>Generate samples using reverse SDE.</p> <p>Parameters:</p> <ul> <li><code>num_samples</code> (<code>int</code>): Number of samples</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>num_steps</code> (<code>int</code>): Number of integration steps</li> <li><code>return_trajectory</code> (<code>bool</code>): Return full trajectory</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array | list[jax.Array]</code>: Samples or trajectory</li> </ul> <p>Example:</p> <pre><code>model = ScoreDiffusionModel(config=config, rngs=rngs)\n\n# Generate samples\nsamples = model.sample(16, num_steps=1000, rngs=rngs)\n\n# Get full trajectory\ntrajectory = model.sample(4, num_steps=1000, return_trajectory=True, rngs=rngs)\nprint(f\"Trajectory length: {len(trajectory)}\")  # 1000 steps\n</code></pre>"},{"location":"api/models/diffusion/#latent-diffusion-models","title":"Latent Diffusion Models","text":""},{"location":"api/models/diffusion/#ldmmodel","title":"LDMModel","text":"<p>Latent Diffusion Model combining VAE and diffusion in latent space.</p> <p>Purpose: Applies diffusion in a compressed latent space for efficient high-resolution generation. Foundation of Stable Diffusion.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel","title":"artifex.generative_models.models.diffusion.latent.LDMModel","text":"<pre><code>LDMModel(config: LatentDiffusionConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>DDPMModel</code></p> <p>Latent Diffusion Model implementation.</p> <p>This model combines a VAE for encoding/decoding with a diffusion model that operates in the latent space.</p> <p>Uses nested LatentDiffusionConfig with: - backbone: BackboneConfig (polymorphic) for the denoising network - noise_schedule: NoiseScheduleConfig for the diffusion schedule - encoder: EncoderConfig for encoding to latent space - decoder: DecoderConfig for decoding from latent space - latent_scale_factor: Scaling factor for latent codes</p> <p>Supports both 1D and 2D backbones: - UNet1D: Works with (batch, sequence, channels) format - UNet/DiT: Works with (batch, H, W, C) spatial format</p> <p>Flat latent codes from the MLP encoder are automatically reshaped to the appropriate format for the backbone.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LatentDiffusionConfig</code> <p>LatentDiffusionConfig with nested configs for backbone,     noise_schedule, encoder, and decoder.     The backbone field accepts any BackboneConfig type and the     appropriate backbone is created based on backbone_type.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.original_input_dim","title":"original_input_dim  <code>instance-attribute</code>","text":"<pre><code>original_input_dim = input_shape\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.scale_factor","title":"scale_factor  <code>instance-attribute</code>","text":"<pre><code>scale_factor = latent_scale_factor\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder = MLPEncoder(config=encoder, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = MLPDecoder(config=decoder, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.use_pretrained_vae","title":"use_pretrained_vae  <code>instance-attribute</code>","text":"<pre><code>use_pretrained_vae = False\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = _latent_spatial_shape\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.output_dim","title":"output_dim  <code>property</code>","text":"<pre><code>output_dim\n</code></pre> <p>Get output dimensions.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.encode","title":"encode","text":"<pre><code>encode(x: Array) -&gt; tuple[Array, Array]\n</code></pre> <p>Encode input to latent space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>tuple[Array, Array]</code> <p>Tuple of (latent_code, posterior_params)</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.decode","title":"decode","text":"<pre><code>decode(z: Array) -&gt; Array\n</code></pre> <p>Decode latent code to output space.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Array</code> <p>Latent code</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Decoded output</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.reparameterize","title":"reparameterize","text":"<pre><code>reparameterize(\n    mean: Array, logvar: Array, *, rngs: Rngs\n) -&gt; Array\n</code></pre> <p>Reparameterization trick.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Array</code> <p>Mean of the latent distribution</p> required <code>logvar</code> <code>Array</code> <p>Log variance of the latent distribution</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Sampled latent code</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.denoise","title":"denoise","text":"<pre><code>denoise(x: Array, t: Array) -&gt; Array\n</code></pre> <p>Predict noise from noisy input using the backbone.</p> <p>Handles reshaping between flat and spatial formats as needed by the backbone type.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Noisy input (flat format: batch, latent_dim)</p> required <code>t</code> <code>Array</code> <p>Timestep indices</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Predicted noise (same shape as input)</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.sample","title":"sample","text":"<pre><code>sample(\n    num_samples: int, *, return_trajectory: bool = False\n) -&gt; Array | list[Array]\n</code></pre> <p>Sample from the model by generating in latent space.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>return_trajectory</code> <code>bool</code> <p>If True, return full trajectory</p> <code>False</code> <p>Returns:</p> Type Description <code>Array | list[Array]</code> <p>Generated samples (decoded to image space) or trajectory</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.LDMModel.loss","title":"loss","text":"<pre><code>loss(\n    x: Array,\n    t: Array | None = None,\n    *,\n    rngs: Rngs | None = None,\n) -&gt; Array\n</code></pre> <p>Compute LDM loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input images</p> required <code>t</code> <code>Array | None</code> <p>Timesteps (optional)</p> <code>None</code> <code>rngs</code> <code>Rngs | None</code> <p>Random number generators</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Loss value</p>"},{"location":"api/models/diffusion/#initialization_4","title":"Initialization","text":"<pre><code>LDMModel(\n    *,\n    config: ModelConfig,\n    rngs: nnx.Rngs,\n    **kwargs\n)\n</code></pre> <p>Configuration Parameters:</p> Parameter Type Default Description <code>latent_dim</code> <code>int</code> 8 Latent space dimension <code>encoder_hidden_dims</code> <code>list[int]</code> [32, 64] Encoder layer sizes <code>decoder_hidden_dims</code> <code>list[int]</code> [64, 32] Decoder layer sizes <code>encoder_type</code> <code>str</code> \"simple\" Encoder type (\"simple\" or \"vae\") <code>decoder_type</code> <code>str</code> \"simple\" Decoder type <code>scale_factor</code> <code>float</code> 0.18215 Latent scaling factor"},{"location":"api/models/diffusion/#methods_4","title":"Methods","text":""},{"location":"api/models/diffusion/#encodex","title":"<code>encode(x)</code>","text":"<p>Encode input to latent space.</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input images</li> </ul> <p>Returns:</p> <ul> <li><code>tuple[jax.Array, jax.Array]</code>: <code>(mean, logvar)</code> of latent distribution</li> </ul> <p>Example:</p> <pre><code>model = LDMModel(config=config, rngs=rngs)\n\n# Encode images to latent space\nimages = jax.random.normal(rngs.sample(), (8, 64, 64, 3))\nmean, logvar = model.encode(images)\n\nprint(f\"Latent mean: {mean.shape}\")      # (8, 16)\nprint(f\"Latent logvar: {logvar.shape}\")  # (8, 16)\n</code></pre>"},{"location":"api/models/diffusion/#decodez","title":"<code>decode(z)</code>","text":"<p>Decode latent code to image space.</p> <p>Parameters:</p> <ul> <li><code>z</code> (<code>jax.Array</code>): Latent codes</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Decoded images</li> </ul> <p>Example:</p> <pre><code># Sample latent code\nz = jax.random.normal(rngs.sample(), (8, 16))\n\n# Decode to images\nimages = model.decode(z)\nprint(f\"Decoded images: {images.shape}\")  # (8, 64, 64, 3)\n</code></pre>"},{"location":"api/models/diffusion/#reparameterizemean-logvar-rngs","title":"<code>reparameterize(mean, logvar, *, rngs)</code>","text":"<p>Reparameterization trick for sampling.</p> <p>Parameters:</p> <ul> <li><code>mean</code> (<code>jax.Array</code>): Mean of latent distribution</li> <li><code>logvar</code> (<code>jax.Array</code>): Log variance of latent distribution</li> <li><code>rngs</code> (<code>nnx.Rngs</code>): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Sampled latent code</li> </ul> <p>Mathematical Formula:</p> \\[ z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\]"},{"location":"api/models/diffusion/#samplenum_samples-rngsnone-return_trajectoryfalse","title":"<code>sample(num_samples, *, rngs=None, return_trajectory=False)</code>","text":"<p>Generate samples (automatically encoded/decoded).</p> <p>Parameters:</p> <ul> <li><code>num_samples</code> (<code>int</code>): Number of samples</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>return_trajectory</code> (<code>bool</code>): Return full trajectory</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array | list[jax.Array]</code>: Generated images</li> </ul> <p>Example:</p> <pre><code># Generate high-resolution images efficiently\nsamples = model.sample(16, rngs=rngs)\nprint(f\"Generated: {samples.shape}\")  # (16, 64, 64, 3)\n\n# Diffusion happens in compressed 16D latent space!\n# 8x faster than pixel-space diffusion\n</code></pre>"},{"location":"api/models/diffusion/#diffusion-transformers","title":"Diffusion Transformers","text":""},{"location":"api/models/diffusion/#ditmodel","title":"DiTModel","text":"<p>Diffusion model using Vision Transformer backbone.</p> <p>Purpose: Replaces U-Net with Transformer for better scalability and state-of-the-art quality at large model sizes.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel","title":"artifex.generative_models.models.diffusion.dit.DiTModel","text":"<pre><code>DiTModel(\n    config: DiTConfig,\n    backbone_fn: Callable[[DiTConfig, Rngs], Module]\n    | None = None,\n    *,\n    rngs: Rngs,\n)\n</code></pre> <p>               Bases: <code>GenerativeModel</code></p> <p>Diffusion model using Transformer backbone instead of U-Net.</p> <p>Implements Diffusion Transformers (DiT) which replace the U-Net backbone with a Vision Transformer for improved scalability and performance.</p> <p>Uses nested DiTConfig with: - noise_schedule: NoiseScheduleConfig for the diffusion schedule - patch_size, hidden_size, depth, num_heads, mlp_ratio: Transformer architecture - num_classes: Number of classes for conditional generation - cfg_scale: Classifier-free guidance scale</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DiTConfig</code> <p>DiTConfig with DiT-specific parameters and noise_schedule</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <code>backbone_fn</code> <code>Callable[[DiTConfig, Rngs], Module] | None</code> <p>Optional custom backbone function</p> <code>None</code>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.num_classes","title":"num_classes  <code>instance-attribute</code>","text":"<pre><code>num_classes = num_classes\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.cfg_scale","title":"cfg_scale  <code>instance-attribute</code>","text":"<pre><code>cfg_scale = cfg_scale\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.learn_sigma","title":"learn_sigma  <code>instance-attribute</code>","text":"<pre><code>learn_sigma = learn_sigma\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_shape\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.in_channels","title":"in_channels  <code>instance-attribute</code>","text":"<pre><code>in_channels = input_shape[0]\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.img_size","title":"img_size  <code>instance-attribute</code>","text":"<pre><code>img_size = input_shape[1]\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.noise_schedule","title":"noise_schedule  <code>instance-attribute</code>","text":"<pre><code>noise_schedule: NoiseSchedule = create_noise_schedule(\n    noise_schedule\n)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.betas","title":"betas  <code>instance-attribute</code>","text":"<pre><code>betas = betas\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.alphas","title":"alphas  <code>instance-attribute</code>","text":"<pre><code>alphas = alphas\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.alphas_cumprod","title":"alphas_cumprod  <code>instance-attribute</code>","text":"<pre><code>alphas_cumprod = alphas_cumprod\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone = backbone_fn(config, rngs)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.sample_step","title":"sample_step","text":"<pre><code>sample_step(\n    x_t: Array,\n    t: Array,\n    *,\n    rngs: Rngs | None = None,\n    y: Array | None = None,\n    cfg_scale: float | None = None,\n) -&gt; Array\n</code></pre> <p>Single sampling step with optional classifier-free guidance.</p> <p>Parameters:</p> Name Type Description Default <code>x_t</code> <code>Array</code> <p>Current noisy sample [batch, height, width, channels]</p> required <code>t</code> <code>Array</code> <p>Current timestep [batch]</p> required <code>rngs</code> <code>Rngs | None</code> <p>Random number generators</p> <code>None</code> <code>y</code> <code>Array | None</code> <p>Optional class labels for conditional generation</p> <code>None</code> <code>cfg_scale</code> <code>float | None</code> <p>Classifier-free guidance scale</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Denoised sample [batch, height, width, channels]</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.dit.DiTModel.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    rngs: Rngs,\n    num_steps: int = 1000,\n    y: Array | None = None,\n    cfg_scale: float | None = None,\n    img_size: int | None = None,\n) -&gt; Array\n</code></pre> <p>Generate samples using DiT.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <code>num_steps</code> <code>int</code> <p>Number of diffusion steps</p> <code>1000</code> <code>y</code> <code>Array | None</code> <p>Optional class labels for conditional generation</p> <code>None</code> <code>cfg_scale</code> <code>float | None</code> <p>Classifier-free guidance scale</p> <code>None</code> <code>img_size</code> <code>int | None</code> <p>Image size (uses config default if not specified)</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples [n_samples, height, width, channels]</p>"},{"location":"api/models/diffusion/#initialization_5","title":"Initialization","text":"<pre><code>DiTModel(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs,\n    backbone_fn: Optional[Callable] = None,\n    **kwargs\n)\n</code></pre> <p>Configuration Parameters:</p> Parameter Type Default Description <code>img_size</code> <code>int</code> 32 Image size <code>patch_size</code> <code>int</code> 2 Patch size for Vision Transformer <code>hidden_size</code> <code>int</code> 512 Transformer hidden dimension <code>depth</code> <code>int</code> 12 Number of transformer layers <code>num_heads</code> <code>int</code> 8 Number of attention heads <code>mlp_ratio</code> <code>float</code> 4.0 MLP expansion ratio <code>num_classes</code> <code>int | None</code> None Number of classes for conditioning <code>dropout_rate</code> <code>float</code> 0.0 Dropout rate <code>learn_sigma</code> <code>bool</code> False Learn variance <code>cfg_scale</code> <code>float</code> 1.0 Classifier-free guidance scale"},{"location":"api/models/diffusion/#methods_5","title":"Methods","text":""},{"location":"api/models/diffusion/#__call__x-t-ynone-deterministicfalse-cfg_scalenone","title":"<code>__call__(x, t, y=None, *, deterministic=False, cfg_scale=None)</code>","text":"<p>Forward pass through DiT model.</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input images <code>(batch, H, W, C)</code></li> <li><code>t</code> (<code>jax.Array</code>): Timesteps <code>(batch,)</code></li> <li><code>y</code> (<code>jax.Array | None</code>): Optional class labels</li> <li><code>deterministic</code> (<code>bool</code>): Whether to apply dropout</li> <li><code>cfg_scale</code> (<code>float | None</code>): Classifier-free guidance scale</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Predicted noise</li> </ul> <p>Example:</p> <pre><code>model = DiTModel(config, rngs=rngs)\n\n# Forward pass\nx = jax.random.normal(rngs.sample(), (4, 32, 32, 3))\nt = jnp.array([100, 200, 300, 400])\ny = jnp.array([0, 1, 2, 3])  # Class labels\n\nnoise_pred = model(x, t, y=y, deterministic=False)\n</code></pre>"},{"location":"api/models/diffusion/#generaten_samples1-rngs-num_steps1000-ynone-cfg_scalenone-img_sizenone-kwargs","title":"<code>generate(n_samples=1, *, rngs, num_steps=1000, y=None, cfg_scale=None, img_size=None, **kwargs)</code>","text":"<p>Generate samples using DiT.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code> (<code>int</code>): Number of samples</li> <li><code>rngs</code> (<code>nnx.Rngs</code>): Random number generators</li> <li><code>num_steps</code> (<code>int</code>): Number of diffusion steps</li> <li><code>y</code> (<code>jax.Array | None</code>): Class labels for conditional generation</li> <li><code>cfg_scale</code> (<code>float | None</code>): Classifier-free guidance scale</li> <li><code>img_size</code> (<code>int | None</code>): Image size</li> <li><code>**kwargs</code>: Additional arguments</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated samples</li> </ul> <p>Example:</p> <pre><code># Unconditional generation\nsamples = model.generate(n_samples=16, rngs=rngs, num_steps=1000)\n\n# Conditional generation with classifier-free guidance\nclass_labels = jnp.array([i % 10 for i in range(16)])\nsamples = model.generate(\n    n_samples=16,\n    y=class_labels,\n    cfg_scale=4.0,  # Strong conditioning\n    rngs=rngs,\n    num_steps=1000\n)\n</code></pre>"},{"location":"api/models/diffusion/#guidance-techniques","title":"Guidance Techniques","text":""},{"location":"api/models/diffusion/#classifierfreeguidance","title":"ClassifierFreeGuidance","text":"<p>Classifier-free guidance for conditional generation.</p> <p>Purpose: Enables strong conditioning without needing a separate classifier by training a single model to handle both conditional and unconditional generation.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.ClassifierFreeGuidance","title":"artifex.generative_models.models.diffusion.guidance.ClassifierFreeGuidance","text":"<pre><code>ClassifierFreeGuidance(\n    guidance_scale: float = 7.5,\n    unconditional_conditioning: Any | None = None,\n)\n</code></pre> <p>Classifier-free guidance for conditional diffusion models.</p> <p>This implements the classifier-free guidance technique that allows trading off between sample diversity and adherence to conditioning.</p> <p>Parameters:</p> Name Type Description Default <code>guidance_scale</code> <code>float</code> <p>Guidance strength (higher = more conditioning)</p> <code>7.5</code> <code>unconditional_conditioning</code> <code>Any | None</code> <p>Unconditional conditioning token/embedding</p> <code>None</code>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.ClassifierFreeGuidance.guidance_scale","title":"guidance_scale  <code>instance-attribute</code>","text":"<pre><code>guidance_scale = guidance_scale\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.ClassifierFreeGuidance.unconditional_conditioning","title":"unconditional_conditioning  <code>instance-attribute</code>","text":"<pre><code>unconditional_conditioning = unconditional_conditioning\n</code></pre>"},{"location":"api/models/diffusion/#initialization_6","title":"Initialization","text":"<pre><code>ClassifierFreeGuidance(\n    guidance_scale: float = 7.5,\n    unconditional_conditioning: Any | None = None\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>guidance_scale</code> <code>float</code> 7.5 Guidance strength (higher = stronger conditioning) <code>unconditional_conditioning</code> <code>Any | None</code> None Unconditional token/embedding"},{"location":"api/models/diffusion/#methods_6","title":"Methods","text":""},{"location":"api/models/diffusion/#__call__model-x-t-conditioning-rngsnone-kwargs","title":"<code>__call__(model, x, t, conditioning, *, rngs=None, **kwargs)</code>","text":"<p>Apply classifier-free guidance.</p> <p>Parameters:</p> <ul> <li><code>model</code> (<code>DiffusionModel</code>): Diffusion model</li> <li><code>x</code> (<code>jax.Array</code>): Noisy input</li> <li><code>t</code> (<code>jax.Array</code>): Timesteps</li> <li><code>conditioning</code> (<code>Any</code>): Conditioning information</li> <li><code>rngs</code> (<code>nnx.Rngs | None</code>): Random number generators</li> <li><code>**kwargs</code>: Additional arguments</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Guided noise prediction</li> </ul> <p>Mathematical Formula:</p> \\[ \\tilde{\\epsilon} = \\epsilon_\\theta(x_t, \\emptyset) + w \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\emptyset)) \\] <p>Example:</p> <pre><code>from artifex.generative_models.models.diffusion.guidance import ClassifierFreeGuidance\n\n# Create guidance\ncfg = ClassifierFreeGuidance(guidance_scale=7.5)\n\n# Use during sampling\nx_t = noisy_sample\nt = timesteps\nconditioning = class_labels\n\nguided_noise = cfg(model, x_t, t, conditioning, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#classifierguidance","title":"ClassifierGuidance","text":"<p>Classifier guidance using a pre-trained classifier.</p> <p>Purpose: Uses gradients from a pre-trained classifier to guide generation towards desired classes.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.ClassifierGuidance","title":"artifex.generative_models.models.diffusion.guidance.ClassifierGuidance","text":"<pre><code>ClassifierGuidance(\n    classifier: Module,\n    guidance_scale: float = 1.0,\n    class_label: int | None = None,\n)\n</code></pre> <p>Classifier guidance for diffusion models.</p> <p>Uses a pre-trained classifier to guide the generation process towards desired classes.</p> <p>Parameters:</p> Name Type Description Default <code>classifier</code> <code>Module</code> <p>Pre-trained classifier model</p> required <code>guidance_scale</code> <code>float</code> <p>Guidance strength</p> <code>1.0</code> <code>class_label</code> <code>int | None</code> <p>Target class label for guidance</p> <code>None</code>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.ClassifierGuidance.classifier","title":"classifier  <code>instance-attribute</code>","text":"<pre><code>classifier = classifier\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.ClassifierGuidance.guidance_scale","title":"guidance_scale  <code>instance-attribute</code>","text":"<pre><code>guidance_scale = guidance_scale\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.ClassifierGuidance.class_label","title":"class_label  <code>instance-attribute</code>","text":"<pre><code>class_label = class_label\n</code></pre>"},{"location":"api/models/diffusion/#initialization_7","title":"Initialization","text":"<pre><code>ClassifierGuidance(\n    classifier: nnx.Module,\n    guidance_scale: float = 1.0,\n    class_label: int | None = None\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>classifier</code> <code>nnx.Module</code> Pre-trained classifier model <code>guidance_scale</code> <code>float</code> Guidance strength <code>class_label</code> <code>int | None</code> Target class for guidance"},{"location":"api/models/diffusion/#methods_7","title":"Methods","text":""},{"location":"api/models/diffusion/#__call__model-x-t-rngsnone-class_labelnone-kwargs","title":"<code>__call__(model, x, t, *, rngs=None, class_label=None, **kwargs)</code>","text":"<p>Apply classifier guidance.</p> <p>Mathematical Formula:</p> \\[ \\tilde{\\epsilon} = \\epsilon_\\theta(x_t) - w \\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{x_t} \\log p_\\phi(y | x_t) \\] <p>Example:</p> <pre><code>from artifex.generative_models.models.diffusion.guidance import ClassifierGuidance\n\n# Load pre-trained classifier\nclassifier = load_classifier()\n\n# Create classifier guidance\ncg = ClassifierGuidance(\n    classifier=classifier,\n    guidance_scale=1.0,\n    class_label=5  # Generate class 5\n)\n\n# Use during sampling\nguided_noise = cg(model, x_t, t, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#guideddiffusionmodel","title":"GuidedDiffusionModel","text":"<p>Diffusion model with built-in guidance support.</p> <p>Purpose: Extends base diffusion model to support various guidance techniques during generation.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.GuidedDiffusionModel","title":"artifex.generative_models.models.diffusion.guidance.GuidedDiffusionModel","text":"<pre><code>GuidedDiffusionModel(\n    config: Any,\n    *,\n    rngs: Rngs,\n    guidance_method: str | None = None,\n    guidance_scale: float = 7.5,\n    classifier: Module | None = None,\n)\n</code></pre> <p>               Bases: <code>DiffusionModel</code></p> <p>Diffusion model with built-in guidance support.</p> <p>This extends the base diffusion model to support various guidance techniques during generation.</p> <p>Uses the polymorphic backbone system - backbone type is determined by config.backbone.backbone_type discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Any</code> <p>Model configuration with nested BackboneConfig.     The backbone is created based on backbone_type.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <code>guidance_method</code> <code>str | None</code> <p>Type of guidance (\"classifier_free\", \"classifier\", None)</p> <code>None</code> <code>guidance_scale</code> <code>float</code> <p>Guidance strength</p> <code>7.5</code> <code>classifier</code> <code>Module | None</code> <p>Classifier for classifier guidance</p> <code>None</code>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.GuidedDiffusionModel.guidance_method","title":"guidance_method  <code>instance-attribute</code>","text":"<pre><code>guidance_method = guidance_method\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.GuidedDiffusionModel.guidance_scale","title":"guidance_scale  <code>instance-attribute</code>","text":"<pre><code>guidance_scale = guidance_scale\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.GuidedDiffusionModel.guidance","title":"guidance  <code>instance-attribute</code>","text":"<pre><code>guidance = ClassifierFreeGuidance(\n    guidance_scale=guidance_scale,\n    unconditional_conditioning=getattr(\n        config, \"unconditional_token\", None\n    ),\n)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.GuidedDiffusionModel.guided_sample_step","title":"guided_sample_step","text":"<pre><code>guided_sample_step(\n    x: Array,\n    t: Array,\n    conditioning: Any | None = None,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Single sampling step with guidance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Current sample</p> required <code>t</code> <code>Array</code> <p>Timesteps</p> required <code>conditioning</code> <code>Any | None</code> <p>Conditioning information</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Guided noise prediction</p> Note <p>NNX models store RNGs at init time, no need to pass rngs.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.guidance.GuidedDiffusionModel.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    rngs: Rngs | None = None,\n    conditioning: Any | None = None,\n    shape: tuple[int, ...] | None = None,\n    clip_denoised: bool = True,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Generate samples with guidance.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>rngs</code> <code>Rngs | None</code> <p>Random number generators</p> <code>None</code> <code>conditioning</code> <code>Any | None</code> <p>Conditioning information for guided generation</p> <code>None</code> <code>shape</code> <code>tuple[int, ...] | None</code> <p>Sample shape</p> <code>None</code> <code>clip_denoised</code> <code>bool</code> <p>Whether to clip denoised samples</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/diffusion/#initialization_8","title":"Initialization","text":"<pre><code>GuidedDiffusionModel(\n    config: ModelConfig,\n    backbone_fn: Callable,\n    *,\n    rngs: nnx.Rngs,\n    guidance_method: str | None = None,\n    guidance_scale: float = 7.5,\n    classifier: nnx.Module | None = None\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>guidance_method</code> <code>str | None</code> Guidance type (\"classifier_free\", \"classifier\", None) <code>guidance_scale</code> <code>float</code> Guidance strength <code>classifier</code> <code>nnx.Module | None</code> Classifier for classifier guidance <p>Example:</p> <pre><code>from artifex.generative_models.models.diffusion.guidance import GuidedDiffusionModel\n\n# Create model with classifier-free guidance\nmodel = GuidedDiffusionModel(\n    config,\n    backbone_fn,\n    rngs=rngs,\n    guidance_method=\"classifier_free\",\n    guidance_scale=7.5\n)\n\n# Generate with conditioning\nsamples = model.generate(\n    n_samples=16,\n    conditioning=class_labels,\n    rngs=rngs\n)\n</code></pre>"},{"location":"api/models/diffusion/#guidance-utility-functions","title":"Guidance Utility Functions","text":""},{"location":"api/models/diffusion/#apply_guidancenoise_pred_cond-noise_pred_uncond-guidance_scale","title":"<code>apply_guidance(noise_pred_cond, noise_pred_uncond, guidance_scale)</code>","text":"<p>Apply classifier-free guidance formula.</p> <p>Parameters:</p> <ul> <li><code>noise_pred_cond</code> (<code>jax.Array</code>): Conditional noise prediction</li> <li><code>noise_pred_uncond</code> (<code>jax.Array</code>): Unconditional noise prediction</li> <li><code>guidance_scale</code> (<code>float</code>): Guidance strength</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Guided noise prediction</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.diffusion.guidance import apply_guidance\n\n# Get predictions\nnoise_cond = model(x_t, t, conditioning=labels, rngs=rngs)\nnoise_uncond = model(x_t, t, conditioning=None, rngs=rngs)\n\n# Apply guidance\nguided = apply_guidance(noise_cond, noise_uncond, guidance_scale=7.5)\n</code></pre>"},{"location":"api/models/diffusion/#linear_guidance_schedulestep-total_steps-start_scale10-end_scale75","title":"<code>linear_guidance_schedule(step, total_steps, start_scale=1.0, end_scale=7.5)</code>","text":"<p>Linear guidance scale schedule.</p> <p>Parameters:</p> <ul> <li><code>step</code> (<code>int</code>): Current step</li> <li><code>total_steps</code> (<code>int</code>): Total number of steps</li> <li><code>start_scale</code> (<code>float</code>): Starting guidance scale</li> <li><code>end_scale</code> (<code>float</code>): Ending guidance scale</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: Guidance scale for current step</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.diffusion.guidance import linear_guidance_schedule\n\n# Gradually increase guidance during sampling\nfor step in range(total_steps):\n    scale = linear_guidance_schedule(step, total_steps, start_scale=1.0, end_scale=7.5)\n    # Use scale for this step\n</code></pre>"},{"location":"api/models/diffusion/#cosine_guidance_schedulestep-total_steps-start_scale10-end_scale75","title":"<code>cosine_guidance_schedule(step, total_steps, start_scale=1.0, end_scale=7.5)</code>","text":"<p>Cosine guidance scale schedule.</p> <p>Example:</p> <pre><code>from artifex.generative_models.models.diffusion.guidance import cosine_guidance_schedule\n\n# Use cosine schedule (higher guidance at beginning and end)\nfor step in range(total_steps):\n    scale = cosine_guidance_schedule(step, total_steps)\n    # Use scale for this step\n</code></pre>"},{"location":"api/models/diffusion/#auxiliary-classes","title":"Auxiliary Classes","text":""},{"location":"api/models/diffusion/#simpleencoder","title":"SimpleEncoder","text":"<p>Simple MLP encoder for Latent Diffusion Models.</p> <p>Purpose: Encodes images to latent space with mean and log variance.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder","title":"artifex.generative_models.models.diffusion.latent.SimpleEncoder","text":"<pre><code>SimpleEncoder(\n    input_dim: tuple[int, ...],\n    latent_dim: int,\n    hidden_dims: list | None = None,\n    *,\n    rngs: Rngs,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Simple encoder for latent diffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>tuple[int, ...]</code> <p>Input dimensions (H, W, C) for images or (D,) for vectors</p> required <code>latent_dim</code> <code>int</code> <p>Latent dimension</p> required <code>hidden_dims</code> <code>list | None</code> <p>Hidden layer dimensions</p> <code>None</code> <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_dim\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder.hidden_dims","title":"hidden_dims  <code>instance-attribute</code>","text":"<pre><code>hidden_dims = hidden_dims or [32, 64]\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder.is_image","title":"is_image  <code>instance-attribute</code>","text":"<pre><code>is_image = (\n    isinstance(input_dim, (tuple, list))\n    and len(input_dim) &gt;= 2\n)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder.flat_dim","title":"flat_dim  <code>instance-attribute</code>","text":"<pre><code>flat_dim = input_dim[0] * input_dim[1] * input_dim[2]\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = List([])\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder.mean_layer","title":"mean_layer  <code>instance-attribute</code>","text":"<pre><code>mean_layer = Linear(in_features, latent_dim, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleEncoder.logvar_layer","title":"logvar_layer  <code>instance-attribute</code>","text":"<pre><code>logvar_layer = Linear(in_features, latent_dim, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#initialization_9","title":"Initialization","text":"<pre><code>SimpleEncoder(\n    input_dim: tuple[int, ...],\n    latent_dim: int,\n    hidden_dims: list | None = None,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre>"},{"location":"api/models/diffusion/#simpledecoder","title":"SimpleDecoder","text":"<p>Simple MLP decoder for Latent Diffusion Models.</p> <p>Purpose: Decodes latent codes back to image space.</p>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleDecoder","title":"artifex.generative_models.models.diffusion.latent.SimpleDecoder","text":"<pre><code>SimpleDecoder(\n    latent_dim: int,\n    output_dim: tuple[int, ...],\n    hidden_dims: list | None = None,\n    *,\n    rngs: Rngs | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Simple decoder for latent diffusion model.</p> <p>Parameters:</p> Name Type Description Default <code>latent_dim</code> <code>int</code> <p>Latent dimension</p> required <code>output_dim</code> <code>tuple[int, ...]</code> <p>Output dimensions (H, W, C) for images or (D,) for vectors</p> required <code>hidden_dims</code> <code>list | None</code> <p>Hidden layer dimensions (in reverse order)</p> <code>None</code> <code>rngs</code> <code>Rngs | None</code> <p>Random number generators</p> <code>None</code>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleDecoder.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleDecoder.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = output_dim\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleDecoder.hidden_dims","title":"hidden_dims  <code>instance-attribute</code>","text":"<pre><code>hidden_dims = hidden_dims or [64, 32]\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleDecoder.is_image","title":"is_image  <code>instance-attribute</code>","text":"<pre><code>is_image = (\n    isinstance(output_dim, (tuple, list))\n    and len(output_dim) &gt;= 2\n)\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleDecoder.flat_dim","title":"flat_dim  <code>instance-attribute</code>","text":"<pre><code>flat_dim = output_dim[0] * output_dim[1] * output_dim[2]\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleDecoder.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = List([])\n</code></pre>"},{"location":"api/models/diffusion/#artifex.generative_models.models.diffusion.latent.SimpleDecoder.output_layer","title":"output_layer  <code>instance-attribute</code>","text":"<pre><code>output_layer = Linear(in_features, flat_dim, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#initialization_10","title":"Initialization","text":"<pre><code>SimpleDecoder(\n    latent_dim: int,\n    output_dim: tuple[int, ...],\n    hidden_dims: list | None = None,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre>"},{"location":"api/models/diffusion/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api/models/diffusion/#modelconfig-for-diffusion-models","title":"ModelConfig for Diffusion Models","text":"<p>Complete reference of configuration parameters for all diffusion models.</p>"},{"location":"api/models/diffusion/#base-parameters","title":"Base Parameters","text":"Parameter Type Required Description <code>name</code> <code>str</code> Yes Model name <code>model_class</code> <code>str</code> Yes Model class name <code>input_dim</code> <code>tuple[int, ...]</code> Yes Input dimensions (H, W, C) <code>hidden_dims</code> <code>list[int]</code> No Hidden layer dimensions <code>output_dim</code> <code>int | tuple</code> No Output dimensions <code>activation</code> <code>str</code> No Activation function <code>parameters</code> <code>dict</code> No Model-specific parameters"},{"location":"api/models/diffusion/#ddpm-parameters","title":"DDPM Parameters","text":"<pre><code>{\n    \"noise_steps\": 1000,      # Number of timesteps\n    \"beta_start\": 1e-4,       # Initial noise level\n    \"beta_end\": 0.02,         # Final noise level\n    \"beta_schedule\": \"linear\" # Noise schedule\n}\n</code></pre>"},{"location":"api/models/diffusion/#ddim-parameters","title":"DDIM Parameters","text":"<pre><code>{\n    \"noise_steps\": 1000,      # Training steps\n    \"ddim_steps\": 50,         # Sampling steps\n    \"eta\": 0.0,               # Stochasticity\n    \"skip_type\": \"uniform\",   # Timestep selection\n    \"beta_start\": 1e-4,\n    \"beta_end\": 0.02\n}\n</code></pre>"},{"location":"api/models/diffusion/#score-based-parameters","title":"Score-Based Parameters","text":"<pre><code>{\n    \"sigma_min\": 0.01,        # Minimum noise level\n    \"sigma_max\": 1.0,         # Maximum noise level\n    \"score_scaling\": 1.0,     # Score scaling factor\n    \"noise_steps\": 1000\n}\n</code></pre>"},{"location":"api/models/diffusion/#latent-diffusion-parameters","title":"Latent Diffusion Parameters","text":"<pre><code>{\n    \"latent_dim\": 16,                    # Latent space dimension\n    \"encoder_hidden_dims\": [64, 128],    # Encoder architecture\n    \"decoder_hidden_dims\": [128, 64],    # Decoder architecture\n    \"encoder_type\": \"simple\",            # Encoder type\n    \"scale_factor\": 0.18215,             # Latent scaling\n    \"noise_steps\": 1000\n}\n</code></pre>"},{"location":"api/models/diffusion/#dit-parameters","title":"DiT Parameters","text":"<pre><code>{\n    \"img_size\": 32,           # Image size\n    \"patch_size\": 4,          # Patch size\n    \"hidden_size\": 512,       # Transformer dimension\n    \"depth\": 12,              # Number of layers\n    \"num_heads\": 8,           # Attention heads\n    \"mlp_ratio\": 4.0,         # MLP expansion\n    \"num_classes\": 10,        # Number of classes\n    \"dropout_rate\": 0.1,      # Dropout rate\n    \"learn_sigma\": False,     # Learn variance\n    \"cfg_scale\": 2.0,         # Guidance scale\n    \"noise_steps\": 1000\n}\n</code></pre>"},{"location":"api/models/diffusion/#quick-reference","title":"Quick Reference","text":""},{"location":"api/models/diffusion/#model-selection-guide","title":"Model Selection Guide","text":"Model Best For Sampling Speed Memory Quality DDPMModel Standard use, learning Slow (1000 steps) High \u2b50\u2b50\u2b50\u2b50\u2b50 DDIMModel Fast inference Fast (50 steps) High \u2b50\u2b50\u2b50\u2b50 ScoreDiffusionModel Research, flexibility Medium High \u2b50\u2b50\u2b50\u2b50 LDMModel High-res, efficiency Fast Medium \u2b50\u2b50\u2b50\u2b50 DiTModel Scalability, SOTA Medium Very High \u2b50\u2b50\u2b50\u2b50\u2b50"},{"location":"api/models/diffusion/#common-usage-patterns","title":"Common Usage Patterns","text":"<pre><code># Basic DDPM\nmodel = DDPMModel(config, rngs=rngs)\nsamples = model.generate(16, rngs=rngs)\n\n# Fast DDIM\nmodel = DDIMModel(config, rngs=rngs)\nsamples = model.ddim_sample(16, steps=50, rngs=rngs)\n\n# Latent Diffusion\nmodel = LDMModel(config=config, rngs=rngs)\nsamples = model.sample(16, rngs=rngs)\n\n# DiT with conditioning\nmodel = DiTModel(config, rngs=rngs)\nsamples = model.generate(16, y=labels, cfg_scale=4.0, rngs=rngs)\n</code></pre>"},{"location":"api/models/diffusion/#see-also","title":"See Also","text":"<ul> <li>Diffusion Concepts: Theory and mathematical foundations</li> <li>User Guide: Practical usage examples</li> <li>MNIST Tutorial: Complete working example</li> <li>Core API: Base generative model classes</li> <li>Configuration API: Configuration system</li> </ul>"},{"location":"api/models/ebm/","title":"Energy-Based Models API Reference","text":"<p>Complete API documentation for energy-based models (EBMs) in Artifex.</p> <p>Coming Soon</p> <p>Energy-based model implementations are planned for a future release. This documentation will be updated when the feature is available.</p>"},{"location":"api/models/ebm/#overview","title":"Overview","text":"<p>Energy-based models will include:</p> <ul> <li>Score Matching: Noise-contrastive estimation</li> <li>Contrastive Divergence: CD-k training algorithms</li> <li>Stein Variational Gradient Descent: Particle-based inference</li> <li>Langevin Dynamics: MCMC-based sampling</li> </ul>"},{"location":"api/models/ebm/#planned-api","title":"Planned API","text":""},{"location":"api/models/ebm/#base-class","title":"Base Class","text":"<pre><code>from artifex.generative_models.models.ebm.base import EnergyBasedModel\n\nmodel = EnergyBasedModel(\n    config: EBMConfig,\n    *,\n    rngs: nnx.Rngs,\n)\n</code></pre>"},{"location":"api/models/ebm/#configuration","title":"Configuration","text":"<pre><code>from artifex.generative_models.core.configuration import EBMConfig\n\nconfig = EBMConfig(\n    name=\"ebm_model\",\n    energy_net_hidden_dims=(256, 128),\n    mcmc_steps=10,\n    step_size=0.01,\n)\n</code></pre>"},{"location":"api/models/ebm/#energy-function","title":"Energy Function","text":"<pre><code># Define energy function\nenergy = model.energy(x)  # Lower = more likely\n\n# Compute log probability\nlog_prob = -energy\n</code></pre>"},{"location":"api/models/ebm/#sampling","title":"Sampling","text":"<pre><code># Sample using Langevin dynamics\nsamples = model.sample(\n    n_samples=100,\n    n_steps=1000,\n    step_size=0.01,\n)\n</code></pre>"},{"location":"api/models/ebm/#related-documentation","title":"Related Documentation","text":"<ul> <li>Energy-Based Models Guide - Conceptual overview</li> <li>EBM Concepts - Understanding energy-based modeling</li> <li>MCMC Sampling - Sampling algorithms for EBMs</li> </ul>"},{"location":"api/models/ebm/#references","title":"References","text":"<ul> <li>LeCun et al., \"A Tutorial on Energy-Based Learning\" (2006)</li> <li>Du &amp; Mordatch, \"Implicit Generation and Modeling with Energy-Based Models\" (2019)</li> <li>Song &amp; Ermon, \"Generative Modeling by Estimating Gradients of the Data Distribution\" (2019)</li> </ul>"},{"location":"api/models/flow/","title":"Flow Models API Reference","text":"<p>Complete API documentation for normalizing flow models in Artifex.</p>"},{"location":"api/models/flow/#overview","title":"Overview","text":"<p>This document provides detailed API reference for all flow-based model classes, including:</p> <ul> <li>Base Classes: <code>NormalizingFlow</code>, <code>FlowLayer</code></li> <li>RealNVP: Coupling-based flows with affine transformations</li> <li>Glow: Multi-scale flow with ActNorm and invertible convolutions</li> <li>MAF: Masked Autoregressive Flow for fast density estimation</li> <li>IAF: Inverse Autoregressive Flow for fast sampling</li> <li>Neural Spline Flows: Spline-based transformations for higher expressiveness</li> </ul>"},{"location":"api/models/flow/#base-classes","title":"Base Classes","text":""},{"location":"api/models/flow/#normalizingflow","title":"<code>NormalizingFlow</code>","text":"<p>Base class for all normalizing flow models.</p> <pre><code>from artifex.generative_models.models.flow.base import NormalizingFlow\n</code></pre> <p>Initialization:</p> <pre><code>model = NormalizingFlow(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs,\n    precision: jax.lax.Precision | None = None\n)\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>config</code> <code>ModelConfig</code> Model configuration object <code>rngs</code> <code>nnx.Rngs</code> Random number generators (required) <code>precision</code> <code>jax.lax.Precision \\| None</code> JAX operation precision (optional) <p>Configuration Parameters (in <code>config.parameters</code>):</p> Parameter Type Default Description <code>base_distribution</code> <code>str</code> <code>\"normal\"</code> Base distribution type (<code>\"normal\"</code> or <code>\"uniform\"</code>) <code>base_distribution_params</code> <code>dict</code> <code>{}</code> Base distribution parameters <p>Attributes:</p> <ul> <li><code>input_dim</code>: Input dimension (from config)</li> <li><code>latent_dim</code>: Latent dimension (defaults to input_dim)</li> <li><code>flow_layers</code>: List of flow layers</li> <li><code>log_prob_fn</code>: Base distribution log probability function</li> <li><code>sample_fn</code>: Base distribution sampling function</li> </ul> <p>Methods:</p>"},{"location":"api/models/flow/#forward","title":"<code>forward</code>","text":"<p>Transform data to latent space.</p> <pre><code>z, log_det = model.forward(\n    x: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; tuple[jax.Array, jax.Array]\n</code></pre> <p>Parameters:</p> <ul> <li><code>x</code>: Input data of shape <code>(batch_size, ...)</code></li> <li><code>rngs</code>: Optional random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>z</code>: Latent representation</li> <li><code>log_det</code>: Log-determinant of Jacobian</li> </ul> <p>Example:</p> <pre><code>import jax.numpy as jnp\nfrom flax import nnx\n\n# Forward transformation\nx = jnp.ones((16, 64))\nz, log_det = model.forward(x, rngs=rngs)\n\nprint(f\"Latent shape: {z.shape}\")  # (16, 64)\nprint(f\"Log-det shape: {log_det.shape}\")  # (16,)\n</code></pre>"},{"location":"api/models/flow/#inverse","title":"<code>inverse</code>","text":"<p>Transform latent to data space.</p> <pre><code>x, log_det = model.inverse(\n    z: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; tuple[jax.Array, jax.Array]\n</code></pre> <p>Parameters:</p> <ul> <li><code>z</code>: Latent code of shape <code>(batch_size, ...)</code></li> <li><code>rngs</code>: Optional random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>x</code>: Reconstructed data</li> <li><code>log_det</code>: Log-determinant of Jacobian</li> </ul> <p>Example:</p> <pre><code># Sample from base distribution\nz = jax.random.normal(rngs.sample(), (16, 64))\n\n# Transform to data space\nx, log_det = model.inverse(z, rngs=rngs)\nprint(f\"Generated data shape: {x.shape}\")  # (16, 64)\n</code></pre>"},{"location":"api/models/flow/#log_prob","title":"<code>log_prob</code>","text":"<p>Compute exact log probability of data.</p> <pre><code>log_prob = model.log_prob(\n    x: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; jax.Array\n</code></pre> <p>Parameters:</p> <ul> <li><code>x</code>: Input data of shape <code>(batch_size, ...)</code></li> <li><code>rngs</code>: Optional random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>log_prob</code>: Log probability for each sample, shape <code>(batch_size,)</code></li> </ul> <p>Example:</p> <pre><code># Compute log probability\nx = jnp.ones((16, 64))\nlog_prob = model.log_prob(x, rngs=rngs)\n\nprint(f\"Mean log-likelihood: {jnp.mean(log_prob):.3f}\")\n</code></pre>"},{"location":"api/models/flow/#generate-sample","title":"<code>generate</code> / <code>sample</code>","text":"<p>Generate samples from the model.</p> <pre><code>samples = model.generate(\n    n_samples: int = 1,\n    *,\n    rngs: nnx.Rngs | None = None,\n    **kwargs\n) -&gt; jax.Array\n</code></pre> <p>Parameters:</p> <ul> <li><code>n_samples</code>: Number of samples to generate</li> <li><code>rngs</code>: Optional random number generators</li> <li><code>**kwargs</code>: Additional keyword arguments</li> </ul> <p>Returns:</p> <ul> <li><code>samples</code>: Generated samples of shape <code>(n_samples, ...)</code></li> </ul> <p>Example:</p> <pre><code># Generate 16 samples\nsamples = model.generate(n_samples=16, rngs=rngs)\nprint(f\"Samples shape: {samples.shape}\")\n</code></pre>"},{"location":"api/models/flow/#__call__","title":"<code>__call__</code>","text":"<p>Forward pass returning dictionary of outputs.</p> <pre><code>outputs = model(\n    x: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None,\n    training: bool = False,\n    **kwargs\n) -&gt; dict[str, Any]\n</code></pre> <p>Returns:</p> <p>Dictionary containing:</p> <ul> <li><code>z</code>: Latent representation</li> <li><code>logdet</code>: Log-determinant</li> <li><code>log_prob</code>: Log probability of data</li> <li><code>log_prob_x</code>: Alias for <code>log_prob</code></li> </ul> <p>Example:</p> <pre><code>outputs = model(x, rngs=rngs, training=True)\nloss = -jnp.mean(outputs[\"log_prob\"])\n</code></pre>"},{"location":"api/models/flow/#loss_fn","title":"<code>loss_fn</code>","text":"<p>Compute negative log-likelihood loss.</p> <pre><code>loss_dict = model.loss_fn(\n    batch: Any,\n    model_outputs: dict[str, Any],\n    *,\n    rngs: nnx.Rngs | None = None,\n    **kwargs\n) -&gt; dict[str, Any]\n</code></pre> <p>Parameters:</p> <ul> <li><code>batch</code>: Input batch data</li> <li><code>model_outputs</code>: Outputs from forward pass</li> <li><code>rngs</code>: Optional random number generators</li> </ul> <p>Returns:</p> <p>Dictionary containing:</p> <ul> <li><code>loss</code>: Negative log-likelihood loss</li> <li><code>nll_loss</code>: Same as <code>loss</code></li> <li><code>log_prob</code>: Mean log probability</li> <li><code>avg_log_prob</code>: Same as <code>log_prob</code></li> </ul>"},{"location":"api/models/flow/#flowlayer","title":"<code>FlowLayer</code>","text":"<p>Base class for flow layer transformations.</p> <pre><code>from artifex.generative_models.models.flow.base import FlowLayer\n</code></pre> <p>Initialization:</p> <pre><code>layer = FlowLayer(\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Abstract Methods:</p>"},{"location":"api/models/flow/#forward_1","title":"<code>forward</code>","text":"<p>Forward transformation.</p> <pre><code>y, log_det = layer.forward(\n    x: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; tuple[jax.Array, jax.Array]\n</code></pre>"},{"location":"api/models/flow/#inverse_1","title":"<code>inverse</code>","text":"<p>Inverse transformation.</p> <pre><code>x, log_det = layer.inverse(\n    y: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; tuple[jax.Array, jax.Array]\n</code></pre>"},{"location":"api/models/flow/#realnvp","title":"RealNVP","text":""},{"location":"api/models/flow/#realnvp_1","title":"<code>RealNVP</code>","text":"<p>Real-valued Non-Volume Preserving flow using affine coupling layers.</p> <pre><code>from artifex.generative_models.models.flow import RealNVP\n</code></pre> <p>Initialization:</p> <pre><code>model = RealNVP(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Configuration Parameters (in <code>config.parameters</code>):</p> Parameter Type Default Description <code>num_coupling_layers</code> <code>int</code> <code>8</code> Number of coupling transformations <code>mask_type</code> <code>str</code> <code>\"checkerboard\"</code> Masking pattern (<code>\"checkerboard\"</code> or <code>\"channel-wise\"</code>) <code>base_distribution</code> <code>str</code> <code>\"normal\"</code> Base distribution type <code>base_distribution_params</code> <code>dict</code> <code>{}</code> Base distribution parameters <p>Example:</p> <pre><code>from artifex.generative_models.core.configuration import ModelConfig\nfrom artifex.generative_models.models.flow import RealNVP\nfrom flax import nnx\n\n# Configure RealNVP\nconfig = ModelConfig(\n    name=\"realnvp\",\n    model_class=\"artifex.generative_models.models.flow.RealNVP\",\n    input_dim=784,\n    output_dim=784,\n    hidden_dims=[512, 512],\n    parameters={\n        \"num_coupling_layers\": 8,\n        \"mask_type\": \"checkerboard\",\n    }\n)\n\n# Create model\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = RealNVP(config, rngs=rngs)\n\n# Use model\nimport jax.numpy as jnp\nx = jax.random.normal(rngs.sample(), (32, 784))\n\n# Density estimation\nlog_prob = model.log_prob(x, rngs=rngs)\nprint(f\"Log probability: {jnp.mean(log_prob):.3f}\")\n\n# Generation\nsamples = model.generate(n_samples=16, rngs=rngs)\nprint(f\"Generated shape: {samples.shape}\")\n</code></pre> <p>Methods:</p> <p>Inherits all methods from <code>NormalizingFlow</code> base class.</p>"},{"location":"api/models/flow/#couplinglayer","title":"<code>CouplingLayer</code>","text":"<p>Affine coupling layer for RealNVP.</p> <pre><code>from artifex.generative_models.models.flow.real_nvp import CouplingLayer\n</code></pre> <p>Initialization:</p> <pre><code>layer = CouplingLayer(\n    mask: jax.Array,\n    hidden_dims: list[int],\n    scale_activation: Callable[[jax.Array], jax.Array] = jax.nn.tanh,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>mask</code> <code>jax.Array</code> - Binary mask (1 = unchanged, 0 = transformed) <code>hidden_dims</code> <code>list[int]</code> - Hidden dimensions for scale/translation networks <code>scale_activation</code> <code>Callable</code> <code>jax.nn.tanh</code> Activation for scale factor <code>rngs</code> <code>nnx.Rngs</code> - Random number generators (required) <p>Example:</p> <pre><code>import jax.numpy as jnp\n\n# Create alternating mask\nmask = jnp.arange(64) % 2  # [0, 1, 0, 1, ...]\n\n# Create coupling layer\nlayer = CouplingLayer(\n    mask=mask,\n    hidden_dims=[256, 256],\n    rngs=rngs\n)\n\n# Forward transformation\nx = jax.random.normal(rngs.sample(), (16, 64))\ny, log_det = layer.forward(x, rngs=rngs)\n\n# Inverse transformation\nx_recon, log_det_inv = layer.inverse(y, rngs=rngs)\n\n# Verify invertibility\nerror = jnp.max(jnp.abs(x - x_recon))\nprint(f\"Reconstruction error: {error:.6f}\")\n</code></pre>"},{"location":"api/models/flow/#glow","title":"Glow","text":""},{"location":"api/models/flow/#glow_1","title":"<code>Glow</code>","text":"<p>Multi-scale flow with ActNorm, invertible 1\u00d71 convolutions, and coupling.</p> <pre><code>from artifex.generative_models.models.flow import Glow\n</code></pre> <p>Initialization:</p> <pre><code>model = Glow(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Configuration Parameters (in <code>config.parameters</code>):</p> Parameter Type Default Description <code>image_shape</code> <code>tuple[int, int, int]</code> <code>(32, 32, 3)</code> Input image shape (H, W, C) <code>num_scales</code> <code>int</code> <code>3</code> Number of multi-scale levels <code>blocks_per_scale</code> <code>int</code> <code>6</code> Number of flow blocks per scale <code>hidden_dims</code> <code>list[int]</code> <code>[512, 512]</code> Hidden dimensions for coupling <p>Example:</p> <pre><code>from artifex.generative_models.models.flow import Glow\n\n# Configure Glow for 32x32 RGB images\nconfig = ModelConfig(\n    name=\"glow\",\n    model_class=\"artifex.generative_models.models.flow.Glow\",\n    input_dim=(32, 32, 3),\n    hidden_dims=[512, 512],\n    parameters={\n        \"image_shape\": (32, 32, 3),\n        \"num_scales\": 3,\n        \"blocks_per_scale\": 6,\n    }\n)\n\n# Create Glow model\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = Glow(config, rngs=rngs)\n\n# Training\nimages = jax.random.normal(rngs.sample(), (16, 32, 32, 3))\noutputs = model(images, rngs=rngs)\nloss = -jnp.mean(outputs[\"log_prob\"])\n\n# Generation\nsamples = model.generate(n_samples=16, rngs=rngs)\n</code></pre> <p>Methods:</p> <p>Inherits from <code>NormalizingFlow</code> with image-specific generation.</p>"},{"location":"api/models/flow/#glowblock","title":"<code>GlowBlock</code>","text":"<p>Single Glow block: ActNorm \u2192 1\u00d71 Conv \u2192 Coupling.</p> <pre><code>from artifex.generative_models.models.flow.glow import GlowBlock\n</code></pre> <p>Initialization:</p> <pre><code>block = GlowBlock(\n    num_channels: int,\n    hidden_dims: list[int] | None = None,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_channels</code>: Number of channels in input</li> <li><code>hidden_dims</code>: Hidden dimensions for coupling layer</li> <li><code>rngs</code>: Random number generators</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.flow.glow import GlowBlock\n\n# Create Glow block for 32-channel input\nblock = GlowBlock(\n    num_channels=32,\n    hidden_dims=[512, 512],\n    rngs=rngs\n)\n\n# Forward pass\nx = jax.random.normal(rngs.sample(), (16, 8, 8, 32))\ny, log_det = block.forward(x, rngs=rngs)\n\n# Inverse pass\nx_recon, log_det_inv = block.inverse(y, rngs=rngs)\n</code></pre>"},{"location":"api/models/flow/#actnormlayer","title":"<code>ActNormLayer</code>","text":"<p>Activation normalization with learnable scale and bias.</p> <pre><code>from artifex.generative_models.models.flow.glow import ActNormLayer\n</code></pre> <p>Initialization:</p> <pre><code>layer = ActNormLayer(\n    num_channels: int,\n    *,\n    rngs: nnx.Rngs | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_channels</code>: Number of channels to normalize</li> <li><code>rngs</code>: Optional random number generators</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.flow.glow import ActNormLayer\n\n# Create ActNorm layer\nlayer = ActNormLayer(num_channels=32, rngs=rngs)\n\n# Forward (initializes from data on first call)\nx = jax.random.normal(rngs.sample(), (16, 8, 8, 32))\ny, log_det = layer.forward(x, rngs=rngs)\n\n# After initialization, parameters are learned\nprint(f\"Scale: {layer.logs.value.shape}\")  # (1, 1, 32)\nprint(f\"Bias: {layer.bias.value.shape}\")   # (1, 1, 32)\n</code></pre> <p>Features:</p> <ul> <li>Data-dependent initialization on first forward pass</li> <li>Learnable per-channel scale and bias</li> <li>Efficient Jacobian computation</li> </ul>"},{"location":"api/models/flow/#invertibleconv1x1","title":"<code>InvertibleConv1x1</code>","text":"<p>Invertible 1\u00d71 convolution for channel mixing.</p> <pre><code>from artifex.generative_models.models.flow.glow import InvertibleConv1x1\n</code></pre> <p>Initialization:</p> <pre><code>layer = InvertibleConv1x1(\n    num_channels: int,\n    *,\n    rngs: nnx.Rngs | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_channels</code>: Number of channels</li> <li><code>rngs</code>: Optional random number generators</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.flow.glow import InvertibleConv1x1\n\n# Create invertible 1x1 conv\nlayer = InvertibleConv1x1(num_channels=32, rngs=rngs)\n\n# Forward\nx = jax.random.normal(rngs.sample(), (16, 8, 8, 32))\ny, log_det = layer.forward(x, rngs=rngs)\n\n# Inverse\nx_recon, log_det_inv = layer.inverse(y, rngs=rngs)\n\n# Verify invertibility\nerror = jnp.max(jnp.abs(x - x_recon))\nprint(f\"Reconstruction error: {error:.6f}\")  # Should be ~0\n</code></pre> <p>Features:</p> <ul> <li>Initialized as orthogonal matrix (via QR decomposition)</li> <li>Efficient Jacobian: \\(h \\cdot w \\cdot \\log|\\det(W)|\\)</li> <li>Fully invertible</li> </ul>"},{"location":"api/models/flow/#affinecouplinglayer","title":"<code>AffineCouplingLayer</code>","text":"<p>Affine coupling layer for Glow (similar to RealNVP but channel-split).</p> <pre><code>from artifex.generative_models.models.flow.glow import AffineCouplingLayer\n</code></pre> <p>Initialization:</p> <pre><code>layer = AffineCouplingLayer(\n    num_channels: int,\n    hidden_dims: list[int] | None = None,\n    *,\n    rngs: nnx.Rngs | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_channels</code>: Number of input channels</li> <li><code>hidden_dims</code>: Hidden dimensions for conditioning network</li> <li><code>rngs</code>: Optional random number generators</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.flow.glow import AffineCouplingLayer\n\n# Create affine coupling layer\nlayer = AffineCouplingLayer(\n    num_channels=32,\n    hidden_dims=[512, 512],\n    rngs=rngs\n)\n\n# Forward\nx = jax.random.normal(rngs.sample(), (16, 8, 8, 32))\ny, log_det = layer.forward(x, rngs=rngs)\n</code></pre>"},{"location":"api/models/flow/#maf-masked-autoregressive-flow","title":"MAF (Masked Autoregressive Flow)","text":""},{"location":"api/models/flow/#maf","title":"<code>MAF</code>","text":"<p>Masked Autoregressive Flow for fast density estimation.</p> <pre><code>from artifex.generative_models.models.flow import MAF\n</code></pre> <p>Initialization:</p> <pre><code>model = MAF(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Configuration Parameters (in <code>config.parameters</code>):</p> Parameter Type Default Description <code>num_layers</code> <code>int</code> <code>5</code> Number of MAF layers <code>reverse_ordering</code> <code>bool</code> <code>True</code> Alternate variable ordering between layers <p>Example:</p> <pre><code>from artifex.generative_models.models.flow import MAF\n\n# Configure MAF\nconfig = ModelConfig(\n    name=\"maf\",\n    model_class=\"artifex.generative_models.models.flow.MAF\",\n    input_dim=64,\n    output_dim=64,\n    hidden_dims=[512],\n    parameters={\n        \"num_layers\": 5,\n        \"reverse_ordering\": True,\n    }\n)\n\n# Create MAF\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = MAF(config, rngs=rngs)\n\n# Fast density estimation (parallel)\nx = jax.random.normal(rngs.sample(), (100, 64))\nlog_prob = model.log_prob(x, rngs=rngs)  # Fast!\n\n# Slow sampling (sequential)\nsamples = model.sample(n_samples=10, rngs=rngs)  # Slower\n</code></pre> <p>Trade-offs:</p> <ul> <li>Fast Forward: \\(O(1)\\) passes for density estimation</li> <li>Slow Inverse: \\(O(d)\\) sequential passes for sampling</li> <li>Best for applications where density estimation is primary</li> </ul>"},{"location":"api/models/flow/#maflayer","title":"<code>MAFLayer</code>","text":"<p>Single MAF transformation layer.</p> <pre><code>from artifex.generative_models.models.flow.maf import MAFLayer\n</code></pre> <p>Initialization:</p> <pre><code>layer = MAFLayer(\n    input_dim: int,\n    hidden_dims: Sequence[int],\n    *,\n    rngs: nnx.Rngs,\n    order: jax.Array | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>input_dim</code>: Input dimension</li> <li><code>hidden_dims</code>: Hidden dimensions for MADE network</li> <li><code>rngs</code>: Random number generators (required)</li> <li><code>order</code>: Variable ordering (None for natural ordering)</li> </ul>"},{"location":"api/models/flow/#made","title":"<code>MADE</code>","text":"<p>Masked Autoencoder for Distribution Estimation.</p> <pre><code>from artifex.generative_models.models.flow.made import MADE\n</code></pre> <p>Initialization:</p> <pre><code>made = MADE(\n    input_dim: int,\n    hidden_dims: Sequence[int],\n    output_multiplier: int = 2,\n    *,\n    rngs: nnx.Rngs,\n    order: jax.Array | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>input_dim</code>: Input dimension</li> <li><code>hidden_dims</code>: Hidden layer dimensions</li> <li><code>output_multiplier</code>: Output dim multiplier (2 for mean and scale)</li> <li><code>rngs</code>: Random number generators (required)</li> <li><code>order</code>: Variable ordering</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.flow.made import MADE\n\n# Create MADE network\nmade = MADE(\n    input_dim=64,\n    hidden_dims=[512, 512],\n    output_multiplier=2,  # For mean and log_scale\n    rngs=rngs\n)\n\n# Forward pass\nx = jax.random.normal(rngs.sample(), (16, 64))\nmu, log_scale = made(x)  # Returns (16, 64) for each\n\nprint(f\"Mean shape: {mu.shape}\")       # (16, 64)\nprint(f\"Log scale shape: {log_scale.shape}\")  # (16, 64)\n</code></pre>"},{"location":"api/models/flow/#iaf-inverse-autoregressive-flow","title":"IAF (Inverse Autoregressive Flow)","text":""},{"location":"api/models/flow/#iaf","title":"<code>IAF</code>","text":"<p>Inverse Autoregressive Flow for fast sampling.</p> <pre><code>from artifex.generative_models.models.flow import IAF\n</code></pre> <p>Initialization:</p> <pre><code>model = IAF(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Configuration Parameters (in <code>config.parameters</code>):</p> Parameter Type Default Description <code>num_layers</code> <code>int</code> <code>5</code> Number of IAF layers <code>reverse_ordering</code> <code>bool</code> <code>True</code> Alternate variable ordering <p>Example:</p> <pre><code>from artifex.generative_models.models.flow import IAF\n\n# Configure IAF\nconfig = ModelConfig(\n    name=\"iaf\",\n    model_class=\"artifex.generative_models.models.flow.IAF\",\n    input_dim=64,\n    output_dim=64,\n    hidden_dims=[512],\n    parameters={\n        \"num_layers\": 5,\n        \"reverse_ordering\": True,\n    }\n)\n\n# Create IAF\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = IAF(config, rngs=rngs)\n\n# Fast sampling (parallel)\nsamples = model.sample(n_samples=100, rngs=rngs)  # Fast!\n\n# Slow density estimation (sequential)\nlog_prob = model.log_prob(samples, rngs=rngs)  # Slower\n</code></pre> <p>Trade-offs:</p> <ul> <li>Fast Inverse: \\(O(1)\\) passes for sampling/generation</li> <li>Slow Forward: \\(O(d)\\) sequential passes for density estimation</li> <li>Best for applications where sampling is primary (e.g., variational inference)</li> </ul>"},{"location":"api/models/flow/#iaflayer","title":"<code>IAFLayer</code>","text":"<p>Single IAF transformation layer.</p> <pre><code>from artifex.generative_models.models.flow.iaf import IAFLayer\n</code></pre> <p>Initialization:</p> <pre><code>layer = IAFLayer(\n    input_dim: int,\n    hidden_dims: Sequence[int],\n    *,\n    rngs: nnx.Rngs,\n    order: jax.Array | None = None\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>input_dim</code>: Input dimension</li> <li><code>hidden_dims</code>: Hidden dimensions for MADE network</li> <li><code>rngs</code>: Random number generators (required)</li> <li><code>order</code>: Variable ordering</li> </ul>"},{"location":"api/models/flow/#neural-spline-flows","title":"Neural Spline Flows","text":""},{"location":"api/models/flow/#neuralsplineflow","title":"<code>NeuralSplineFlow</code>","text":"<p>Flow using rational quadratic spline transformations.</p> <pre><code>from artifex.generative_models.models.flow import NeuralSplineFlow\n</code></pre> <p>Initialization:</p> <pre><code>model = NeuralSplineFlow(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Configuration Parameters (in <code>config.metadata[\"flow_params\"]</code>):</p> Parameter Type Default Description <code>num_layers</code> <code>int</code> <code>8</code> Number of spline coupling layers <code>num_bins</code> <code>int</code> <code>8</code> Number of spline bins/segments <code>tail_bound</code> <code>float</code> <code>3.0</code> Spline domain bounds <code>base_distribution</code> <code>str</code> <code>\"normal\"</code> Base distribution type <p>Example:</p> <pre><code>from artifex.generative_models.models.flow import NeuralSplineFlow\n\n# Configure Neural Spline Flow\nconfig = ModelConfig(\n    name=\"spline_flow\",\n    model_class=\"artifex.generative_models.models.flow.NeuralSplineFlow\",\n    input_dim=64,\n    hidden_dims=[128, 128],\n    metadata={\n        \"flow_params\": {\n            \"num_layers\": 8,\n            \"num_bins\": 8,\n            \"tail_bound\": 3.0,\n        }\n    }\n)\n\n# Create model\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = NeuralSplineFlow(config, rngs=rngs)\n\n# More expressive transformations\nx = jax.random.normal(rngs.sample(), (32, 64))\nlog_prob = model.log_prob(x, rngs=rngs)\n\n# Generate samples\nsamples = model.generate(n_samples=16, rngs=rngs)\n</code></pre> <p>Features:</p> <ul> <li>More expressive than affine transformations</li> <li>Monotonic by construction (ensures invertibility)</li> <li>Smooth with controlled derivatives</li> <li>Bounded domain with identity outside bounds</li> </ul>"},{"location":"api/models/flow/#splinecouplinglayer","title":"<code>SplineCouplingLayer</code>","text":"<p>Coupling layer with spline transformations.</p> <pre><code>from artifex.generative_models.models.flow.neural_spline import SplineCouplingLayer\n</code></pre> <p>Initialization:</p> <pre><code>layer = SplineCouplingLayer(\n    mask: jax.Array,\n    hidden_dims: list[int] = [128, 128],\n    num_bins: int = 8,\n    tail_bound: float = 3.0,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>mask</code> <code>jax.Array</code> - Binary mask for coupling <code>hidden_dims</code> <code>list[int]</code> <code>[128, 128]</code> Hidden dimensions for conditioning network <code>num_bins</code> <code>int</code> <code>8</code> Number of spline bins <code>tail_bound</code> <code>float</code> <code>3.0</code> Spline domain bounds \\([-b, b]\\) <code>rngs</code> <code>nnx.Rngs</code> - Random number generators (required) <p>Example:</p> <pre><code>from artifex.generative_models.models.flow.neural_spline import SplineCouplingLayer\nimport jax.numpy as jnp\n\n# Create alternating mask\nmask = jnp.arange(64) % 2\n\n# Create spline coupling layer\nlayer = SplineCouplingLayer(\n    mask=mask,\n    hidden_dims=[256, 256],\n    num_bins=8,\n    tail_bound=3.0,\n    rngs=rngs\n)\n\n# Forward\nx = jax.random.normal(rngs.sample(), (16, 64))\ny, log_det = layer.forward(x, rngs=rngs)\n\n# Inverse\nx_recon, log_det_inv = layer.inverse(y, rngs=rngs)\n</code></pre>"},{"location":"api/models/flow/#rationalquadraticsplinetransform","title":"<code>RationalQuadraticSplineTransform</code>","text":"<p>Rational quadratic spline transformation for single dimension.</p> <pre><code>from artifex.generative_models.models.flow.neural_spline import (\n    RationalQuadraticSplineTransform\n)\n</code></pre> <p>Initialization:</p> <pre><code>transform = RationalQuadraticSplineTransform(\n    num_bins: int = 8,\n    tail_bound: float = 3.0,\n    min_bin_width: float = 1e-3,\n    min_bin_height: float = 1e-3,\n    min_derivative: float = 1e-3,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_bins</code>: Number of spline bins</li> <li><code>tail_bound</code>: Domain bounds \\([-b, b]\\)</li> <li><code>min_bin_width</code>: Minimum bin width (for numerical stability)</li> <li><code>min_bin_height</code>: Minimum bin height</li> <li><code>min_derivative</code>: Minimum derivative</li> <li><code>rngs</code>: Random number generators</li> </ul> <p>Method:</p> <pre><code>y, log_det = transform.apply_spline(\n    x: jax.Array,\n    widths: jax.Array,\n    heights: jax.Array,\n    derivatives: jax.Array,\n    inverse: bool = False\n) -&gt; tuple[jax.Array, jax.Array]\n</code></pre> <p>Example:</p> <pre><code>from artifex.generative_models.models.flow.neural_spline import (\n    RationalQuadraticSplineTransform\n)\n\n# Create spline transform\nspline = RationalQuadraticSplineTransform(\n    num_bins=8,\n    tail_bound=3.0,\n    rngs=rngs\n)\n\n# Generate spline parameters (typically from neural network)\nbatch_size, dim = 16, 64\nwidths = jax.random.uniform(rngs.sample(), (batch_size, dim, 8))\nheights = jax.random.uniform(rngs.sample(), (batch_size, dim, 8))\nderivatives = jax.random.uniform(rngs.sample(), (batch_size, dim, 9))\n\n# Constrain parameters\nwidths, heights, derivatives = spline._constrain_parameters(\n    widths, heights, derivatives\n)\n\n# Apply spline transformation\nx = jax.random.normal(rngs.sample(), (batch_size, dim))\ny, log_det = spline.apply_spline(\n    x, widths, heights, derivatives, inverse=False\n)\n\nprint(f\"Transformed shape: {y.shape}\")  # (16, 64)\nprint(f\"Log-det shape: {log_det.shape}\")  # (16,)\n</code></pre>"},{"location":"api/models/flow/#conditional-flows","title":"Conditional Flows","text":""},{"location":"api/models/flow/#conditionalnormalizingflow","title":"<code>ConditionalNormalizingFlow</code>","text":"<p>Base class for conditional normalizing flows.</p> <pre><code>from artifex.generative_models.models.flow.conditional import ConditionalNormalizingFlow\n</code></pre> <p>Initialization:</p> <pre><code>model = ConditionalNormalizingFlow(\n    config: ModelConfig,\n    *,\n    rngs: nnx.Rngs\n)\n</code></pre> <p>Additional Methods:</p> <pre><code># Conditional forward pass\nz, log_det = model.forward(x, condition=c, rngs=rngs)\n\n# Conditional generation\nsamples = model.generate(n_samples=16, condition=c, rngs=rngs)\n\n# Conditional log probability\nlog_prob = model.log_prob(x, condition=c, rngs=rngs)\n</code></pre>"},{"location":"api/models/flow/#conditionalrealnvp","title":"<code>ConditionalRealNVP</code>","text":"<p>RealNVP with conditional generation.</p> <pre><code>from artifex.generative_models.models.flow.conditional import ConditionalRealNVP\n</code></pre> <p>Configuration:</p> <p>Add <code>condition_dim</code> to parameters:</p> <pre><code>config = ModelConfig(\n    name=\"conditional_realnvp\",\n    model_class=\"artifex.generative_models.models.flow.ConditionalRealNVP\",\n    input_dim=784,\n    output_dim=784,\n    hidden_dims=[512, 512],\n    parameters={\n        \"num_coupling_layers\": 8,\n        \"condition_dim\": 10,  # e.g., one-hot class labels\n    }\n)\n</code></pre> <p>Example:</p> <pre><code>from artifex.generative_models.models.flow.conditional import ConditionalRealNVP\nimport jax.numpy as jnp\n\n# Create conditional model\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = ConditionalRealNVP(config, rngs=rngs)\n\n# Prepare conditioning (e.g., class labels)\nbatch_size = 16\nclass_labels = jax.random.randint(rngs.sample(), (batch_size,), 0, 10)\ncondition = jax.nn.one_hot(class_labels, 10)\n\n# Conditional density estimation\nx = jax.random.normal(rngs.sample(), (batch_size, 784))\nlog_prob = model.log_prob(x, condition=condition, rngs=rngs)\n\n# Conditional generation\nsamples = model.generate(\n    n_samples=16,\n    condition=condition,\n    rngs=rngs\n)\n</code></pre>"},{"location":"api/models/flow/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api/models/flow/#model-configuration","title":"Model Configuration","text":"<p>All flow models use <code>ModelConfig</code> for configuration:</p> <pre><code>from artifex.generative_models.core.configuration import ModelConfig\n\nconfig = ModelConfig(\n    name: str,                    # Model name\n    model_class: str,             # Full class path\n    input_dim: int | tuple,       # Input dimensions\n    output_dim: int | tuple,      # Output dimensions (often same as input)\n    hidden_dims: list[int],       # Hidden layer dimensions\n    parameters: dict,             # Model-specific parameters\n    metadata: dict = {},          # Additional metadata\n)\n</code></pre>"},{"location":"api/models/flow/#realnvp-configuration","title":"RealNVP Configuration","text":"<pre><code>config = ModelConfig(\n    name=\"realnvp\",\n    model_class=\"artifex.generative_models.models.flow.RealNVP\",\n    input_dim=784,\n    output_dim=784,\n    hidden_dims=[512, 512],\n    parameters={\n        \"num_coupling_layers\": 8,\n        \"mask_type\": \"checkerboard\",  # or \"channel-wise\"\n        \"base_distribution\": \"normal\",\n        \"base_distribution_params\": {\"loc\": 0.0, \"scale\": 1.0},\n    }\n)\n</code></pre>"},{"location":"api/models/flow/#glow-configuration","title":"Glow Configuration","text":"<pre><code>config = ModelConfig(\n    name=\"glow\",\n    model_class=\"artifex.generative_models.models.flow.Glow\",\n    input_dim=(32, 32, 3),\n    hidden_dims=[512, 512],\n    parameters={\n        \"image_shape\": (32, 32, 3),\n        \"num_scales\": 3,\n        \"blocks_per_scale\": 6,\n    }\n)\n</code></pre>"},{"location":"api/models/flow/#maf-configuration","title":"MAF Configuration","text":"<pre><code>config = ModelConfig(\n    name=\"maf\",\n    model_class=\"artifex.generative_models.models.flow.MAF\",\n    input_dim=64,\n    output_dim=64,\n    hidden_dims=[512],\n    parameters={\n        \"num_layers\": 5,\n        \"reverse_ordering\": True,\n    }\n)\n</code></pre>"},{"location":"api/models/flow/#iaf-configuration","title":"IAF Configuration","text":"<pre><code>config = ModelConfig(\n    name=\"iaf\",\n    model_class=\"artifex.generative_models.models.flow.IAF\",\n    input_dim=64,\n    output_dim=64,\n    hidden_dims=[512],\n    parameters={\n        \"num_layers\": 5,\n        \"reverse_ordering\": True,\n    }\n)\n</code></pre>"},{"location":"api/models/flow/#neural-spline-flow-configuration","title":"Neural Spline Flow Configuration","text":"<pre><code>config = ModelConfig(\n    name=\"spline_flow\",\n    model_class=\"artifex.generative_models.models.flow.NeuralSplineFlow\",\n    input_dim=64,\n    hidden_dims=[128, 128],\n    metadata={\n        \"flow_params\": {\n            \"num_layers\": 8,\n            \"num_bins\": 8,\n            \"tail_bound\": 3.0,\n            \"base_distribution\": \"normal\",\n        }\n    }\n)\n</code></pre>"},{"location":"api/models/flow/#common-patterns","title":"Common Patterns","text":""},{"location":"api/models/flow/#training-pattern","title":"Training Pattern","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\n\n# Create model and optimizer (wrt=nnx.Param required in NNX 0.11.0+)\nmodel = RealNVP(config, rngs=rngs)\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n# Training step (JIT-compiled for speed)\n@nnx.jit\ndef train_step(model, optimizer, batch, rngs):\n    def loss_fn(model):\n        outputs = model(batch, rngs=rngs, training=True)\n        return -jnp.mean(outputs[\"log_prob\"])\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n    return {\"loss\": loss}\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        metrics = train_step(model, optimizer, batch, rngs)\n        print(f\"Loss: {metrics['loss']:.3f}\")\n</code></pre>"},{"location":"api/models/flow/#density-estimation-pattern","title":"Density Estimation Pattern","text":"<pre><code># Compute log-likelihood\nlog_probs = model.log_prob(data, rngs=rngs)\n\n# Average log-likelihood\navg_ll = jnp.mean(log_probs)\n\n# Bits per dimension\ninput_dim = jnp.prod(jnp.array(config.input_dim))\nbpd = -avg_ll / (input_dim * jnp.log(2))\n\nprint(f\"Average log-likelihood: {avg_ll:.3f}\")\nprint(f\"Bits per dimension: {bpd:.3f}\")\n</code></pre>"},{"location":"api/models/flow/#generation-pattern","title":"Generation Pattern","text":"<pre><code># Generate samples\nsamples = model.generate(n_samples=16, rngs=rngs)\n\n# With temperature\nz = jax.random.normal(rngs.sample(), (16, latent_dim))\nz = z * temperature  # temperature &gt; 1: more diverse\nsamples, _ = model.inverse(z, rngs=rngs)\n</code></pre>"},{"location":"api/models/flow/#anomaly-detection-pattern","title":"Anomaly Detection Pattern","text":"<pre><code># Compute log-likelihood for training data\ntrain_log_probs = model.log_prob(train_data, rngs=rngs)\n\n# Set threshold (e.g., 5th percentile)\nthreshold = jnp.percentile(train_log_probs, 5)\n\n# Detect anomalies in test data\ntest_log_probs = model.log_prob(test_data, rngs=rngs)\nis_anomaly = test_log_probs &lt; threshold\n\nprint(f\"Detected {jnp.sum(is_anomaly)} anomalies\")\n</code></pre>"},{"location":"api/models/flow/#quick-reference","title":"Quick Reference","text":""},{"location":"api/models/flow/#architecture-comparison","title":"Architecture Comparison","text":"Model Forward Inverse Use Case RealNVP Fast Fast Balanced, general purpose Glow Fast Fast High-quality images MAF Fast Slow Density estimation IAF Slow Fast Fast sampling, VI Spline Fast Fast High expressiveness"},{"location":"api/models/flow/#common-workflows","title":"Common Workflows","text":"<p>Density Estimation:</p> <pre><code>model = MAF(config, rngs=rngs)\nlog_prob = model.log_prob(data, rngs=rngs)\n</code></pre> <p>Fast Sampling:</p> <pre><code>model = IAF(config, rngs=rngs)\nsamples = model.generate(n_samples=100, rngs=rngs)\n</code></pre> <p>Image Generation:</p> <pre><code>model = Glow(config, rngs=rngs)\nsamples = model.generate(n_samples=16, rngs=rngs)\n</code></pre> <p>High Expressiveness:</p> <pre><code>model = NeuralSplineFlow(config, rngs=rngs)\nlog_prob = model.log_prob(data, rngs=rngs)\n</code></pre>"},{"location":"api/models/flow/#see-also","title":"See Also","text":"<ul> <li>User Guide: Flow Models Guide for practical examples</li> <li>Concepts: Flow Explained for theory</li> <li>Tutorial: Flow MNIST Example for hands-on learning</li> </ul>"},{"location":"api/models/flow/#references","title":"References","text":"<ul> <li>Dinh et al. (2016): \"Density estimation using Real NVP\"</li> <li>Kingma &amp; Dhariwal (2018): \"Glow: Generative Flow with Invertible 1x1 Convolutions\"</li> <li>Papamakarios et al. (2017): \"Masked Autoregressive Flow for Density Estimation\"</li> <li>Kingma et al. (2016): \"Improved Variational Inference with Inverse Autoregressive Flow\"</li> <li>Durkan et al. (2019): \"Neural Spline Flows\"</li> </ul>"},{"location":"api/models/gan/","title":"GAN API Reference","text":"<p>Complete API reference for all GAN model classes in Artifex.</p>"},{"location":"api/models/gan/#overview","title":"Overview","text":"<p>The GAN module provides implementations of various Generative Adversarial Network architectures:</p> <ul> <li>Base GAN: Standard generator and discriminator</li> <li>DCGAN: Deep convolutional architecture</li> <li>WGAN: Wasserstein distance with gradient penalty</li> <li>LSGAN: Least squares loss</li> <li>Conditional GAN: Class-conditioned generation</li> <li>CycleGAN: Unpaired image-to-image translation</li> <li>PatchGAN: Patch-based discrimination</li> </ul>"},{"location":"api/models/gan/#base-classes","title":"Base Classes","text":""},{"location":"api/models/gan/#generator","title":"Generator","text":"<p>Basic generator network that transforms latent vectors into data samples.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>hidden_dims</code> <code>list[int]</code> Required Hidden layer dimensions <code>output_shape</code> <code>tuple</code> Required Shape of generated samples (batch, C, H, W) <code>latent_dim</code> <code>int</code> Required Dimension of latent space <code>activation</code> <code>str</code> <code>\"relu\"</code> Activation function name <code>batch_norm</code> <code>bool</code> <code>True</code> Whether to use batch normalization <code>dropout_rate</code> <code>float</code> <code>0.0</code> Dropout rate <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator","title":"artifex.generative_models.models.gan.Generator","text":"<pre><code>Generator(config: GeneratorConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Generator network for GAN.</p> <p>Base generator using fully-connected (Dense) layers. For convolutional generators, use DCGANGenerator or other specialized subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GeneratorConfig</code> <p>GeneratorConfig with network architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not GeneratorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.hidden_dims","title":"hidden_dims  <code>instance-attribute</code>","text":"<pre><code>hidden_dims = hidden_dims\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.output_shape","title":"output_shape  <code>instance-attribute</code>","text":"<pre><code>output_shape = output_shape\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.batch_norm","title":"batch_norm  <code>instance-attribute</code>","text":"<pre><code>batch_norm = batch_norm\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.dropout_rate","title":"dropout_rate  <code>instance-attribute</code>","text":"<pre><code>dropout_rate = dropout_rate\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.activation_fn","title":"activation_fn  <code>instance-attribute</code>","text":"<pre><code>activation_fn = _get_activation_fn(activation)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = List(layers_list)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.output_layer","title":"output_layer  <code>instance-attribute</code>","text":"<pre><code>output_layer = Linear(\n    in_features=last_dim,\n    out_features=output_size,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.bn_layers","title":"bn_layers  <code>instance-attribute</code>","text":"<pre><code>bn_layers = List(\n    [(BatchNorm(dim, rngs=rngs)) for dim in hidden_dims]\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Generator.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(rate=dropout_rate, rngs=rngs)\n</code></pre>"},{"location":"api/models/gan/#__call__z-trainingfalse","title":"<code>__call__(z, training=False)</code>","text":"<p>Generate samples from latent vectors.</p> <p>Parameters:</p> <ul> <li><code>z</code> (<code>jax.Array</code>): Latent vectors of shape <code>(batch_size, latent_dim)</code></li> <li><code>training</code> (<code>bool</code>): Whether in training mode (affects batch norm and dropout)</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated samples of shape <code>(batch_size, *output_shape[1:])</code></li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import Generator\nfrom flax import nnx\nimport jax.numpy as jnp\n\n# Create generator\ngenerator = Generator(\n    hidden_dims=[256, 512, 1024],\n    output_shape=(1, 1, 28, 28),  # MNIST\n    latent_dim=100,\n    activation=\"relu\",\n    batch_norm=True,\n    rngs=nnx.Rngs(params=0),\n)\n\n# Generate samples\nz = jnp.ones((32, 100))  # Batch of latent vectors\nsamples = generator(z, training=False)\nprint(samples.shape)  # (32, 1, 28, 28)\n</code></pre>"},{"location":"api/models/gan/#discriminator","title":"Discriminator","text":"<p>Basic discriminator network that classifies samples as real or fake.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>hidden_dims</code> <code>list[int]</code> Required Hidden layer dimensions <code>activation</code> <code>str</code> <code>\"leaky_relu\"</code> Activation function name <code>leaky_relu_slope</code> <code>float</code> <code>0.2</code> Negative slope for LeakyReLU <code>batch_norm</code> <code>bool</code> <code>False</code> Whether to use batch normalization <code>dropout_rate</code> <code>float</code> <code>0.3</code> Dropout rate <code>use_spectral_norm</code> <code>bool</code> <code>False</code> Whether to use spectral normalization <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator","title":"artifex.generative_models.models.gan.Discriminator","text":"<pre><code>Discriminator(config: DiscriminatorConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Discriminator network for GAN.</p> <p>Base discriminator using fully-connected (Dense) layers. For convolutional discriminators, use DCGANDiscriminator or other specialized subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DiscriminatorConfig</code> <p>DiscriminatorConfig with network architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not DiscriminatorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.hidden_dims","title":"hidden_dims  <code>instance-attribute</code>","text":"<pre><code>hidden_dims = hidden_dims\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.input_shape","title":"input_shape  <code>instance-attribute</code>","text":"<pre><code>input_shape = input_shape\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = activation\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.leaky_relu_slope","title":"leaky_relu_slope  <code>instance-attribute</code>","text":"<pre><code>leaky_relu_slope = leaky_relu_slope\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.batch_norm","title":"batch_norm  <code>instance-attribute</code>","text":"<pre><code>batch_norm = batch_norm\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.dropout_rate","title":"dropout_rate  <code>instance-attribute</code>","text":"<pre><code>dropout_rate = dropout_rate\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.use_spectral_norm","title":"use_spectral_norm  <code>instance-attribute</code>","text":"<pre><code>use_spectral_norm = use_spectral_norm\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.activation_fn","title":"activation_fn  <code>instance-attribute</code>","text":"<pre><code>activation_fn = _get_activation_fn(\n    activation, leaky_relu_slope\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = List(layers_list)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.output_layer","title":"output_layer  <code>instance-attribute</code>","text":"<pre><code>output_layer = Linear(\n    in_features=curr_dim, out_features=1, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.bn_layers","title":"bn_layers  <code>instance-attribute</code>","text":"<pre><code>bn_layers = List(\n    [(BatchNorm(dim, rngs=rngs)) for dim in hidden_dims]\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.Discriminator.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = Dropout(rate=dropout_rate, rngs=rngs)\n</code></pre>"},{"location":"api/models/gan/#initialize_layersinput_shape-rngsnone","title":"<code>initialize_layers(input_shape, rngs=None)</code>","text":"<p>Initialize layers based on input shape.</p> <p>Parameters:</p> <ul> <li><code>input_shape</code> (<code>tuple</code>): Shape of input data <code>(batch, C, H, W)</code></li> <li><code>rngs</code> (<code>nnx.Rngs</code>, optional): Random number generators</li> </ul>"},{"location":"api/models/gan/#__call__x-trainingfalse","title":"<code>__call__(x, training=False)</code>","text":"<p>Classify samples as real or fake.</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input samples of shape <code>(batch_size, C, H, W)</code></li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Discrimination scores of shape <code>(batch_size, 1)</code>, values in <code>[0, 1]</code></li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import Discriminator\nfrom flax import nnx\nimport jax.numpy as jnp\n\n# Create discriminator\ndiscriminator = Discriminator(\n    hidden_dims=[512, 256, 128],\n    activation=\"leaky_relu\",\n    leaky_relu_slope=0.2,\n    batch_norm=False,\n    dropout_rate=0.3,\n    rngs=nnx.Rngs(params=0, dropout=1),\n)\n\n# Classify samples\nsamples = jnp.ones((32, 1, 28, 28))\nscores = discriminator(samples, training=True)\nprint(scores.shape)  # (32, 1)\nprint(f\"Scores range: [{scores.min():.3f}, {scores.max():.3f}]\")\n</code></pre>"},{"location":"api/models/gan/#gan","title":"GAN","text":"<p>Complete GAN model combining generator and discriminator.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>config</code> <code>object</code> Required Model configuration object <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <code>precision</code> <code>jax.lax.Precision</code> <code>None</code> Numerical precision <p>Configuration Object:</p> <p>The <code>config</code> must have the following structure:</p> <pre><code>class GANConfig:\n    latent_dim: int = 100                  # Latent space dimension\n    loss_type: str = \"vanilla\"             # Loss type: \"vanilla\", \"wasserstein\", \"least_squares\", \"hinge\"\n    gradient_penalty_weight: float = 0.0   # Weight for gradient penalty (WGAN-GP)\n\n    class generator:\n        hidden_dims: list[int]             # Generator hidden dimensions\n        output_shape: tuple                # Output shape (batch, C, H, W)\n        activation: str = \"relu\"\n        batch_norm: bool = True\n        dropout_rate: float = 0.0\n\n    class discriminator:\n        hidden_dims: list[int]             # Discriminator hidden dimensions\n        activation: str = \"leaky_relu\"\n        leaky_relu_slope: float = 0.2\n        batch_norm: bool = False\n        dropout_rate: float = 0.3\n        use_spectral_norm: bool = False\n</code></pre> <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN","title":"artifex.generative_models.models.gan.GAN","text":"<pre><code>GAN(\n    config: GANConfig,\n    *,\n    rngs: Rngs,\n    precision: Precision | None = None,\n)\n</code></pre> <p>               Bases: <code>GenerativeModel</code></p> <p>Generative Adversarial Network (GAN) implementation.</p> <p>Base GAN using fully-connected Generator and Discriminator. For convolutional GANs, use DCGAN or other specialized subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GANConfig</code> <p>GANConfig with nested GeneratorConfig and DiscriminatorConfig</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <code>precision</code> <code>Precision | None</code> <p>Numerical precision for computations</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None or missing 'sample' stream</p> <code>TypeError</code> <p>If config is not GANConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.rngs","title":"rngs  <code>instance-attribute</code>","text":"<pre><code>rngs = rngs\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.loss_type","title":"loss_type  <code>instance-attribute</code>","text":"<pre><code>loss_type = loss_type\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.gradient_penalty_weight","title":"gradient_penalty_weight  <code>instance-attribute</code>","text":"<pre><code>gradient_penalty_weight = gradient_penalty_weight\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.generator","title":"generator  <code>instance-attribute</code>","text":"<pre><code>generator = Generator(config=generator, rngs=rngs)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.discriminator","title":"discriminator  <code>instance-attribute</code>","text":"<pre><code>discriminator = Discriminator(\n    config=discriminator, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    batch_size: int | None = None,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Generate samples from the generator.</p> <p>Note: Uses stored self.rngs for sampling. RNG automatically advances each call.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>batch_size</code> <code>int | None</code> <p>Alternative way to specify number of samples (for compatibility)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples of shape (num_samples, *output_shape[1:])</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.GAN.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn(\n    batch: dict[str, Any],\n    model_outputs: dict[str, Any],\n    **kwargs: Any,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compute GAN loss for training.</p> <p>Note: Uses stored self.rngs for sampling. RNG automatically advances each call.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>Input batch containing real data (dict with 'x' or 'data' key, or raw array)</p> required <code>model_outputs</code> <code>dict[str, Any]</code> <p>Model outputs (unused for GAN loss computation)</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing: - loss: Total combined loss (generator + discriminator) - generator_loss: Generator loss component - discriminator_loss: Discriminator loss component - real_scores_mean: Mean discriminator score on real data - fake_scores_mean: Mean discriminator score on fake data</p>"},{"location":"api/models/gan/#__call__x-rngsnone-trainingfalse-kwargs","title":"<code>__call__(x, rngs=None, training=False, **kwargs)</code>","text":"<p>Forward pass through the GAN (runs discriminator only).</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input data</li> <li><code>rngs</code> (<code>nnx.Rngs</code>, optional): Random number generators</li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary with keys:</li> <li><code>\"real_scores\"</code>: Discriminator scores for real data</li> <li><code>\"fake_scores\"</code>: <code>None</code> (computed in <code>loss_fn</code>)</li> <li><code>\"fake_samples\"</code>: <code>None</code> (computed in <code>loss_fn</code>)</li> </ul>"},{"location":"api/models/gan/#generaten_samples1-rngsnone-batch_sizenone-kwargs","title":"<code>generate(n_samples=1, rngs=None, batch_size=None, **kwargs)</code>","text":"<p>Generate samples from the generator.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code> (<code>int</code>): Number of samples to generate</li> <li><code>rngs</code> (<code>nnx.Rngs</code>, optional): Random number generators</li> <li><code>batch_size</code> (<code>int</code>, optional): Alternative to <code>n_samples</code></li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated samples</li> </ul>"},{"location":"api/models/gan/#loss_fnbatch-model_outputs-rngsnone-kwargs","title":"<code>loss_fn(batch, model_outputs, rngs=None, **kwargs)</code>","text":"<p>Compute GAN loss for training.</p> <p>Parameters:</p> <ul> <li><code>batch</code> (<code>dict</code> or <code>jax.Array</code>): Input batch (real data)</li> <li><code>model_outputs</code> (<code>dict</code>): Model outputs (unused for GAN)</li> <li><code>rngs</code> (<code>nnx.Rngs</code>, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary with losses:</li> <li><code>\"loss\"</code>: Total loss (generator + discriminator)</li> <li><code>\"generator_loss\"</code>: Generator loss</li> <li><code>\"discriminator_loss\"</code>: Discriminator loss</li> <li><code>\"real_scores_mean\"</code>: Mean discriminator score for real samples</li> <li><code>\"fake_scores_mean\"</code>: Mean discriminator score for fake samples</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import GAN\nfrom flax import nnx\n\n# Create configuration\nclass GANConfig:\n    latent_dim = 100\n    loss_type = \"vanilla\"\n\n    class generator:\n        hidden_dims = [256, 512]\n        output_shape = (1, 1, 28, 28)\n        activation = \"relu\"\n        batch_norm = True\n        dropout_rate = 0.0\n\n    class discriminator:\n        hidden_dims = [512, 256]\n        activation = \"leaky_relu\"\n        leaky_relu_slope = 0.2\n        batch_norm = False\n        dropout_rate = 0.3\n        use_spectral_norm = False\n\n# Create GAN\ngan = GAN(GANConfig(), rngs=nnx.Rngs(params=0, dropout=1, sample=2))\n\n# Generate samples\nsamples = gan.generate(n_samples=16, rngs=nnx.Rngs(sample=0))\n\n# Compute loss\nimport jax.numpy as jnp\nbatch = jnp.ones((32, 1, 28, 28))\nlosses = gan.loss_fn(batch, None, rngs=nnx.Rngs(sample=0))\nprint(f\"Generator Loss: {losses['generator_loss']:.4f}\")\nprint(f\"Discriminator Loss: {losses['discriminator_loss']:.4f}\")\n</code></pre>"},{"location":"api/models/gan/#dcgan","title":"DCGAN","text":""},{"location":"api/models/gan/#dcgangenerator","title":"DCGANGenerator","text":"<p>Deep Convolutional GAN generator using transposed convolutions.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>output_shape</code> <code>tuple[int, ...]</code> Required Output image shape <code>(C, H, W)</code> <code>latent_dim</code> <code>int</code> <code>100</code> Latent space dimension <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(256, 128, 64, 32)</code> Channel dimensions per layer <code>activation</code> <code>callable</code> <code>jax.nn.relu</code> Activation function <code>batch_norm</code> <code>bool</code> <code>True</code> Use batch normalization <code>dropout_rate</code> <code>float</code> <code>0.0</code> Dropout rate <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANGenerator","title":"artifex.generative_models.models.gan.DCGANGenerator","text":"<pre><code>DCGANGenerator(config: ConvGeneratorConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Deep Convolutional GAN Generator.</p> <p>Uses transposed convolutions for progressive upsampling from latent vector to output image. All configuration comes from ConvGeneratorConfig.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConvGeneratorConfig</code> <p>ConvGeneratorConfig with all architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConvGeneratorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANGenerator.init_h","title":"init_h  <code>instance-attribute</code>","text":"<pre><code>init_h = max(\n    1, height // stride_factor**num_upsample_layers\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANGenerator.init_w","title":"init_w  <code>instance-attribute</code>","text":"<pre><code>init_w = max(1, width // stride_factor**num_upsample_layers)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANGenerator.initial_linear","title":"initial_linear  <code>instance-attribute</code>","text":"<pre><code>initial_linear = Linear(\n    in_features=latent_dim,\n    out_features=initial_features,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANGenerator.conv_transpose_layers","title":"conv_transpose_layers  <code>instance-attribute</code>","text":"<pre><code>conv_transpose_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANGenerator.dcgan_batch_norms","title":"dcgan_batch_norms  <code>instance-attribute</code>","text":"<pre><code>dcgan_batch_norms = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANGenerator.output_conv","title":"output_conv  <code>instance-attribute</code>","text":"<pre><code>output_conv = ConvTranspose(\n    in_features=hidden_dims_list[-1],\n    out_features=channels,\n    kernel_size=kernel_size,\n    strides=stride,\n    padding=padding,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#__call__z-trainingtrue","title":"<code>__call__(z, training=True)</code>","text":"<p>Generate images from latent vectors.</p> <p>Parameters:</p> <ul> <li><code>z</code> (<code>jax.Array</code>): Latent vectors of shape <code>(batch_size, latent_dim)</code></li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated images of shape <code>(batch_size, C, H, W)</code></li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import DCGANGenerator\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\ngenerator = DCGANGenerator(\n    output_shape=(3, 64, 64),            # RGB 64\u00d764 images\n    latent_dim=100,\n    hidden_dims=(256, 128, 64, 32),\n    activation=jax.nn.relu,\n    batch_norm=True,\n    rngs=nnx.Rngs(params=0),\n)\n\n# Generate samples\nz = jax.random.normal(jax.random.key(0), (16, 100))\nimages = generator(z, training=False)\nprint(images.shape)  # (16, 3, 64, 64)\n</code></pre>"},{"location":"api/models/gan/#dcgandiscriminator","title":"DCGANDiscriminator","text":"<p>Deep Convolutional GAN discriminator using strided convolutions.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>input_shape</code> <code>tuple[int, ...]</code> Required Input image shape <code>(C, H, W)</code> <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(32, 64, 128, 256)</code> Channel dimensions per layer <code>activation</code> <code>callable</code> <code>jax.nn.leaky_relu</code> Activation function <code>leaky_relu_slope</code> <code>float</code> <code>0.2</code> Negative slope for LeakyReLU <code>batch_norm</code> <code>bool</code> <code>False</code> Use batch normalization <code>dropout_rate</code> <code>float</code> <code>0.3</code> Dropout rate <code>use_spectral_norm</code> <code>bool</code> <code>True</code> Use spectral normalization <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANDiscriminator","title":"artifex.generative_models.models.gan.DCGANDiscriminator","text":"<pre><code>DCGANDiscriminator(\n    config: ConvDiscriminatorConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Discriminator</code></p> <p>Deep Convolutional GAN Discriminator.</p> <p>Uses strided convolutions for progressive downsampling from input image to binary classification. All configuration comes from ConvDiscriminatorConfig.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConvDiscriminatorConfig</code> <p>ConvDiscriminatorConfig with all architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConvDiscriminatorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANDiscriminator.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANDiscriminator.dcgan_batch_norms","title":"dcgan_batch_norms  <code>instance-attribute</code>","text":"<pre><code>dcgan_batch_norms = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGANDiscriminator.final_linear","title":"final_linear  <code>instance-attribute</code>","text":"<pre><code>final_linear = Linear(\n    in_features=final_features,\n    out_features=output_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#__call__x-trainingtrue","title":"<code>__call__(x, training=True)</code>","text":"<p>Classify images as real or fake.</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input images of shape <code>(batch_size, C, H, W)</code></li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Discrimination scores of shape <code>(batch_size, 1)</code></li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import DCGANDiscriminator\nfrom flax import nnx\nimport jax.numpy as jnp\n\ndiscriminator = DCGANDiscriminator(\n    input_shape=(3, 64, 64),\n    hidden_dims=(32, 64, 128, 256),\n    activation=jax.nn.leaky_relu,\n    leaky_relu_slope=0.2,\n    batch_norm=False,\n    dropout_rate=0.3,\n    use_spectral_norm=True,\n    rngs=nnx.Rngs(params=0, dropout=1),\n)\n\n# Classify images\nimages = jnp.ones((16, 3, 64, 64))\nscores = discriminator(images, training=True)\nprint(scores.shape)  # (16, 1)\n</code></pre>"},{"location":"api/models/gan/#dcgan_1","title":"DCGAN","text":"<p>Complete Deep Convolutional GAN model.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>config</code> <code>DCGANConfiguration</code> Required DCGAN configuration <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Configuration:</p> <p>Use <code>DCGANConfiguration</code> from <code>artifex.generative_models.core.configuration.gan</code>:</p> <pre><code>from artifex.generative_models.core.configuration.gan import DCGANConfiguration\n\nconfig = DCGANConfiguration(\n    image_size=64,                        # Image size (H=W)\n    channels=3,                           # Number of channels\n    latent_dim=100,                       # Latent dimension\n    gen_hidden_dims=(256, 128, 64, 32),  # Generator channels\n    disc_hidden_dims=(32, 64, 128, 256), # Discriminator channels\n    loss_type=\"vanilla\",                  # Loss type\n    generator_lr=0.0002,                  # Generator learning rate\n    discriminator_lr=0.0002,              # Discriminator learning rate\n    beta1=0.5,                            # Adam \u03b21\n    beta2=0.999,                          # Adam \u03b22\n)\n</code></pre> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import DCGAN\nfrom artifex.generative_models.core.configuration.gan import DCGANConfiguration\nfrom flax import nnx\n\nconfig = DCGANConfiguration(\n    image_size=64,\n    channels=3,\n    latent_dim=100,\n    gen_hidden_dims=(256, 128, 64, 32),\n    disc_hidden_dims=(32, 64, 128, 256),\n    loss_type=\"vanilla\",\n)\n\ndcgan = DCGAN(config, rngs=nnx.Rngs(params=0, dropout=1, sample=2))\n\n# Generate samples\nsamples = dcgan.generate(n_samples=16, rngs=nnx.Rngs(sample=0))\nprint(samples.shape)  # (16, 3, 64, 64)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN","title":"artifex.generative_models.models.gan.DCGAN","text":"<pre><code>DCGAN(config: DCGANConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>GAN</code></p> <p>Deep Convolutional GAN (DCGAN) model.</p> <p>Uses DCGANConfig which contains ConvGeneratorConfig and ConvDiscriminatorConfig for complete architecture specification.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DCGANConfig</code> <p>DCGANConfig with nested ConvGeneratorConfig and ConvDiscriminatorConfig</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not DCGANConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.rngs","title":"rngs  <code>instance-attribute</code>","text":"<pre><code>rngs = rngs\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.generator","title":"generator  <code>instance-attribute</code>","text":"<pre><code>generator = DCGANGenerator(config=generator, rngs=rngs)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.discriminator","title":"discriminator  <code>instance-attribute</code>","text":"<pre><code>discriminator = DCGANDiscriminator(\n    config=discriminator, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.loss_type","title":"loss_type  <code>instance-attribute</code>","text":"<pre><code>loss_type = loss_type\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.gradient_penalty_weight","title":"gradient_penalty_weight  <code>instance-attribute</code>","text":"<pre><code>gradient_penalty_weight = gradient_penalty_weight\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.generator_lr","title":"generator_lr  <code>instance-attribute</code>","text":"<pre><code>generator_lr = generator_lr\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.discriminator_lr","title":"discriminator_lr  <code>instance-attribute</code>","text":"<pre><code>discriminator_lr = discriminator_lr\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.beta1","title":"beta1  <code>instance-attribute</code>","text":"<pre><code>beta1 = beta1\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.DCGAN.beta2","title":"beta2  <code>instance-attribute</code>","text":"<pre><code>beta2 = beta2\n</code></pre>"},{"location":"api/models/gan/#wgan","title":"WGAN","text":""},{"location":"api/models/gan/#wgangenerator","title":"WGANGenerator","text":"<p>Wasserstein GAN generator with convolutional architecture.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>output_shape</code> <code>tuple[int, ...]</code> Required Output image shape <code>(C, H, W)</code> <code>latent_dim</code> <code>int</code> <code>100</code> Latent space dimension <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(1024, 512, 256)</code> Channel dimensions <code>activation</code> <code>callable</code> <code>jax.nn.relu</code> Activation function <code>batch_norm</code> <code>bool</code> <code>True</code> Use batch normalization <code>dropout_rate</code> <code>float</code> <code>0.0</code> Dropout rate <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANGenerator","title":"artifex.generative_models.models.gan.WGANGenerator","text":"<pre><code>WGANGenerator(config: ConvGeneratorConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Wasserstein GAN Generator using convolutional architecture.</p> <p>Based on the PyTorch WGAN-GP reference implementation: - Uses ConvTranspose layers like DCGAN - BatchNorm is typically used in WGAN generators - Tanh activation at output</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConvGeneratorConfig</code> <p>ConvGeneratorConfig with all architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConvGeneratorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANGenerator.wgan_activation_fn","title":"wgan_activation_fn  <code>instance-attribute</code>","text":"<pre><code>wgan_activation_fn = _get_activation_fn(activation)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANGenerator.initial_linear","title":"initial_linear  <code>instance-attribute</code>","text":"<pre><code>initial_linear = Linear(\n    in_features=latent_dim,\n    out_features=init_h * init_w * hidden_dims_list[0],\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANGenerator.initial_bn","title":"initial_bn  <code>instance-attribute</code>","text":"<pre><code>initial_bn = BatchNorm(\n    num_features=hidden_dims_list[0],\n    use_running_average=batch_norm_use_running_avg,\n    momentum=batch_norm_momentum,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANGenerator.conv_transpose_layers","title":"conv_transpose_layers  <code>instance-attribute</code>","text":"<pre><code>conv_transpose_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANGenerator.wgan_batch_norm_layers","title":"wgan_batch_norm_layers  <code>instance-attribute</code>","text":"<pre><code>wgan_batch_norm_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANGenerator.output_conv","title":"output_conv  <code>instance-attribute</code>","text":"<pre><code>output_conv = ConvTranspose(\n    in_features=hidden_dims_list[-1],\n    out_features=channels,\n    kernel_size=kernel_size,\n    strides=stride,\n    padding=padding,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#__call__z-trainingtrue_1","title":"<code>__call__(z, training=True)</code>","text":"<p>Generate images from latent vectors.</p> <p>Parameters:</p> <ul> <li><code>z</code> (<code>jax.Array</code>): Latent vectors of shape <code>(batch_size, latent_dim)</code></li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated images of shape <code>(batch_size, C, H, W)</code></li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import WGANGenerator\nfrom flax import nnx\nimport jax\n\ngenerator = WGANGenerator(\n    output_shape=(3, 64, 64),\n    latent_dim=100,\n    hidden_dims=(1024, 512, 256),\n    activation=jax.nn.relu,\n    batch_norm=True,\n    rngs=nnx.Rngs(params=0),\n)\n\nz = jax.random.normal(jax.random.key(0), (16, 100))\nimages = generator(z, training=False)\nprint(images.shape)  # (16, 3, 64, 64)\n</code></pre>"},{"location":"api/models/gan/#wgandiscriminator","title":"WGANDiscriminator","text":"<p>Wasserstein GAN discriminator (critic) with instance normalization.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>input_shape</code> <code>tuple[int, ...]</code> Required Input image shape <code>(C, H, W)</code> <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(256, 512, 1024)</code> Channel dimensions <code>activation</code> <code>callable</code> <code>jax.nn.leaky_relu</code> Activation function <code>leaky_relu_slope</code> <code>float</code> <code>0.2</code> Negative slope for LeakyReLU <code>use_instance_norm</code> <code>bool</code> <code>True</code> Use instance normalization <code>dropout_rate</code> <code>float</code> <code>0.0</code> Dropout rate <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANDiscriminator","title":"artifex.generative_models.models.gan.WGANDiscriminator","text":"<pre><code>WGANDiscriminator(\n    config: ConvDiscriminatorConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Discriminator</code></p> <p>Wasserstein GAN Discriminator (Critic) using convolutional architecture.</p> <p>Key differences from standard discriminator: - Uses InstanceNorm instead of BatchNorm (as per WGAN-GP paper) - No sigmoid activation at the end (outputs raw scores) - LeakyReLU activation</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConvDiscriminatorConfig</code> <p>ConvDiscriminatorConfig with all architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConvDiscriminatorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANDiscriminator.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANDiscriminator.norm_layers","title":"norm_layers  <code>instance-attribute</code>","text":"<pre><code>norm_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANDiscriminator.output_conv","title":"output_conv  <code>instance-attribute</code>","text":"<pre><code>output_conv = Conv(\n    in_features=hidden_dims_list[-1],\n    out_features=1,\n    kernel_size=(4, 4),\n    strides=(1, 1),\n    padding=\"VALID\",\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGANDiscriminator.wgan_use_instance_norm","title":"wgan_use_instance_norm  <code>instance-attribute</code>","text":"<pre><code>wgan_use_instance_norm = use_instance_norm\n</code></pre>"},{"location":"api/models/gan/#__call__x-trainingtrue_1","title":"<code>__call__(x, training=True)</code>","text":"<p>Compute critic scores (no sigmoid activation).</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input images of shape <code>(batch_size, C, H, W)</code></li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Raw critic scores (no sigmoid) of shape <code>(batch_size,)</code></li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import WGANDiscriminator\nfrom flax import nnx\nimport jax.numpy as jnp\n\ndiscriminator = WGANDiscriminator(\n    input_shape=(3, 64, 64),\n    hidden_dims=(256, 512, 1024),\n    activation=jax.nn.leaky_relu,\n    use_instance_norm=True,\n    rngs=nnx.Rngs(params=0),\n)\n\nimages = jnp.ones((16, 3, 64, 64))\nscores = discriminator(images, training=True)\nprint(scores.shape)  # (16,)\n# Note: No sigmoid, scores can be any real number\n</code></pre>"},{"location":"api/models/gan/#wgan_1","title":"WGAN","text":"<p>Complete Wasserstein GAN with Gradient Penalty (WGAN-GP).</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>config</code> <code>ModelConfig</code> Required Model configuration <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <code>precision</code> <code>jax.lax.Precision</code> <code>None</code> Numerical precision <p>Configuration:</p> <pre><code>from artifex.generative_models.core.configuration import ModelConfig\n\nconfig = ModelConfig(\n    input_dim=100,                        # Latent dimension\n    output_dim=(3, 64, 64),               # Output image shape\n    hidden_dims=None,                     # Use defaults\n    metadata={\n        \"gan_params\": {\n            \"gen_hidden_dims\": (1024, 512, 256),\n            \"disc_hidden_dims\": (256, 512, 1024),\n            \"gradient_penalty_weight\": 10.0,     # Lambda for GP\n            \"critic_iterations\": 5,               # Critic updates per generator\n        }\n    }\n)\n</code></pre> <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN","title":"artifex.generative_models.models.gan.WGAN","text":"<pre><code>WGAN(config: WGANConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Wasserstein GAN with Gradient Penalty (WGAN-GP) model.</p> <p>Based on the PyTorch reference implementation with proper convolutional architecture.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WGANConfig</code> <p>WGANConfig with nested ConvGeneratorConfig and ConvDiscriminatorConfig</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not WGANConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.generator","title":"generator  <code>instance-attribute</code>","text":"<pre><code>generator = WGANGenerator(config=gen_config, rngs=rngs)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.discriminator","title":"discriminator  <code>instance-attribute</code>","text":"<pre><code>discriminator = WGANDiscriminator(\n    config=disc_config, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.rngs","title":"rngs  <code>instance-attribute</code>","text":"<pre><code>rngs = rngs\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.lambda_gp","title":"lambda_gp  <code>instance-attribute</code>","text":"<pre><code>lambda_gp = gradient_penalty_weight\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.n_critic","title":"n_critic  <code>instance-attribute</code>","text":"<pre><code>n_critic = critic_iterations\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    rngs: Rngs | None = None,\n    batch_size: int | None = None,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Generate samples from the generator.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>rngs</code> <code>Rngs | None</code> <p>Random number generator</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Alternative way to specify number of samples (for compatibility)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.discriminator_loss","title":"discriminator_loss","text":"<pre><code>discriminator_loss(\n    real_samples: Array, fake_samples: Array, rngs: Rngs\n) -&gt; Array\n</code></pre> <p>Compute WGAN-GP discriminator loss.</p> <p>Parameters:</p> Name Type Description Default <code>real_samples</code> <code>Array</code> <p>Real samples from dataset.</p> required <code>fake_samples</code> <code>Array</code> <p>Generated fake samples.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Discriminator loss value.</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.WGAN.generator_loss","title":"generator_loss","text":"<pre><code>generator_loss(fake_samples: Array) -&gt; Array\n</code></pre> <p>Compute WGAN generator loss.</p> <p>Parameters:</p> Name Type Description Default <code>fake_samples</code> <code>Array</code> <p>Generated fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Generator loss value.</p>"},{"location":"api/models/gan/#generaten_samples1-rngsnone-batch_sizenone-kwargs_1","title":"<code>generate(n_samples=1, rngs=None, batch_size=None, **kwargs)</code>","text":"<p>Generate samples from generator.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code> (<code>int</code>): Number of samples</li> <li><code>rngs</code> (<code>nnx.Rngs</code>, optional): Random number generators</li> <li><code>batch_size</code> (<code>int</code>, optional): Alternative to <code>n_samples</code></li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated samples</li> </ul>"},{"location":"api/models/gan/#discriminator_lossreal_samples-fake_samples-rngs","title":"<code>discriminator_loss(real_samples, fake_samples, rngs)</code>","text":"<p>Compute WGAN-GP discriminator loss with gradient penalty.</p> <p>Parameters:</p> <ul> <li><code>real_samples</code> (<code>jax.Array</code>): Real images</li> <li><code>fake_samples</code> (<code>jax.Array</code>): Generated images</li> <li><code>rngs</code> (<code>nnx.Rngs</code>): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Discriminator loss (scalar)</li> </ul> <p>Loss Formula: $$ \\mathcal{L}D = \\mathbb{E}[D(G(z))] - \\mathbb{E}[D(x)] + \\lambda \\mathbb{E}[(|\\nabla)|_2 - 1)^2] $$}} D(\\hat{x</p>"},{"location":"api/models/gan/#generator_lossfake_samples","title":"<code>generator_loss(fake_samples)</code>","text":"<p>Compute WGAN generator loss.</p> <p>Parameters:</p> <ul> <li><code>fake_samples</code> (<code>jax.Array</code>): Generated images</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generator loss (scalar)</li> </ul> <p>Loss Formula: $$ \\mathcal{L}_G = -\\mathbb{E}[D(G(z))] $$</p> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import WGAN\nfrom artifex.generative_models.core.configuration import ModelConfig\nfrom flax import nnx\n\nconfig = ModelConfig(\n    input_dim=100,\n    output_dim=(3, 64, 64),\n    metadata={\n        \"gan_params\": {\n            \"gen_hidden_dims\": (1024, 512, 256),\n            \"disc_hidden_dims\": (256, 512, 1024),\n            \"gradient_penalty_weight\": 10.0,\n            \"critic_iterations\": 5,\n        }\n    }\n)\n\nwgan = WGAN(config, rngs=nnx.Rngs(params=0, sample=1))\n\n# Generate samples\nsamples = wgan.generate(n_samples=16, rngs=nnx.Rngs(sample=0))\nprint(samples.shape)  # (16, 3, 64, 64)\n\n# Training step\nimport jax\nreal_samples = jax.random.normal(jax.random.key(0), (32, 3, 64, 64))\nz = jax.random.normal(jax.random.key(1), (32, 100))\nfake_samples = wgan.generator(z, training=True)\n\ndisc_loss = wgan.discriminator_loss(real_samples, fake_samples, rngs=nnx.Rngs(params=2))\ngen_loss = wgan.generator_loss(fake_samples)\n</code></pre>"},{"location":"api/models/gan/#compute_gradient_penalty","title":"compute_gradient_penalty","text":"<p>Compute gradient penalty for WGAN-GP.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Function Signature:</p> <pre><code>def compute_gradient_penalty(\n    discriminator: WGANDiscriminator,\n    real_samples: jax.Array,\n    fake_samples: jax.Array,\n    rngs: nnx.Rngs,\n    lambda_gp: float = 10.0,\n) -&gt; jax.Array\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>discriminator</code> <code>WGANDiscriminator</code> Required Discriminator network <code>real_samples</code> <code>jax.Array</code> Required Real images <code>fake_samples</code> <code>jax.Array</code> Required Generated images <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <code>lambda_gp</code> <code>float</code> <code>10.0</code> Gradient penalty weight <p>Returns:</p> <ul> <li><code>jax.Array</code>: Gradient penalty loss (scalar)</li> </ul> <p>Formula: $$ \\text{GP} = \\lambda \\mathbb{E}{\\hat{x}}[(|\\nabla)|_2 - 1)^2] $$}} D(\\hat{x</p> <p>where \\(\\hat{x} = \\epsilon x + (1-\\epsilon)G(z)\\) is a random interpolation.</p> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import compute_gradient_penalty, WGANDiscriminator\nfrom flax import nnx\nimport jax\n\ndiscriminator = WGANDiscriminator(\n    input_shape=(3, 64, 64),\n    rngs=nnx.Rngs(params=0),\n)\n\nreal_samples = jax.random.normal(jax.random.key(0), (32, 3, 64, 64))\nfake_samples = jax.random.normal(jax.random.key(1), (32, 3, 64, 64))\n\ngp = compute_gradient_penalty(\n    discriminator,\n    real_samples,\n    fake_samples,\n    rngs=nnx.Rngs(params=2),\n    lambda_gp=10.0,\n)\nprint(f\"Gradient Penalty: {gp:.4f}\")\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.compute_gradient_penalty","title":"artifex.generative_models.models.gan.compute_gradient_penalty","text":"<pre><code>compute_gradient_penalty(\n    discriminator: WGANDiscriminator,\n    real_samples: Array,\n    fake_samples: Array,\n    rngs: Rngs,\n    lambda_gp: float = 10.0,\n) -&gt; Array\n</code></pre> <p>Compute gradient penalty for WGAN-GP.</p> <p>The gradient penalty enforces the Lipschitz constraint by penalizing the discriminator when the gradient norm deviates from 1.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator</code> <code>WGANDiscriminator</code> <p>The discriminator network.</p> required <code>real_samples</code> <code>Array</code> <p>Real samples from the dataset.</p> required <code>fake_samples</code> <code>Array</code> <p>Generated fake samples.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for interpolation.</p> required <code>lambda_gp</code> <code>float</code> <p>Gradient penalty weight.</p> <code>10.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Gradient penalty loss value.</p>"},{"location":"api/models/gan/#lsgan","title":"LSGAN","text":""},{"location":"api/models/gan/#lsgangenerator","title":"LSGANGenerator","text":"<p>Least Squares GAN generator (same architecture as DCGAN).</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters: Same as <code>DCGANGenerator</code></p> <p>Methods: Same as <code>DCGANGenerator</code></p> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import LSGANGenerator\nfrom flax import nnx\nimport jax\n\ngenerator = LSGANGenerator(\n    output_shape=(3, 64, 64),\n    latent_dim=100,\n    hidden_dims=(512, 256, 128, 64),\n    rngs=nnx.Rngs(params=0),\n)\n\nz = jax.random.normal(jax.random.key(0), (16, 100))\nimages = generator(z, training=False)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANGenerator","title":"artifex.generative_models.models.gan.LSGANGenerator","text":"<pre><code>LSGANGenerator(config: ConvGeneratorConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Least Squares GAN Generator using convolutional architecture.</p> <p>LSGAN uses the same architecture as DCGAN but with least squares loss instead of the standard adversarial loss.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConvGeneratorConfig</code> <p>ConvGeneratorConfig with all architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConvGeneratorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANGenerator.lsgan_activation_fn","title":"lsgan_activation_fn  <code>instance-attribute</code>","text":"<pre><code>lsgan_activation_fn = _get_activation(activation)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANGenerator.initial_linear","title":"initial_linear  <code>instance-attribute</code>","text":"<pre><code>initial_linear = Linear(\n    in_features=latent_dim,\n    out_features=init_h * init_w * hidden_dims_list[0],\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANGenerator.initial_bn","title":"initial_bn  <code>instance-attribute</code>","text":"<pre><code>initial_bn = BatchNorm(\n    num_features=hidden_dims_list[0],\n    use_running_average=batch_norm_use_running_avg,\n    momentum=batch_norm_momentum,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANGenerator.conv_transpose_layers","title":"conv_transpose_layers  <code>instance-attribute</code>","text":"<pre><code>conv_transpose_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANGenerator.lsgan_batch_norm_layers","title":"lsgan_batch_norm_layers  <code>instance-attribute</code>","text":"<pre><code>lsgan_batch_norm_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANGenerator.output_conv","title":"output_conv  <code>instance-attribute</code>","text":"<pre><code>output_conv = ConvTranspose(\n    in_features=hidden_dims_list[-1],\n    out_features=channels,\n    kernel_size=kernel_size,\n    strides=stride,\n    padding=padding,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#lsgandiscriminator","title":"LSGANDiscriminator","text":"<p>Least Squares GAN discriminator (no sigmoid activation).</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters: Same as <code>DCGANDiscriminator</code></p> <p>Key Difference: Output layer has no sigmoid activation (outputs raw logits for least squares loss).</p> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import LSGANDiscriminator\nfrom flax import nnx\nimport jax.numpy as jnp\n\ndiscriminator = LSGANDiscriminator(\n    input_shape=(3, 64, 64),\n    hidden_dims=(64, 128, 256, 512),\n    rngs=nnx.Rngs(params=0, dropout=1),\n)\n\nimages = jnp.ones((16, 3, 64, 64))\nscores = discriminator(images, training=True)\n# Note: Scores are raw logits, not in [0, 1]\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANDiscriminator","title":"artifex.generative_models.models.gan.LSGANDiscriminator","text":"<pre><code>LSGANDiscriminator(\n    config: ConvDiscriminatorConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Discriminator</code></p> <p>Least Squares GAN Discriminator using convolutional architecture.</p> <p>LSGAN discriminator uses the same architecture as DCGAN discriminator but with least squares loss instead of sigmoid cross-entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConvDiscriminatorConfig</code> <p>ConvDiscriminatorConfig with all architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConvDiscriminatorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANDiscriminator.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANDiscriminator.lsgan_batch_norm_layers","title":"lsgan_batch_norm_layers  <code>instance-attribute</code>","text":"<pre><code>lsgan_batch_norm_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANDiscriminator.output_layer","title":"output_layer  <code>instance-attribute</code>","text":"<pre><code>output_layer: Linear = Linear(\n    in_features=final_features, out_features=1, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGANDiscriminator.lsgan_dropout","title":"lsgan_dropout  <code>instance-attribute</code>","text":"<pre><code>lsgan_dropout = Dropout(rate=dropout_rate, rngs=rngs)\n</code></pre>"},{"location":"api/models/gan/#lsgan_1","title":"LSGAN","text":"<p>Complete Least Squares GAN model.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters: Same as base <code>GAN</code></p> <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN","title":"artifex.generative_models.models.gan.LSGAN","text":"<pre><code>LSGAN(config: LSGANConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Least Squares GAN implementation.</p> <p>LSGAN replaces the log loss in the original GAN formulation with a least squares loss, which provides more stable training and better quality gradients for the generator.</p> Reference <p>Mao et al. \"Least Squares Generative Adversarial Networks\" (2017)</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LSGANConfig</code> <p>LSGANConfig with nested ConvGeneratorConfig and ConvDiscriminatorConfig</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not LSGANConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.loss_type","title":"loss_type  <code>instance-attribute</code>","text":"<pre><code>loss_type = 'least_squares'\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.generator","title":"generator  <code>instance-attribute</code>","text":"<pre><code>generator = LSGANGenerator(config=gen_config, rngs=rngs)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.discriminator","title":"discriminator  <code>instance-attribute</code>","text":"<pre><code>discriminator = LSGANDiscriminator(\n    config=disc_config, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.a","title":"a  <code>instance-attribute</code>","text":"<pre><code>a = a\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.b","title":"b  <code>instance-attribute</code>","text":"<pre><code>b = b\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.c","title":"c  <code>instance-attribute</code>","text":"<pre><code>c = c\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.generator_loss","title":"generator_loss","text":"<pre><code>generator_loss(\n    fake_scores: Array,\n    target_real: float = 1.0,\n    reduction: str = \"mean\",\n) -&gt; Array\n</code></pre> <p>Compute LSGAN generator loss.</p> <p>Parameters:</p> Name Type Description Default <code>fake_scores</code> <code>Array</code> <p>Discriminator scores for fake samples</p> required <code>target_real</code> <code>float</code> <p>Target value for fake samples (usually 1.0)</p> <code>1.0</code> <code>reduction</code> <code>str</code> <p>Reduction method ('mean', 'sum', 'none')</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generator loss</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.discriminator_loss","title":"discriminator_loss","text":"<pre><code>discriminator_loss(\n    real_scores: Array,\n    fake_scores: Array,\n    target_real: float = 1.0,\n    target_fake: float = 0.0,\n    reduction: str = \"mean\",\n) -&gt; Array\n</code></pre> <p>Compute LSGAN discriminator loss.</p> <p>Parameters:</p> Name Type Description Default <code>real_scores</code> <code>Array</code> <p>Discriminator scores for real samples</p> required <code>fake_scores</code> <code>Array</code> <p>Discriminator scores for fake samples</p> required <code>target_real</code> <code>float</code> <p>Target value for real samples (usually 1.0)</p> <code>1.0</code> <code>target_fake</code> <code>float</code> <p>Target value for fake samples (usually 0.0)</p> <code>0.0</code> <code>reduction</code> <code>str</code> <p>Reduction method ('mean', 'sum', 'none')</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Array</code> <p>Discriminator loss</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.LSGAN.training_step","title":"training_step","text":"<pre><code>training_step(\n    real_images: Array, latent_vectors: Array\n) -&gt; dict[str, Array]\n</code></pre> <p>Perform a single training step.</p> <p>Note: Use model.train() for training mode and model.eval() for evaluation mode.</p> <p>Parameters:</p> Name Type Description Default <code>real_images</code> <code>Array</code> <p>Batch of real images</p> required <code>latent_vectors</code> <code>Array</code> <p>Batch of latent vectors</p> required <p>Returns:</p> Type Description <code>dict[str, Array]</code> <p>Dictionary with loss values and generated images</p>"},{"location":"api/models/gan/#generator_lossfake_scores-target_real10-reductionmean","title":"<code>generator_loss(fake_scores, target_real=1.0, reduction=\"mean\")</code>","text":"<p>Compute LSGAN generator loss.</p> <p>Formula: $$ \\mathcal{L}_G = \\frac{1}{2}\\mathbb{E}[(D(G(z)) - c)^2] $$</p> <p>where \\(c\\) is the target value for fake samples (usually 1.0).</p>"},{"location":"api/models/gan/#discriminator_lossreal_scores-fake_scores-target_real10-target_fake00-reductionmean","title":"<code>discriminator_loss(real_scores, fake_scores, target_real=1.0, target_fake=0.0, reduction=\"mean\")</code>","text":"<p>Compute LSGAN discriminator loss.</p> <p>Formula: $$ \\mathcal{L}_D = \\frac{1}{2}\\mathbb{E}[(D(x) - b)^2] + \\frac{1}{2}\\mathbb{E}[D(G(z))^2] $$</p> <p>where \\(b\\) is the target for real samples (usually 1.0).</p> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import LSGAN\nfrom artifex.generative_models.core.configuration import ModelConfig\nfrom flax import nnx\n\nconfig = ModelConfig(\n    input_dim=100,\n    output_dim=(3, 64, 64),\n    hidden_dims=[512, 256, 128, 64],\n)\n\nlsgan = LSGAN(config, rngs=nnx.Rngs(params=0, dropout=1, sample=2))\n\n# Generate samples\nsamples = lsgan.generate(n_samples=16, rngs=nnx.Rngs(sample=0))\n\n# Compute losses\nimport jax\nreal_images = jax.random.normal(jax.random.key(0), (32, 3, 64, 64))\nz = jax.random.normal(jax.random.key(1), (32, 100))\nfake_images = lsgan.generator(z, training=True)\n\nreal_scores = lsgan.discriminator(real_images, training=True)\nfake_scores = lsgan.discriminator(fake_images, training=True)\n\ngen_loss = lsgan.generator_loss(fake_scores)\ndisc_loss = lsgan.discriminator_loss(real_scores, fake_scores)\n</code></pre>"},{"location":"api/models/gan/#conditional-gan","title":"Conditional GAN","text":""},{"location":"api/models/gan/#conditionalgenerator","title":"ConditionalGenerator","text":"<p>Conditional GAN generator that takes class labels as input.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>output_shape</code> <code>tuple[int, ...]</code> Required Output image shape <code>(C, H, W)</code> <code>num_classes</code> <code>int</code> Required Number of classes <code>latent_dim</code> <code>int</code> <code>100</code> Latent space dimension <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(512, 256, 128, 64)</code> Channel dimensions <code>activation</code> <code>callable</code> <code>jax.nn.relu</code> Activation function <code>batch_norm</code> <code>bool</code> <code>True</code> Use batch normalization <code>dropout_rate</code> <code>float</code> <code>0.0</code> Dropout rate <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator","title":"artifex.generative_models.models.gan.ConditionalGenerator","text":"<pre><code>ConditionalGenerator(\n    config: ConditionalGeneratorConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Generator</code></p> <p>Conditional GAN Generator using convolutional architecture.</p> <p>The generator is conditioned on class labels by concatenating the label embedding with the noise vector before passing through the network.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConditionalGeneratorConfig</code> <p>ConditionalGeneratorConfig with network architecture and conditional parameters (num_classes, embedding_dim via config.conditional)</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConditionalGeneratorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.num_classes","title":"num_classes  <code>instance-attribute</code>","text":"<pre><code>num_classes = num_classes\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.embedding_dim","title":"embedding_dim  <code>instance-attribute</code>","text":"<pre><code>embedding_dim = embedding_dim\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.activation_fn","title":"activation_fn  <code>instance-attribute</code>","text":"<pre><code>activation_fn = _get_activation_fn(activation)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.label_embedding","title":"label_embedding  <code>instance-attribute</code>","text":"<pre><code>label_embedding = Linear(\n    in_features=num_classes,\n    out_features=embedding_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.init_h","title":"init_h  <code>instance-attribute</code>","text":"<pre><code>init_h = target_size // 2 ** total_upsample_layers\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.init_w","title":"init_w  <code>instance-attribute</code>","text":"<pre><code>init_w = target_size // 2 ** total_upsample_layers\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.initial_projection","title":"initial_projection  <code>instance-attribute</code>","text":"<pre><code>initial_projection = Linear(\n    in_features=combined_input_dim,\n    out_features=init_h * init_w * hidden_dims_list[0],\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.initial_bn","title":"initial_bn  <code>instance-attribute</code>","text":"<pre><code>initial_bn = BatchNorm(\n    num_features=hidden_dims_list[0],\n    use_running_average=batch_norm_use_running_avg,\n    momentum=batch_norm_momentum,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.conv_transpose_layers","title":"conv_transpose_layers  <code>instance-attribute</code>","text":"<pre><code>conv_transpose_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.batch_norm_layers","title":"batch_norm_layers  <code>instance-attribute</code>","text":"<pre><code>batch_norm_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGenerator.output_conv","title":"output_conv  <code>instance-attribute</code>","text":"<pre><code>output_conv = ConvTranspose(\n    in_features=hidden_dims_list[-1],\n    out_features=channels,\n    kernel_size=kernel_size,\n    strides=stride,\n    padding=padding,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#__call__z-labels-trainingtrue","title":"<code>__call__(z, labels, training=True)</code>","text":"<p>Generate samples conditioned on labels.</p> <p>Parameters:</p> <ul> <li><code>z</code> (<code>jax.Array</code>): Latent vectors of shape <code>(batch_size, latent_dim)</code></li> <li><code>labels</code> (<code>jax.Array</code>): One-hot encoded labels of shape <code>(batch_size, num_classes)</code></li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated images of shape <code>(batch_size, C, H, W)</code></li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import ConditionalGenerator\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\ngenerator = ConditionalGenerator(\n    output_shape=(1, 28, 28),           # MNIST\n    num_classes=10,\n    latent_dim=100,\n    hidden_dims=(512, 256, 128, 64),\n    rngs=nnx.Rngs(params=0),\n)\n\n# Generate specific digits\nz = jax.random.normal(jax.random.key(0), (10, 100))\nlabels = jax.nn.one_hot(jnp.arange(10), 10)  # One of each digit\nimages = generator(z, labels, training=False)\nprint(images.shape)  # (10, 1, 28, 28)\n</code></pre>"},{"location":"api/models/gan/#conditionaldiscriminator","title":"ConditionalDiscriminator","text":"<p>Conditional GAN discriminator that takes class labels as input.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>input_shape</code> <code>tuple[int, ...]</code> Required Input image shape <code>(C, H, W)</code> <code>num_classes</code> <code>int</code> Required Number of classes <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(64, 128, 256, 512)</code> Channel dimensions <code>activation</code> <code>callable</code> <code>jax.nn.leaky_relu</code> Activation function <code>leaky_relu_slope</code> <code>float</code> <code>0.2</code> Negative slope for LeakyReLU <code>batch_norm</code> <code>bool</code> <code>False</code> Use batch normalization <code>dropout_rate</code> <code>float</code> <code>0.0</code> Dropout rate <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator","title":"artifex.generative_models.models.gan.ConditionalDiscriminator","text":"<pre><code>ConditionalDiscriminator(\n    config: ConditionalDiscriminatorConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Discriminator</code></p> <p>Conditional GAN Discriminator using convolutional architecture.</p> <p>The discriminator is conditioned on class labels by concatenating the label embedding with the input image before passing through the network.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConditionalDiscriminatorConfig</code> <p>ConditionalDiscriminatorConfig with network architecture and conditional parameters (num_classes, embedding_dim via config.conditional)</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConditionalDiscriminatorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.input_shape","title":"input_shape  <code>instance-attribute</code>","text":"<pre><code>input_shape = input_shape\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.num_classes","title":"num_classes  <code>instance-attribute</code>","text":"<pre><code>num_classes = num_classes\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.embedding_dim","title":"embedding_dim  <code>instance-attribute</code>","text":"<pre><code>embedding_dim = embedding_dim\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.leaky_relu_slope","title":"leaky_relu_slope  <code>instance-attribute</code>","text":"<pre><code>leaky_relu_slope = leaky_relu_slope\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.expected_height","title":"expected_height  <code>instance-attribute</code>","text":"<pre><code>expected_height = height\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.expected_width","title":"expected_width  <code>instance-attribute</code>","text":"<pre><code>expected_width = width\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.expected_channels","title":"expected_channels  <code>instance-attribute</code>","text":"<pre><code>expected_channels = channels\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.label_embedding","title":"label_embedding  <code>instance-attribute</code>","text":"<pre><code>label_embedding = Linear(\n    in_features=num_classes,\n    out_features=height * width,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalDiscriminator.output_conv","title":"output_conv  <code>instance-attribute</code>","text":"<pre><code>output_conv = Conv(\n    in_features=hidden_dims_list[-1],\n    out_features=1,\n    kernel_size=kernel_size,\n    strides=stride_first,\n    padding=padding,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#__call__x-labels-trainingtrue","title":"<code>__call__(x, labels, training=True)</code>","text":"<p>Classify samples conditioned on labels.</p> <p>Parameters:</p> <ul> <li><code>x</code> (<code>jax.Array</code>): Input images of shape <code>(batch_size, C, H, W)</code></li> <li><code>labels</code> (<code>jax.Array</code>): One-hot encoded labels of shape <code>(batch_size, num_classes)</code></li> <li><code>training</code> (<code>bool</code>): Whether in training mode</li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Discrimination scores of shape <code>(batch_size,)</code></li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import ConditionalDiscriminator\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\ndiscriminator = ConditionalDiscriminator(\n    input_shape=(1, 28, 28),\n    num_classes=10,\n    hidden_dims=(64, 128, 256, 512),\n    rngs=nnx.Rngs(params=0, dropout=1),\n)\n\n# Classify samples with labels\nimages = jax.random.normal(jax.random.key(0), (32, 1, 28, 28))\nlabels = jax.nn.one_hot(jnp.zeros(32, dtype=int), 10)  # All zeros\nscores = discriminator(images, labels, training=True)\nprint(scores.shape)  # (32,)\n</code></pre>"},{"location":"api/models/gan/#conditionalgan","title":"ConditionalGAN","text":"<p>Complete Conditional GAN model.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>config</code> <code>ModelConfig</code> Required Model configuration <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <code>precision</code> <code>jax.lax.Precision</code> <code>None</code> Numerical precision <p>Configuration:</p> <pre><code>from artifex.generative_models.core.configuration import ModelConfig\n\nconfig = ModelConfig(\n    input_dim=100,                        # Latent dimension\n    output_dim=(1, 28, 28),               # MNIST shape\n    metadata={\n        \"gan_params\": {\n            \"num_classes\": 10,                      # Number of classes\n            \"gen_hidden_dims\": (512, 256, 128, 64),\n            \"discriminator_features\": [64, 128, 256, 512],\n        }\n    }\n)\n</code></pre> <p>Methods:</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN","title":"artifex.generative_models.models.gan.ConditionalGAN","text":"<pre><code>ConditionalGAN(config: ConditionalGANConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Conditional Generative Adversarial Network (CGAN).</p> <p>Based on \"Conditional Generative Adversarial Nets\" by Mirza &amp; Osindero (2014). The generator and discriminator are both conditioned on class labels.</p> <p>Uses composition pattern: conditional parameters (num_classes, embedding_dim) are embedded in the nested ConditionalGeneratorConfig and ConditionalDiscriminatorConfig via ConditionalParams.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConditionalGANConfig</code> <p>ConditionalGANConfig with nested ConditionalGeneratorConfig and ConditionalDiscriminatorConfig. All parameters are specified in the config objects.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not ConditionalGANConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.generator","title":"generator  <code>instance-attribute</code>","text":"<pre><code>generator = ConditionalGenerator(\n    config=gen_config, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.discriminator","title":"discriminator  <code>instance-attribute</code>","text":"<pre><code>discriminator = ConditionalDiscriminator(\n    config=disc_config, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.rngs","title":"rngs  <code>instance-attribute</code>","text":"<pre><code>rngs = rngs\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.num_classes","title":"num_classes  <code>instance-attribute</code>","text":"<pre><code>num_classes = num_classes\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    labels: Array | None = None,\n    *,\n    rngs: Rngs | None = None,\n    batch_size: int | None = None,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Generate conditional samples from the generator.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>labels</code> <code>Array | None</code> <p>One-hot encoded labels of shape (n_samples, num_classes)</p> <code>None</code> <code>rngs</code> <code>Rngs | None</code> <p>Random number generator</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Alternative way to specify number of samples (for compatibility)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.discriminator_loss","title":"discriminator_loss","text":"<pre><code>discriminator_loss(\n    real_samples: Array,\n    fake_samples: Array,\n    real_labels: Array,\n    fake_labels: Array,\n) -&gt; Array\n</code></pre> <p>Compute conditional discriminator loss.</p> <p>Parameters:</p> Name Type Description Default <code>real_samples</code> <code>Array</code> <p>Real samples from dataset.</p> required <code>fake_samples</code> <code>Array</code> <p>Generated fake samples.</p> required <code>real_labels</code> <code>Array</code> <p>Labels for real samples.</p> required <code>fake_labels</code> <code>Array</code> <p>Labels for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Discriminator loss value.</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.ConditionalGAN.generator_loss","title":"generator_loss","text":"<pre><code>generator_loss(\n    fake_samples: Array, fake_labels: Array\n) -&gt; Array\n</code></pre> <p>Compute conditional generator loss.</p> <p>Parameters:</p> Name Type Description Default <code>fake_samples</code> <code>Array</code> <p>Generated fake samples.</p> required <code>fake_labels</code> <code>Array</code> <p>Labels for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Generator loss value.</p>"},{"location":"api/models/gan/#generaten_samples1-labelsnone-rngsnone-batch_sizenone-kwargs","title":"<code>generate(n_samples=1, labels=None, rngs=None, batch_size=None, **kwargs)</code>","text":"<p>Generate conditional samples.</p> <p>Parameters:</p> <ul> <li><code>n_samples</code> (<code>int</code>): Number of samples</li> <li><code>labels</code> (<code>jax.Array</code>, optional): One-hot encoded labels. If <code>None</code>, random labels are used.</li> <li><code>rngs</code> (<code>nnx.Rngs</code>, optional): Random number generators</li> <li><code>batch_size</code> (<code>int</code>, optional): Alternative to <code>n_samples</code></li> </ul> <p>Returns:</p> <ul> <li><code>jax.Array</code>: Generated samples</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import ConditionalGAN\nfrom artifex.generative_models.core.configuration import ModelConfig\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\nconfig = ModelConfig(\n    input_dim=100,\n    output_dim=(1, 28, 28),\n    metadata={\n        \"gan_params\": {\n            \"num_classes\": 10,\n            \"gen_hidden_dims\": (512, 256, 128, 64),\n            \"discriminator_features\": [64, 128, 256, 512],\n        }\n    }\n)\n\ncgan = ConditionalGAN(config, rngs=nnx.Rngs(params=0, dropout=1, sample=2))\n\n# Generate specific digits\nlabels = jax.nn.one_hot(jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 10)\nsamples = cgan.generate(n_samples=10, labels=labels, rngs=nnx.Rngs(sample=0))\nprint(samples.shape)  # (10, 1, 28, 28)\n</code></pre>"},{"location":"api/models/gan/#cyclegan","title":"CycleGAN","text":""},{"location":"api/models/gan/#cyclegangenerator","title":"CycleGANGenerator","text":"<p>CycleGAN generator for image-to-image translation.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>output_shape</code> <code>tuple[int, ...]</code> Required Output image shape <code>(C, H, W)</code> <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(64, 128, 256)</code> Channel dimensions <code>num_residual_blocks</code> <code>int</code> <code>9</code> Number of ResNet blocks <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator","title":"artifex.generative_models.models.gan.CycleGANGenerator","text":"<pre><code>CycleGANGenerator(\n    config: CycleGANGeneratorConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>CycleGAN Generator for image-to-image translation.</p> <p>Uses a ResNet-based architecture with reflection padding as described in the original CycleGAN paper. This follows the pytorch reference implementation more closely.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CycleGANGeneratorConfig</code> <p>CycleGANGeneratorConfig with network architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not CycleGANGeneratorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.input_shape","title":"input_shape  <code>instance-attribute</code>","text":"<pre><code>input_shape = input_shape\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.output_shape","title":"output_shape  <code>instance-attribute</code>","text":"<pre><code>output_shape = output_shape\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.hidden_dims","title":"hidden_dims  <code>instance-attribute</code>","text":"<pre><code>hidden_dims = list(hidden_dims)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.n_residual_blocks","title":"n_residual_blocks  <code>instance-attribute</code>","text":"<pre><code>n_residual_blocks = n_residual_blocks\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.batch_norm","title":"batch_norm  <code>instance-attribute</code>","text":"<pre><code>batch_norm = batch_norm\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.dropout_rate","title":"dropout_rate  <code>instance-attribute</code>","text":"<pre><code>dropout_rate = dropout_rate\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.use_skip_connections","title":"use_skip_connections  <code>instance-attribute</code>","text":"<pre><code>use_skip_connections = use_skip_connections\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = getattr(nnx, activation_name)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.initial_conv","title":"initial_conv  <code>instance-attribute</code>","text":"<pre><code>initial_conv = Conv(\n    in_features=input_channels,\n    out_features=hidden_dims[0],\n    kernel_size=(7, 7),\n    strides=(1, 1),\n    padding=\"SAME\",\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.initial_norm","title":"initial_norm  <code>instance-attribute</code>","text":"<pre><code>initial_norm = BatchNorm(\n    num_features=hidden_dims[0], rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.downsample_layers","title":"downsample_layers  <code>instance-attribute</code>","text":"<pre><code>downsample_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.downsample_norms","title":"downsample_norms  <code>instance-attribute</code>","text":"<pre><code>downsample_norms = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.residual_blocks","title":"residual_blocks  <code>instance-attribute</code>","text":"<pre><code>residual_blocks = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.upsample_layers","title":"upsample_layers  <code>instance-attribute</code>","text":"<pre><code>upsample_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.upsample_norms","title":"upsample_norms  <code>instance-attribute</code>","text":"<pre><code>upsample_norms = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.output_conv","title":"output_conv  <code>instance-attribute</code>","text":"<pre><code>output_conv = Conv(\n    in_features=hidden_dims[0],\n    out_features=output_channels,\n    kernel_size=(7, 7),\n    strides=(1, 1),\n    padding=\"SAME\",\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANGenerator.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = None\n</code></pre>"},{"location":"api/models/gan/#cyclegandiscriminator","title":"CycleGANDiscriminator","text":"<p>CycleGAN PatchGAN discriminator.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>input_shape</code> <code>tuple[int, ...]</code> Required Input image shape <code>(C, H, W)</code> <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(64, 128, 256, 512)</code> Channel dimensions <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator","title":"artifex.generative_models.models.gan.CycleGANDiscriminator","text":"<pre><code>CycleGANDiscriminator(\n    config: PatchGANDiscriminatorConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>CycleGAN Discriminator (PatchGAN-style).</p> <p>Uses a PatchGAN discriminator that classifies patches of the input as real or fake, rather than the entire image.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PatchGANDiscriminatorConfig</code> <p>PatchGANDiscriminatorConfig with network architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not PatchGANDiscriminatorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.input_shape","title":"input_shape  <code>instance-attribute</code>","text":"<pre><code>input_shape = input_shape\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.hidden_dims","title":"hidden_dims  <code>instance-attribute</code>","text":"<pre><code>hidden_dims = list(hidden_dims)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.batch_norm","title":"batch_norm  <code>instance-attribute</code>","text":"<pre><code>batch_norm = batch_norm\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.dropout_rate","title":"dropout_rate  <code>instance-attribute</code>","text":"<pre><code>dropout_rate = dropout_rate\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = lambda x: leaky_relu(\n    x, negative_slope=leaky_relu_slope\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.norm_layers","title":"norm_layers  <code>instance-attribute</code>","text":"<pre><code>norm_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.final_conv","title":"final_conv  <code>instance-attribute</code>","text":"<pre><code>final_conv = Conv(\n    in_features=hidden_dims[-1],\n    out_features=1,\n    kernel_size=kernel_size,\n    strides=(1, 1),\n    padding=padding,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGANDiscriminator.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = None\n</code></pre>"},{"location":"api/models/gan/#cyclegan_1","title":"CycleGAN","text":"<p>Complete CycleGAN model for unpaired image-to-image translation.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Key Features:</p> <ul> <li>Two generators: <code>G: X \u2192 Y</code> and <code>F: Y \u2192 X</code></li> <li>Two discriminators: <code>D_X</code> and <code>D_Y</code></li> <li>Cycle consistency loss: <code>x \u2192 G(x) \u2192 F(G(x)) \u2248 x</code></li> <li>Identity loss (optional): <code>F(x) \u2248 x</code> if <code>x \u2208 X</code></li> </ul> <p>Parameters:</p> Parameter Type Default Description <code>input_shape_x</code> <code>tuple</code> Required Domain X image shape <code>(C, H, W)</code> <code>input_shape_y</code> <code>tuple</code> Required Domain Y image shape <code>(C, H, W)</code> <code>gen_hidden_dims</code> <code>tuple</code> <code>(64, 128, 256)</code> Generator channels <code>disc_hidden_dims</code> <code>tuple</code> <code>(64, 128, 256)</code> Discriminator channels <code>cycle_weight</code> <code>float</code> <code>10.0</code> Cycle consistency weight <code>identity_weight</code> <code>float</code> <code>0.5</code> Identity loss weight <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import CycleGAN\nfrom flax import nnx\nimport jax\n\ncyclegan = CycleGAN(\n    input_shape_x=(3, 256, 256),         # Horses\n    input_shape_y=(3, 256, 256),         # Zebras\n    gen_hidden_dims=(64, 128, 256),\n    disc_hidden_dims=(64, 128, 256),\n    cycle_weight=10.0,\n    identity_weight=0.5,\n    rngs=nnx.Rngs(params=0, dropout=1),\n)\n\n# Translate horse to zebra\nhorse_images = jax.random.normal(jax.random.key(0), (4, 3, 256, 256))\nzebra_images = cyclegan.generator_g(horse_images, training=False)\nprint(zebra_images.shape)  # (4, 3, 256, 256)\n\n# Translate zebra back to horse (cycle consistency)\nreconstructed_horses = cyclegan.generator_f(zebra_images, training=False)\nprint(reconstructed_horses.shape)  # (4, 3, 256, 256)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN","title":"artifex.generative_models.models.gan.CycleGAN","text":"<pre><code>CycleGAN(config: CycleGANConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>GenerativeModel</code></p> <p>CycleGAN for unpaired image-to-image translation.</p> <p>Implements the complete CycleGAN architecture with two generators and two discriminators for bidirectional domain translation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CycleGANConfig</code> <p>CycleGANConfig with nested CycleGANGeneratorConfig and    PatchGANDiscriminatorConfig objects</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not CycleGANConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.input_shape_a","title":"input_shape_a  <code>instance-attribute</code>","text":"<pre><code>input_shape_a = input_shape_a\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.input_shape_b","title":"input_shape_b  <code>instance-attribute</code>","text":"<pre><code>input_shape_b = input_shape_b\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.lambda_cycle","title":"lambda_cycle  <code>instance-attribute</code>","text":"<pre><code>lambda_cycle = lambda_cycle\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.lambda_identity","title":"lambda_identity  <code>instance-attribute</code>","text":"<pre><code>lambda_identity = lambda_identity\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.generator_a_to_b","title":"generator_a_to_b  <code>instance-attribute</code>","text":"<pre><code>generator_a_to_b = CycleGANGenerator(\n    config=gen_a_to_b_config, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.generator_b_to_a","title":"generator_b_to_a  <code>instance-attribute</code>","text":"<pre><code>generator_b_to_a = CycleGANGenerator(\n    config=gen_b_to_a_config, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.discriminator_a","title":"discriminator_a  <code>instance-attribute</code>","text":"<pre><code>discriminator_a = CycleGANDiscriminator(\n    config=disc_a_config, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.discriminator_b","title":"discriminator_b  <code>instance-attribute</code>","text":"<pre><code>discriminator_b = CycleGANDiscriminator(\n    config=disc_b_config, rngs=rngs\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    rngs: Rngs | None = None,\n    batch_size: int | None = None,\n    domain: str = \"a_to_b\",\n    input_images: Array | None = None,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Generate translated images.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate (ignored if input_images provided).</p> <code>1</code> <code>rngs</code> <code>Rngs | None</code> <p>Random number generators.</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Alternative way to specify number of samples (for compatibility).</p> <code>None</code> <code>domain</code> <code>str</code> <p>Translation direction (\"a_to_b\" or \"b_to_a\").</p> <code>'a_to_b'</code> <code>input_images</code> <code>Array | None</code> <p>Input images to translate. If None, generates random input.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Translated images.</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.compute_cycle_loss","title":"compute_cycle_loss","text":"<pre><code>compute_cycle_loss(\n    real_a: Array, real_b: Array\n) -&gt; tuple[Array, Array]\n</code></pre> <p>Compute cycle consistency losses.</p> <p>Parameters:</p> Name Type Description Default <code>real_a</code> <code>Array</code> <p>Real images from domain A.</p> required <code>real_b</code> <code>Array</code> <p>Real images from domain B.</p> required <p>Returns:</p> Type Description <code>tuple[Array, Array]</code> <p>Tuple of (cycle_loss_a, cycle_loss_b).</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.compute_identity_loss","title":"compute_identity_loss","text":"<pre><code>compute_identity_loss(\n    real_a: Array, real_b: Array\n) -&gt; tuple[Array, Array]\n</code></pre> <p>Compute identity losses.</p> <p>Identity loss encourages generators to preserve color composition when translating images that already belong to the target domain.</p> <p>Parameters:</p> Name Type Description Default <code>real_a</code> <code>Array</code> <p>Real images from domain A.</p> required <code>real_b</code> <code>Array</code> <p>Real images from domain B.</p> required <p>Returns:</p> Type Description <code>tuple[Array, Array]</code> <p>Tuple of (identity_loss_a, identity_loss_b).</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.CycleGAN.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn(\n    batch: dict[str, Any],\n    model_outputs: dict[str, Any],\n    *,\n    rngs: Rngs | None = None,\n    **kwargs: Any,\n) -&gt; dict[str, Any]\n</code></pre> <p>Compute total CycleGAN loss.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Any]</code> <p>Batch containing real images from both domains.</p> required <code>model_outputs</code> <code>dict[str, Any]</code> <p>Model outputs (not used in basic implementation).</p> required <code>rngs</code> <code>Rngs | None</code> <p>Random number generators.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing loss and metrics.</p>"},{"location":"api/models/gan/#patchgan","title":"PatchGAN","text":""},{"location":"api/models/gan/#patchgandiscriminator","title":"PatchGANDiscriminator","text":"<p>PatchGAN discriminator that outputs N\u00d7N array of patch predictions.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>input_shape</code> <code>tuple[int, ...]</code> Required Input image shape <code>(C, H, W)</code> <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(64, 128, 256, 512)</code> Channel dimensions <code>kernel_size</code> <code>int</code> <code>4</code> Convolution kernel size <code>stride</code> <code>int</code> <code>2</code> Convolution stride <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Returns: N\u00d7N array of patch classifications instead of single scalar</p> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import PatchGANDiscriminator\nfrom flax import nnx\nimport jax.numpy as jnp\n\ndiscriminator = PatchGANDiscriminator(\n    input_shape=(3, 256, 256),\n    hidden_dims=(64, 128, 256, 512),\n    kernel_size=4,\n    stride=2,\n    rngs=nnx.Rngs(params=0, dropout=1),\n)\n\nimages = jnp.ones((16, 3, 256, 256))\npatch_scores = discriminator(images, training=True)\nprint(patch_scores.shape)  # (16, H', W', 1) - array of patch predictions\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator","title":"artifex.generative_models.models.gan.PatchGANDiscriminator","text":"<pre><code>PatchGANDiscriminator(\n    config: PatchGANDiscriminatorConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Discriminator</code></p> <p>PatchGAN Discriminator for image-to-image translation.</p> <p>The PatchGAN discriminator classifies whether N\u00d7N patches in an image are real or fake, rather than classifying the entire image. This is particularly effective for image translation tasks where local texture and structure are important.</p> Reference <p>Isola et al. \"Image-to-Image Translation with Conditional Adversarial Networks\" (2017) Wang et al. \"High-Resolution Image Synthesis and Semantic Manipulation\" (2018)</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>PatchGANDiscriminatorConfig</code> <p>PatchGANDiscriminatorConfig with all architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None</p> <code>TypeError</code> <p>If config is not PatchGANDiscriminatorConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_num_filters","title":"patchgan_num_filters  <code>instance-attribute</code>","text":"<pre><code>patchgan_num_filters = num_filters\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_num_layers","title":"patchgan_num_layers  <code>instance-attribute</code>","text":"<pre><code>patchgan_num_layers = num_layers\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_use_bias","title":"patchgan_use_bias  <code>instance-attribute</code>","text":"<pre><code>patchgan_use_bias = use_bias\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_kernel_size","title":"patchgan_kernel_size  <code>instance-attribute</code>","text":"<pre><code>patchgan_kernel_size = kernel_size\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_stride","title":"patchgan_stride  <code>instance-attribute</code>","text":"<pre><code>patchgan_stride = stride\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_last_kernel_size","title":"patchgan_last_kernel_size  <code>instance-attribute</code>","text":"<pre><code>patchgan_last_kernel_size = last_kernel_size\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_activation_fn","title":"patchgan_activation_fn  <code>instance-attribute</code>","text":"<pre><code>patchgan_activation_fn = lambda x: leaky_relu(\n    x, negative_slope=leaky_relu_slope\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_conv_layers","title":"patchgan_conv_layers  <code>instance-attribute</code>","text":"<pre><code>patchgan_conv_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_batch_norm_layers","title":"patchgan_batch_norm_layers  <code>instance-attribute</code>","text":"<pre><code>patchgan_batch_norm_layers = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.initial_conv","title":"initial_conv  <code>instance-attribute</code>","text":"<pre><code>initial_conv = Conv(\n    in_features=channels,\n    out_features=num_filters,\n    kernel_size=kernel_size,\n    strides=stride,\n    padding=\"SAME\",\n    use_bias=True,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.final_conv","title":"final_conv  <code>instance-attribute</code>","text":"<pre><code>final_conv = Conv(\n    in_features=in_channels,\n    out_features=1,\n    kernel_size=last_kernel_size,\n    strides=(1, 1),\n    padding=\"SAME\",\n    use_bias=True,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.PatchGANDiscriminator.patchgan_dropout","title":"patchgan_dropout  <code>instance-attribute</code>","text":"<pre><code>patchgan_dropout = Dropout(rate=dropout_rate, rngs=rngs)\n</code></pre>"},{"location":"api/models/gan/#multiscalepatchgandiscriminator","title":"MultiScalePatchGANDiscriminator","text":"<p>Multi-scale PatchGAN discriminator operating at multiple resolutions.</p> <p>Module: <code>artifex.generative_models.models.gan</code></p> <p>Parameters:</p> Parameter Type Default Description <code>input_shape</code> <code>tuple[int, ...]</code> Required Input image shape <code>(C, H, W)</code> <code>hidden_dims</code> <code>tuple[int, ...]</code> <code>(64, 128, 256)</code> Channel dimensions <code>num_scales</code> <code>int</code> <code>3</code> Number of scales <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators <p>Returns: List of predictions at different scales</p> <p>Example:</p> <pre><code>from artifex.generative_models.models.gan import MultiScalePatchGANDiscriminator\nfrom flax import nnx\nimport jax.numpy as jnp\n\ndiscriminator = MultiScalePatchGANDiscriminator(\n    input_shape=(3, 256, 256),\n    hidden_dims=(64, 128, 256),\n    num_scales=3,\n    rngs=nnx.Rngs(params=0, dropout=1),\n)\n\nimages = jnp.ones((16, 3, 256, 256))\npredictions = discriminator(images, training=True)\n# predictions is a list of 3 arrays at different scales\nfor i, pred in enumerate(predictions):\n    print(f\"Scale {i}: {pred.shape}\")\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator","title":"artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator","text":"<pre><code>MultiScalePatchGANDiscriminator(\n    config: MultiScalePatchGANConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Multi-scale PatchGAN discriminator.</p> <p>Processes images at multiple scales using several PatchGAN discriminators. This allows the discriminator to capture both fine-grained and coarse-grained features at different resolutions.</p> Reference <p>Wang et al. \"High-Resolution Image Synthesis and Semantic Manipulation\" (2018)</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MultiScalePatchGANConfig</code> <p>MultiScalePatchGANConfig with all architecture parameters</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rngs is None or configuration is invalid</p> <code>TypeError</code> <p>If config is not MultiScalePatchGANConfig</p>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator.input_shape","title":"input_shape  <code>instance-attribute</code>","text":"<pre><code>input_shape = input_shape\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator.num_discriminators","title":"num_discriminators  <code>instance-attribute</code>","text":"<pre><code>num_discriminators = num_discriminators\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator.num_layers_per_disc","title":"num_layers_per_disc  <code>instance-attribute</code>","text":"<pre><code>num_layers_per_disc = list(num_layers_per_disc)\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator.use_avg_pool","title":"use_avg_pool  <code>instance-attribute</code>","text":"<pre><code>use_avg_pool = True\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator.discriminators","title":"discriminators  <code>instance-attribute</code>","text":"<pre><code>discriminators = List([])\n</code></pre>"},{"location":"api/models/gan/#artifex.generative_models.models.gan.MultiScalePatchGANDiscriminator.downsample_image","title":"downsample_image","text":"<pre><code>downsample_image(x: Array, factor: int) -&gt; Array\n</code></pre> <p>Downsample image by given factor using average pooling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input image tensor (B, H, W, C)</p> required <code>factor</code> <code>int</code> <p>Downsampling factor</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Downsampled image</p>"},{"location":"api/models/gan/#loss-functions","title":"Loss Functions","text":"<p>See Adversarial Loss Functions for detailed documentation of:</p> <ul> <li><code>vanilla_generator_loss</code></li> <li><code>vanilla_discriminator_loss</code></li> <li><code>least_squares_generator_loss</code></li> <li><code>least_squares_discriminator_loss</code></li> <li><code>wasserstein_generator_loss</code></li> <li><code>wasserstein_discriminator_loss</code></li> <li><code>hinge_generator_loss</code></li> <li><code>hinge_discriminator_loss</code></li> </ul>"},{"location":"api/models/gan/#summary","title":"Summary","text":"<p>This API reference covered all GAN model classes:</p> <ul> <li>Base Classes: <code>Generator</code>, <code>Discriminator</code>, <code>GAN</code></li> <li>DCGAN: <code>DCGANGenerator</code>, <code>DCGANDiscriminator</code>, <code>DCGAN</code></li> <li>WGAN: <code>WGANGenerator</code>, <code>WGANDiscriminator</code>, <code>WGAN</code>, <code>compute_gradient_penalty</code></li> <li>LSGAN: <code>LSGANGenerator</code>, <code>LSGANDiscriminator</code>, <code>LSGAN</code></li> <li>Conditional GAN: <code>ConditionalGenerator</code>, <code>ConditionalDiscriminator</code>, <code>ConditionalGAN</code></li> <li>CycleGAN: <code>CycleGANGenerator</code>, <code>CycleGANDiscriminator</code>, <code>CycleGAN</code></li> <li>PatchGAN: <code>PatchGANDiscriminator</code>, <code>MultiScalePatchGANDiscriminator</code></li> </ul>"},{"location":"api/models/gan/#see-also","title":"See Also","text":"<ul> <li>GAN Concepts: Theory and mathematical foundations</li> <li>GAN User Guide: Practical usage examples</li> <li>GAN MNIST Example: Complete training tutorial</li> <li>Adversarial Losses: Loss function reference</li> </ul>"},{"location":"api/models/protein-point-cloud/","title":"Protein Point Cloud Model API","text":"<p>Coming Soon</p> <p>This page is under development. Check back for protein point cloud model API documentation.</p>"},{"location":"api/models/protein-point-cloud/#overview","title":"Overview","text":"<p>API reference for protein point cloud generation models.</p>"},{"location":"api/models/protein-point-cloud/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling Guide</li> <li>Point Clouds Guide</li> </ul>"},{"location":"api/models/vae/","title":"VAE API Reference","text":"<p>Complete API documentation for Variational Autoencoder models in Artifex.</p>"},{"location":"api/models/vae/#module-overview","title":"Module Overview","text":"<pre><code>from artifex.generative_models.models.vae import (\n    VAE,                    # Base VAE class\n    BetaVAE,               # \u03b2-VAE with disentanglement\n    BetaVAEWithCapacity,   # \u03b2-VAE with capacity control\n    ConditionalVAE,        # Conditional VAE\n    VQVAE,                 # Vector Quantized VAE\n)\n\nfrom artifex.generative_models.models.vae.encoders import (\n    MLPEncoder,            # Fully-connected encoder\n    CNNEncoder,            # Convolutional encoder\n    ResNetEncoder,         # ResNet-based encoder\n    ConditionalEncoder,    # Conditional wrapper\n)\n\nfrom artifex.generative_models.models.vae.decoders import (\n    MLPDecoder,            # Fully-connected decoder\n    CNNDecoder,            # Transposed convolutional decoder\n    ResNetDecoder,         # ResNet-based decoder\n    ConditionalDecoder,    # Conditional wrapper\n)\n</code></pre>"},{"location":"api/models/vae/#base-classes","title":"Base Classes","text":""},{"location":"api/models/vae/#vae","title":"VAE","text":"<p>The base VAE class implementing standard Variational Autoencoder functionality.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE","title":"artifex.generative_models.models.vae.base.VAE","text":"<pre><code>VAE(\n    config: VAEConfig,\n    *,\n    rngs: Rngs,\n    precision: Precision | None = None,\n)\n</code></pre> <p>               Bases: <code>GenerativeModel</code></p> <p>Base class for Variational Autoencoders.</p> <p>This class provides a foundation for implementing various VAE models using Flax NNX. All VAE models should inherit from this class and implement the required methods.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>VAEConfig</code> <p>VAEConfig with encoder, decoder, encoder_type, and kl_weight settings</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <code>precision</code> <code>Precision | None</code> <p>Numerical precision for computations</p> <code>None</code>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.kl_weight","title":"kl_weight  <code>instance-attribute</code>","text":"<pre><code>kl_weight = kl_weight\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder = create_encoder(encoder, encoder_type, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = create_decoder(decoder, encoder_type, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.encode","title":"encode","text":"<pre><code>encode(x: Array) -&gt; tuple[Array, Array]\n</code></pre> <p>Encode input to the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input data</p> required <p>Returns:</p> Type Description <code>tuple[Array, Array]</code> <p>Tuple of (mean, log_var) of the latent distribution</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If encoder output format is invalid</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.decode","title":"decode","text":"<pre><code>decode(z: Array) -&gt; Array\n</code></pre> <p>Decode latent vectors to the output space.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Array</code> <p>Latent vectors</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Reconstructed outputs</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If decoder output format is invalid</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.reparameterize","title":"reparameterize","text":"<pre><code>reparameterize(mean: Array, log_var: Array) -&gt; Array\n</code></pre> <p>Apply the reparameterization trick.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Array</code> <p>Mean vectors of the latent distribution</p> required <code>log_var</code> <code>Array</code> <p>Log variance vectors of the latent distribution</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Sampled latent vectors</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn(\n    params: dict | None = None,\n    batch: dict | None = None,\n    rng: Array | None = None,\n    x: Array | None = None,\n    outputs: dict[str, Array] | None = None,\n    beta: float | None = None,\n    reconstruction_loss_fn: Callable | None = None,\n    **kwargs: Any,\n) -&gt; dict[str, Array]\n</code></pre> <p>Calculate loss for VAE.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict | None</code> <p>Model parameters (optional, for compatibility with Trainer)</p> <code>None</code> <code>batch</code> <code>dict | None</code> <p>Input batch (optional, for compatibility with Trainer)</p> <code>None</code> <code>rng</code> <code>Array | None</code> <p>Random number generator (optional, for compatibility with Trainer)</p> <code>None</code> <code>x</code> <code>Array | None</code> <p>Input data (if not provided in batch)</p> <code>None</code> <code>outputs</code> <code>dict[str, Array] | None</code> <p>Dictionary of model outputs from forward pass</p> <code>None</code> <code>beta</code> <code>float | None</code> <p>Weight for KL divergence term</p> <code>None</code> <code>reconstruction_loss_fn</code> <code>Callable | None</code> <p>Optional custom reconstruction loss function. Signature: fn(predictions, targets) -&gt; loss (JAX/Optax convention)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Array]</code> <p>Dictionary of loss components</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.sample","title":"sample","text":"<pre><code>sample(\n    n_samples: int = 1, *, temperature: float = 1.0\n) -&gt; Array\n</code></pre> <p>Sample from the prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate. Must be a Python int (static value). When JIT-compiling, mark this as a static argument.</p> <code>1</code> <code>temperature</code> <code>float</code> <p>Scaling factor for the standard deviation (higher = more diverse)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p> Note <p>When JIT-compiling functions that call this method, mark n_samples as static: Example: <code>@nnx.jit(static_argnums=(1,))</code> or <code>@jax.jit(static_argnums=(1,))</code></p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.reconstruct","title":"reconstruct","text":"<pre><code>reconstruct(\n    x: Array, *, deterministic: bool = False\n) -&gt; Array\n</code></pre> <p>Reconstruct inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input data</p> required <code>deterministic</code> <code>bool</code> <p>If True, use mean of the latent distribution instead of sampling</p> <code>False</code> <p>Returns:</p> Type Description <code>Array</code> <p>Reconstructed outputs</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    temperature: float = 1.0,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Generate samples from the model.</p> <p>This is an alias for the sample method to maintain consistency with the GenerativeModel interface.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>temperature</code> <code>float</code> <p>Scaling factor for the standard deviation (higher = more diverse)</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments (unused, for compatibility)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.interpolate","title":"interpolate","text":"<pre><code>interpolate(x1: Array, x2: Array, steps: int = 10) -&gt; Array\n</code></pre> <p>Interpolate between two inputs in latent space.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>Array</code> <p>First input</p> required <code>x2</code> <code>Array</code> <p>Second input</p> required <code>steps</code> <code>int</code> <p>Number of interpolation steps (including endpoints). Must be a Python int (static value) when JIT-compiling.</p> <code>10</code> <p>Returns:</p> Type Description <code>Array</code> <p>Interpolated outputs</p> Note <p>When JIT-compiling functions that call this method, mark steps as static: Example: <code>@nnx.jit(static_argnums=(3,))</code> for the third argument.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.base.VAE.latent_traversal","title":"latent_traversal","text":"<pre><code>latent_traversal(\n    x: Array,\n    dim: int,\n    range_vals: tuple[float, float] = (-3.0, 3.0),\n    steps: int = 10,\n) -&gt; Array\n</code></pre> <p>Traverse a single dimension of the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input data</p> required <code>dim</code> <code>int</code> <p>Dimension to traverse</p> required <code>range_vals</code> <code>tuple[float, float]</code> <p>Range of values for traversal</p> <code>(-3.0, 3.0)</code> <code>steps</code> <code>int</code> <p>Number of steps in the traversal</p> <code>10</code> <p>Returns:</p> Type Description <code>Array</code> <p>Decoded outputs from the traversal</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dim is out of range</p>"},{"location":"api/models/vae/#class-definition","title":"Class Definition","text":"<pre><code>class VAE(GenerativeModel):\n    \"\"\"Base class for Variational Autoencoders.\"\"\"\n\n    def __init__(\n        self,\n        encoder: nnx.Module,\n        decoder: nnx.Module,\n        latent_dim: int,\n        *,\n        rngs: nnx.Rngs,\n        kl_weight: float = 1.0,\n        precision: jax.lax.Precision | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize a VAE.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>encoder</code> <code>nnx.Module</code> Required Encoder network mapping inputs to latent distributions <code>decoder</code> <code>nnx.Module</code> Required Decoder network mapping latent codes to reconstructions <code>latent_dim</code> <code>int</code> Required Dimensionality of the latent space (must be positive) <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators for initialization and sampling <code>kl_weight</code> <code>float</code> <code>1.0</code> Weight for KL divergence term (\u03b2 parameter) <code>precision</code> <code>jax.lax.Precision \\| None</code> <code>None</code> Numerical precision for computations"},{"location":"api/models/vae/#methods","title":"Methods","text":""},{"location":"api/models/vae/#encode","title":"<code>encode</code>","text":"<pre><code>def encode(\n    self,\n    x: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; tuple[jax.Array, jax.Array]:\n    \"\"\"Encode input to latent distribution parameters.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>x</code> (Array): Input data with shape <code>(batch_size, *input_shape)</code></li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>tuple[Array, Array]</code>: Mean and log-variance of latent distribution</li> <li><code>mean</code>: Shape <code>(batch_size, latent_dim)</code></li> <li><code>log_var</code>: Shape <code>(batch_size, latent_dim)</code></li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If encoder output format is invalid</li> </ul> <p>Example:</p> <pre><code>mean, log_var = vae.encode(x, rngs=rngs)\nprint(f\"Mean shape: {mean.shape}\")        # (32, 20)\nprint(f\"Log-var shape: {log_var.shape}\")  # (32, 20)\n</code></pre>"},{"location":"api/models/vae/#decode","title":"<code>decode</code>","text":"<pre><code>def decode(\n    self,\n    z: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; jax.Array:\n    \"\"\"Decode latent vectors to reconstructions.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>z</code> (Array): Latent vectors with shape <code>(batch_size, latent_dim)</code></li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Reconstructed outputs with shape <code>(batch_size, *output_shape)</code></li> </ul> <p>Example:</p> <pre><code>z = jax.random.normal(key, (32, 20))\nreconstructed = vae.decode(z, rngs=rngs)\nprint(f\"Reconstruction shape: {reconstructed.shape}\")  # (32, 784)\n</code></pre>"},{"location":"api/models/vae/#reparameterize","title":"<code>reparameterize</code>","text":"<pre><code>@nnx.jit\ndef reparameterize(\n    self,\n    mean: jax.Array,\n    log_var: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; jax.Array:\n    \"\"\"Apply the reparameterization trick.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>mean</code> (Array): Mean vectors with shape <code>(batch_size, latent_dim)</code></li> <li><code>log_var</code> (Array): Log-variance vectors with shape <code>(batch_size, latent_dim)</code></li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Sampled latent vectors with shape <code>(batch_size, latent_dim)</code></li> </ul> <p>Details:</p> <p>Implements the reparameterization trick: \\(z = \\mu + \\sigma \\odot \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)</p> <p>Includes numerical stability via log-variance clipping to <code>[-20, 20]</code>.</p> <p>Example:</p> <pre><code>mean, log_var = vae.encode(x, rngs=rngs)\nz = vae.reparameterize(mean, log_var, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#__call__","title":"<code>__call__</code>","text":"<pre><code>def __call__(\n    self,\n    x: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"Forward pass through the VAE.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>x</code> (Array): Input data with shape <code>(batch_size, *input_shape)</code></li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary containing:</li> <li><code>reconstructed</code> (Array): Reconstructed outputs</li> <li><code>reconstruction</code> (Array): Alias for compatibility</li> <li><code>mean</code> (Array): Latent mean vectors</li> <li><code>log_var</code> (Array): Latent log-variance vectors</li> <li><code>logvar</code> (Array): Alias for compatibility</li> <li><code>z</code> (Array): Sampled latent vectors</li> </ul> <p>Example:</p> <pre><code>outputs = vae(x, rngs=rngs)\nprint(outputs.keys())\n# dict_keys(['reconstructed', 'reconstruction', 'mean', 'log_var', 'logvar', 'z'])\n</code></pre>"},{"location":"api/models/vae/#loss_fn","title":"<code>loss_fn</code>","text":"<pre><code>def loss_fn(\n    self,\n    params: dict | None = None,\n    batch: dict | None = None,\n    rng: jax.Array | None = None,\n    x: jax.Array | None = None,\n    outputs: dict[str, jax.Array] | None = None,\n    beta: float | None = None,\n    reconstruction_loss_fn: Callable | None = None,\n    **kwargs,\n) -&gt; dict[str, jax.Array]:\n    \"\"\"Calculate VAE loss (ELBO).\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>params</code> (dict, optional): Model parameters (for Trainer compatibility)</li> <li><code>batch</code> (dict, optional): Input batch (for Trainer compatibility)</li> <li><code>rng</code> (Array, optional): Random number generator</li> <li><code>x</code> (Array, optional): Input data if not in batch</li> <li><code>outputs</code> (dict, optional): Pre-computed model outputs</li> <li><code>beta</code> (float, optional): KL divergence weight override</li> <li><code>reconstruction_loss_fn</code> (Callable, optional): Custom reconstruction loss</li> <li><code>**kwargs</code>: Additional arguments</li> </ul> <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary containing:</li> <li><code>reconstruction_loss</code> (Array): Reconstruction error</li> <li><code>recon_loss</code> (Array): Alias for compatibility</li> <li><code>kl_loss</code> (Array): KL divergence</li> <li><code>total_loss</code> (Array): Combined loss</li> <li><code>loss</code> (Array): Alias for compatibility</li> </ul> <p>Loss Formula:</p> \\[ \\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] + \\beta \\cdot D_{KL}(q(z|x) \\| p(z)) \\] <p>Example:</p> <pre><code>outputs = vae(x, rngs=rngs)\nlosses = vae.loss_fn(x=x, outputs=outputs)\n\nprint(f\"Reconstruction: {losses['reconstruction_loss']:.4f}\")\nprint(f\"KL Divergence: {losses['kl_loss']:.4f}\")\nprint(f\"Total Loss: {losses['total_loss']:.4f}\")\n</code></pre>"},{"location":"api/models/vae/#sample","title":"<code>sample</code>","text":"<pre><code>def sample(\n    self,\n    n_samples: int = 1,\n    *,\n    temperature: float = 1.0,\n    rngs: nnx.Rngs | None = None\n) -&gt; jax.Array:\n    \"\"\"Sample from the prior distribution.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>n_samples</code> (int): Number of samples to generate</li> <li><code>temperature</code> (float): Scaling factor for sampling diversity (higher = more diverse)</li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Generated samples with shape <code>(n_samples, *output_shape)</code></li> </ul> <p>Example:</p> <pre><code># Generate 16 samples\nsamples = vae.sample(n_samples=16, temperature=1.0, rngs=rngs)\n\n# More diverse samples\nhot_samples = vae.sample(n_samples=16, temperature=2.0, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#generate","title":"<code>generate</code>","text":"<pre><code>def generate(\n    self,\n    n_samples: int = 1,\n    *,\n    temperature: float = 1.0,\n    rngs: nnx.Rngs | None = None,\n    **kwargs,\n) -&gt; jax.Array:\n    \"\"\"Generate samples (alias for sample).\"\"\"\n</code></pre> <p>Alias for <code>sample()</code> to maintain consistency with <code>GenerativeModel</code> interface.</p>"},{"location":"api/models/vae/#reconstruct","title":"<code>reconstruct</code>","text":"<pre><code>def reconstruct(\n    self,\n    x: jax.Array,\n    *,\n    deterministic: bool = False,\n    rngs: nnx.Rngs | None = None\n) -&gt; jax.Array:\n    \"\"\"Reconstruct inputs.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>x</code> (Array): Input data</li> <li><code>deterministic</code> (bool): If True, use mean instead of sampling</li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Reconstructed outputs</li> </ul> <p>Example:</p> <pre><code># Stochastic reconstruction\nrecon = vae.reconstruct(x, deterministic=False, rngs=rngs)\n\n# Deterministic reconstruction (use latent mean)\ndet_recon = vae.reconstruct(x, deterministic=True, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#interpolate","title":"<code>interpolate</code>","text":"<pre><code>def interpolate(\n    self,\n    x1: jax.Array,\n    x2: jax.Array,\n    steps: int = 10,\n    *,\n    rngs: nnx.Rngs | None = None,\n) -&gt; jax.Array:\n    \"\"\"Interpolate between two inputs in latent space.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>x1</code> (Array): First input</li> <li><code>x2</code> (Array): Second input</li> <li><code>steps</code> (int): Number of interpolation steps (including endpoints)</li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Interpolated outputs with shape <code>(steps, *output_shape)</code></li> </ul> <p>Example:</p> <pre><code>x1 = test_images[0]\nx2 = test_images[1]\ninterpolation = vae.interpolate(x1, x2, steps=10, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#latent_traversal","title":"<code>latent_traversal</code>","text":"<pre><code>def latent_traversal(\n    self,\n    x: jax.Array,\n    dim: int,\n    range_vals: tuple[float, float] = (-3.0, 3.0),\n    steps: int = 10,\n    *,\n    rngs: nnx.Rngs | None = None,\n) -&gt; jax.Array:\n    \"\"\"Traverse a single latent dimension.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>x</code> (Array): Input data</li> <li><code>dim</code> (int): Dimension to traverse (0 to latent_dim-1)</li> <li><code>range_vals</code> (tuple): Range of values for traversal</li> <li><code>steps</code> (int): Number of traversal steps</li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>Array</code>: Decoded outputs from traversal with shape <code>(steps, *output_shape)</code></li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If dimension is out of range</li> </ul> <p>Example:</p> <pre><code># Traverse dimension 5\ntraversal = vae.latent_traversal(\n    x=test_image,\n    dim=5,\n    range_vals=(-3.0, 3.0),\n    steps=15,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#vae-variants","title":"VAE Variants","text":""},{"location":"api/models/vae/#betavae","title":"BetaVAE","text":"<p>\u03b2-VAE for learning disentangled representations.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAE","title":"artifex.generative_models.models.vae.beta_vae.BetaVAE","text":"<pre><code>BetaVAE(config: BetaVAEConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>VAE</code></p> <p>Beta Variational Autoencoder implementation.</p> <p>Beta-VAE is a variant of VAE that introduces a hyperparameter beta to the KL divergence term in the loss function. This allows for better control over the disentanglement of latent representations.</p> <p>By setting beta &gt; 1, the model is encouraged to learn more disentangled representations, but potentially at the cost of reconstruction quality.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BetaVAEConfig</code> <p>BetaVAEConfig with encoder, decoder, encoder_type, and beta settings</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator for initialization</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAE.beta_default","title":"beta_default  <code>instance-attribute</code>","text":"<pre><code>beta_default = beta_default\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAE.beta_warmup_steps","title":"beta_warmup_steps  <code>instance-attribute</code>","text":"<pre><code>beta_warmup_steps = beta_warmup_steps\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAE.reconstruction_loss_type","title":"reconstruction_loss_type  <code>instance-attribute</code>","text":"<pre><code>reconstruction_loss_type = reconstruction_loss_type\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAE.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn(\n    params: dict | None = None,\n    batch: dict | None = None,\n    rng: Array | None = None,\n    x: Array | None = None,\n    outputs: dict[str, Array] | None = None,\n    beta: float | None = None,\n    **kwargs: Any,\n) -&gt; dict[str, Array]\n</code></pre> <p>Calculate loss for BetaVAE.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict | None</code> <p>Model parameters (optional, for compatibility with Trainer)</p> <code>None</code> <code>batch</code> <code>dict | None</code> <p>Input batch (optional, for compatibility with Trainer)</p> <code>None</code> <code>rng</code> <code>Array | None</code> <p>Random number generator (optional, for Trainer compatibility)</p> <code>None</code> <code>x</code> <code>Array | None</code> <p>Input data (if not provided in batch)</p> <code>None</code> <code>outputs</code> <code>dict[str, Array] | None</code> <p>Dictionary of model outputs from forward pass</p> <code>None</code> <code>beta</code> <code>float | None</code> <p>Weight for KL divergence term. If None, uses model's beta_default with optional warmup.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments including current training step</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Array]</code> <p>Dictionary of loss components</p>"},{"location":"api/models/vae/#class-definition_1","title":"Class Definition","text":"<pre><code>class BetaVAE(VAE):\n    \"\"\"Beta Variational Autoencoder for disentanglement.\"\"\"\n\n    def __init__(\n        self,\n        encoder: nnx.Module,\n        decoder: nnx.Module,\n        latent_dim: int,\n        beta_default: float = 1.0,\n        beta_warmup_steps: int = 0,\n        reconstruction_loss_type: str = \"mse\",\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; None:\n        \"\"\"Initialize a BetaVAE.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>encoder</code> <code>nnx.Module</code> Required Encoder network <code>decoder</code> <code>nnx.Module</code> Required Decoder network <code>latent_dim</code> <code>int</code> Required Latent space dimension <code>beta_default</code> <code>float</code> <code>1.0</code> Default \u03b2 value for KL weighting <code>beta_warmup_steps</code> <code>int</code> <code>0</code> Steps for \u03b2 annealing (0 = no annealing) <code>reconstruction_loss_type</code> <code>str</code> <code>\"mse\"</code> Loss type: <code>\"mse\"</code> or <code>\"bce\"</code> <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/vae/#key-differences-from-vae","title":"Key Differences from VAE","text":"<p>Modified Loss Function:</p> \\[ \\mathcal{L}_\\beta = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] + \\beta \\cdot D_{KL}(q(z|x) \\| p(z)) \\] <p>Beta Annealing:</p> <p>When <code>beta_warmup_steps &gt; 0</code>, \u03b2 increases linearly from 0 to <code>beta_default</code>:</p> \\[ \\beta(t) = \\min\\left(\\beta_{\\text{default}}, \\beta_{\\text{default}} \\cdot \\frac{t}{T_{\\text{warmup}}}\\right) \\]"},{"location":"api/models/vae/#example","title":"Example","text":"<pre><code>beta_vae = BetaVAE(\n    encoder=encoder,\n    decoder=decoder,\n    latent_dim=32,\n    beta_default=4.0,              # Higher \u03b2 encourages disentanglement\n    beta_warmup_steps=10000,       # Gradually increase \u03b2\n    reconstruction_loss_type=\"mse\",\n    rngs=rngs,\n)\n\n# Training step with beta annealing\nfor step in range(num_steps):\n    outputs = beta_vae(batch, rngs=rngs)\n    losses = beta_vae.loss_fn(x=batch, outputs=outputs, step=step)\n    # losses['beta'] contains current \u03b2 value\n</code></pre>"},{"location":"api/models/vae/#betavaewithcapacity","title":"BetaVAEWithCapacity","text":"<p>\u03b2-VAE with Burgess et al. capacity control mechanism.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAEWithCapacity","title":"artifex.generative_models.models.vae.beta_vae.BetaVAEWithCapacity","text":"<pre><code>BetaVAEWithCapacity(\n    config: BetaVAEWithCapacityConfig, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>BetaVAE</code></p> <p>Beta-VAE with Burgess et al. capacity control.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>BetaVAEWithCapacityConfig</code> <p>BetaVAEWithCapacityConfig with encoder, decoder, beta, and capacity settings</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator for initialization</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAEWithCapacity.use_capacity_control","title":"use_capacity_control  <code>instance-attribute</code>","text":"<pre><code>use_capacity_control = use_capacity_control\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAEWithCapacity.capacity_max","title":"capacity_max  <code>instance-attribute</code>","text":"<pre><code>capacity_max = capacity_max\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAEWithCapacity.capacity_num_iter","title":"capacity_num_iter  <code>instance-attribute</code>","text":"<pre><code>capacity_num_iter = capacity_num_iter\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAEWithCapacity.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = gamma\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.beta_vae.BetaVAEWithCapacity.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn(\n    params: dict | None = None,\n    batch: dict | None = None,\n    rng: Array | None = None,\n    x: Array | None = None,\n    outputs: dict[str, Array] | None = None,\n    beta: float | None = None,\n    step: int = 0,\n    **kwargs,\n) -&gt; dict[str, Array]\n</code></pre> <p>Calculate loss with optional capacity control.</p>"},{"location":"api/models/vae/#class-definition_2","title":"Class Definition","text":"<pre><code>class BetaVAEWithCapacity(BetaVAE):\n    \"\"\"\u03b2-VAE with capacity control.\"\"\"\n\n    def __init__(\n        self,\n        encoder: nnx.Module,\n        decoder: nnx.Module,\n        latent_dim: int,\n        beta_default: float = 1.0,\n        beta_warmup_steps: int = 0,\n        reconstruction_loss_type: str = \"mse\",\n        use_capacity_control: bool = False,\n        capacity_max: float = 25.0,\n        capacity_num_iter: int = 25000,\n        gamma: float = 1000.0,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; None:\n        \"\"\"Initialize BetaVAE with capacity control.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#additional-parameters","title":"Additional Parameters","text":"Parameter Type Default Description <code>use_capacity_control</code> <code>bool</code> <code>False</code> Enable capacity control <code>capacity_max</code> <code>float</code> <code>25.0</code> Maximum capacity in nats <code>capacity_num_iter</code> <code>int</code> <code>25000</code> Steps to reach max capacity <code>gamma</code> <code>float</code> <code>1000.0</code> Weight for capacity loss"},{"location":"api/models/vae/#capacity-loss","title":"Capacity Loss","text":"\\[ \\mathcal{L}_C = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] + \\gamma \\cdot |D_{KL}(q(z|x) \\| p(z)) - C(t)| \\] <p>Where capacity \\(C(t)\\) increases linearly:</p> \\[ C(t) = \\min\\left(C_{\\max}, C_{\\max} \\cdot \\frac{t}{T_{\\text{capacity}}}\\right) \\]"},{"location":"api/models/vae/#example_1","title":"Example","text":"<pre><code>capacity_vae = BetaVAEWithCapacity(\n    encoder=encoder,\n    decoder=decoder,\n    latent_dim=32,\n    beta_default=4.0,\n    use_capacity_control=True,\n    capacity_max=25.0,\n    capacity_num_iter=25000,\n    gamma=1000.0,\n    rngs=rngs,\n)\n\n# Loss includes capacity terms\nlosses = capacity_vae.loss_fn(x=batch, outputs=outputs, step=step)\nprint(losses['current_capacity'])    # Current C value\nprint(losses['capacity_loss'])       # \u03b3 * |KL - C|\nprint(losses['kl_capacity_diff'])    # KL - C\n</code></pre>"},{"location":"api/models/vae/#conditionalvae","title":"ConditionalVAE","text":"<p>Conditional VAE for class-conditional generation.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE","title":"artifex.generative_models.models.vae.conditional.ConditionalVAE","text":"<pre><code>ConditionalVAE(\n    config: ConditionalVAEConfig,\n    *,\n    rngs: Rngs,\n    precision: Precision | None = None,\n)\n</code></pre> <p>               Bases: <code>VAE</code></p> <p>Conditional Variational Autoencoder implementation.</p> <p>Extends the base VAE with conditioning capabilities by incorporating additional information at both encoding and decoding steps. This follows the standard CVAE pattern using one-hot concatenation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ConditionalVAEConfig</code> <p>ConditionalVAEConfig with encoder, decoder, encoder_type, conditioning settings</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators</p> required <code>precision</code> <code>Precision | None</code> <p>Numerical precision for computations</p> <code>None</code>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.rngs","title":"rngs  <code>instance-attribute</code>","text":"<pre><code>rngs = rngs\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.precision","title":"precision  <code>instance-attribute</code>","text":"<pre><code>precision = precision\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.kl_weight","title":"kl_weight  <code>instance-attribute</code>","text":"<pre><code>kl_weight = kl_weight\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.condition_dim","title":"condition_dim  <code>instance-attribute</code>","text":"<pre><code>condition_dim = condition_dim\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.condition_type","title":"condition_type  <code>instance-attribute</code>","text":"<pre><code>condition_type = condition_type\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder = create_encoder(\n    encoder,\n    encoder_type,\n    conditional=True,\n    num_classes=condition_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = create_decoder(\n    decoder,\n    encoder_type,\n    conditional=True,\n    num_classes=condition_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.encode","title":"encode","text":"<pre><code>encode(\n    x: Array, y: Array | None = None\n) -&gt; tuple[Array, Array]\n</code></pre> <p>Encode input to the latent space with conditioning.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input data</p> required <code>y</code> <code>Array | None</code> <p>Conditioning information (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Array, Array]</code> <p>Tuple of (mean, log_var) of the latent distribution</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.decode","title":"decode","text":"<pre><code>decode(z: Array, y: Array | None = None) -&gt; Array\n</code></pre> <p>Decode latent vectors with conditioning.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Array</code> <p>Latent vectors</p> required <code>y</code> <code>Array | None</code> <p>Conditioning information (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Array</code> <p>Reconstructed output</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.sample","title":"sample","text":"<pre><code>sample(\n    n_samples: int = 1,\n    *,\n    temperature: float = 1.0,\n    y: Array | None = None,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Sample from the model with conditioning.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>temperature</code> <code>float</code> <p>Temperature parameter controlling randomness</p> <code>1.0</code> <code>y</code> <code>Array | None</code> <p>Conditioning information (optional)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for compatibility</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    temperature: float = 1.0,\n    y: Array | None = None,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Generate samples from the model with conditioning.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>temperature</code> <code>float</code> <p>Temperature parameter controlling randomness</p> <code>1.0</code> <code>y</code> <code>Array | None</code> <p>Conditioning information (optional)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.conditional.ConditionalVAE.reconstruct","title":"reconstruct","text":"<pre><code>reconstruct(\n    x: Array,\n    *,\n    y: Array | None = None,\n    deterministic: bool = False,\n) -&gt; Array\n</code></pre> <p>Reconstruct inputs with conditioning.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input data</p> required <code>y</code> <code>Array | None</code> <p>Conditioning information (optional)</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>If True, use mean instead of sampling</p> <code>False</code> <p>Returns:</p> Type Description <code>Array</code> <p>Reconstructed outputs</p>"},{"location":"api/models/vae/#class-definition_3","title":"Class Definition","text":"<pre><code>class ConditionalVAE(VAE):\n    \"\"\"Conditional Variational Autoencoder.\"\"\"\n\n    def __init__(\n        self,\n        encoder: nnx.Module,\n        decoder: nnx.Module,\n        latent_dim: int,\n        condition_dim: int = 10,\n        condition_type: str = \"concat\",\n        *,\n        rngs: nnx.Rngs | None = None,\n    ):\n        \"\"\"Initialize Conditional VAE.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>encoder</code> <code>nnx.Module</code> Required Conditional encoder <code>decoder</code> <code>nnx.Module</code> Required Conditional decoder <code>latent_dim</code> <code>int</code> Required Latent dimension <code>condition_dim</code> <code>int</code> <code>10</code> Conditioning dimension <code>condition_type</code> <code>str</code> <code>\"concat\"</code> Conditioning strategy <code>rngs</code> <code>nnx.Rngs</code> <code>None</code> Random number generators"},{"location":"api/models/vae/#modified-methods","title":"Modified Methods","text":""},{"location":"api/models/vae/#__call___1","title":"<code>__call__</code>","text":"<pre><code>def __call__(\n    self,\n    x: jax.Array,\n    y: jax.Array | None = None,\n    *,\n    rngs: nnx.Rngs | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Forward pass with conditioning.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#encode_1","title":"<code>encode</code>","text":"<pre><code>def encode(\n    self,\n    x: jax.Array,\n    y: jax.Array | None = None,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; tuple[jax.Array, jax.Array]:\n    \"\"\"Encode with conditioning.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#decode_1","title":"<code>decode</code>","text":"<pre><code>def decode(\n    self,\n    z: jax.Array,\n    y: jax.Array | None = None,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; jax.Array:\n    \"\"\"Decode with conditioning.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#sample_1","title":"<code>sample</code>","text":"<pre><code>def sample(\n    self,\n    n_samples: int = 1,\n    *,\n    temperature: float = 1.0,\n    y: jax.Array | None = None,\n    rngs: nnx.Rngs | None = None,\n) -&gt; jax.Array:\n    \"\"\"Sample with conditioning.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#example_2","title":"Example","text":"<pre><code># Create conditional encoder/decoder\nconditional_encoder = ConditionalEncoder(\n    encoder=base_encoder,\n    num_classes=10,\n    embed_dim=128,\n    rngs=rngs,\n)\n\nconditional_decoder = ConditionalDecoder(\n    decoder=base_decoder,\n    num_classes=10,\n    embed_dim=128,\n    rngs=rngs,\n)\n\ncvae = ConditionalVAE(\n    encoder=conditional_encoder,\n    decoder=conditional_decoder,\n    latent_dim=32,\n    condition_dim=10,\n    rngs=rngs,\n)\n\n# Forward pass with class labels\nlabels = jnp.array([0, 1, 2, 3, 4])\noutputs = cvae(x, y=labels, rngs=rngs)\n\n# Generate specific classes\ntarget_labels = jnp.array([5, 5, 5, 5])\nsamples = cvae.sample(n_samples=4, y=target_labels, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#vqvae","title":"VQVAE","text":"<p>Vector Quantized VAE with discrete latent representations.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE","title":"artifex.generative_models.models.vae.vq_vae.VQVAE","text":"<pre><code>VQVAE(config: VQVAEConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>VAE</code></p> <p>Vector Quantized Variational Autoencoder implementation.</p> <p>Extends the base VAE with discrete latent variables using a codebook.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>VQVAEConfig</code> <p>VQVAEConfig with encoder, decoder, encoder_type, and VQ settings</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators for initialization</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.num_embeddings","title":"num_embeddings  <code>instance-attribute</code>","text":"<pre><code>num_embeddings = num_embeddings\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.embedding_dim","title":"embedding_dim  <code>instance-attribute</code>","text":"<pre><code>embedding_dim = embedding_dim\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.commitment_cost","title":"commitment_cost  <code>instance-attribute</code>","text":"<pre><code>commitment_cost = commitment_cost\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.embeddings","title":"embeddings  <code>instance-attribute</code>","text":"<pre><code>embeddings = Embed(\n    num_embeddings=num_embeddings,\n    features=embedding_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.quantize","title":"quantize","text":"<pre><code>quantize(encoding: Array) -&gt; tuple[Array, dict[str, Array]]\n</code></pre> <p>Quantize the input using the codebook.</p> <p>Parameters:</p> Name Type Description Default <code>encoding</code> <code>Array</code> <p>Continuous encoding from the encoder</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Array]]</code> <p>Tuple of (quantized encoding, auxiliary dict containing losses and indices)</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.encode","title":"encode","text":"<pre><code>encode(x: Array) -&gt; tuple[Array, Array]\n</code></pre> <p>Encode input to continuous latent representation (before quantization).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input data, shape [batch_size, ...]</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Tuple of (mean, log_var) for VAE interface compatibility.</p> <code>Array</code> <p>For VQ-VAE, log_var is zeros since encoding is deterministic.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.decode","title":"decode","text":"<pre><code>decode(z: Array) -&gt; Array\n</code></pre> <p>Decode latent representation to reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Array</code> <p>Quantized representation</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Reconstructed output</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn(\n    x: Array, outputs: dict[str, Array], **kwargs: Any\n) -&gt; dict[str, Array]\n</code></pre> <p>Compute the VQ-VAE loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Input data</p> required <code>outputs</code> <code>dict[str, Array]</code> <p>Dictionary of model outputs from forward pass</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Array]</code> <p>Dictionary of loss components</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.sample","title":"sample","text":"<pre><code>sample(\n    n_samples: int = 1, temperature: float = 1.0, **kwargs\n) -&gt; Array\n</code></pre> <p>Sample from the model. Args:     n_samples: Number of samples to generate     temperature: Temperature parameter controlling randomness         (higher = more random)     **kwargs: Additional keyword arguments for compatibility</p> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.vq_vae.VQVAE.generate","title":"generate","text":"<pre><code>generate(\n    n_samples: int = 1,\n    *,\n    temperature: float = 1.0,\n    **kwargs: Any,\n) -&gt; Array\n</code></pre> <p>Generate samples from the model.</p> <p>Implements the required method from GenerativeModel base class.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of samples to generate</p> <code>1</code> <code>temperature</code> <code>float</code> <p>Temperature parameter controlling randomness</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Generated samples</p>"},{"location":"api/models/vae/#class-definition_4","title":"Class Definition","text":"<pre><code>class VQVAE(VAE):\n    \"\"\"Vector Quantized Variational Autoencoder.\"\"\"\n\n    def __init__(\n        self,\n        encoder: nnx.Module,\n        decoder: nnx.Module,\n        latent_dim: int,\n        num_embeddings: int = 512,\n        embedding_dim: int = 64,\n        commitment_cost: float = 0.25,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize VQ-VAE.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>encoder</code> <code>nnx.Module</code> Required Encoder network <code>decoder</code> <code>nnx.Module</code> Required Decoder network <code>latent_dim</code> <code>int</code> Required Latent dimension <code>num_embeddings</code> <code>int</code> <code>512</code> Codebook size (number of embeddings) <code>embedding_dim</code> <code>int</code> <code>64</code> Dimension of each embedding <code>commitment_cost</code> <code>float</code> <code>0.25</code> Weight for commitment loss <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/vae/#key-method-quantize","title":"Key Method: <code>quantize</code>","text":"<pre><code>def quantize(\n    self,\n    encoding: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None\n) -&gt; tuple[jax.Array, dict[str, Any]]:\n    \"\"\"Quantize continuous encoding using codebook.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>encoding</code> (Array): Continuous encoding from encoder</li> <li><code>rngs</code> (Rngs, optional): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>tuple</code>: (quantized encoding, auxiliary info)</li> <li><code>quantized</code> (Array): Discrete quantized vectors</li> <li><code>aux</code> (dict): Contains <code>commitment_loss</code>, <code>codebook_loss</code>, <code>encoding_indices</code></li> </ul> <p>Quantization Process:</p> <ol> <li>Find nearest codebook embedding for each encoding vector</li> <li>Replace encoding with codebook embedding</li> <li>Use straight-through estimator for gradients</li> </ol>"},{"location":"api/models/vae/#loss-function","title":"Loss Function","text":"<p>VQ-VAE uses a specialized loss:</p> \\[ \\mathcal{L}_{VQ} = \\|x - \\hat{x}\\|^2 + \\|sg[z_e] - e\\|^2 + \\beta \\|z_e - sg[e]\\|^2 \\] <p>Where:</p> <ul> <li>First term: Reconstruction loss</li> <li>Second term: Codebook loss (update embeddings)</li> <li>Third term: Commitment loss (encourage encoder to commit)</li> </ul>"},{"location":"api/models/vae/#example_3","title":"Example","text":"<pre><code>vqvae = VQVAE(\n    encoder=encoder,\n    decoder=decoder,\n    latent_dim=64,\n    num_embeddings=512,    # 512 discrete codes\n    embedding_dim=64,\n    commitment_cost=0.25,\n    rngs=rngs,\n)\n\n# Forward pass includes quantization\noutputs = vqvae(x, rngs=rngs)\nprint(outputs['z_e'])              # Pre-quantization encoding\nprint(outputs['quantized'])        # Quantized (discrete) encoding\nprint(outputs['commitment_loss'])  # Commitment loss component\nprint(outputs['codebook_loss'])    # Codebook loss component\n\n# Loss includes VQ-specific terms\nlosses = vqvae.loss_fn(x=batch, outputs=outputs)\nprint(losses['reconstruction_loss'])\nprint(losses['commitment_loss'])\nprint(losses['codebook_loss'])\n</code></pre>"},{"location":"api/models/vae/#encoders","title":"Encoders","text":""},{"location":"api/models/vae/#mlpencoder","title":"MLPEncoder","text":"<p>Fully-connected encoder for flattened inputs.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.MLPEncoder","title":"artifex.generative_models.models.vae.encoders.MLPEncoder","text":"<pre><code>MLPEncoder(config: EncoderConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Simple MLP encoder for VAE.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EncoderConfig</code> <p>EncoderConfig with hidden_dims, latent_dim, activation, input_shape</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.MLPEncoder.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.MLPEncoder.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone = MLP(\n    hidden_dims=hidden_dims,\n    activation=activation,\n    in_features=input_features,\n    use_batch_norm=use_batch_norm,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.MLPEncoder.mean_proj","title":"mean_proj  <code>instance-attribute</code>","text":"<pre><code>mean_proj = Linear(\n    in_features=hidden_dims[-1],\n    out_features=latent_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.MLPEncoder.logvar_proj","title":"logvar_proj  <code>instance-attribute</code>","text":"<pre><code>logvar_proj = Linear(\n    in_features=hidden_dims[-1],\n    out_features=latent_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#class-definition_5","title":"Class Definition","text":"<pre><code>class MLPEncoder(nnx.Module):\n    \"\"\"MLP encoder for VAE.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dims: list,\n        latent_dim: int,\n        activation: str = \"relu\",\n        input_dim: tuple | None = None,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize MLP encoder.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>hidden_dims</code> <code>list[int]</code> Required Hidden layer dimensions <code>latent_dim</code> <code>int</code> Required Latent space dimension <code>activation</code> <code>str</code> <code>\"relu\"</code> Activation function <code>input_dim</code> <code>tuple \\| None</code> <code>None</code> Input dimensions for shape inference <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/vae/#example_4","title":"Example","text":"<pre><code>encoder = MLPEncoder(\n    hidden_dims=[512, 256, 128],\n    latent_dim=32,\n    activation=\"relu\",\n    input_dim=(784,),\n    rngs=rngs,\n)\n\nmean, log_var = encoder(x, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#cnnencoder","title":"CNNEncoder","text":"<p>Convolutional encoder for image inputs.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.CNNEncoder","title":"artifex.generative_models.models.vae.encoders.CNNEncoder","text":"<pre><code>CNNEncoder(config: EncoderConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>CNN-based encoder for VAE.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>EncoderConfig</code> <p>EncoderConfig with hidden_dims, latent_dim, activation, input_shape</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.CNNEncoder.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.CNNEncoder.cnn","title":"cnn  <code>instance-attribute</code>","text":"<pre><code>cnn = CNN(\n    hidden_dims=hidden_dims,\n    activation=activation,\n    in_features=in_channels,\n    use_batch_norm=use_batch_norm,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.CNNEncoder.flatten","title":"flatten  <code>instance-attribute</code>","text":"<pre><code>flatten = Flatten(rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.CNNEncoder.mean_proj","title":"mean_proj  <code>instance-attribute</code>","text":"<pre><code>mean_proj = Linear(\n    in_features=flattened_size,\n    out_features=latent_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.CNNEncoder.logvar_proj","title":"logvar_proj  <code>instance-attribute</code>","text":"<pre><code>logvar_proj = Linear(\n    in_features=flattened_size,\n    out_features=latent_dim,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#class-definition_6","title":"Class Definition","text":"<pre><code>class CNNEncoder(nnx.Module):\n    \"\"\"CNN encoder for VAE.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dims: list,\n        latent_dim: int,\n        activation: str = \"relu\",\n        input_dim: tuple | None = None,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize CNN encoder.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_5","title":"Parameters","text":"Parameter Type Default Description <code>hidden_dims</code> <code>list[int]</code> Required Channel dimensions for conv layers <code>latent_dim</code> <code>int</code> Required Latent space dimension <code>activation</code> <code>str</code> <code>\"relu\"</code> Activation function <code>input_dim</code> <code>tuple \\| None</code> <code>None</code> Input shape (H, W, C) <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/vae/#architecture","title":"Architecture","text":"<ul> <li>Series of convolutional layers with stride 2</li> <li>Each layer reduces spatial dimensions by half</li> <li>Global pooling before projecting to latent space</li> </ul>"},{"location":"api/models/vae/#example_5","title":"Example","text":"<pre><code>encoder = CNNEncoder(\n    hidden_dims=[32, 64, 128, 256],\n    latent_dim=64,\n    activation=\"relu\",\n    input_dim=(28, 28, 1),\n    rngs=rngs,\n)\n\n# Input shape: (batch, 28, 28, 1)\nmean, log_var = encoder(images, rngs=rngs)\n# Output shapes: (batch, 64), (batch, 64)\n</code></pre>"},{"location":"api/models/vae/#conditionalencoder","title":"ConditionalEncoder","text":"<p>Wrapper that adds conditioning to any encoder.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.ConditionalEncoder","title":"artifex.generative_models.models.vae.encoders.ConditionalEncoder","text":"<pre><code>ConditionalEncoder(\n    encoder: Module, num_classes: int, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Wrapper that adds conditioning to any encoder.</p> <p>This wrapper adds class conditioning to an existing encoder by converting labels to one-hot and concatenating them with the input. This follows the standard CVAE pattern from popular implementations.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>Base encoder to wrap</p> required <code>num_classes</code> <code>int</code> <p>Number of classes for conditioning</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators (for API consistency)</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.ConditionalEncoder.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder = encoder\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.encoders.ConditionalEncoder.num_classes","title":"num_classes  <code>instance-attribute</code>","text":"<pre><code>num_classes = num_classes\n</code></pre>"},{"location":"api/models/vae/#class-definition_7","title":"Class Definition","text":"<pre><code>class ConditionalEncoder(nnx.Module):\n    \"\"\"Conditional encoder wrapper.\"\"\"\n\n    def __init__(\n        self,\n        encoder: nnx.Module,\n        num_classes: int,\n        embed_dim: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize conditional encoder.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_6","title":"Parameters","text":"Parameter Type Default Description <code>encoder</code> <code>nnx.Module</code> Required Base encoder to wrap <code>num_classes</code> <code>int</code> Required Number of conditioning classes <code>embed_dim</code> <code>int</code> Required Embedding dimension for labels <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/vae/#example_6","title":"Example","text":"<pre><code>base_encoder = MLPEncoder(\n    hidden_dims=[512, 256],\n    latent_dim=32,\n    rngs=rngs,\n)\n\nconditional_encoder = ConditionalEncoder(\n    encoder=base_encoder,\n    num_classes=10,\n    embed_dim=128,\n    rngs=rngs,\n)\n\n# Pass class labels as integers or one-hot\nlabels = jnp.array([0, 1, 2, 3])\nmean, log_var = conditional_encoder(x, condition=labels, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#decoders","title":"Decoders","text":""},{"location":"api/models/vae/#mlpdecoder","title":"MLPDecoder","text":"<p>Fully-connected decoder.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.MLPDecoder","title":"artifex.generative_models.models.vae.decoders.MLPDecoder","text":"<pre><code>MLPDecoder(config: DecoderConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Simple MLP decoder for VAE.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DecoderConfig</code> <p>DecoderConfig with hidden_dims, output_shape, latent_dim, activation</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.MLPDecoder.output_shape","title":"output_shape  <code>instance-attribute</code>","text":"<pre><code>output_shape = output_shape\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.MLPDecoder.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.MLPDecoder.output_size","title":"output_size  <code>instance-attribute</code>","text":"<pre><code>output_size = 1\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.MLPDecoder.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone = MLP(\n    hidden_dims=decoder_dims,\n    activation=activation,\n    in_features=latent_dim,\n    use_batch_norm=use_batch_norm,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.MLPDecoder.output_layer","title":"output_layer  <code>instance-attribute</code>","text":"<pre><code>output_layer = Linear(\n    in_features=decoder_dims[-1],\n    out_features=output_size,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.MLPDecoder.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = SigmoidActivation\n</code></pre>"},{"location":"api/models/vae/#class-definition_8","title":"Class Definition","text":"<pre><code>class MLPDecoder(nnx.Module):\n    \"\"\"MLP decoder for VAE.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dims: list[int],\n        output_dim: tuple[int, ...],\n        latent_dim: int,\n        activation: str = \"relu\",\n        *,\n        rngs: nnx.Rngs | None = None,\n    ):\n        \"\"\"Initialize MLP decoder.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_7","title":"Parameters","text":"Parameter Type Default Description <code>hidden_dims</code> <code>list[int]</code> Required Hidden layer dimensions (reversed from encoder) <code>output_dim</code> <code>tuple</code> Required Output reconstruction shape <code>latent_dim</code> <code>int</code> Required Latent space dimension <code>activation</code> <code>str</code> <code>\"relu\"</code> Activation function <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/vae/#example_7","title":"Example","text":"<pre><code>decoder = MLPDecoder(\n    hidden_dims=[128, 256, 512],  # Reversed from encoder\n    output_dim=(784,),\n    latent_dim=32,\n    activation=\"relu\",\n    rngs=rngs,\n)\n\nreconstructed = decoder(z)  # Shape: (batch, 784)\n</code></pre>"},{"location":"api/models/vae/#cnndecoder","title":"CNNDecoder","text":"<p>Transposed convolutional decoder for images.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder","title":"artifex.generative_models.models.vae.decoders.CNNDecoder","text":"<pre><code>CNNDecoder(config: DecoderConfig, *, rngs: Rngs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>CNN-based decoder for VAE.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DecoderConfig</code> <p>DecoderConfig with hidden_dims, output_shape, latent_dim, activation</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.output_shape","title":"output_shape  <code>instance-attribute</code>","text":"<pre><code>output_shape = output_shape\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.latent_dim","title":"latent_dim  <code>instance-attribute</code>","text":"<pre><code>latent_dim = latent_dim\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.initial_h","title":"initial_h  <code>instance-attribute</code>","text":"<pre><code>initial_h = max(enc_h, 1)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.initial_w","title":"initial_w  <code>instance-attribute</code>","text":"<pre><code>initial_w = max(enc_w, 1)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.initial_features","title":"initial_features  <code>instance-attribute</code>","text":"<pre><code>initial_features = hidden_dims[0]\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.project","title":"project  <code>instance-attribute</code>","text":"<pre><code>project = Linear(\n    in_features=latent_dim,\n    out_features=initial_size,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.cnn","title":"cnn  <code>instance-attribute</code>","text":"<pre><code>cnn = CNN(\n    hidden_dims=decoder_dims,\n    activation=activation,\n    use_transpose=True,\n    in_features=initial_features,\n    use_batch_norm=use_batch_norm,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.output_channels","title":"output_channels  <code>instance-attribute</code>","text":"<pre><code>output_channels = output_shape[2]\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.output_conv","title":"output_conv  <code>instance-attribute</code>","text":"<pre><code>output_conv = Conv(\n    in_features=final_channels,\n    out_features=output_channels,\n    kernel_size=(3, 3),\n    padding=\"SAME\",\n    rngs=rngs,\n)\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.CNNDecoder.activation","title":"activation  <code>instance-attribute</code>","text":"<pre><code>activation = SigmoidActivation\n</code></pre>"},{"location":"api/models/vae/#class-definition_9","title":"Class Definition","text":"<pre><code>class CNNDecoder(nnx.Module):\n    \"\"\"CNN decoder with transposed convolutions.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dims: list[int],\n        output_dim: tuple[int, ...],\n        latent_dim: int,\n        activation: str = \"relu\",\n        *,\n        rngs: nnx.Rngs | None = None,\n    ):\n        \"\"\"Initialize CNN decoder.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_8","title":"Parameters","text":"Parameter Type Default Description <code>hidden_dims</code> <code>list[int]</code> Required Channel dimensions (reversed from encoder) <code>output_dim</code> <code>tuple</code> Required Output shape (H, W, C) <code>latent_dim</code> <code>int</code> Required Latent dimension <code>activation</code> <code>str</code> <code>\"relu\"</code> Activation function <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/vae/#example_8","title":"Example","text":"<pre><code>decoder = CNNDecoder(\n    hidden_dims=[256, 128, 64, 32],  # Reversed channels\n    output_dim=(28, 28, 1),\n    latent_dim=64,\n    activation=\"relu\",\n    rngs=rngs,\n)\n\n# Input: (batch, 64)\nreconstructed = decoder(z)  # Output: (batch, 28, 28, 1)\n</code></pre>"},{"location":"api/models/vae/#conditionaldecoder","title":"ConditionalDecoder","text":"<p>Wrapper that adds conditioning to any decoder.</p>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.ConditionalDecoder","title":"artifex.generative_models.models.vae.decoders.ConditionalDecoder","text":"<pre><code>ConditionalDecoder(\n    decoder: Module, num_classes: int, *, rngs: Rngs\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Wrapper that adds conditioning to any decoder.</p> <p>This wrapper adds class conditioning to an existing decoder by converting labels to one-hot and concatenating them with the latent vector. This follows the standard CVAE pattern from popular implementations.</p> <p>Parameters:</p> Name Type Description Default <code>decoder</code> <code>Module</code> <p>Base decoder to wrap</p> required <code>num_classes</code> <code>int</code> <p>Number of classes for conditioning</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generators (for API consistency)</p> required"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.ConditionalDecoder.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = decoder\n</code></pre>"},{"location":"api/models/vae/#artifex.generative_models.models.vae.decoders.ConditionalDecoder.num_classes","title":"num_classes  <code>instance-attribute</code>","text":"<pre><code>num_classes = num_classes\n</code></pre>"},{"location":"api/models/vae/#class-definition_10","title":"Class Definition","text":"<pre><code>class ConditionalDecoder(nnx.Module):\n    \"\"\"Conditional decoder wrapper.\"\"\"\n\n    def __init__(\n        self,\n        decoder: nnx.Module,\n        num_classes: int,\n        embed_dim: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize conditional decoder.\"\"\"\n</code></pre>"},{"location":"api/models/vae/#parameters_9","title":"Parameters","text":"Parameter Type Default Description <code>decoder</code> <code>nnx.Module</code> Required Base decoder to wrap <code>num_classes</code> <code>int</code> Required Number of conditioning classes <code>embed_dim</code> <code>int</code> Required Embedding dimension <code>rngs</code> <code>nnx.Rngs</code> Required Random number generators"},{"location":"api/models/vae/#example_9","title":"Example","text":"<pre><code>base_decoder = MLPDecoder(\n    hidden_dims=[128, 256, 512],\n    output_dim=(784,),\n    latent_dim=32,\n    rngs=rngs,\n)\n\nconditional_decoder = ConditionalDecoder(\n    decoder=base_decoder,\n    num_classes=10,\n    embed_dim=128,\n    rngs=rngs,\n)\n\nlabels = jnp.array([0, 1, 2, 3])\nreconstructed = conditional_decoder(z, condition=labels, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#utility-functions","title":"Utility Functions","text":""},{"location":"api/models/vae/#create_encoder_unified","title":"<code>create_encoder_unified</code>","text":"<pre><code>def create_encoder_unified(\n    *,\n    config: ModelConfig,\n    encoder_type: str,\n    conditional: bool = False,\n    num_classes: int | None = None,\n    embed_dim: int | None = None,\n    rngs: nnx.Rngs,\n) -&gt; nnx.Module:\n    \"\"\"Create encoder from unified configuration.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>config</code> (ModelConfig): Model configuration</li> <li><code>encoder_type</code> (str): Type of encoder: <code>\"dense\"</code>, <code>\"cnn\"</code>, <code>\"resnet\"</code></li> <li><code>conditional</code> (bool): Whether to wrap in conditional encoder</li> <li><code>num_classes</code> (int, optional): Number of classes for conditioning</li> <li><code>embed_dim</code> (int, optional): Embedding dimension</li> <li><code>rngs</code> (Rngs): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>nnx.Module</code>: Configured encoder</li> </ul>"},{"location":"api/models/vae/#create_decoder_unified","title":"<code>create_decoder_unified</code>","text":"<pre><code>def create_decoder_unified(\n    *,\n    config: ModelConfig,\n    decoder_type: str,\n    conditional: bool = False,\n    num_classes: int | None = None,\n    embed_dim: int | None = None,\n    rngs: nnx.Rngs,\n) -&gt; nnx.Module:\n    \"\"\"Create decoder from unified configuration.\"\"\"\n</code></pre> <p>Parameters:</p> <ul> <li><code>config</code> (ModelConfig): Model configuration</li> <li><code>decoder_type</code> (str): Type of decoder: <code>\"dense\"</code>, <code>\"cnn\"</code>, <code>\"resnet\"</code></li> <li><code>conditional</code> (bool): Whether to wrap in conditional decoder</li> <li><code>num_classes</code> (int, optional): Number of classes for conditioning</li> <li><code>embed_dim</code> (int, optional): Embedding dimension</li> <li><code>rngs</code> (Rngs): Random number generators</li> </ul> <p>Returns:</p> <ul> <li><code>nnx.Module</code>: Configured decoder</li> </ul>"},{"location":"api/models/vae/#complete-example","title":"Complete Example","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.models.vae import BetaVAE\nfrom artifex.generative_models.models.vae.encoders import CNNEncoder\nfrom artifex.generative_models.models.vae.decoders import CNNDecoder\n\n# Initialize\nrngs = nnx.Rngs(params=0, dropout=1, sample=2)\n\n# Create encoder\nencoder = CNNEncoder(\n    hidden_dims=[32, 64, 128, 256],\n    latent_dim=64,\n    activation=\"relu\",\n    input_dim=(28, 28, 1),\n    rngs=rngs,\n)\n\n# Create decoder\ndecoder = CNNDecoder(\n    hidden_dims=[256, 128, 64, 32],\n    output_dim=(28, 28, 1),\n    latent_dim=64,\n    activation=\"relu\",\n    rngs=rngs,\n)\n\n# Create \u03b2-VAE\nmodel = BetaVAE(\n    encoder=encoder,\n    decoder=decoder,\n    latent_dim=64,\n    beta_default=4.0,\n    beta_warmup_steps=10000,\n    reconstruction_loss_type=\"mse\",\n    rngs=rngs,\n)\n\n# Forward pass\nx = jnp.ones((32, 28, 28, 1))\noutputs = model(x, rngs=rngs)\n\n# Calculate loss\nlosses = model.loss_fn(x=x, outputs=outputs, step=5000)\nprint(f\"Total Loss: {losses['total_loss']:.4f}\")\nprint(f\"Beta: {losses['beta']:.4f}\")\n\n# Generate samples\nsamples = model.sample(n_samples=16, temperature=1.0, rngs=rngs)\n\n# Latent traversal\ntraversal = model.latent_traversal(x[0], dim=10, steps=15, rngs=rngs)\n</code></pre>"},{"location":"api/models/vae/#see-also","title":"See Also","text":"<ul> <li>VAE Concepts \u2014 Theory and mathematical foundations</li> <li>VAE User Guide \u2014 Practical usage and examples</li> <li>Training Guide \u2014 Training VAE models</li> <li>Loss Functions \u2014 Available loss functions</li> <li>Configuration \u2014 Configuration system</li> </ul>"},{"location":"api/training/trainer/","title":"Trainer API Reference","text":"<p>Complete API reference for Artifex's training system.</p>"},{"location":"api/training/trainer/#overview","title":"Overview","text":"<p>The training system provides a robust infrastructure for training generative models, with ongoing work towards production-ready status. The main components are:</p> <ul> <li><code>Trainer</code>: Main training class handling the complete training loop</li> <li><code>TrainingState</code>: Immutable state container for training</li> <li>Configuration Classes: Type-safe configuration with Pydantic</li> </ul>"},{"location":"api/training/trainer/#trainer","title":"Trainer","text":"<p>Main class for training generative models.</p> <pre><code>from artifex.generative_models.training import Trainer\n</code></pre>"},{"location":"api/training/trainer/#constructor","title":"Constructor","text":"<pre><code>Trainer(\n    model: Any,\n    training_config: TrainingConfig,\n    optimizer: optax.GradientTransformation | None = None,\n    train_data_loader: Callable | None = None,\n    val_data_loader: Callable | None = None,\n    workdir: str | None = None,\n    rng: jax.Array | None = None,\n    loss_fn: Callable | None = None,\n    metrics_logger: MetricsLogger | None = None,\n    logger: Logger | None = None,\n    checkpoint_dir: str | None = None,\n    save_interval: int = 1000,\n    log_callback: Callable | None = None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>model</code> <code>Any</code> required Model to train (must have parameters) <code>training_config</code> <code>TrainingConfig</code> required Type-safe training configuration <code>optimizer</code> <code>optax.GradientTransformation \\| None</code> <code>None</code> Custom optimizer (auto-created if None) <code>train_data_loader</code> <code>Callable \\| None</code> <code>None</code> Function returning training data iterator <code>val_data_loader</code> <code>Callable \\| None</code> <code>None</code> Function returning validation data iterator <code>workdir</code> <code>str \\| None</code> <code>None</code> Working directory for outputs <code>rng</code> <code>jax.Array \\| None</code> <code>None</code> JAX random key (default: PRNGKey(0)) <code>loss_fn</code> <code>Callable \\| None</code> <code>None</code> Custom loss function <code>metrics_logger</code> <code>MetricsLogger \\| None</code> <code>None</code> Metrics logger instance <code>logger</code> <code>Logger \\| None</code> <code>None</code> General logger instance <code>checkpoint_dir</code> <code>str \\| None</code> <code>None</code> Checkpoint directory (default: workdir) <code>save_interval</code> <code>int</code> <code>1000</code> Save checkpoint every N steps <code>log_callback</code> <code>Callable \\| None</code> <code>None</code> Custom logging callback <p>Example:</p> <pre><code>from artifex.generative_models.training import Trainer\nfrom artifex.generative_models.core.configuration import (\n    TrainingConfig,\n    OptimizerConfig,\n)\n\n# Create training configuration\noptimizer_config = OptimizerConfig(\n    name=\"adam\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n)\n\ntraining_config = TrainingConfig(\n    name=\"vae_training\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=optimizer_config,\n)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    train_data_loader=train_loader,\n    val_data_loader=val_loader,\n    workdir=\"./experiments/vae\",\n)\n</code></pre>"},{"location":"api/training/trainer/#methods","title":"Methods","text":""},{"location":"api/training/trainer/#train_step","title":"train_step","text":"<p>Execute a single training step.</p> <pre><code>def train_step(\n    batch: dict[str, Any]\n) -&gt; dict[str, Any]\n</code></pre> <p>Parameters:</p> <ul> <li><code>batch</code> (<code>dict[str, Any]</code>): Batch of training data</li> </ul> <p>Returns:</p> <ul> <li><code>dict[str, Any]</code>: Training metrics including loss</li> </ul> <p>Example:</p> <pre><code>batch = {\"images\": images, \"labels\": labels}\nmetrics = trainer.train_step(batch)\nprint(f\"Loss: {metrics['loss']:.4f}\")\n</code></pre>"},{"location":"api/training/trainer/#validate_step","title":"validate_step","text":"<p>Execute a single validation step.</p> <pre><code>def validate_step(\n    batch: dict[str, Any]\n) -&gt; dict[str, Any]\n</code></pre> <p>Parameters:</p> <ul> <li><code>batch</code> (<code>dict[str, Any]</code>): Batch of validation data</li> </ul> <p>Returns:</p> <ul> <li><code>dict[str, Any]</code>: Validation metrics including loss</li> </ul> <p>Example:</p> <pre><code>val_batch = {\"images\": val_images, \"labels\": val_labels}\nval_metrics = trainer.validate_step(val_batch)\nprint(f\"Val Loss: {val_metrics['loss']:.4f}\")\n</code></pre>"},{"location":"api/training/trainer/#train_epoch","title":"train_epoch","text":"<p>Train for one complete epoch.</p> <pre><code>def train_epoch() -&gt; dict[str, Any]\n</code></pre> <p>Returns:</p> <ul> <li><code>dict[str, Any]</code>: Average metrics for the epoch</li> </ul> <p>Example:</p> <pre><code>for epoch in range(num_epochs):\n    metrics = trainer.train_epoch()\n    print(f\"Epoch {epoch + 1}: Loss = {metrics['loss']:.4f}\")\n</code></pre> <p>Notes:</p> <ul> <li>Automatically saves checkpoints based on <code>save_frequency</code></li> <li>Calls training step for each batch in the data loader</li> <li>Returns averaged metrics over all batches</li> </ul>"},{"location":"api/training/trainer/#train","title":"train","text":"<p>Complete training loop with validation.</p> <pre><code>def train(\n    train_data: Any,\n    num_epochs: int,\n    batch_size: int,\n    val_data: Any | None = None,\n    val_interval: int = 100,\n) -&gt; dict[str, Any]\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>train_data</code> <code>Any</code> required Training data <code>num_epochs</code> <code>int</code> required Number of training epochs <code>batch_size</code> <code>int</code> required Batch size <code>val_data</code> <code>Any \\| None</code> <code>None</code> Validation data <code>val_interval</code> <code>int</code> <code>100</code> Validate every N steps <p>Returns:</p> <ul> <li><code>dict[str, Any]</code>: Final metrics after training</li> </ul> <p>Example:</p> <pre><code>final_metrics = trainer.train(\n    train_data=train_data,\n    num_epochs=100,\n    batch_size=128,\n    val_data=val_data,\n    val_interval=100,\n)\n</code></pre>"},{"location":"api/training/trainer/#evaluate","title":"evaluate","text":"<p>Evaluate model on a dataset.</p> <pre><code>def evaluate(\n    data: Any,\n    batch_size: int\n) -&gt; dict[str, Any]\n</code></pre> <p>Parameters:</p> <ul> <li><code>data</code> (<code>Any</code>): Evaluation data</li> <li><code>batch_size</code> (<code>int</code>): Batch size for evaluation</li> </ul> <p>Returns:</p> <ul> <li><code>dict[str, Any]</code>: Average evaluation metrics</li> </ul> <p>Example:</p> <pre><code>test_metrics = trainer.evaluate(test_data, batch_size=128)\nprint(f\"Test Loss: {test_metrics['loss']:.4f}\")\n</code></pre>"},{"location":"api/training/trainer/#generate_samples","title":"generate_samples","text":"<p>Generate samples from the trained model.</p> <pre><code>def generate_samples(\n    num_samples: int,\n    seed: int | None = None,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Parameters:</p> <ul> <li><code>num_samples</code> (<code>int</code>): Number of samples to generate</li> <li><code>seed</code> (<code>int | None</code>): Random seed for reproducibility</li> <li><code>**kwargs</code>: Additional arguments for model's generate method</li> </ul> <p>Returns:</p> <ul> <li><code>Any</code>: Generated samples</li> </ul> <p>Example:</p> <pre><code># Generate 16 samples\nsamples = trainer.generate_samples(num_samples=16, seed=42)\n\n# With temperature sampling\nsamples = trainer.generate_samples(\n    num_samples=16,\n    temperature=0.8,\n)\n</code></pre> <p>Raises:</p> <ul> <li><code>NotImplementedError</code>: If model doesn't have a <code>generate</code> method</li> </ul>"},{"location":"api/training/trainer/#save_checkpoint","title":"save_checkpoint","text":"<p>Save training checkpoint.</p> <pre><code>def save_checkpoint(\n    path: str | None = None\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>path</code> (<code>str | None</code>): Path to save checkpoint (default: auto-generated)</li> </ul> <p>Example:</p> <pre><code># Auto-generated path\ntrainer.save_checkpoint()\n\n# Custom path\ntrainer.save_checkpoint(\"./checkpoints/best_model.pkl\")\n</code></pre> <p>Notes:</p> <ul> <li>Saves complete training state (params, opt_state, step, rng)</li> <li>Creates checkpoint directory if it doesn't exist</li> <li>Uses pickle serialization</li> </ul>"},{"location":"api/training/trainer/#load_checkpoint","title":"load_checkpoint","text":"<p>Load training checkpoint.</p> <pre><code>def load_checkpoint(\n    path: str\n) -&gt; None\n</code></pre> <p>Parameters:</p> <ul> <li><code>path</code> (<code>str</code>): Path to checkpoint file</li> </ul> <p>Example:</p> <pre><code># Load checkpoint\ntrainer.load_checkpoint(\"./checkpoints/checkpoint_5000.pkl\")\n\n# Resume training\ntrainer.train_epoch()\n</code></pre> <p>Notes:</p> <ul> <li>Restores complete training state</li> <li>Updates internal state in-place</li> </ul>"},{"location":"api/training/trainer/#attributes","title":"Attributes","text":"Attribute Type Description <code>model</code> <code>Any</code> Model being trained <code>training_config</code> <code>TrainingConfig</code> Training configuration <code>optimizer</code> <code>optax.GradientTransformation</code> Optimizer instance <code>state</code> <code>dict[str, Any]</code> Current training state <code>train_metrics</code> <code>list[dict]</code> Training metrics history <code>val_metrics</code> <code>list[dict]</code> Validation metrics history <code>steps_per_epoch</code> <code>int</code> Number of steps per epoch"},{"location":"api/training/trainer/#trainingstate","title":"TrainingState","text":"<p>Immutable container for training state.</p> <pre><code>from artifex.generative_models.training.trainer import TrainingState\n</code></pre>"},{"location":"api/training/trainer/#constructor_1","title":"Constructor","text":"<pre><code>TrainingState(\n    step: int,\n    params: Any,\n    opt_state: optax.OptState,\n    rng: jax.Array,\n    best_loss: float = float(\"inf\"),\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>step</code> <code>int</code> required Current training step <code>params</code> <code>Any</code> required Model parameters <code>opt_state</code> <code>optax.OptState</code> required Optimizer state <code>rng</code> <code>jax.Array</code> required JAX random key <code>best_loss</code> <code>float</code> <code>float(\"inf\")</code> Best validation loss"},{"location":"api/training/trainer/#class-methods","title":"Class Methods","text":""},{"location":"api/training/trainer/#create","title":"create","text":"<p>Create a new training state.</p> <pre><code>@classmethod\ndef create(\n    cls,\n    params: Any,\n    opt_state: optax.OptState,\n    rng: jax.Array,\n    step: int = 0,\n    best_loss: float = float(\"inf\"),\n) -&gt; \"TrainingState\"\n</code></pre> <p>Example:</p> <pre><code>import jax\nimport optax\n\n# Create optimizer\noptimizer = optax.adam(1e-3)\nopt_state = optimizer.init(model_params)\n\n# Create training state\nstate = TrainingState.create(\n    params=model_params,\n    opt_state=opt_state,\n    rng=jax.random.PRNGKey(42),\n    step=0,\n)\n</code></pre>"},{"location":"api/training/trainer/#properties","title":"Properties","text":"Property Type Description <code>step</code> <code>int</code> Current training step <code>params</code> <code>Any</code> Model parameters (PyTree) <code>opt_state</code> <code>optax.OptState</code> Optimizer state <code>rng</code> <code>jax.Array</code> JAX random key <code>best_loss</code> <code>float</code> Best validation loss seen"},{"location":"api/training/trainer/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/training/trainer/#trainingconfig","title":"TrainingConfig","text":"<p>Type-safe training configuration.</p> <pre><code>from artifex.generative_models.core.configuration import TrainingConfig\n</code></pre> <p>Fields:</p> Field Type Required Description <code>name</code> <code>str</code> \u2705 Configuration name <code>batch_size</code> <code>int</code> \u2705 Batch size (\u2265 1) <code>num_epochs</code> <code>int</code> \u2705 Number of epochs (\u2265 1) <code>optimizer</code> <code>OptimizerConfig</code> \u2705 Optimizer config <code>scheduler</code> <code>SchedulerConfig \\| None</code> \u274c LR scheduler config <code>gradient_clip_norm</code> <code>float \\| None</code> \u274c Gradient clipping norm <code>checkpoint_dir</code> <code>Path</code> \u274c Checkpoint directory <code>save_frequency</code> <code>int</code> \u274c Save every N steps <code>max_checkpoints</code> <code>int</code> \u274c Max checkpoints to keep <code>log_frequency</code> <code>int</code> \u274c Log every N steps <code>use_wandb</code> <code>bool</code> \u274c Use W&amp;B logging <code>wandb_project</code> <code>str \\| None</code> \u274c W&amp;B project name <p>Example:</p> <pre><code>training_config = TrainingConfig(\n    name=\"vae_training\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=optimizer_config,\n    scheduler=scheduler_config,\n    save_frequency=5000,\n    log_frequency=100,\n)\n</code></pre>"},{"location":"api/training/trainer/#optimizerconfig","title":"OptimizerConfig","text":"<p>Configure optimizers.</p> <pre><code>from artifex.generative_models.core.configuration import OptimizerConfig\n</code></pre> <p>Fields:</p> Field Type Required Description <code>name</code> <code>str</code> \u2705 Configuration name <code>optimizer_type</code> <code>str</code> \u2705 Optimizer type <code>learning_rate</code> <code>float</code> \u2705 Learning rate (&gt; 0) <code>weight_decay</code> <code>float</code> \u274c Weight decay (\u2265 0) <code>beta1</code> <code>float</code> \u274c Beta1 for Adam <code>beta2</code> <code>float</code> \u274c Beta2 for Adam <code>eps</code> <code>float</code> \u274c Epsilon for stability <code>momentum</code> <code>float</code> \u274c Momentum for SGD <code>nesterov</code> <code>bool</code> \u274c Use Nesterov momentum <code>gradient_clip_norm</code> <code>float \\| None</code> \u274c Gradient clip by norm <code>gradient_clip_value</code> <code>float \\| None</code> \u274c Gradient clip by value <p>Supported Optimizer Types:</p> <ul> <li><code>\"adam\"</code>: Adam optimizer</li> <li><code>\"adamw\"</code>: AdamW with weight decay</li> <li><code>\"sgd\"</code>: Stochastic Gradient Descent</li> <li><code>\"rmsprop\"</code>: RMSProp</li> <li><code>\"adagrad\"</code>: AdaGrad</li> </ul> <p>Example:</p> <pre><code>optimizer_config = OptimizerConfig(\n    name=\"adamw\",\n    optimizer_type=\"adamw\",\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    gradient_clip_norm=1.0,\n)\n</code></pre>"},{"location":"api/training/trainer/#schedulerconfig","title":"SchedulerConfig","text":"<p>Configure learning rate schedules.</p> <pre><code>from artifex.generative_models.core.configuration import SchedulerConfig\n</code></pre> <p>Fields:</p> Field Type Required Description <code>name</code> <code>str</code> \u2705 Configuration name <code>scheduler_type</code> <code>str</code> \u2705 Scheduler type <code>warmup_steps</code> <code>int</code> \u274c Warmup steps <code>min_lr_ratio</code> <code>float</code> \u274c Min LR ratio <code>cycle_length</code> <code>int \\| None</code> \u274c Cosine cycle length <code>total_steps</code> <code>int \\| None</code> \u274c Total steps (linear) <code>decay_rate</code> <code>float</code> \u274c Exponential decay rate <code>decay_steps</code> <code>int</code> \u274c Exponential decay steps <code>step_size</code> <code>int</code> \u274c Step schedule size <code>gamma</code> <code>float</code> \u274c Step/multistep gamma <code>milestones</code> <code>list[int]</code> \u274c Multistep milestones <p>Supported Scheduler Types:</p> <ul> <li><code>\"constant\"</code>: Constant learning rate</li> <li><code>\"linear\"</code>: Linear decay</li> <li><code>\"cosine\"</code>: Cosine annealing</li> <li><code>\"exponential\"</code>: Exponential decay</li> <li><code>\"step\"</code>: Step-wise decay</li> <li><code>\"multistep\"</code>: Multiple milestone decay</li> </ul> <p>Example:</p> <pre><code>scheduler_config = SchedulerConfig(\n    name=\"cosine_warmup\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    cycle_length=50000,\n    min_lr_ratio=0.1,\n)\n</code></pre>"},{"location":"api/training/trainer/#custom-loss-functions","title":"Custom Loss Functions","text":"<p>Implement custom loss functions for specialized training:</p> <pre><code>def custom_loss_fn(params, batch, rng):\n    \"\"\"Custom loss function.\n\n    Args:\n        params: Model parameters\n        batch: Batch of data\n        rng: JAX random key\n\n    Returns:\n        Tuple of (loss, metrics_dict)\n    \"\"\"\n    # Forward pass\n    model_outputs = model.apply(params, batch, rngs=rng)\n\n    # Compute loss\n    loss = jnp.mean((model_outputs - batch[\"targets\"]) ** 2)\n\n    # Additional metrics\n    metrics = {\n        \"mse\": loss,\n        \"mae\": jnp.mean(jnp.abs(model_outputs - batch[\"targets\"])),\n    }\n\n    return loss, metrics\n\n# Use custom loss function\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    loss_fn=custom_loss_fn,\n)\n</code></pre>"},{"location":"api/training/trainer/#logging-callbacks","title":"Logging Callbacks","text":"<p>Artifex provides built-in logging callbacks for common experiment tracking tools:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n    TensorBoardLoggerCallback,\n    TensorBoardLoggerConfig,\n    ProgressBarCallback,\n)\n\n# W&amp;B logging\nwandb_callback = WandbLoggerCallback(WandbLoggerConfig(\n    project=\"my-project\",\n    name=\"experiment-1\",\n))\n\n# TensorBoard logging\ntb_callback = TensorBoardLoggerCallback(TensorBoardLoggerConfig(\n    log_dir=\"logs/experiment-1\",\n))\n\n# Progress bar\nprogress_callback = ProgressBarCallback()\n\n# Use callbacks with trainer\ntrainer.fit(callbacks=[wandb_callback, tb_callback, progress_callback])\n</code></pre> <p>See Logging Callbacks for full documentation.</p>"},{"location":"api/training/trainer/#custom-logging-callbacks","title":"Custom Logging Callbacks","text":"<p>For custom logging needs, implement a callback:</p> <pre><code>from artifex.generative_models.training.callbacks import BaseCallback\n\nclass CustomLoggerCallback(BaseCallback):\n    \"\"\"Custom logging callback.\"\"\"\n\n    def on_batch_end(self, trainer, batch, logs=None):\n        if logs and trainer.global_step % 100 == 0:\n            # Your custom logging logic\n            print(f\"Step {trainer.global_step}: {logs}\")\n\n# Use custom callback\ntrainer.fit(callbacks=[CustomLoggerCallback()])\n</code></pre>"},{"location":"api/training/trainer/#complete-training-example","title":"Complete Training Example","text":"<p>Full example with all components:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    ModelConfig,\n    TrainingConfig,\n    OptimizerConfig,\n    SchedulerConfig,\n)\nfrom artifex.generative_models.factory import create_model\nfrom artifex.generative_models.training import Trainer\nfrom flax import nnx\n\n# 1. Model configuration\nmodel_config = ModelConfig(\n    name=\"vae_mnist\",\n    model_class=\"artifex.generative_models.models.vae.base.VAE\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[512, 256],\n    output_dim=64,\n    parameters={\"beta\": 1.0},\n)\n\n# 2. Create model\nrngs = nnx.Rngs(42)\nmodel = create_model(config=model_config, rngs=rngs)\n\n# 3. Optimizer configuration\noptimizer_config = OptimizerConfig(\n    name=\"adamw\",\n    optimizer_type=\"adamw\",\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    gradient_clip_norm=1.0,\n)\n\n# 4. Scheduler configuration\nscheduler_config = SchedulerConfig(\n    name=\"cosine_warmup\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    cycle_length=50000,\n    min_lr_ratio=0.1,\n)\n\n# 5. Training configuration\ntraining_config = TrainingConfig(\n    name=\"vae_training\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=optimizer_config,\n    scheduler=scheduler_config,\n    save_frequency=5000,\n    log_frequency=100,\n    checkpoint_dir=\"./checkpoints/vae\",\n)\n\n# 6. Create trainer\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    train_data_loader=train_loader,\n    val_data_loader=val_loader,\n    workdir=\"./experiments/vae\",\n)\n\n# 7. Training loop\nfor epoch in range(training_config.num_epochs):\n    # Train epoch\n    train_metrics = trainer.train_epoch()\n    print(f\"Epoch {epoch + 1}: Train Loss = {train_metrics['loss']:.4f}\")\n\n    # Validate\n    val_metrics = trainer.evaluate(val_data, batch_size=128)\n    print(f\"Epoch {epoch + 1}: Val Loss = {val_metrics['loss']:.4f}\")\n\n    # Save best model\n    if val_metrics['loss'] &lt; trainer.state.get('best_loss', float('inf')):\n        trainer.save_checkpoint(\"./checkpoints/vae/best_model.pkl\")\n\n# 8. Generate samples\nsamples = trainer.generate_samples(num_samples=16, seed=42)\n</code></pre>"},{"location":"api/training/trainer/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/training/trainer/#custom-training-step","title":"Custom Training Step","text":"<p>Override the training step for specialized training:</p> <pre><code>from functools import partial\nimport jax\nimport optax\nfrom flax import nnx\n\nclass CustomTrainer(Trainer):\n    \"\"\"Custom trainer with specialized training step.\"\"\"\n\n    def _train_step(self, state, batch):\n        \"\"\"Custom training step with additional processing.\"\"\"\n        rng, step_rng = jax.random.split(state[\"rng\"])\n\n        # Custom preprocessing\n        batch = self.preprocess_batch(batch, step_rng)\n\n        # Custom loss computation\n        def loss_fn(params):\n            loss, metrics = self.compute_custom_loss(params, batch, step_rng)\n            return loss, metrics\n\n        (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n            state[\"params\"]\n        )\n\n        # Custom gradient processing\n        grads = self.process_gradients(grads)\n\n        # Update parameters\n        updates, opt_state = self.optimizer.update(\n            grads, state[\"opt_state\"], state[\"params\"]\n        )\n        params = optax.apply_updates(state[\"params\"], updates)\n\n        new_state = {\n            \"step\": state[\"step\"] + 1,\n            \"params\": params,\n            \"opt_state\": opt_state,\n            \"rng\": rng,\n        }\n\n        return new_state, metrics\n\n    def preprocess_batch(self, batch, rng):\n        \"\"\"Custom batch preprocessing.\"\"\"\n        # Your preprocessing logic\n        return batch\n\n    def compute_custom_loss(self, params, batch, rng):\n        \"\"\"Custom loss computation.\"\"\"\n        # Your loss logic\n        pass\n\n    def process_gradients(self, grads):\n        \"\"\"Custom gradient processing.\"\"\"\n        # Your gradient processing logic\n        return grads\n\n# Use custom trainer\ntrainer = CustomTrainer(\n    model=model,\n    training_config=training_config,\n    train_data_loader=train_loader,\n)\n</code></pre>"},{"location":"api/training/trainer/#distributed-training","title":"Distributed Training","text":"<p>Extend trainer for distributed training:</p> <pre><code>import jax\n\nclass DistributedTrainer(Trainer):\n    \"\"\"Trainer for distributed training across devices.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Get available devices\n        self.devices = jax.devices()\n        print(f\"Training on {len(self.devices)} devices\")\n\n        # Replicate model parameters\n        self.replicated_params = jax.device_put_replicated(\n            self.state[\"params\"],\n            self.devices\n        )\n\n    @partial(jax.pmap, axis_name=\"devices\")\n    def distributed_train_step(self, state, batch):\n        \"\"\"Training step parallelized across devices.\"\"\"\n        def loss_fn(params):\n            outputs = self.model.apply(params, batch, training=True)\n            return outputs[\"loss\"], outputs\n\n        (loss, outputs), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n            state[\"params\"]\n        )\n\n        # Average gradients across devices\n        grads = jax.lax.pmean(grads, axis_name=\"devices\")\n\n        # Update\n        updates, opt_state = self.optimizer.update(grads, state[\"opt_state\"])\n        params = optax.apply_updates(state[\"params\"], updates)\n\n        new_state = {\n            \"step\": state[\"step\"] + 1,\n            \"params\": params,\n            \"opt_state\": opt_state,\n            \"rng\": state[\"rng\"],\n        }\n\n        return new_state, loss\n\n# Use distributed trainer\ndistributed_trainer = DistributedTrainer(\n    model=model,\n    training_config=training_config,\n    train_data_loader=train_loader,\n)\n</code></pre>"},{"location":"api/training/trainer/#type-hints","title":"Type Hints","text":"<p>The Trainer API uses type hints for clarity:</p> <pre><code>from typing import Any, Callable\nimport jax\nimport jax.numpy as jnp\nimport optax\n\n# Type aliases\nBatch = dict[str, jax.Array]\nMetrics = dict[str, float]\nLossFn = Callable[[Any, Batch, jax.Array], tuple[float, Metrics]]\n\n# Usage in custom code\ndef my_loss_fn(\n    params: Any,\n    batch: Batch,\n    rng: jax.Array\n) -&gt; tuple[float, Metrics]:\n    \"\"\"Type-safe loss function.\"\"\"\n    pass\n</code></pre>"},{"location":"api/training/trainer/#summary","title":"Summary","text":"<p>The Trainer API provides:</p> <p>\u2705 Simple Interface: Easy to use for common cases \u2705 Type-Safe: Pydantic-based configuration \u2705 Flexible: Extensible for custom training logic \u2705 Research-Focused: Checkpointing, logging, monitoring for experimentation \u2705 Well-Documented: Complete API reference with examples</p>"},{"location":"api/training/trainer/#see-also","title":"See Also","text":"<ul> <li>Training Guide - Practical training examples</li> <li>Training Overview - Architecture and concepts</li> <li>Configuration Guide - Configuration system details</li> <li>Core API - Core model interfaces</li> </ul> <p>For practical examples, see the Training Guide.</p>"},{"location":"architecture/extensions/","title":"Extensions Architecture","text":"<p>Coming Soon</p> <p>This page is under development. Check back for comprehensive extensions architecture documentation.</p>"},{"location":"architecture/extensions/#overview","title":"Overview","text":"<p>The extensions architecture enables modular addition of domain-specific features.</p>"},{"location":"architecture/extensions/#related-documentation","title":"Related Documentation","text":"<ul> <li>Extensions Guide</li> <li>Custom Extensions</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks &amp; Evaluation","text":"<p>Artifex provides a comprehensive benchmarking framework for evaluating generative models across different modalities. This includes standardized metrics, evaluation protocols, and benchmark suites for rigorous model comparison.</p>"},{"location":"benchmarks/#overview","title":"Overview","text":"<ul> <li> <p> Image Metrics</p> <p>FID, Inception Score, LPIPS, Precision/Recall for image generation quality</p> <p> Image Metrics</p> </li> <li> <p> Text Metrics</p> <p>Perplexity, BLEU, ROUGE, and diversity scores for text generation</p> <p> Text Metrics</p> </li> <li> <p> Audio Metrics</p> <p>MCD, spectral metrics, and perceptual quality for audio generation</p> <p> Audio Metrics</p> </li> <li> <p> Disentanglement</p> <p>MIG, SAP, DCI, and FactorVAE scores for latent space evaluation</p> <p> Disentanglement Metrics</p> </li> </ul>"},{"location":"benchmarks/#quick-start","title":"Quick Start","text":"<p>Run benchmarks on your models with just a few lines of code:</p> <pre><code>from artifex.benchmarks import BenchmarkRunner\nfrom artifex.benchmarks.metrics.image import FIDMetric, InceptionScoreMetric\nfrom artifex.benchmarks.suites.image_suite import ImageBenchmarkSuite\n\n# Create benchmark suite\nsuite = ImageBenchmarkSuite(\n    metrics=[FIDMetric(), InceptionScoreMetric()],\n    dataset_name=\"cifar10\",\n)\n\n# Run benchmarks\nresults = suite.evaluate(model, num_samples=10000)\n\n# Print results\nprint(f\"FID: {results['fid']:.2f}\")\nprint(f\"IS: {results['inception_score']:.2f} +/- {results['inception_score_std']:.2f}\")\n</code></pre>"},{"location":"benchmarks/#image-metrics","title":"Image Metrics","text":"<p>Metrics for evaluating image generation quality, diversity, and realism.</p>"},{"location":"benchmarks/#fid-frechet-inception-distance","title":"FID (Frechet Inception Distance)","text":"<p>Measures the distance between feature distributions of real and generated images.</p> <pre><code>from artifex.benchmarks.metrics.image import FIDMetric\n\nfid_metric = FIDMetric()\nfid_score = fid_metric.compute(real_images, generated_images)\nprint(f\"FID: {fid_score:.2f}\")  # Lower is better\n</code></pre> <p>Interpretation:</p> <ul> <li>FID &lt; 10: Excellent quality</li> <li>FID 10-50: Good quality</li> <li>FID 50-100: Moderate quality</li> <li>FID &gt; 100: Poor quality</li> </ul>"},{"location":"benchmarks/#inception-score-is","title":"Inception Score (IS)","text":"<p>Measures both quality and diversity of generated images.</p> <pre><code>from artifex.benchmarks.metrics.image import InceptionScoreMetric\n\nis_metric = InceptionScoreMetric()\nis_score, is_std = is_metric.compute(generated_images)\nprint(f\"IS: {is_score:.2f} +/- {is_std:.2f}\")  # Higher is better\n</code></pre>"},{"location":"benchmarks/#lpips-learned-perceptual-image-patch-similarity","title":"LPIPS (Learned Perceptual Image Patch Similarity)","text":"<p>Measures perceptual similarity using deep features.</p> <pre><code>from artifex.benchmarks.metrics.image import LPIPSMetric\n\nlpips_metric = LPIPSMetric()\nlpips_score = lpips_metric.compute(image1, image2)\nprint(f\"LPIPS: {lpips_score:.4f}\")  # Lower means more similar\n</code></pre>"},{"location":"benchmarks/#precision-and-recall","title":"Precision and Recall","text":"<p>Measures coverage and quality separately.</p> <pre><code>from artifex.benchmarks.metrics.precision_recall import PrecisionRecallMetric\n\npr_metric = PrecisionRecallMetric()\nprecision, recall = pr_metric.compute(real_images, generated_images)\nprint(f\"Precision: {precision:.3f}\")  # Quality (higher is better)\nprint(f\"Recall: {recall:.3f}\")        # Coverage (higher is better)\n</code></pre>"},{"location":"benchmarks/#documentation","title":"Documentation","text":"<ul> <li>Image Metrics - Detailed image metric implementations</li> <li>Unified Image Metrics - Combined image evaluation</li> <li>Inception Metrics - FID and IS details</li> <li>Precision/Recall - P/R implementation</li> </ul>"},{"location":"benchmarks/#text-metrics","title":"Text Metrics","text":"<p>Metrics for evaluating text generation quality and coherence.</p>"},{"location":"benchmarks/#perplexity","title":"Perplexity","text":"<p>Measures how well a model predicts text.</p> <pre><code>from artifex.benchmarks.metrics.text import PerplexityMetric\n\nppl_metric = PerplexityMetric()\nperplexity = ppl_metric.compute(model, test_text)\nprint(f\"Perplexity: {perplexity:.2f}\")  # Lower is better\n</code></pre>"},{"location":"benchmarks/#bleu-score","title":"BLEU Score","text":"<p>Measures n-gram overlap with reference text.</p> <pre><code>from artifex.benchmarks.metrics.text import BLEUMetric\n\nbleu_metric = BLEUMetric()\nbleu_score = bleu_metric.compute(generated_text, reference_text)\nprint(f\"BLEU: {bleu_score:.3f}\")  # Higher is better (0-1)\n</code></pre>"},{"location":"benchmarks/#diversity-metrics","title":"Diversity Metrics","text":"<p>Measures vocabulary diversity and uniqueness.</p> <pre><code>from artifex.benchmarks.metrics.diversity import DiversityMetric\n\ndiversity_metric = DiversityMetric()\nscores = diversity_metric.compute(generated_texts)\nprint(f\"Distinct-1: {scores['distinct_1']:.3f}\")\nprint(f\"Distinct-2: {scores['distinct_2']:.3f}\")\n</code></pre>"},{"location":"benchmarks/#documentation_1","title":"Documentation","text":"<ul> <li>Text Metrics - Text evaluation metrics</li> <li>Perplexity - Perplexity implementation</li> <li>Diversity - Diversity metrics</li> </ul>"},{"location":"benchmarks/#audio-metrics","title":"Audio Metrics","text":"<p>Metrics for evaluating audio generation quality.</p>"},{"location":"benchmarks/#mcd-mel-cepstral-distortion","title":"MCD (Mel-Cepstral Distortion)","text":"<p>Measures spectral difference between audio signals.</p> <pre><code>from artifex.benchmarks.metrics.audio import MCDMetric\n\nmcd_metric = MCDMetric()\nmcd_score = mcd_metric.compute(generated_audio, reference_audio)\nprint(f\"MCD: {mcd_score:.2f} dB\")  # Lower is better\n</code></pre>"},{"location":"benchmarks/#spectral-metrics","title":"Spectral Metrics","text":"<p>Measures frequency-domain characteristics.</p> <pre><code>from artifex.benchmarks.metrics.audio import SpectralMetrics\n\nspectral_metrics = SpectralMetrics()\nresults = spectral_metrics.compute(generated_audio, reference_audio)\nprint(f\"Spectral Convergence: {results['spectral_convergence']:.4f}\")\nprint(f\"Log STFT Magnitude: {results['log_stft_magnitude']:.4f}\")\n</code></pre>"},{"location":"benchmarks/#documentation_2","title":"Documentation","text":"<ul> <li>Audio Metrics - Audio evaluation metrics</li> </ul>"},{"location":"benchmarks/#disentanglement-metrics","title":"Disentanglement Metrics","text":"<p>Metrics for evaluating VAE latent space quality.</p>"},{"location":"benchmarks/#mig-mutual-information-gap","title":"MIG (Mutual Information Gap)","text":"<p>Measures how well individual latents capture single factors.</p> <pre><code>from artifex.benchmarks.metrics.disentanglement import MIGMetric\n\nmig_metric = MIGMetric()\nmig_score = mig_metric.compute(model, dataset)\nprint(f\"MIG: {mig_score:.3f}\")  # Higher is better (0-1)\n</code></pre>"},{"location":"benchmarks/#sap-separated-attribute-predictability","title":"SAP (Separated Attribute Predictability)","text":"<p>Measures predictability of factors from latents.</p> <pre><code>from artifex.benchmarks.metrics.disentanglement import SAPMetric\n\nsap_metric = SAPMetric()\nsap_score = sap_metric.compute(model, dataset)\nprint(f\"SAP: {sap_score:.3f}\")  # Higher is better (0-1)\n</code></pre>"},{"location":"benchmarks/#dci-disentanglement-completeness-informativeness","title":"DCI (Disentanglement, Completeness, Informativeness)","text":"<p>Comprehensive disentanglement evaluation.</p> <pre><code>from artifex.benchmarks.metrics.disentanglement import DCIMetric\n\ndci_metric = DCIMetric()\nresults = dci_metric.compute(model, dataset)\nprint(f\"Disentanglement: {results['disentanglement']:.3f}\")\nprint(f\"Completeness: {results['completeness']:.3f}\")\nprint(f\"Informativeness: {results['informativeness']:.3f}\")\n</code></pre>"},{"location":"benchmarks/#documentation_3","title":"Documentation","text":"<ul> <li>Disentanglement Metrics - Latent space evaluation</li> </ul>"},{"location":"benchmarks/#benchmark-suites","title":"Benchmark Suites","text":"<p>Pre-configured evaluation protocols for different use cases.</p>"},{"location":"benchmarks/#image-suite","title":"Image Suite","text":"<pre><code>from artifex.benchmarks.suites.image_suite import ImageBenchmarkSuite\n\nsuite = ImageBenchmarkSuite(\n    dataset_name=\"cifar10\",\n    metrics=[\"fid\", \"is\", \"lpips\", \"precision_recall\"],\n)\n\nresults = suite.run(model, num_samples=50000)\n</code></pre>"},{"location":"benchmarks/#multi-beta-vae-suite","title":"Multi-Beta VAE Suite","text":"<pre><code>from artifex.benchmarks.suites.multi_beta_vae_suite import MultiBetaVAESuite\n\nsuite = MultiBetaVAESuite(\n    beta_values=[0.1, 0.5, 1.0, 2.0, 4.0],\n    metrics=[\"reconstruction\", \"kl\", \"mig\", \"sap\"],\n)\n\nresults = suite.run(model, dataset)\n</code></pre>"},{"location":"benchmarks/#geometric-suite","title":"Geometric Suite","text":"<pre><code>from artifex.benchmarks.suites.geometric_suite import GeometricBenchmarkSuite\n\nsuite = GeometricBenchmarkSuite(\n    metrics=[\"chamfer\", \"earth_mover\", \"coverage\"],\n)\n\nresults = suite.run(model, point_cloud_dataset)\n</code></pre>"},{"location":"benchmarks/#documentation_4","title":"Documentation","text":"<ul> <li>Image Suite - Image benchmark suite</li> <li>Text Suite - Text benchmark suite</li> <li>Audio Suite - Audio benchmark suite</li> <li>Geometric Suite - 3D geometry benchmarks</li> <li>Multi-Beta VAE Suite - VAE disentanglement</li> <li>Standard Suite - Standard evaluation protocols</li> </ul>"},{"location":"benchmarks/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Measure computational performance of models.</p>"},{"location":"benchmarks/#latency","title":"Latency","text":"<pre><code>from artifex.benchmarks.performance.latency import LatencyBenchmark\n\nlatency_bench = LatencyBenchmark()\nresults = latency_bench.measure(model, batch_size=32, num_iterations=100)\nprint(f\"Mean latency: {results['mean_ms']:.2f} ms\")\nprint(f\"P99 latency: {results['p99_ms']:.2f} ms\")\n</code></pre>"},{"location":"benchmarks/#throughput","title":"Throughput","text":"<pre><code>from artifex.benchmarks.performance.throughput import ThroughputBenchmark\n\nthroughput_bench = ThroughputBenchmark()\nresults = throughput_bench.measure(model, batch_sizes=[1, 8, 32, 64, 128])\nprint(f\"Peak throughput: {results['peak_samples_per_sec']:.1f} samples/sec\")\n</code></pre>"},{"location":"benchmarks/#memory","title":"Memory","text":"<pre><code>from artifex.benchmarks.performance.memory import MemoryBenchmark\n\nmemory_bench = MemoryBenchmark()\nresults = memory_bench.measure(model, batch_size=32)\nprint(f\"Peak memory: {results['peak_mb']:.1f} MB\")\n</code></pre>"},{"location":"benchmarks/#documentation_5","title":"Documentation","text":"<ul> <li>Latency - Latency measurements</li> <li>Throughput - Throughput benchmarks</li> <li>Memory - Memory profiling</li> <li>Scaling - Scaling analysis</li> <li>Optimization - Performance optimization</li> </ul>"},{"location":"benchmarks/#visualization","title":"Visualization","text":"<p>Tools for visualizing benchmark results.</p> <pre><code>from artifex.benchmarks.visualization.plots import plot_metrics\nfrom artifex.benchmarks.visualization.comparison import compare_models\n\n# Plot metrics over training\nplot_metrics(training_history, metrics=[\"fid\", \"is\"])\n\n# Compare multiple models\ncompare_models(\n    models={\"VAE\": vae_results, \"GAN\": gan_results, \"Diffusion\": diffusion_results},\n    metrics=[\"fid\", \"is\", \"lpips\"],\n)\n</code></pre>"},{"location":"benchmarks/#documentation_6","title":"Documentation","text":"<ul> <li>Plots - Plotting utilities</li> <li>Comparison - Model comparison</li> <li>Dashboard - Interactive dashboard</li> <li>Image Grid - Sample visualization</li> </ul>"},{"location":"benchmarks/#datasets","title":"Datasets","text":"<p>Benchmark datasets available in Artifex.</p> Dataset Modality Size Use Case CIFAR-10 Image 60K General image benchmarks CelebA Image 200K Face generation FFHQ Image 70K High-quality faces QM9 Molecular 134K Molecular generation CrossDocked Protein 22M Protein-ligand docking"},{"location":"benchmarks/#documentation_7","title":"Documentation","text":"<ul> <li>CelebA - CelebA dataset</li> <li>FFHQ - FFHQ dataset</li> <li>QM9 - QM9 molecular dataset</li> <li>CrossDocked - Protein-ligand dataset</li> <li>Synthetic Datasets - Synthetic data generation</li> </ul>"},{"location":"benchmarks/#best-practices","title":"Best Practices","text":"<p>DO</p> <ul> <li>Use standardized evaluation protocols for fair comparison</li> <li>Report confidence intervals with multiple runs</li> <li>Use appropriate metrics for your modality</li> <li>Compare against established baselines</li> </ul> <p>DON'T</p> <ul> <li>Don't cherry-pick metrics that favor your model</li> <li>Don't use different sample sizes for comparisons</li> <li>Don't ignore statistical significance</li> <li>Don't compare models trained on different datasets</li> </ul>"},{"location":"benchmarks/#summary","title":"Summary","text":"<p>The Artifex benchmarking framework provides:</p> <ul> <li>Image Metrics: FID, IS, LPIPS, Precision/Recall</li> <li>Text Metrics: Perplexity, BLEU, ROUGE, diversity</li> <li>Audio Metrics: MCD, spectral analysis</li> <li>Disentanglement: MIG, SAP, DCI, FactorVAE</li> <li>Performance: Latency, throughput, memory profiling</li> <li>Visualization: Plots, comparisons, dashboards</li> </ul> <p>Use benchmark suites for standardized evaluation and fair model comparison.</p>"},{"location":"benchmarks/audio/","title":"Audio","text":"<p>Module: <code>benchmarks.metrics.audio</code></p> <p>Source: <code>benchmarks/metrics/audio.py</code></p>"},{"location":"benchmarks/audio/#overview","title":"Overview","text":"<p>Audio-specific metrics for generative model evaluation.</p>"},{"location":"benchmarks/audio/#classes","title":"Classes","text":""},{"location":"benchmarks/audio/#melcepstralmetric","title":"MelCepstralMetric","text":"<pre><code>class MelCepstralMetric\n</code></pre>"},{"location":"benchmarks/audio/#spectralmetric","title":"SpectralMetric","text":"<pre><code>class SpectralMetric\n</code></pre>"},{"location":"benchmarks/audio/#functions","title":"Functions","text":""},{"location":"benchmarks/audio/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/audio/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/audio/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/audio/#compute_1","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/audio/#create_mcd_metric","title":"create_mcd_metric","text":"<pre><code>def create_mcd_metric()\n</code></pre>"},{"location":"benchmarks/audio/#create_spectral_metric","title":"create_spectral_metric","text":"<pre><code>def create_spectral_metric()\n</code></pre>"},{"location":"benchmarks/audio/#validate_inputs","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/audio/#validate_inputs_1","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/audio/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 8</li> <li>Imports: 4</li> </ul>"},{"location":"benchmarks/audio_benchmarks/","title":"Audio Benchmarks","text":"<p>Module: <code>benchmarks.datasets.audio_benchmarks</code></p> <p>Source: <code>benchmarks/datasets/audio_benchmarks.py</code></p>"},{"location":"benchmarks/audio_benchmarks/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/audio_protocols/","title":"Audio Protocols","text":"<p>Module: <code>benchmarks.protocols.audio_protocols</code></p> <p>Source: <code>benchmarks/protocols/audio_protocols.py</code></p>"},{"location":"benchmarks/audio_protocols/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/audio_suite/","title":"Audio Suite","text":"<p>Module: <code>benchmarks.suites.audio_suite</code></p> <p>Source: <code>benchmarks/suites/audio_suite.py</code></p>"},{"location":"benchmarks/audio_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/base/","title":"Base","text":"<p>Module: <code>benchmarks.model_adapters.base</code></p> <p>Source: <code>benchmarks/model_adapters/base.py</code></p>"},{"location":"benchmarks/base/#overview","title":"Overview","text":"<p>Base model adapters for benchmarks.</p> <p>This module provides the base adapter classes for different model types to be used with the benchmark system.</p>"},{"location":"benchmarks/base/#classes","title":"Classes","text":""},{"location":"benchmarks/base/#benchmarkmodeladapter","title":"BenchmarkModelAdapter","text":"<pre><code>class BenchmarkModelAdapter\n</code></pre>"},{"location":"benchmarks/base/#nnxmodeladapter","title":"NNXModelAdapter","text":"<pre><code>class NNXModelAdapter\n</code></pre>"},{"location":"benchmarks/base/#functions","title":"Functions","text":""},{"location":"benchmarks/base/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/base/#adapt_model","title":"adapt_model","text":"<pre><code>def adapt_model()\n</code></pre>"},{"location":"benchmarks/base/#can_adapt","title":"can_adapt","text":"<pre><code>def can_adapt()\n</code></pre>"},{"location":"benchmarks/base/#can_adapt_1","title":"can_adapt","text":"<pre><code>def can_adapt()\n</code></pre>"},{"location":"benchmarks/base/#model_name","title":"model_name","text":"<pre><code>def model_name()\n</code></pre>"},{"location":"benchmarks/base/#predict","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"benchmarks/base/#predict_1","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"benchmarks/base/#register_adapter","title":"register_adapter","text":"<pre><code>def register_adapter()\n</code></pre>"},{"location":"benchmarks/base/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"benchmarks/base/#sample_1","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"benchmarks/base/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 10</li> <li>Imports: 5</li> </ul>"},{"location":"benchmarks/benchmark_runner/","title":"Benchmark Runner","text":"<p>Module: <code>benchmarks.cli.benchmark_runner</code></p> <p>Source: <code>benchmarks/cli/benchmark_runner.py</code></p>"},{"location":"benchmarks/benchmark_runner/#overview","title":"Overview","text":"<p>CLI runner for benchmarks.</p>"},{"location":"benchmarks/benchmark_runner/#classes","title":"Classes","text":""},{"location":"benchmarks/benchmark_runner/#dummymodel","title":"DummyModel","text":"<pre><code>class DummyModel\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#functions","title":"Functions","text":""},{"location":"benchmarks/benchmark_runner/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#load_model","title":"load_model","text":"<pre><code>def load_model()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#model_name","title":"model_name","text":"<pre><code>def model_name()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#predict","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#run_benchmark","title":"run_benchmark","text":"<pre><code>def run_benchmark()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#run_benchmark_suite","title":"run_benchmark_suite","text":"<pre><code>def run_benchmark_suite()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#run_command","title":"run_command","text":"<pre><code>def run_command()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#suite_command","title":"suite_command","text":"<pre><code>def suite_command()\n</code></pre>"},{"location":"benchmarks/benchmark_runner/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 9</li> <li>Imports: 9</li> </ul>"},{"location":"benchmarks/caching/","title":"Caching","text":"<p>Module: <code>benchmarks.utils.caching</code></p> <p>Source: <code>benchmarks/utils/caching.py</code></p>"},{"location":"benchmarks/caching/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/celeba/","title":"Celeba","text":"<p>Module: <code>benchmarks.datasets.celeba</code></p> <p>Source: <code>benchmarks/datasets/celeba.py</code></p>"},{"location":"benchmarks/celeba/#overview","title":"Overview","text":"<p>CelebA dataset for image generation benchmarks.</p> <p>This module provides a dataset implementation for the CelebA dataset, which is used for face generation and attribute manipulation benchmarks.</p>"},{"location":"benchmarks/celeba/#classes","title":"Classes","text":""},{"location":"benchmarks/celeba/#celebadataset","title":"CelebADataset","text":"<pre><code>class CelebADataset\n</code></pre>"},{"location":"benchmarks/celeba/#functions","title":"Functions","text":""},{"location":"benchmarks/celeba/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/celeba/#get_batch","title":"get_batch","text":"<pre><code>def get_batch()\n</code></pre>"},{"location":"benchmarks/celeba/#get_split","title":"get_split","text":"<pre><code>def get_split()\n</code></pre>"},{"location":"benchmarks/celeba/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 3</li> <li>Imports: 8</li> </ul>"},{"location":"benchmarks/comparison/","title":"Comparison","text":"<p>Module: <code>benchmarks.visualization.comparison</code></p> <p>Source: <code>benchmarks/visualization/comparison.py</code></p>"},{"location":"benchmarks/comparison/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/core/","title":"Core","text":"<p>Module: <code>benchmarks.protocols.core</code></p> <p>Source: <code>benchmarks/protocols/core.py</code></p>"},{"location":"benchmarks/core/#overview","title":"Overview","text":"<p>Core benchmark protocols and infrastructure.</p>"},{"location":"benchmarks/core/#classes","title":"Classes","text":""},{"location":"benchmarks/core/#benchmarkconfig","title":"BenchmarkConfig","text":"<pre><code>class BenchmarkConfig\n</code></pre>"},{"location":"benchmarks/core/#benchmarkresult","title":"BenchmarkResult","text":"<pre><code>class BenchmarkResult\n</code></pre>"},{"location":"benchmarks/core/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 0</li> <li>Imports: 3</li> </ul>"},{"location":"benchmarks/crossdocked/","title":"Crossdocked","text":"<p>Module: <code>benchmarks.datasets.crossdocked</code></p> <p>Source: <code>benchmarks/datasets/crossdocked.py</code></p>"},{"location":"benchmarks/crossdocked/#overview","title":"Overview","text":"<p>CrossDocked2020 dataset implementation.</p> <p>This module provides a dataset interface for the CrossDocked2020 dataset, which contains protein-ligand complexes for co-design benchmarks.</p>"},{"location":"benchmarks/crossdocked/#classes","title":"Classes","text":""},{"location":"benchmarks/crossdocked/#crossdockeddataset","title":"CrossDockedDataset","text":"<pre><code>class CrossDockedDataset\n</code></pre>"},{"location":"benchmarks/crossdocked/#functions","title":"Functions","text":""},{"location":"benchmarks/crossdocked/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/crossdocked/#extract_pocket","title":"extract_pocket","text":"<pre><code>def extract_pocket()\n</code></pre>"},{"location":"benchmarks/crossdocked/#get_batch","title":"get_batch","text":"<pre><code>def get_batch()\n</code></pre>"},{"location":"benchmarks/crossdocked/#get_dataset_info","title":"get_dataset_info","text":"<pre><code>def get_dataset_info()\n</code></pre>"},{"location":"benchmarks/crossdocked/#get_statistics","title":"get_statistics","text":"<pre><code>def get_statistics()\n</code></pre>"},{"location":"benchmarks/crossdocked/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 5</li> <li>Imports: 7</li> </ul>"},{"location":"benchmarks/dashboard/","title":"Dashboard","text":"<p>Module: <code>benchmarks.visualization.dashboard</code></p> <p>Source: <code>benchmarks/visualization/dashboard.py</code></p>"},{"location":"benchmarks/dashboard/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/dataset_loaders/","title":"Dataset Loaders","text":"<p>Module: <code>benchmarks.datasets.dataset_loaders</code></p> <p>Source: <code>benchmarks/datasets/dataset_loaders.py</code></p>"},{"location":"benchmarks/dataset_loaders/#overview","title":"Overview","text":"<p>Dataset loaders for benchmark datasets.</p> <p>This module implements the base dataset class for benchmarks, along with utility functions for loading and registering datasets.</p>"},{"location":"benchmarks/dataset_loaders/#classes","title":"Classes","text":""},{"location":"benchmarks/dataset_loaders/#benchmarkdataset","title":"BenchmarkDataset","text":"<pre><code>class BenchmarkDataset\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#benchmarkdatasetconfig","title":"BenchmarkDatasetConfig","text":"<pre><code>class BenchmarkDatasetConfig\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#functions","title":"Functions","text":""},{"location":"benchmarks/dataset_loaders/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#get_dataset","title":"get_dataset","text":"<pre><code>def get_dataset()\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#list_datasets","title":"list_datasets","text":"<pre><code>def list_datasets()\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#load","title":"load","text":"<pre><code>def load()\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#load_dataset","title":"load_dataset","text":"<pre><code>def load_dataset()\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#register_dataset","title":"register_dataset","text":"<pre><code>def register_dataset()\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#save","title":"save","text":"<pre><code>def save()\n</code></pre>"},{"location":"benchmarks/dataset_loaders/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 7</li> <li>Imports: 7</li> </ul>"},{"location":"benchmarks/disentanglement/","title":"Disentanglement","text":"<p>Module: <code>benchmarks.metrics.disentanglement</code></p> <p>Source: <code>benchmarks/metrics/disentanglement.py</code></p>"},{"location":"benchmarks/disentanglement/#overview","title":"Overview","text":"<p>Disentanglement metrics for generative models.</p> <p>This module provides metrics for evaluating disentanglement in generative models, particularly for VAE models with controllable generation capabilities.</p>"},{"location":"benchmarks/disentanglement/#classes","title":"Classes","text":""},{"location":"benchmarks/disentanglement/#disentanglementmetric","title":"DisentanglementMetric","text":"<pre><code>class DisentanglementMetric\n</code></pre>"},{"location":"benchmarks/disentanglement/#mutualinformationgapmetric","title":"MutualInformationGapMetric","text":"<pre><code>class MutualInformationGapMetric\n</code></pre>"},{"location":"benchmarks/disentanglement/#separationmetric","title":"SeparationMetric","text":"<pre><code>class SeparationMetric\n</code></pre>"},{"location":"benchmarks/disentanglement/#functions","title":"Functions","text":""},{"location":"benchmarks/disentanglement/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/disentanglement/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/disentanglement/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/disentanglement/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/disentanglement/#compute_1","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/disentanglement/#compute_2","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/disentanglement/#create_dci_metric","title":"create_dci_metric","text":"<pre><code>def create_dci_metric()\n</code></pre>"},{"location":"benchmarks/disentanglement/#create_mig_metric","title":"create_mig_metric","text":"<pre><code>def create_mig_metric()\n</code></pre>"},{"location":"benchmarks/disentanglement/#create_sap_metric","title":"create_sap_metric","text":"<pre><code>def create_sap_metric()\n</code></pre>"},{"location":"benchmarks/disentanglement/#validate_inputs","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/disentanglement/#validate_inputs_1","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/disentanglement/#validate_inputs_2","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/disentanglement/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 12</li> <li>Imports: 9</li> </ul>"},{"location":"benchmarks/diversity/","title":"Diversity","text":"<p>Module: <code>benchmarks.metrics.diversity</code></p> <p>Source: <code>benchmarks/metrics/diversity.py</code></p>"},{"location":"benchmarks/diversity/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/ffhq/","title":"Ffhq","text":"<p>Module: <code>benchmarks.datasets.ffhq</code></p> <p>Source: <code>benchmarks/datasets/ffhq.py</code></p>"},{"location":"benchmarks/ffhq/#overview","title":"Overview","text":"<p>FFHQ Dataset Implementation for StyleGAN3 Benchmarks.</p> <p>This module provides FFHQ (Flickr-Faces-HQ) dataset integration for high-resolution face generation benchmarks with StyleGAN3.</p> <p>Key Features:</p> <ul> <li>High-resolution face image dataset (256x256, 1024x1024)</li> <li>Mock data generation for development and testing</li> <li>Efficient batching and preprocessing</li> <li>Few-shot adaptation capabilities</li> </ul>"},{"location":"benchmarks/ffhq/#classes","title":"Classes","text":""},{"location":"benchmarks/ffhq/#celebadataset","title":"CelebADataset","text":"<pre><code>class CelebADataset\n</code></pre>"},{"location":"benchmarks/ffhq/#ffhqdataset","title":"FFHQDataset","text":"<pre><code>class FFHQDataset\n</code></pre>"},{"location":"benchmarks/ffhq/#functions","title":"Functions","text":""},{"location":"benchmarks/ffhq/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"benchmarks/ffhq/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"benchmarks/ffhq/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/ffhq/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/ffhq/#get_few_shot_batch","title":"get_few_shot_batch","text":"<pre><code>def get_few_shot_batch()\n</code></pre>"},{"location":"benchmarks/ffhq/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 5</li> <li>Imports: 4</li> </ul>"},{"location":"benchmarks/geometric/","title":"Geometric","text":"<p>Module: <code>benchmarks.metrics.geometric</code></p> <p>Source: <code>benchmarks/metrics/geometric.py</code></p>"},{"location":"benchmarks/geometric/#overview","title":"Overview","text":"<p>Geometric metrics for point cloud and 3D shape evaluation.</p>"},{"location":"benchmarks/geometric/#classes","title":"Classes","text":""},{"location":"benchmarks/geometric/#pointcloudmetrics","title":"PointCloudMetrics","text":"<pre><code>class PointCloudMetrics\n</code></pre>"},{"location":"benchmarks/geometric/#functions","title":"Functions","text":""},{"location":"benchmarks/geometric/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/geometric/#chamfer_distance_single","title":"chamfer_distance_single","text":"<pre><code>def chamfer_distance_single()\n</code></pre>"},{"location":"benchmarks/geometric/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/geometric/#compute_density_single","title":"compute_density_single","text":"<pre><code>def compute_density_single()\n</code></pre>"},{"location":"benchmarks/geometric/#compute_metric","title":"compute_metric","text":"<pre><code>def compute_metric()\n</code></pre>"},{"location":"benchmarks/geometric/#compute_metrics","title":"compute_metrics","text":"<pre><code>def compute_metrics()\n</code></pre>"},{"location":"benchmarks/geometric/#create_point_cloud_metric","title":"create_point_cloud_metric","text":"<pre><code>def create_point_cloud_metric()\n</code></pre>"},{"location":"benchmarks/geometric/#emd_single","title":"emd_single","text":"<pre><code>def emd_single()\n</code></pre>"},{"location":"benchmarks/geometric/#get_distance_stats","title":"get_distance_stats","text":"<pre><code>def get_distance_stats()\n</code></pre>"},{"location":"benchmarks/geometric/#stats_single","title":"stats_single","text":"<pre><code>def stats_single()\n</code></pre>"},{"location":"benchmarks/geometric/#validate_inputs","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/geometric/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 11</li> <li>Imports: 5</li> </ul>"},{"location":"benchmarks/geometric_suite/","title":"Geometric Suite","text":"<p>Module: <code>benchmarks.suites.geometric_suite</code></p> <p>Source: <code>benchmarks/suites/geometric_suite.py</code></p>"},{"location":"benchmarks/geometric_suite/#overview","title":"Overview","text":"<p>Geometric benchmarks suite for point cloud and 3D data generation.</p>"},{"location":"benchmarks/geometric_suite/#classes","title":"Classes","text":""},{"location":"benchmarks/geometric_suite/#geometricbenchmarksuite","title":"GeometricBenchmarkSuite","text":"<pre><code>class GeometricBenchmarkSuite\n</code></pre>"},{"location":"benchmarks/geometric_suite/#pointcloudgenerationbenchmark","title":"PointCloudGenerationBenchmark","text":"<pre><code>class PointCloudGenerationBenchmark\n</code></pre>"},{"location":"benchmarks/geometric_suite/#functions","title":"Functions","text":""},{"location":"benchmarks/geometric_suite/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#get_benchmark_info","title":"get_benchmark_info","text":"<pre><code>def get_benchmark_info()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#get_performance_targets","title":"get_performance_targets","text":"<pre><code>def get_performance_targets()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#get_suite_summary","title":"get_suite_summary","text":"<pre><code>def get_suite_summary()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#run_all_benchmarks","title":"run_all_benchmarks","text":"<pre><code>def run_all_benchmarks()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#run_evaluation","title":"run_evaluation","text":"<pre><code>def run_evaluation()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#run_training","title":"run_training","text":"<pre><code>def run_training()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#validate_performance","title":"validate_performance","text":"<pre><code>def validate_performance()\n</code></pre>"},{"location":"benchmarks/geometric_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 10</li> <li>Imports: 13</li> </ul>"},{"location":"benchmarks/image/","title":"Image","text":"<p>Module: <code>benchmarks.metrics.image</code></p> <p>Source: <code>benchmarks/metrics/image.py</code></p>"},{"location":"benchmarks/image/#overview","title":"Overview","text":"<p>Image metrics for generative models.</p> <p>This module provides metrics for evaluating the quality of generated images, including FID, IS (Inception Score), LPIPS, and SSIM metrics for image generation.</p>"},{"location":"benchmarks/image/#classes","title":"Classes","text":""},{"location":"benchmarks/image/#fidmetric","title":"FIDMetric","text":"<pre><code>class FIDMetric\n</code></pre>"},{"location":"benchmarks/image/#ismetric","title":"ISMetric","text":"<pre><code>class ISMetric\n</code></pre>"},{"location":"benchmarks/image/#lpipsmetric","title":"LPIPSMetric","text":"<pre><code>class LPIPSMetric\n</code></pre>"},{"location":"benchmarks/image/#mockinceptionmodel","title":"MockInceptionModel","text":"<pre><code>class MockInceptionModel\n</code></pre>"},{"location":"benchmarks/image/#ssimmetric","title":"SSIMMetric","text":"<pre><code>class SSIMMetric\n</code></pre>"},{"location":"benchmarks/image/#functions","title":"Functions","text":""},{"location":"benchmarks/image/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/image/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/image/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/image/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/image/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/image/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/image/#compute_1","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/image/#compute_2","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/image/#compute_3","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/image/#create_fid_metric","title":"create_fid_metric","text":"<pre><code>def create_fid_metric()\n</code></pre>"},{"location":"benchmarks/image/#create_is_metric","title":"create_is_metric","text":"<pre><code>def create_is_metric()\n</code></pre>"},{"location":"benchmarks/image/#create_lpips_metric","title":"create_lpips_metric","text":"<pre><code>def create_lpips_metric()\n</code></pre>"},{"location":"benchmarks/image/#create_ssim_metric","title":"create_ssim_metric","text":"<pre><code>def create_ssim_metric()\n</code></pre>"},{"location":"benchmarks/image/#extract_features","title":"extract_features","text":"<pre><code>def extract_features()\n</code></pre>"},{"location":"benchmarks/image/#validate_inputs","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/image/#validate_inputs_1","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/image/#validate_inputs_2","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/image/#validate_inputs_3","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/image/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 5</li> <li>Functions: 18</li> <li>Imports: 10</li> </ul>"},{"location":"benchmarks/image_benchmarks/","title":"Image Benchmarks","text":"<p>Module: <code>benchmarks.datasets.image_benchmarks</code></p> <p>Source: <code>benchmarks/datasets/image_benchmarks.py</code></p>"},{"location":"benchmarks/image_benchmarks/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/image_grid/","title":"Image Grid","text":"<p>Module: <code>benchmarks.visualization.image_grid</code></p> <p>Source: <code>benchmarks/visualization/image_grid.py</code></p>"},{"location":"benchmarks/image_grid/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/image_protocols/","title":"Image Protocols","text":"<p>Module: <code>benchmarks.protocols.image_protocols</code></p> <p>Source: <code>benchmarks/protocols/image_protocols.py</code></p>"},{"location":"benchmarks/image_protocols/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/image_suite/","title":"Image Suite","text":"<p>Module: <code>benchmarks.suites.image_suite</code></p> <p>Source: <code>benchmarks/suites/image_suite.py</code></p>"},{"location":"benchmarks/image_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/image_unified/","title":"Image Unified","text":"<p>Module: <code>benchmarks.metrics.image_unified</code></p> <p>Source: <code>benchmarks/metrics/image_unified.py</code></p>"},{"location":"benchmarks/image_unified/#overview","title":"Overview","text":"<p>Image metrics for generative models using unified configuration system.</p> <p>This module provides metrics for evaluating the quality of generated images, including FID, IS (Inception Score), LPIPS, and SSIM metrics for image generation.</p>"},{"location":"benchmarks/image_unified/#classes","title":"Classes","text":""},{"location":"benchmarks/image_unified/#fidmetric","title":"FIDMetric","text":"<pre><code>class FIDMetric\n</code></pre>"},{"location":"benchmarks/image_unified/#lpipsmetric","title":"LPIPSMetric","text":"<pre><code>class LPIPSMetric\n</code></pre>"},{"location":"benchmarks/image_unified/#mockinceptionmodel","title":"MockInceptionModel","text":"<pre><code>class MockInceptionModel\n</code></pre>"},{"location":"benchmarks/image_unified/#functions","title":"Functions","text":""},{"location":"benchmarks/image_unified/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/image_unified/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/image_unified/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/image_unified/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/image_unified/#compute_1","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/image_unified/#create_fid_metric","title":"create_fid_metric","text":"<pre><code>def create_fid_metric()\n</code></pre>"},{"location":"benchmarks/image_unified/#create_lpips_metric","title":"create_lpips_metric","text":"<pre><code>def create_lpips_metric()\n</code></pre>"},{"location":"benchmarks/image_unified/#extract_features","title":"extract_features","text":"<pre><code>def extract_features()\n</code></pre>"},{"location":"benchmarks/image_unified/#validate_inputs","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/image_unified/#validate_inputs_1","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/image_unified/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 10</li> <li>Imports: 8</li> </ul>"},{"location":"benchmarks/inception_metrics/","title":"Inception Metrics","text":"<p>Module: <code>benchmarks.metrics.inception_metrics</code></p> <p>Source: <code>benchmarks/metrics/inception_metrics.py</code></p>"},{"location":"benchmarks/inception_metrics/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/latency/","title":"Latency","text":"<p>Module: <code>benchmarks.performance.latency</code></p> <p>Source: <code>benchmarks/performance/latency.py</code></p>"},{"location":"benchmarks/latency/#overview","title":"Overview","text":"<p>Latency benchmark for generative models.</p> <p>This module provides benchmarks for measuring inference latency of generative models. It supports both sampling-based and prediction-based inference.</p>"},{"location":"benchmarks/latency/#classes","title":"Classes","text":""},{"location":"benchmarks/latency/#latencybenchmark","title":"LatencyBenchmark","text":"<pre><code>class LatencyBenchmark\n</code></pre>"},{"location":"benchmarks/latency/#functions","title":"Functions","text":""},{"location":"benchmarks/latency/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/latency/#measure_inference_latency","title":"measure_inference_latency","text":"<pre><code>def measure_inference_latency()\n</code></pre>"},{"location":"benchmarks/latency/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/latency/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 3</li> <li>Imports: 7</li> </ul>"},{"location":"benchmarks/likelihood/","title":"Likelihood","text":"<p>Module: <code>benchmarks.metrics.likelihood</code></p> <p>Source: <code>benchmarks/metrics/likelihood.py</code></p>"},{"location":"benchmarks/likelihood/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/memory/","title":"Memory","text":"<p>Module: <code>benchmarks.performance.memory</code></p> <p>Source: <code>benchmarks/performance/memory.py</code></p>"},{"location":"benchmarks/memory/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/model_adapters/","title":"Model Adapters","text":"<p>Module: <code>benchmarks.model_adapters</code></p> <p>Source: <code>benchmarks/model_adapters.py</code></p>"},{"location":"benchmarks/model_adapters/#overview","title":"Overview","text":"<p>Model adapters for benchmarks.</p> <p>This module provides adapters for different model types to be used with the benchmark system. Adapters implement the ModelProtocol interface required by benchmarks.</p>"},{"location":"benchmarks/model_adapters/#classes","title":"Classes","text":""},{"location":"benchmarks/model_adapters/#benchmarkmodeladapter","title":"BenchmarkModelAdapter","text":"<pre><code>class BenchmarkModelAdapter\n</code></pre>"},{"location":"benchmarks/model_adapters/#nnxmodeladapter","title":"NNXModelAdapter","text":"<pre><code>class NNXModelAdapter\n</code></pre>"},{"location":"benchmarks/model_adapters/#functions","title":"Functions","text":""},{"location":"benchmarks/model_adapters/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/model_adapters/#adapt_model","title":"adapt_model","text":"<pre><code>def adapt_model()\n</code></pre>"},{"location":"benchmarks/model_adapters/#can_adapt","title":"can_adapt","text":"<pre><code>def can_adapt()\n</code></pre>"},{"location":"benchmarks/model_adapters/#can_adapt_1","title":"can_adapt","text":"<pre><code>def can_adapt()\n</code></pre>"},{"location":"benchmarks/model_adapters/#model_name","title":"model_name","text":"<pre><code>def model_name()\n</code></pre>"},{"location":"benchmarks/model_adapters/#predict","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"benchmarks/model_adapters/#predict_1","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"benchmarks/model_adapters/#register_adapter","title":"register_adapter","text":"<pre><code>def register_adapter()\n</code></pre>"},{"location":"benchmarks/model_adapters/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"benchmarks/model_adapters/#sample_1","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"benchmarks/model_adapters/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 10</li> <li>Imports: 5</li> </ul>"},{"location":"benchmarks/molecular_flows/","title":"Molecular Flows","text":"<p>Module: <code>benchmarks.metrics.molecular_flows</code></p> <p>Source: <code>benchmarks/metrics/molecular_flows.py</code></p>"},{"location":"benchmarks/molecular_flows/#overview","title":"Overview","text":"<p>Molecular flows evaluation metrics for SE(3)-equivariant models.</p>"},{"location":"benchmarks/molecular_flows/#classes","title":"Classes","text":""},{"location":"benchmarks/molecular_flows/#molecularflowsmetrics","title":"MolecularFlowsMetrics","text":"<pre><code>class MolecularFlowsMetrics\n</code></pre>"},{"location":"benchmarks/molecular_flows/#functions","title":"Functions","text":""},{"location":"benchmarks/molecular_flows/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/molecular_flows/#chemical_validity","title":"chemical_validity","text":"<pre><code>def chemical_validity()\n</code></pre>"},{"location":"benchmarks/molecular_flows/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/molecular_flows/#conformational_diversity","title":"conformational_diversity","text":"<pre><code>def conformational_diversity()\n</code></pre>"},{"location":"benchmarks/molecular_flows/#energy_consistency","title":"energy_consistency","text":"<pre><code>def energy_consistency()\n</code></pre>"},{"location":"benchmarks/molecular_flows/#validate_inputs","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/molecular_flows/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 6</li> <li>Imports: 6</li> </ul>"},{"location":"benchmarks/multi_beta_vae_suite/","title":"Multi Beta Vae Suite","text":"<p>Module: <code>benchmarks.suites.multi_beta_vae_suite</code></p> <p>Source: <code>benchmarks/suites/multi_beta_vae_suite.py</code></p>"},{"location":"benchmarks/multi_beta_vae_suite/#overview","title":"Overview","text":"<p>Multi-\u03b2 VAE controllable generation benchmark suite.</p> <p>This module provides a comprehensive benchmark suite for evaluating multi-\u03b2 VAE controllable generation models, targeting the Week 9-12 objectives:</p> <ul> <li>MIG Score &gt;0.3 (Mutual Information Gap for disentanglement)</li> <li>FID Score &lt;50 on CelebA (Fr\u00e9chet Inception Distance)</li> <li>Reconstruction Quality: LPIPS &lt;0.2, SSIM &gt;0.8</li> <li>Training Efficiency: &lt;8h per epoch on CelebA subset</li> </ul>"},{"location":"benchmarks/multi_beta_vae_suite/#classes","title":"Classes","text":""},{"location":"benchmarks/multi_beta_vae_suite/#multibetavaebenchmark","title":"MultiBetaVAEBenchmark","text":"<pre><code>class MultiBetaVAEBenchmark\n</code></pre>"},{"location":"benchmarks/multi_beta_vae_suite/#multibetavaebenchmarksuite","title":"MultiBetaVAEBenchmarkSuite","text":"<pre><code>class MultiBetaVAEBenchmarkSuite\n</code></pre>"},{"location":"benchmarks/multi_beta_vae_suite/#functions","title":"Functions","text":""},{"location":"benchmarks/multi_beta_vae_suite/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/multi_beta_vae_suite/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/multi_beta_vae_suite/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/multi_beta_vae_suite/#run_all","title":"run_all","text":"<pre><code>def run_all()\n</code></pre>"},{"location":"benchmarks/multi_beta_vae_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 4</li> <li>Imports: 10</li> </ul>"},{"location":"benchmarks/multimodal_benchmarks/","title":"Multimodal Benchmarks","text":"<p>Module: <code>benchmarks.datasets.multimodal_benchmarks</code></p> <p>Source: <code>benchmarks/datasets/multimodal_benchmarks.py</code></p>"},{"location":"benchmarks/multimodal_benchmarks/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/multimodal_protocols/","title":"Multimodal Protocols","text":"<p>Module: <code>benchmarks.protocols.multimodal_protocols</code></p> <p>Source: <code>benchmarks/protocols/multimodal_protocols.py</code></p>"},{"location":"benchmarks/multimodal_protocols/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/multimodal_suite/","title":"Multimodal Suite","text":"<p>Module: <code>benchmarks.suites.multimodal_suite</code></p> <p>Source: <code>benchmarks/suites/multimodal_suite.py</code></p>"},{"location":"benchmarks/multimodal_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/optimization/","title":"Optimization","text":"<p>Module: <code>benchmarks.performance.optimization</code></p> <p>Source: <code>benchmarks/performance/optimization.py</code></p>"},{"location":"benchmarks/optimization/#overview","title":"Overview","text":"<p>Optimization benchmark for generative models.</p> <p>This module provides benchmarks for evaluating the training performance and optimization strategies for generative models, measuring convergence rates, loss curves, and training efficiency.</p>"},{"location":"benchmarks/optimization/#classes","title":"Classes","text":""},{"location":"benchmarks/optimization/#optimizationbenchmark","title":"OptimizationBenchmark","text":"<pre><code>class OptimizationBenchmark\n</code></pre>"},{"location":"benchmarks/optimization/#optimizationmetricsconfig","title":"OptimizationMetricsConfig","text":"<pre><code>class OptimizationMetricsConfig\n</code></pre>"},{"location":"benchmarks/optimization/#optimizercomparisonbenchmark","title":"OptimizerComparisonBenchmark","text":"<pre><code>class OptimizerComparisonBenchmark\n</code></pre>"},{"location":"benchmarks/optimization/#trainerprotocol","title":"TrainerProtocol","text":"<pre><code>class TrainerProtocol\n</code></pre>"},{"location":"benchmarks/optimization/#trainingconvergencebenchmark","title":"TrainingConvergenceBenchmark","text":"<pre><code>class TrainingConvergenceBenchmark\n</code></pre>"},{"location":"benchmarks/optimization/#trainingcurvepoint","title":"TrainingCurvePoint","text":"<pre><code>class TrainingCurvePoint\n</code></pre>"},{"location":"benchmarks/optimization/#functions","title":"Functions","text":""},{"location":"benchmarks/optimization/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/optimization/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/optimization/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/optimization/#evaluate","title":"evaluate","text":"<pre><code>def evaluate()\n</code></pre>"},{"location":"benchmarks/optimization/#init_3","title":"init","text":"<pre><code>def init()\n</code></pre>"},{"location":"benchmarks/optimization/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/optimization/#run_1","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/optimization/#train_step","title":"train_step","text":"<pre><code>def train_step()\n</code></pre>"},{"location":"benchmarks/optimization/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 6</li> <li>Functions: 8</li> <li>Imports: 7</li> </ul>"},{"location":"benchmarks/optimization_benchmark/","title":"Optimization Benchmark","text":"<p>Module: <code>benchmarks.cli.optimization_benchmark</code></p> <p>Source: <code>benchmarks/cli/optimization_benchmark.py</code></p>"},{"location":"benchmarks/optimization_benchmark/#overview","title":"Overview","text":"<p>Command-line interface for running optimization benchmarks.</p>"},{"location":"benchmarks/optimization_benchmark/#functions","title":"Functions","text":""},{"location":"benchmarks/optimization_benchmark/#create_parser","title":"create_parser","text":"<pre><code>def create_parser()\n</code></pre>"},{"location":"benchmarks/optimization_benchmark/#main","title":"main","text":"<pre><code>def main()\n</code></pre>"},{"location":"benchmarks/optimization_benchmark/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 2</li> <li>Imports: 7</li> </ul>"},{"location":"benchmarks/optimization_plots/","title":"Optimization Plots","text":"<p>Module: <code>benchmarks.visualization.optimization_plots</code></p> <p>Source: <code>benchmarks/visualization/optimization_plots.py</code></p>"},{"location":"benchmarks/optimization_plots/#overview","title":"Overview","text":"<p>Visualization tools for optimization benchmark results.</p>"},{"location":"benchmarks/optimization_plots/#functions","title":"Functions","text":""},{"location":"benchmarks/optimization_plots/#plot_convergence_speed","title":"plot_convergence_speed","text":"<pre><code>def plot_convergence_speed()\n</code></pre>"},{"location":"benchmarks/optimization_plots/#plot_optimizer_comparison","title":"plot_optimizer_comparison","text":"<pre><code>def plot_optimizer_comparison()\n</code></pre>"},{"location":"benchmarks/optimization_plots/#plot_training_curve","title":"plot_training_curve","text":"<pre><code>def plot_training_curve()\n</code></pre>"},{"location":"benchmarks/optimization_plots/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 3</li> <li>Imports: 4</li> </ul>"},{"location":"benchmarks/perplexity/","title":"Perplexity","text":"<p>Module: <code>benchmarks.metrics.perplexity</code></p> <p>Source: <code>benchmarks/metrics/perplexity.py</code></p>"},{"location":"benchmarks/perplexity/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/plots/","title":"Plots","text":"<p>Module: <code>benchmarks.visualization.plots</code></p> <p>Source: <code>benchmarks/visualization/plots.py</code></p>"},{"location":"benchmarks/plots/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/precision_recall/","title":"Precision Recall","text":"<p>Module: <code>benchmarks.metrics.precision_recall</code></p> <p>Source: <code>benchmarks/metrics/precision_recall.py</code></p>"},{"location":"benchmarks/precision_recall/#overview","title":"Overview","text":"<p>Precision-recall metrics for evaluating generative models.</p> <p>This module implements precision and recall metrics for generative models as described in \"Improved Precision and Recall Metric for Assessing Generative Models\" (Kynk\u00e4\u00e4nniemi et al., 2019).</p> <p>The implementation uses clustering to identify modes in the data distribution and computes precision and recall based on cluster coverage.</p>"},{"location":"benchmarks/precision_recall/#classes","title":"Classes","text":""},{"location":"benchmarks/precision_recall/#kmeansmodule","title":"KMeansModule","text":"<pre><code>class KMeansModule\n</code></pre>"},{"location":"benchmarks/precision_recall/#precisionrecallbenchmark","title":"PrecisionRecallBenchmark","text":"<pre><code>class PrecisionRecallBenchmark\n</code></pre>"},{"location":"benchmarks/precision_recall/#functions","title":"Functions","text":""},{"location":"benchmarks/precision_recall/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/precision_recall/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/precision_recall/#compute_cluster_based_metrics","title":"compute_cluster_based_metrics","text":"<pre><code>def compute_cluster_based_metrics()\n</code></pre>"},{"location":"benchmarks/precision_recall/#compute_distance_based_metrics","title":"compute_distance_based_metrics","text":"<pre><code>def compute_distance_based_metrics()\n</code></pre>"},{"location":"benchmarks/precision_recall/#compute_precision_recall","title":"compute_precision_recall","text":"<pre><code>def compute_precision_recall()\n</code></pre>"},{"location":"benchmarks/precision_recall/#fit","title":"fit","text":"<pre><code>def fit()\n</code></pre>"},{"location":"benchmarks/precision_recall/#is_well_separated_clusters","title":"is_well_separated_clusters","text":"<pre><code>def is_well_separated_clusters()\n</code></pre>"},{"location":"benchmarks/precision_recall/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/precision_recall/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 8</li> <li>Imports: 5</li> </ul>"},{"location":"benchmarks/protein_adapters/","title":"Protein Adapters","text":"<p>Module: <code>benchmarks.model_adapters.protein_adapters</code></p> <p>Source: <code>benchmarks/model_adapters/protein_adapters.py</code></p>"},{"location":"benchmarks/protein_adapters/#overview","title":"Overview","text":"<p>Model adapters for protein generative models.</p> <p>This module provides adapters for protein-specific model types to be used with the benchmark system. These adapters implement the ModelProtocol interface and leverage the base NNXModelAdapter functionality.</p>"},{"location":"benchmarks/protein_adapters/#classes","title":"Classes","text":""},{"location":"benchmarks/protein_adapters/#proteinpointcloudadapter","title":"ProteinPointCloudAdapter","text":"<pre><code>class ProteinPointCloudAdapter\n</code></pre>"},{"location":"benchmarks/protein_adapters/#functions","title":"Functions","text":""},{"location":"benchmarks/protein_adapters/#can_adapt","title":"can_adapt","text":"<pre><code>def can_adapt()\n</code></pre>"},{"location":"benchmarks/protein_adapters/#predict","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"benchmarks/protein_adapters/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"benchmarks/protein_adapters/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 3</li> <li>Imports: 6</li> </ul>"},{"location":"benchmarks/protein_benchmarks/","title":"Protein Benchmarks","text":"<p>Module: <code>benchmarks.suites.protein_benchmarks</code></p> <p>Source: <code>benchmarks/suites/protein_benchmarks.py</code></p>"},{"location":"benchmarks/protein_benchmarks/#overview","title":"Overview","text":"<p>Protein model benchmarks.</p> <p>This module provides benchmark suites for protein generative models, including quality metrics and performance metrics.</p>"},{"location":"benchmarks/protein_benchmarks/#classes","title":"Classes","text":""},{"location":"benchmarks/protein_benchmarks/#proteinbenchmarksuite","title":"ProteinBenchmarkSuite","text":"<pre><code>class ProteinBenchmarkSuite\n</code></pre>"},{"location":"benchmarks/protein_benchmarks/#proteinstructurebenchmark","title":"ProteinStructureBenchmark","text":"<pre><code>class ProteinStructureBenchmark\n</code></pre>"},{"location":"benchmarks/protein_benchmarks/#functions","title":"Functions","text":""},{"location":"benchmarks/protein_benchmarks/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_benchmarks/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_benchmarks/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/protein_benchmarks/#run_all","title":"run_all","text":"<pre><code>def run_all()\n</code></pre>"},{"location":"benchmarks/protein_benchmarks/#visualize_results","title":"visualize_results","text":"<pre><code>def visualize_results()\n</code></pre>"},{"location":"benchmarks/protein_benchmarks/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 5</li> <li>Imports: 11</li> </ul>"},{"location":"benchmarks/protein_dataset/","title":"Protein Dataset","text":"<p>Module: <code>benchmarks.datasets.protein_dataset</code></p> <p>Source: <code>benchmarks/datasets/protein_dataset.py</code></p>"},{"location":"benchmarks/protein_dataset/#overview","title":"Overview","text":"<p>Protein datasets for benchmarking.</p> <p>This module provides synthetic protein datasets for benchmarking protein generative models.</p>"},{"location":"benchmarks/protein_dataset/#classes","title":"Classes","text":""},{"location":"benchmarks/protein_dataset/#syntheticproteindataset","title":"SyntheticProteinDataset","text":"<pre><code>class SyntheticProteinDataset\n</code></pre>"},{"location":"benchmarks/protein_dataset/#functions","title":"Functions","text":""},{"location":"benchmarks/protein_dataset/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_dataset/#create_synthetic_protein_dataset","title":"create_synthetic_protein_dataset","text":"<pre><code>def create_synthetic_protein_dataset()\n</code></pre>"},{"location":"benchmarks/protein_dataset/#flatten_coordinates","title":"flatten_coordinates","text":"<pre><code>def flatten_coordinates()\n</code></pre>"},{"location":"benchmarks/protein_dataset/#get_batch","title":"get_batch","text":"<pre><code>def get_batch()\n</code></pre>"},{"location":"benchmarks/protein_dataset/#get_dataset_info","title":"get_dataset_info","text":"<pre><code>def get_dataset_info()\n</code></pre>"},{"location":"benchmarks/protein_dataset/#get_raw_coordinates","title":"get_raw_coordinates","text":"<pre><code>def get_raw_coordinates()\n</code></pre>"},{"location":"benchmarks/protein_dataset/#get_sample_with_metadata","title":"get_sample_with_metadata","text":"<pre><code>def get_sample_with_metadata()\n</code></pre>"},{"location":"benchmarks/protein_dataset/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 7</li> <li>Imports: 6</li> </ul>"},{"location":"benchmarks/protein_ligand/","title":"Protein Ligand","text":"<p>Module: <code>benchmarks.metrics.protein_ligand</code></p> <p>Source: <code>benchmarks/metrics/protein_ligand.py</code></p>"},{"location":"benchmarks/protein_ligand/#overview","title":"Overview","text":"<p>Protein-ligand co-design metrics.</p> <p>This module provides metrics for evaluating protein-ligand co-design models, including binding affinity prediction and molecular validity assessment.</p>"},{"location":"benchmarks/protein_ligand/#classes","title":"Classes","text":""},{"location":"benchmarks/protein_ligand/#bindingaffinitymetric","title":"BindingAffinityMetric","text":"<pre><code>class BindingAffinityMetric\n</code></pre>"},{"location":"benchmarks/protein_ligand/#druglikenessmetric","title":"DrugLikenessMetric","text":"<pre><code>class DrugLikenessMetric\n</code></pre>"},{"location":"benchmarks/protein_ligand/#molecularvaliditymetric","title":"MolecularValidityMetric","text":"<pre><code>class MolecularValidityMetric\n</code></pre>"},{"location":"benchmarks/protein_ligand/#functions","title":"Functions","text":""},{"location":"benchmarks/protein_ligand/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_ligand/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_ligand/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_ligand/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/protein_ligand/#compute_1","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/protein_ligand/#compute_2","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/protein_ligand/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 6</li> <li>Imports: 3</li> </ul>"},{"location":"benchmarks/protein_ligand_suite/","title":"Protein Ligand Suite","text":"<p>Module: <code>benchmarks.suites.protein_ligand_suite</code></p> <p>Source: <code>benchmarks/suites/protein_ligand_suite.py</code></p>"},{"location":"benchmarks/protein_ligand_suite/#overview","title":"Overview","text":"<p>Protein-ligand co-design benchmark suite.</p> <p>This module provides a comprehensive benchmark suite for evaluating protein-ligand co-design models, targeting the Week 5-8 objectives:</p> <ul> <li>Binding affinity RMSE &lt;1.0 kcal/mol</li> <li>Molecular validity &gt;95%</li> <li>Drug-likeness QED &gt;0.7</li> </ul>"},{"location":"benchmarks/protein_ligand_suite/#classes","title":"Classes","text":""},{"location":"benchmarks/protein_ligand_suite/#proteinligandbenchmarksuite","title":"ProteinLigandBenchmarkSuite","text":"<pre><code>class ProteinLigandBenchmarkSuite\n</code></pre>"},{"location":"benchmarks/protein_ligand_suite/#proteinligandcodesignbenchmark","title":"ProteinLigandCoDesignBenchmark","text":"<pre><code>class ProteinLigandCoDesignBenchmark\n</code></pre>"},{"location":"benchmarks/protein_ligand_suite/#functions","title":"Functions","text":""},{"location":"benchmarks/protein_ligand_suite/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_ligand_suite/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_ligand_suite/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/protein_ligand_suite/#run_all","title":"run_all","text":"<pre><code>def run_all()\n</code></pre>"},{"location":"benchmarks/protein_ligand_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 4</li> <li>Imports: 9</li> </ul>"},{"location":"benchmarks/protein_model_adapters/","title":"Protein Model Adapters","text":"<p>Module: <code>benchmarks.protein_model_adapters</code></p> <p>Source: <code>benchmarks/protein_model_adapters.py</code></p>"},{"location":"benchmarks/protein_model_adapters/#overview","title":"Overview","text":"<p>Model adapters for protein models in the artifex benchmarking system.</p> <p>This module provides adapters for protein models to work with the benchmark metrics, particularly the precision-recall benchmarks. All adapters follow the NNX requirements outlined in the critical technical guidelines.</p>"},{"location":"benchmarks/protein_model_adapters/#classes","title":"Classes","text":""},{"location":"benchmarks/protein_model_adapters/#proteinpointcloudadapter","title":"ProteinPointCloudAdapter","text":"<pre><code>class ProteinPointCloudAdapter\n</code></pre>"},{"location":"benchmarks/protein_model_adapters/#functions","title":"Functions","text":""},{"location":"benchmarks/protein_model_adapters/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/protein_model_adapters/#can_adapt","title":"can_adapt","text":"<pre><code>def can_adapt()\n</code></pre>"},{"location":"benchmarks/protein_model_adapters/#predict","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"benchmarks/protein_model_adapters/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"benchmarks/protein_model_adapters/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 4</li> <li>Imports: 6</li> </ul>"},{"location":"benchmarks/qm9/","title":"Qm9","text":"<p>Module: <code>benchmarks.datasets.qm9</code></p> <p>Source: <code>benchmarks/datasets/qm9.py</code></p>"},{"location":"benchmarks/qm9/#overview","title":"Overview","text":"<p>QM9 molecular dataset for SE(3)-equivariant molecular flows.</p>"},{"location":"benchmarks/qm9/#classes","title":"Classes","text":""},{"location":"benchmarks/qm9/#qm9dataset","title":"QM9Dataset","text":"<pre><code>class QM9Dataset\n</code></pre>"},{"location":"benchmarks/qm9/#functions","title":"Functions","text":""},{"location":"benchmarks/qm9/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/qm9/#get_batch","title":"get_batch","text":"<pre><code>def get_batch()\n</code></pre>"},{"location":"benchmarks/qm9/#get_dataset_info","title":"get_dataset_info","text":"<pre><code>def get_dataset_info()\n</code></pre>"},{"location":"benchmarks/qm9/#get_statistics","title":"get_statistics","text":"<pre><code>def get_statistics()\n</code></pre>"},{"location":"benchmarks/qm9/#validate_structure","title":"validate_structure","text":"<pre><code>def validate_structure()\n</code></pre>"},{"location":"benchmarks/qm9/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 5</li> <li>Imports: 6</li> </ul>"},{"location":"benchmarks/registry/","title":"Registry","text":"<p>Module: <code>benchmarks.suites.registry</code></p> <p>Source: <code>benchmarks/suites/registry.py</code></p>"},{"location":"benchmarks/registry/#overview","title":"Overview","text":"<p>Registry for benchmark suites.</p> <p>This module provides a registry for benchmark suites, allowing suites to be registered and discovered by name.</p>"},{"location":"benchmarks/registry/#functions","title":"Functions","text":""},{"location":"benchmarks/registry/#get_suite","title":"get_suite","text":"<pre><code>def get_suite()\n</code></pre>"},{"location":"benchmarks/registry/#list_suites","title":"list_suites","text":"<pre><code>def list_suites()\n</code></pre>"},{"location":"benchmarks/registry/#register_suite","title":"register_suite","text":"<pre><code>def register_suite()\n</code></pre>"},{"location":"benchmarks/registry/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 3</li> <li>Imports: 2</li> </ul>"},{"location":"benchmarks/reporting/","title":"Reporting","text":"<p>Module: <code>benchmarks.utils.reporting</code></p> <p>Source: <code>benchmarks/utils/reporting.py</code></p>"},{"location":"benchmarks/reporting/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/sample_quality/","title":"Sample Quality","text":"<p>Module: <code>benchmarks.metrics.sample_quality</code></p> <p>Source: <code>benchmarks/metrics/sample_quality.py</code></p>"},{"location":"benchmarks/sample_quality/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/scaling/","title":"Scaling","text":"<p>Module: <code>benchmarks.performance.scaling</code></p> <p>Source: <code>benchmarks/performance/scaling.py</code></p>"},{"location":"benchmarks/scaling/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/se3_molecular_flows_suite/","title":"Se3 Molecular Flows Suite","text":"<p>Module: <code>benchmarks.suites.se3_molecular_flows_suite</code></p> <p>Source: <code>benchmarks/suites/se3_molecular_flows_suite.py</code></p>"},{"location":"benchmarks/se3_molecular_flows_suite/#overview","title":"Overview","text":"<p>SE(3)-Equivariant Molecular Flows benchmark suite.</p>"},{"location":"benchmarks/se3_molecular_flows_suite/#classes","title":"Classes","text":""},{"location":"benchmarks/se3_molecular_flows_suite/#se3molecularflowsbenchmark","title":"SE3MolecularFlowsBenchmark","text":"<pre><code>class SE3MolecularFlowsBenchmark\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#se3molecularflowssuite","title":"SE3MolecularFlowsSuite","text":"<pre><code>class SE3MolecularFlowsSuite\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#functions","title":"Functions","text":""},{"location":"benchmarks/se3_molecular_flows_suite/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#compare_models","title":"compare_models","text":"<pre><code>def compare_models()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#create_default_models","title":"create_default_models","text":"<pre><code>def create_default_models()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#evaluate_model","title":"evaluate_model","text":"<pre><code>def evaluate_model()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#get_benchmark_info","title":"get_benchmark_info","text":"<pre><code>def get_benchmark_info()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#get_suite_info","title":"get_suite_info","text":"<pre><code>def get_suite_info()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#meets_performance_targets","title":"meets_performance_targets","text":"<pre><code>def meets_performance_targets()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#run_benchmarks","title":"run_benchmarks","text":"<pre><code>def run_benchmarks()\n</code></pre>"},{"location":"benchmarks/se3_molecular_flows_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 10</li> <li>Imports: 9</li> </ul>"},{"location":"benchmarks/standard/","title":"Standard","text":"<p>Module: <code>benchmarks.suites.standard</code></p> <p>Source: <code>benchmarks/suites/standard.py</code></p>"},{"location":"benchmarks/standard/#overview","title":"Overview","text":"<p>Standard benchmark suites for generative models.</p> <p>This module provides predefined benchmark suites for evaluating different aspects of generative models.</p>"},{"location":"benchmarks/standard/#functions","title":"Functions","text":""},{"location":"benchmarks/standard/#get_performance_suite","title":"get_performance_suite","text":"<pre><code>def get_performance_suite()\n</code></pre>"},{"location":"benchmarks/standard/#get_quality_suite","title":"get_quality_suite","text":"<pre><code>def get_quality_suite()\n</code></pre>"},{"location":"benchmarks/standard/#get_standard_suite","title":"get_standard_suite","text":"<pre><code>def get_standard_suite()\n</code></pre>"},{"location":"benchmarks/standard/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 3</li> <li>Imports: 2</li> </ul>"},{"location":"benchmarks/statistical/","title":"Statistical","text":"<p>Module: <code>benchmarks.utils.statistical</code></p> <p>Source: <code>benchmarks/utils/statistical.py</code></p>"},{"location":"benchmarks/statistical/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/style_metrics/","title":"Style Metrics","text":"<p>Module: <code>benchmarks.metrics.style_metrics</code></p> <p>Source: <code>benchmarks/metrics/style_metrics.py</code></p>"},{"location":"benchmarks/style_metrics/#overview","title":"Overview","text":"<p>StyleGAN-Specific Metrics for Image Generation Evaluation.</p> <p>This module provides comprehensive metrics for evaluating StyleGAN3 performance, including traditional image quality metrics and StyleGAN-specific evaluations.</p> <p>Key Metrics:</p> <ul> <li>Fr\u00e9chet Inception Distance (FID)</li> <li>Inception Score (IS)</li> <li>Learned Perceptual Image Patch Similarity (LPIPS)</li> <li>Style Mixing Quality</li> <li>Few-shot Adaptation Performance</li> <li>Translation and Rotation Equivariance</li> </ul>"},{"location":"benchmarks/style_metrics/#classes","title":"Classes","text":""},{"location":"benchmarks/style_metrics/#equivariancemetric","title":"EquivarianceMetric","text":"<pre><code>class EquivarianceMetric\n</code></pre>"},{"location":"benchmarks/style_metrics/#fidmetric","title":"FIDMetric","text":"<pre><code>class FIDMetric\n</code></pre>"},{"location":"benchmarks/style_metrics/#fewshotadaptationmetric","title":"FewShotAdaptationMetric","text":"<pre><code>class FewShotAdaptationMetric\n</code></pre>"},{"location":"benchmarks/style_metrics/#inceptionv3feature","title":"InceptionV3Feature","text":"<pre><code>class InceptionV3Feature\n</code></pre>"},{"location":"benchmarks/style_metrics/#lpipsmetric","title":"LPIPSMetric","text":"<pre><code>class LPIPSMetric\n</code></pre>"},{"location":"benchmarks/style_metrics/#perceptualnet","title":"PerceptualNet","text":"<pre><code>class PerceptualNet\n</code></pre>"},{"location":"benchmarks/style_metrics/#styleganmetrics","title":"StyleGANMetrics","text":"<pre><code>class StyleGANMetrics\n</code></pre>"},{"location":"benchmarks/style_metrics/#stylemixingmetric","title":"StyleMixingMetric","text":"<pre><code>class StyleMixingMetric\n</code></pre>"},{"location":"benchmarks/style_metrics/#functions","title":"Functions","text":""},{"location":"benchmarks/style_metrics/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#init_5","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#init_6","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#init_7","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/style_metrics/#compute_all_metrics","title":"compute_all_metrics","text":"<pre><code>def compute_all_metrics()\n</code></pre>"},{"location":"benchmarks/style_metrics/#compute_distance","title":"compute_distance","text":"<pre><code>def compute_distance()\n</code></pre>"},{"location":"benchmarks/style_metrics/#compute_fid","title":"compute_fid","text":"<pre><code>def compute_fid()\n</code></pre>"},{"location":"benchmarks/style_metrics/#compute_statistics","title":"compute_statistics","text":"<pre><code>def compute_statistics()\n</code></pre>"},{"location":"benchmarks/style_metrics/#compute_style_mixing_quality","title":"compute_style_mixing_quality","text":"<pre><code>def compute_style_mixing_quality()\n</code></pre>"},{"location":"benchmarks/style_metrics/#evaluate_adaptation","title":"evaluate_adaptation","text":"<pre><code>def evaluate_adaptation()\n</code></pre>"},{"location":"benchmarks/style_metrics/#evaluate_equivariance","title":"evaluate_equivariance","text":"<pre><code>def evaluate_equivariance()\n</code></pre>"},{"location":"benchmarks/style_metrics/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 8</li> <li>Functions: 17</li> <li>Imports: 4</li> </ul>"},{"location":"benchmarks/stylegan3_suite/","title":"Stylegan3 Suite","text":"<p>Module: <code>benchmarks.suites.stylegan3_suite</code></p> <p>Source: <code>benchmarks/suites/stylegan3_suite.py</code></p>"},{"location":"benchmarks/stylegan3_suite/#overview","title":"Overview","text":"<p>StyleGAN3 Benchmark Suite for High-Resolution Image Generation.</p> <p>This module provides a comprehensive benchmark suite for evaluating StyleGAN3 performance on FFHQ dataset with few-shot adaptation capabilities.</p> <p>Key Features:</p> <ul> <li>StyleGAN3 model evaluation with FID, IS, LPIPS metrics</li> <li>Few-shot domain adaptation testing</li> <li>Style mixing and equivariance evaluation</li> <li>Progressive training and evaluation</li> </ul>"},{"location":"benchmarks/stylegan3_suite/#classes","title":"Classes","text":""},{"location":"benchmarks/stylegan3_suite/#stylegan3benchmark","title":"StyleGAN3Benchmark","text":"<pre><code>class StyleGAN3Benchmark\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#stylegan3benchmarkconfig","title":"StyleGAN3BenchmarkConfig","text":"<pre><code>class StyleGAN3BenchmarkConfig\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#stylegan3suite","title":"StyleGAN3Suite","text":"<pre><code>class StyleGAN3Suite\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#functions","title":"Functions","text":""},{"location":"benchmarks/stylegan3_suite/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#create_stylegan3_demo","title":"create_stylegan3_demo","text":"<pre><code>def create_stylegan3_demo()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#get_performance_summary","title":"get_performance_summary","text":"<pre><code>def get_performance_summary()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#get_performance_targets","title":"get_performance_targets","text":"<pre><code>def get_performance_targets()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#run_all_benchmarks","title":"run_all_benchmarks","text":"<pre><code>def run_all_benchmarks()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#run_benchmark","title":"run_benchmark","text":"<pre><code>def run_benchmark()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#run_evaluation","title":"run_evaluation","text":"<pre><code>def run_evaluation()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#run_training","title":"run_training","text":"<pre><code>def run_training()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#setup_model","title":"setup_model","text":"<pre><code>def setup_model()\n</code></pre>"},{"location":"benchmarks/stylegan3_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 10</li> <li>Imports: 11</li> </ul>"},{"location":"benchmarks/synthetic_datasets/","title":"Synthetic Datasets","text":"<p>Module: <code>benchmarks.datasets.synthetic_datasets</code></p> <p>Source: <code>benchmarks/datasets/synthetic_datasets.py</code></p>"},{"location":"benchmarks/synthetic_datasets/#overview","title":"Overview","text":"<p>Synthetic dataset generators for benchmarks.</p> <p>This module provides functions to generate synthetic datasets for benchmarking generative models, including Gaussian mixtures and simple image datasets.</p>"},{"location":"benchmarks/synthetic_datasets/#functions","title":"Functions","text":""},{"location":"benchmarks/synthetic_datasets/#create_gaussian_mixture","title":"create_gaussian_mixture","text":"<pre><code>def create_gaussian_mixture()\n</code></pre>"},{"location":"benchmarks/synthetic_datasets/#create_image_dataset","title":"create_image_dataset","text":"<pre><code>def create_image_dataset()\n</code></pre>"},{"location":"benchmarks/synthetic_datasets/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 2</li> <li>Imports: 3</li> </ul>"},{"location":"benchmarks/text/","title":"Text","text":"<p>Module: <code>benchmarks.metrics.text</code></p> <p>Source: <code>benchmarks/metrics/text.py</code></p>"},{"location":"benchmarks/text/#overview","title":"Overview","text":"<p>Text-specific metrics for generative model evaluation.</p>"},{"location":"benchmarks/text/#classes","title":"Classes","text":""},{"location":"benchmarks/text/#bleumetric","title":"BLEUMetric","text":"<pre><code>class BLEUMetric\n</code></pre>"},{"location":"benchmarks/text/#diversitymetric","title":"DiversityMetric","text":"<pre><code>class DiversityMetric\n</code></pre>"},{"location":"benchmarks/text/#perplexitymetric","title":"PerplexityMetric","text":"<pre><code>class PerplexityMetric\n</code></pre>"},{"location":"benchmarks/text/#rougemetric","title":"ROUGEMetric","text":"<pre><code>class ROUGEMetric\n</code></pre>"},{"location":"benchmarks/text/#functions","title":"Functions","text":""},{"location":"benchmarks/text/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/text/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/text/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/text/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"benchmarks/text/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/text/#compute_1","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/text/#compute_2","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/text/#compute_3","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"benchmarks/text/#create_bleu_metric","title":"create_bleu_metric","text":"<pre><code>def create_bleu_metric()\n</code></pre>"},{"location":"benchmarks/text/#create_diversity_metric","title":"create_diversity_metric","text":"<pre><code>def create_diversity_metric()\n</code></pre>"},{"location":"benchmarks/text/#create_perplexity_metric","title":"create_perplexity_metric","text":"<pre><code>def create_perplexity_metric()\n</code></pre>"},{"location":"benchmarks/text/#create_rouge_metric","title":"create_rouge_metric","text":"<pre><code>def create_rouge_metric()\n</code></pre>"},{"location":"benchmarks/text/#validate_inputs","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/text/#validate_inputs_1","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/text/#validate_inputs_2","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/text/#validate_inputs_3","title":"validate_inputs","text":"<pre><code>def validate_inputs()\n</code></pre>"},{"location":"benchmarks/text/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 16</li> <li>Imports: 7</li> </ul>"},{"location":"benchmarks/text_benchmarks/","title":"Text Benchmarks","text":"<p>Module: <code>benchmarks.datasets.text_benchmarks</code></p> <p>Source: <code>benchmarks/datasets/text_benchmarks.py</code></p>"},{"location":"benchmarks/text_benchmarks/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/text_protocols/","title":"Text Protocols","text":"<p>Module: <code>benchmarks.protocols.text_protocols</code></p> <p>Source: <code>benchmarks/protocols/text_protocols.py</code></p>"},{"location":"benchmarks/text_protocols/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/text_suite/","title":"Text Suite","text":"<p>Module: <code>benchmarks.suites.text_suite</code></p> <p>Source: <code>benchmarks/suites/text_suite.py</code></p>"},{"location":"benchmarks/text_suite/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"benchmarks/throughput/","title":"Throughput","text":"<p>Module: <code>benchmarks.performance.throughput</code></p> <p>Source: <code>benchmarks/performance/throughput.py</code></p>"},{"location":"benchmarks/throughput/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/","title":"Command Line Interface","text":"<p>Artifex provides a powerful CLI for training, generating, evaluating, and serving generative models without writing code.</p>"},{"location":"cli/#overview","title":"Overview","text":"<ul> <li> <p> Training</p> <p>Train models with <code>artifex train</code> using YAML configs</p> </li> <li> <p> Generation</p> <p>Generate samples with <code>artifex generate</code></p> </li> <li> <p> Evaluation</p> <p>Benchmark models with <code>artifex evaluate</code></p> </li> <li> <p> Serving</p> <p>Deploy models with <code>artifex serve</code></p> </li> </ul>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI is installed automatically with Artifex:</p> <pre><code># Verify installation\nartifex --help\n\n# Or run via uv\nuv run artifex --help\n</code></pre>"},{"location":"cli/#quick-start","title":"Quick Start","text":""},{"location":"cli/#train-a-model","title":"Train a Model","text":"<pre><code># Train from config file\nartifex train --config configs/vae_mnist.yaml\n\n# Train with overrides\nartifex train --config configs/vae_mnist.yaml \\\n    --batch-size 128 \\\n    --epochs 100 \\\n    --lr 1e-3\n\n# Resume training from checkpoint\nartifex train --config configs/vae_mnist.yaml \\\n    --resume checkpoints/vae_mnist/latest\n</code></pre>"},{"location":"cli/#generate-samples","title":"Generate Samples","text":"<pre><code># Generate samples from trained model\nartifex generate --checkpoint checkpoints/vae_mnist/best \\\n    --num-samples 100 \\\n    --output generated/\n\n# Generate with specific seed\nartifex generate --checkpoint model.ckpt \\\n    --num-samples 50 \\\n    --seed 42 \\\n    --output samples/\n</code></pre>"},{"location":"cli/#evaluate-model","title":"Evaluate Model","text":"<pre><code># Run evaluation\nartifex evaluate --checkpoint checkpoints/vae_mnist/best \\\n    --dataset mnist \\\n    --metrics fid inception_score\n\n# Evaluate with custom test set\nartifex evaluate --checkpoint model.ckpt \\\n    --data-path /path/to/test/data \\\n    --output results/evaluation.json\n</code></pre>"},{"location":"cli/#serve-model","title":"Serve Model","text":"<pre><code># Start inference server\nartifex serve --checkpoint checkpoints/vae_mnist/best \\\n    --port 8000\n\n# Serve with specific host\nartifex serve --checkpoint model.ckpt \\\n    --host 0.0.0.0 \\\n    --port 8080\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#artifex-train","title":"<code>artifex train</code>","text":"<p>Train generative models from configuration.</p> <pre><code>artifex train [OPTIONS]\n\nOptions:\n  --config PATH          Path to training config file [required]\n  --batch-size INT       Override batch size\n  --epochs INT           Override number of epochs\n  --lr FLOAT             Override learning rate\n  --resume PATH          Resume from checkpoint\n  --output-dir PATH      Output directory for checkpoints\n  --seed INT             Random seed\n  --device TEXT          Device to use (auto/gpu/cpu)\n  --wandb                Enable Weights &amp; Biases logging\n  --mlflow               Enable MLflow logging\n  --help                 Show help message\n</code></pre> <p> Train Command Reference</p>"},{"location":"cli/#artifex-generate","title":"<code>artifex generate</code>","text":"<p>Generate samples from trained models.</p> <pre><code>artifex generate [OPTIONS]\n\nOptions:\n  --checkpoint PATH      Path to model checkpoint [required]\n  --num-samples INT      Number of samples to generate [default: 10]\n  --output PATH          Output directory for samples\n  --seed INT             Random seed for reproducibility\n  --temperature FLOAT    Sampling temperature [default: 1.0]\n  --batch-size INT       Batch size for generation\n  --format TEXT          Output format (png/npy/pt)\n  --help                 Show help message\n</code></pre> <p> Generate Command Reference</p>"},{"location":"cli/#artifex-evaluate","title":"<code>artifex evaluate</code>","text":"<p>Evaluate model performance with metrics.</p> <pre><code>artifex evaluate [OPTIONS]\n\nOptions:\n  --checkpoint PATH      Path to model checkpoint [required]\n  --dataset TEXT         Dataset name or path\n  --data-path PATH       Custom data path\n  --metrics TEXT         Metrics to compute (comma-separated)\n  --output PATH          Output file for results\n  --batch-size INT       Batch size for evaluation\n  --help                 Show help message\n</code></pre> <p> Evaluate Command Reference</p>"},{"location":"cli/#artifex-serve","title":"<code>artifex serve</code>","text":"<p>Deploy model as REST API.</p> <pre><code>artifex serve [OPTIONS]\n\nOptions:\n  --checkpoint PATH      Path to model checkpoint [required]\n  --host TEXT            Host address [default: localhost]\n  --port INT             Port number [default: 8000]\n  --workers INT          Number of workers [default: 1]\n  --help                 Show help message\n</code></pre> <p> Serve Command Reference</p>"},{"location":"cli/#artifex-benchmark","title":"<code>artifex benchmark</code>","text":"<p>Run comprehensive benchmarks.</p> <pre><code>artifex benchmark [OPTIONS]\n\nOptions:\n  --config PATH          Benchmark configuration file\n  --model PATH           Model checkpoint or config\n  --suite TEXT           Benchmark suite to run\n  --output PATH          Output directory for results\n  --help                 Show help message\n</code></pre> <p> Benchmark Command Reference</p>"},{"location":"cli/#artifex-convert","title":"<code>artifex convert</code>","text":"<p>Convert between model formats.</p> <pre><code>artifex convert [OPTIONS]\n\nOptions:\n  --input PATH           Input model path [required]\n  --output PATH          Output model path [required]\n  --format TEXT          Target format (onnx/tflite/safetensors)\n  --help                 Show help message\n</code></pre> <p> Convert Command Reference</p>"},{"location":"cli/#configuration-files","title":"Configuration Files","text":""},{"location":"cli/#training-config-example","title":"Training Config Example","text":"<pre><code># configs/vae_mnist.yaml\nmodel:\n  type: vae\n  latent_dim: 32\n  encoder:\n    hidden_dims: [256, 128]\n    activation: relu\n  decoder:\n    hidden_dims: [128, 256]\n    activation: relu\n\ntraining:\n  batch_size: 128\n  epochs: 100\n  optimizer:\n    type: adam\n    learning_rate: 1e-3\n  scheduler:\n    type: cosine\n    warmup_steps: 1000\n\ndata:\n  dataset: mnist\n  train_split: 0.9\n\nlogging:\n  wandb: true\n  project: artifex-experiments\n  log_interval: 100\n</code></pre>"},{"location":"cli/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code># Set defaults via environment\nexport ARTIFEX_DEVICE=gpu\nexport ARTIFEX_SEED=42\nexport ARTIFEX_OUTPUT_DIR=./outputs\n\n# These become defaults for all commands\nartifex train --config config.yaml\n</code></pre>"},{"location":"cli/#utility-modules","title":"Utility Modules","text":""},{"location":"cli/#configuration-utilities","title":"Configuration Utilities","text":"<ul> <li>config - Configuration loading and validation</li> <li>formatting - Output formatting helpers</li> </ul>"},{"location":"cli/#logging-utilities","title":"Logging Utilities","text":"<ul> <li>logging - Logging configuration</li> <li>progress - Progress bar utilities</li> </ul>"},{"location":"cli/#examples","title":"Examples","text":""},{"location":"cli/#complete-training-workflow","title":"Complete Training Workflow","text":"<pre><code># 1. Train the model\nartifex train --config configs/dcgan_cifar.yaml \\\n    --output-dir experiments/dcgan_001 \\\n    --wandb\n\n# 2. Generate samples\nartifex generate \\\n    --checkpoint experiments/dcgan_001/best.ckpt \\\n    --num-samples 1000 \\\n    --output experiments/dcgan_001/samples/\n\n# 3. Evaluate quality\nartifex evaluate \\\n    --checkpoint experiments/dcgan_001/best.ckpt \\\n    --dataset cifar10 \\\n    --metrics fid inception_score lpips \\\n    --output experiments/dcgan_001/metrics.json\n</code></pre>"},{"location":"cli/#hyperparameter-search","title":"Hyperparameter Search","text":"<pre><code># Run multiple training jobs\nfor lr in 1e-4 1e-3 1e-2; do\n    artifex train --config configs/vae.yaml \\\n        --lr $lr \\\n        --output-dir experiments/vae_lr_$lr\ndone\n</code></pre>"},{"location":"cli/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Guide - Detailed training documentation</li> <li>Configuration System - Config file format</li> <li>Model Deployment - Deployment guide</li> </ul>"},{"location":"cli/__main__/","title":"Main","text":"<p>Module: <code>cli.__main__</code></p> <p>Source: <code>cli/__main__.py</code></p>"},{"location":"cli/__main__/#overview","title":"Overview","text":"<p>CLI entry point for artifex generative models.</p>"},{"location":"cli/__main__/#functions","title":"Functions","text":""},{"location":"cli/__main__/#main_1","title":"main","text":"<pre><code>def main()\n</code></pre>"},{"location":"cli/__main__/#print_help","title":"print_help","text":"<pre><code>def print_help()\n</code></pre>"},{"location":"cli/__main__/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 2</li> <li>Imports: 1</li> </ul>"},{"location":"cli/benchmark/","title":"Benchmark","text":"<p>Module: <code>cli.commands.benchmark</code></p> <p>Source: <code>cli/commands/benchmark.py</code></p>"},{"location":"cli/benchmark/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/config/","title":"Config","text":"<p>Module: <code>cli.utils.config</code></p> <p>Source: <code>cli/utils/config.py</code></p>"},{"location":"cli/config/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/convert/","title":"Convert","text":"<p>Module: <code>cli.commands.convert</code></p> <p>Source: <code>cli/commands/convert.py</code></p>"},{"location":"cli/convert/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/evaluate/","title":"Evaluate","text":"<p>Module: <code>cli.commands.evaluate</code></p> <p>Source: <code>cli/commands/evaluate.py</code></p>"},{"location":"cli/evaluate/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/formatting/","title":"Formatting","text":"<p>Module: <code>cli.utils.formatting</code></p> <p>Source: <code>cli/utils/formatting.py</code></p>"},{"location":"cli/formatting/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/generate/","title":"Generate","text":"<p>Module: <code>cli.commands.generate</code></p> <p>Source: <code>cli/commands/generate.py</code></p>"},{"location":"cli/generate/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/logging/","title":"Logging","text":"<p>Module: <code>cli.utils.logging</code></p> <p>Source: <code>cli/utils/logging.py</code></p>"},{"location":"cli/logging/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/main/","title":"Main","text":"<p>Module: <code>cli.main</code></p> <p>Source: <code>cli/main.py</code></p>"},{"location":"cli/main/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/progress/","title":"Progress","text":"<p>Module: <code>cli.utils.progress</code></p> <p>Source: <code>cli/utils/progress.py</code></p>"},{"location":"cli/progress/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/serve/","title":"Serve","text":"<p>Module: <code>cli.commands.serve</code></p> <p>Source: <code>cli/commands/serve.py</code></p>"},{"location":"cli/serve/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"cli/train/","title":"Train","text":"<p>Module: <code>cli.commands.train</code></p> <p>Source: <code>cli/commands/train.py</code></p>"},{"location":"cli/train/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"community/contributing/","title":"Contributing to Artifex","text":"<p>Thank you for your interest in contributing to Artifex! This guide will help you get started with contributing code, documentation, and other improvements.</p>"},{"location":"community/contributing/#getting-started","title":"Getting Started","text":""},{"location":"community/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork and Clone:</li> </ol> <pre><code># Fork the repository on GitHub\n# Then clone your fork\ngit clone https://github.com/YOUR_USERNAME/artifex.git\ncd artifex\n</code></pre> <ol> <li>Install with Development Dependencies:</li> </ol> <pre><code># Install all development dependencies\nuv sync --all-extras\n\n# Or install specific extras\nuv sync --extra cuda-dev  # For GPU development\n</code></pre> <ol> <li>Install Pre-commit Hooks:</li> </ol> <pre><code># Install pre-commit hooks for code quality\nuv run pre-commit install\n\n# Run hooks on all files\nuv run pre-commit run --all-files\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <pre><code># Run tests to verify setup\nuv run pytest tests/ -v\n</code></pre>"},{"location":"community/contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li>Create a Feature Branch:</li> </ol> <pre><code>git checkout -b feature/my-new-feature\n</code></pre> <ol> <li>Make Changes and Test:</li> </ol> <pre><code># Make your changes\n# ...\n\n# Run tests\nuv run pytest tests/path/to/test_file.py -xvs\n\n# Run linting\nuv run ruff check src/\nuv run ruff format src/\n\n# Run type checking\nuv run pyright src/\n</code></pre> <ol> <li>Commit Changes:</li> </ol> <pre><code># Stage changes\ngit add .\n\n# Commit with descriptive message\ngit commit -m \"feat: add new feature description\"\n</code></pre> <ol> <li>Push and Create Pull Request:</li> </ol> <pre><code># Push to your fork\ngit push origin feature/my-new-feature\n\n# Create pull request on GitHub\n</code></pre>"},{"location":"community/contributing/#code-standards","title":"Code Standards","text":""},{"location":"community/contributing/#flax-nnx-requirements","title":"Flax NNX Requirements","text":"<p>Artifex uses Flax NNX exclusively. All neural network code must use Flax NNX:</p> <pre><code>from flax import nnx\n\nclass MyModule(nnx.Module):\n    def __init__(self, features: int, *, rngs: nnx.Rngs):\n        super().__init__()  # ALWAYS call this\n        self.dense = nnx.Linear(features, features, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        return self.dense(x)\n</code></pre> <p>Do NOT use:</p> <ul> <li>Flax Linen</li> <li>PyTorch or TensorFlow</li> <li>Numpy operations inside modules (use <code>jax.numpy</code>)</li> </ul>"},{"location":"community/contributing/#code-style","title":"Code Style","text":"<ol> <li>Type Hints: All functions must have type hints:</li> </ol> <pre><code>def my_function(x: jax.Array, y: int) -&gt; dict[str, jax.Array]:\n    \"\"\"Function docstring.\"\"\"\n    return {\"result\": x * y}\n</code></pre> <ol> <li>Docstrings: Use Google-style docstrings:</li> </ol> <pre><code>def train_model(config: ModelConfig) -&gt; dict:\n    \"\"\"Train a generative model.\n\n    Args:\n        config: Model configuration\n\n    Returns:\n        Dictionary with training results\n\n    Raises:\n        ValueError: If configuration is invalid\n    \"\"\"\n    pass\n</code></pre> <ol> <li>Formatting: Code must pass Ruff formatting:</li> </ol> <pre><code>uv run ruff format src/\nuv run ruff check src/\n</code></pre> <ol> <li>Type Checking: Code must pass Pyright:</li> </ol> <pre><code>uv run pyright src/\n</code></pre>"},{"location":"community/contributing/#testing","title":"Testing","text":""},{"location":"community/contributing/#writing-tests","title":"Writing Tests","text":"<ol> <li>Test Structure: Mirror source structure:</li> </ol> <pre><code>src/artifex/generative_models/models/vae.py\ntests/artifex/generative_models/models/test_vae.py\n</code></pre> <ol> <li>Test Template:</li> </ol> <pre><code>import pytest\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.models.vae import create_vae_model\n\ndef test_vae_creation():\n    \"\"\"Test VAE model creation.\"\"\"\n    config = ModelConfig(\n        model_type=\"vae\",\n        latent_dim=10,\n        # ...\n    )\n\n    model = create_vae_model(config, rngs=nnx.Rngs(0))\n\n    assert model is not None\n    assert model.latent_dim == 10\n\ndef test_vae_forward_pass():\n    \"\"\"Test VAE forward pass.\"\"\"\n    model = create_vae_model(config, rngs=nnx.Rngs(0))\n\n    x = jnp.ones((2, 28, 28, 1))\n    output = model(x)\n\n    assert \"reconstruction\" in output\n    assert output[\"reconstruction\"].shape == x.shape\n</code></pre> <ol> <li>GPU Tests: Mark GPU-specific tests:</li> </ol> <pre><code>@pytest.mark.gpu\ndef test_gpu_training():\n    \"\"\"Test training on GPU.\"\"\"\n    # GPU-specific test\n    pass\n</code></pre>"},{"location":"community/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/artifex/generative_models/models/test_vae.py -xvs\n\n# Run with coverage\nuv run pytest --cov=src/artifex --cov-report=html\n\n# Run GPU tests (requires CUDA)\nuv run pytest -m gpu\n</code></pre>"},{"location":"community/contributing/#documentation","title":"Documentation","text":""},{"location":"community/contributing/#writing-documentation","title":"Writing Documentation","text":"<ol> <li>Structure: Follow existing patterns</li> <li>Examples: Include working code examples</li> <li>Cross-references: Link to related docs</li> <li>No AI Traces: Never mention AI assistants, Claude, etc.</li> </ol>"},{"location":"community/contributing/#building-documentation","title":"Building Documentation","text":"<pre><code># Install documentation dependencies\nuv sync --extra docs\n\n# Serve documentation locally\nuv run mkdocs serve\n\n# Build documentation\nuv run mkdocs build\n</code></pre>"},{"location":"community/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"community/contributing/#before-submitting","title":"Before Submitting","text":"<ul> <li> Tests pass locally</li> <li> Code is formatted (Ruff)</li> <li> Type checking passes (Pyright)</li> <li> Documentation updated if needed</li> <li> Commit messages are descriptive</li> </ul>"},{"location":"community/contributing/#pr-template","title":"PR Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\n- [ ] Added new tests\n- [ ] All tests pass\n- [ ] Manual testing performed\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Documentation updated\n- [ ] No breaking changes (or documented)\n</code></pre>"},{"location":"community/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated Checks: CI runs tests, linting, type checking</li> <li>Code Review: Maintainer reviews code</li> <li>Revisions: Address feedback if needed</li> <li>Merge: Approved PRs are merged</li> </ol>"},{"location":"community/contributing/#commit-messages","title":"Commit Messages","text":"<p>Use conventional commits format:</p> <pre><code>type(scope): description\n\n[optional body]\n\n[optional footer]\n</code></pre> <p>Types:</p> <ul> <li><code>feat</code>: New feature</li> <li><code>fix</code>: Bug fix</li> <li><code>docs</code>: Documentation</li> <li><code>test</code>: Tests</li> <li><code>refactor</code>: Code refactoring</li> <li><code>perf</code>: Performance improvement</li> <li><code>ci</code>: CI/CD changes</li> </ul> <p>Examples:</p> <pre><code>feat(vae): add \u03b2-VAE implementation\nfix(training): resolve NaN loss issue\ndocs(quickstart): add installation steps\ntest(gan): increase test coverage\n</code></pre>"},{"location":"community/contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"community/contributing/#our-standards","title":"Our Standards","text":"<ul> <li>Be respectful and inclusive</li> <li>Accept constructive criticism</li> <li>Focus on what's best for the community</li> <li>Show empathy towards others</li> </ul>"},{"location":"community/contributing/#unacceptable-behavior","title":"Unacceptable Behavior","text":"<ul> <li>Harassment or discriminatory language</li> <li>Trolling or insulting comments</li> <li>Personal or political attacks</li> <li>Publishing others' private information</li> </ul>"},{"location":"community/contributing/#enforcement","title":"Enforcement","text":"<p>Violations may result in:</p> <ol> <li>Warning</li> <li>Temporary ban</li> <li>Permanent ban</li> </ol> <p>Report issues to maintainers.</p>"},{"location":"community/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Open GitHub issue for bugs/features</li> <li>Discussions: Use GitHub Discussions for questions</li> <li>Documentation: Check docs first</li> </ul>"},{"location":"community/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in:</p> <ul> <li>CONTRIBUTORS.md file</li> <li>Release notes</li> <li>Documentation credits</li> </ul> <p>Thank you for contributing to Artifex!</p>"},{"location":"community/faq/","title":"Frequently Asked Questions","text":"<p>Common questions about Artifex and their answers.</p>"},{"location":"community/faq/#installation","title":"Installation","text":""},{"location":"community/faq/#q-how-do-i-install-artifex","title":"Q: How do I install Artifex?","text":"<p>A: Using uv (recommended):</p> <pre><code>uv pip install artifex\n</code></pre> <p>For GPU support:</p> <pre><code>uv sync --extra cuda12  # For CUDA 12\n</code></pre> <p>See the Installation Guide for more details.</p>"},{"location":"community/faq/#q-do-i-need-a-gpu","title":"Q: Do I need a GPU?","text":"<p>A: No, Artifex works on CPU. However, training large models is much faster on GPU. Artifex automatically uses GPU if available.</p>"},{"location":"community/faq/#q-which-python-version-should-i-use","title":"Q: Which Python version should I use?","text":"<p>A: Python 3.10 or later. We test on Python 3.10, 3.11, and 3.12.</p>"},{"location":"community/faq/#models","title":"Models","text":""},{"location":"community/faq/#q-which-models-does-artifex-support","title":"Q: Which models does Artifex support?","text":"<p>A: Artifex supports:</p> <ul> <li>VAE (Variational Autoencoders)</li> <li>GAN (Generative Adversarial Networks)</li> <li>Diffusion Models (DDPM, DDIM)</li> <li>Flow Models (RealNVP, Glow, Continuous Normalizing Flows)</li> </ul> <p>See Models Overview for details.</p>"},{"location":"community/faq/#q-can-i-use-custom-architectures","title":"Q: Can I use custom architectures?","text":"<p>A: Yes! Artifex provides flexibility for custom models. See Custom Architectures.</p>"},{"location":"community/faq/#q-how-do-i-load-pre-trained-models","title":"Q: How do I load pre-trained models?","text":"<p>A: Use the checkpointing system:</p> <pre><code>from artifex.generative_models.core.checkpointing import load_checkpoint\n\nmodel, step = load_checkpoint(checkpoint_manager, model_template)\n</code></pre>"},{"location":"community/faq/#training","title":"Training","text":""},{"location":"community/faq/#q-how-do-i-train-a-model","title":"Q: How do I train a model?","text":"<p>A: Basic training example:</p> <pre><code>from artifex.generative_models.training.trainer import Trainer\n\ntrainer = Trainer(model_config=config, training_config=train_config)\ntrainer.train(train_dataset, val_dataset)\n</code></pre> <p>See Training Guide for complete examples.</p>"},{"location":"community/faq/#q-training-is-slow-how-can-i-speed-it-up","title":"Q: Training is slow. How can I speed it up?","text":"<p>A: Several options:</p> <ol> <li>Use GPU: Much faster than CPU</li> <li>Increase batch size: Better GPU utilization</li> <li>Use mixed precision: <code>dtype=jnp.bfloat16</code></li> <li>JIT compilation: Happens automatically with JAX</li> <li>Data parallelism: Distribute across multiple GPUs</li> </ol> <p>See Distributed Training.</p>"},{"location":"community/faq/#q-my-models-loss-is-nan-whats-wrong","title":"Q: My model's loss is NaN. What's wrong?","text":"<p>A: Common causes:</p> <ol> <li>Learning rate too high: Try reducing it (e.g., 1e-4 instead of 1e-3)</li> <li>Gradient explosion: Add gradient clipping</li> <li>Numerical instability: Check for divisions by zero or log(0)</li> <li>Bad initialization: Use proper weight initialization</li> </ol>"},{"location":"community/faq/#q-how-do-i-save-checkpoints-during-training","title":"Q: How do I save checkpoints during training?","text":"<p>A: Checkpoints are saved automatically by the Trainer. Configure frequency:</p> <pre><code>training_config = TrainingConfig(\n    num_epochs=100,\n    checkpoint_every=1000,  # Save every 1000 steps\n)\n</code></pre>"},{"location":"community/faq/#data","title":"Data","text":""},{"location":"community/faq/#q-what-data-formats-does-artifex-support","title":"Q: What data formats does Artifex support?","text":"<p>A: Artifex supports:</p> <ul> <li>Images (PNG, JPG, NumPy arrays)</li> <li>Text (tokenized sequences)</li> <li>Audio (waveforms, spectrograms)</li> <li>Multi-modal (combined modalities)</li> </ul> <p>See Data Guide.</p>"},{"location":"community/faq/#q-how-do-i-use-my-own-dataset","title":"Q: How do I use my own dataset?","text":"<p>A: Create a custom dataset:</p> <pre><code>from artifex.generative_models.modalities.base import BaseDataset\n\nclass MyDataset(BaseDataset):\n    def __len__(self):\n        return len(self.data)\n\n    def __iter__(self):\n        for sample in self.data:\n            yield {\"data\": sample}\n</code></pre> <p>See Custom Datasets.</p>"},{"location":"community/faq/#q-can-i-use-data-augmentation","title":"Q: Can I use data augmentation?","text":"<p>A: Yes! Artifex provides augmentation for all modalities:</p> <pre><code>@jax.jit\ndef augment(batch, key):\n    # Apply augmentation\n    batch = random_flip(batch, key)\n    batch = random_crop(batch, key)\n    return batch\n</code></pre>"},{"location":"community/faq/#technical","title":"Technical","text":""},{"location":"community/faq/#q-why-jax-instead-of-pytorchtensorflow","title":"Q: Why JAX instead of PyTorch/TensorFlow?","text":"<p>A: JAX offers:</p> <ul> <li>Automatic differentiation</li> <li>JIT compilation for speed</li> <li>Functional programming style</li> <li>Easy parallelization with vmap/pmap</li> <li>Better for research and experimentation</li> </ul>"},{"location":"community/faq/#q-why-flax-nnx-specifically","title":"Q: Why Flax NNX specifically?","text":"<p>A: Flax NNX is:</p> <ul> <li>Modern and actively developed</li> <li>More Pythonic than Linen</li> <li>Better for complex architectures</li> <li>Easier state management</li> <li>Official Flax direction</li> </ul>"},{"location":"community/faq/#q-can-i-mix-pytorchtensorflow-with-artifex","title":"Q: Can I mix PyTorch/TensorFlow with Artifex?","text":"<p>A: No. Artifex uses JAX/Flax NNX exclusively. However, you can:</p> <ul> <li>Convert PyTorch/TensorFlow weights to JAX</li> <li>Use Artifex for training, export for PyTorch inference</li> <li>Integrate at the data loading level</li> </ul>"},{"location":"community/faq/#q-how-do-i-debug-jax-code","title":"Q: How do I debug JAX code?","text":"<p>A: Debugging tips:</p> <ol> <li>Disable JIT: Run without <code>@jax.jit</code> decorator</li> <li>Print values: Use <code>jax.debug.print()</code></li> <li>Check shapes: Print intermediate shapes</li> <li>Use assertions: Add shape checks</li> <li>Breakpoints: Use <code>jax.debug.breakpoint()</code></li> </ol> <p>See JAX debugging docs for more details.</p>"},{"location":"community/faq/#performance","title":"Performance","text":""},{"location":"community/faq/#q-how-much-memory-do-i-need","title":"Q: How much memory do I need?","text":"<p>A: Depends on model size and batch size:</p> <ul> <li>Small models (VAE): 2-4GB GPU</li> <li>Medium models (StyleGAN): 8-16GB GPU</li> <li>Large models (large diffusion): 24-40GB GPU</li> </ul> <p>Use gradient accumulation for larger models.</p>"},{"location":"community/faq/#q-how-many-gpus-do-i-need","title":"Q: How many GPUs do I need?","text":"<p>A: One GPU is sufficient for most tasks. Multiple GPUs help with:</p> <ul> <li>Larger batch sizes (data parallelism)</li> <li>Larger models (model parallelism)</li> <li>Faster training (distributed training)</li> </ul> <p>See Distributed Training.</p>"},{"location":"community/faq/#q-can-i-train-on-cpu","title":"Q: Can I train on CPU?","text":"<p>A: Yes, but it's slow. Recommended for:</p> <ul> <li>Testing and debugging</li> <li>Small models</li> <li>Limited datasets</li> </ul> <p>Not recommended for production training.</p>"},{"location":"community/faq/#deployment","title":"Deployment","text":""},{"location":"community/faq/#q-how-do-i-deploy-a-trained-model","title":"Q: How do I deploy a trained model?","text":"<p>A: Several options:</p> <ol> <li>Export model: Save with checkpointing</li> <li>Create REST API: Use Flask/FastAPI</li> <li>Containerize: Use Docker</li> <li>Cloud deployment: Deploy to cloud platforms</li> </ol> <p>See Deployment Guide.</p>"},{"location":"community/faq/#q-can-i-convert-models-to-onnx","title":"Q: Can I convert models to ONNX?","text":"<p>A: JAX models can be converted to ONNX for deployment, but it requires additional tools. See integration guides for details.</p>"},{"location":"community/faq/#q-how-do-i-optimize-for-inference","title":"Q: How do I optimize for inference?","text":"<p>A: Optimization techniques:</p> <ol> <li>JIT compilation: Automatic with JAX</li> <li>Batching: Process multiple samples together</li> <li>Mixed precision: Use bfloat16</li> <li>Model pruning: Remove unnecessary parameters</li> </ol> <p>See Optimization.</p>"},{"location":"community/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"community/faq/#q-i-get-out-of-memory-errors","title":"Q: I get \"Out of Memory\" errors","text":"<p>A: Solutions:</p> <ol> <li>Reduce batch size: Smaller batches use less memory</li> <li>Gradient accumulation: Simulate larger batches</li> <li>Gradient checkpointing: Trade compute for memory</li> <li>Mixed precision: Use bfloat16</li> <li>Model parallelism: Split model across devices</li> </ol>"},{"location":"community/faq/#q-tests-are-failing","title":"Q: Tests are failing","text":"<p>A: Common issues:</p> <ol> <li>Missing dependencies: Run <code>uv sync --all-extras</code></li> <li>CUDA not available: Some tests require GPU</li> <li>Outdated code: Pull latest changes</li> <li>Environment issues: Create fresh virtual environment</li> </ol>"},{"location":"community/faq/#q-import-errors","title":"Q: Import errors","text":"<p>A: Check:</p> <ol> <li>Installation: <code>uv pip list | grep artifex</code></li> <li>Python path: Verify artifex is installed</li> <li>Virtual environment: Activate correct environment</li> <li>Dependencies: Install with <code>uv sync</code></li> </ol>"},{"location":"community/faq/#contributing","title":"Contributing","text":""},{"location":"community/faq/#q-how-can-i-contribute","title":"Q: How can I contribute?","text":"<p>A: Many ways to contribute:</p> <ul> <li>Report bugs</li> <li>Suggest features</li> <li>Submit pull requests</li> <li>Improve documentation</li> <li>Help others in discussions</li> </ul> <p>See Contributing Guide.</p>"},{"location":"community/faq/#q-i-found-a-bug-what-should-i-do","title":"Q: I found a bug. What should I do?","text":"<p>A: Please open a GitHub issue with:</p> <ul> <li>Description of the bug</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>System information (OS, Python version, etc.)</li> </ul>"},{"location":"community/faq/#q-can-i-request-a-feature","title":"Q: Can I request a feature?","text":"<p>A: Yes! Open a GitHub issue describing:</p> <ul> <li>Feature description</li> <li>Use case</li> <li>Why it's useful</li> <li>Possible implementation ideas</li> </ul>"},{"location":"community/faq/#getting-help","title":"Getting Help","text":""},{"location":"community/faq/#q-where-can-i-get-help","title":"Q: Where can I get help?","text":"<p>A: Several resources:</p> <ul> <li>Documentation: Start here</li> <li>GitHub Issues: For bugs and features</li> <li>GitHub Discussions: For questions</li> <li>Examples: Check examples directory</li> </ul>"},{"location":"community/faq/#q-documentation-is-unclear","title":"Q: Documentation is unclear","text":"<p>A: Please let us know! Open an issue describing:</p> <ul> <li>Which page is unclear</li> <li>What's confusing</li> <li>What would help</li> </ul> <p>We appreciate feedback to improve docs.</p>"},{"location":"community/faq/#license","title":"License","text":""},{"location":"community/faq/#q-whats-the-license","title":"Q: What's the license?","text":"<p>A: Artifex is open source under the MIT License. You can:</p> <ul> <li>Use commercially</li> <li>Modify the code</li> <li>Distribute</li> <li>Use privately</li> </ul> <p>See LICENSE file for details.</p>"},{"location":"community/faq/#q-can-i-use-artifex-in-my-company","title":"Q: Can I use Artifex in my company?","text":"<p>A: Yes! The MIT License allows commercial use.</p>"},{"location":"configs/","title":"Configuration System","text":"<p>Unified configuration management for all Artifex components, providing type-safe, validated configurations with schema support, templates, and utilities.</p>"},{"location":"configs/#overview","title":"Overview","text":"<ul> <li> <p> Schema-Based Configs</p> <p>Pydantic-based configuration classes with validation and type safety</p> </li> <li> <p> Templates</p> <p>Pre-built configuration templates for common use cases</p> </li> <li> <p> Conversion</p> <p>Convert between YAML, JSON, and Python configuration formats</p> </li> <li> <p> Validation</p> <p>Automatic validation with clear error messages</p> </li> </ul>"},{"location":"configs/#quick-start","title":"Quick Start","text":""},{"location":"configs/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from artifex.configs import load_config, ConfigLoader\n\n# Load from YAML file\nconfig = load_config(\"config.yaml\")\n\n# Load with validation\nloader = ConfigLoader(schema=\"training\")\nconfig = loader.load(\"training_config.yaml\")\n</code></pre>"},{"location":"configs/#creating-configurations","title":"Creating Configurations","text":"<pre><code>from artifex.configs.schema import TrainingConfig, DataConfig\n\n# Create training configuration\ntraining_config = TrainingConfig(\n    batch_size=128,\n    num_epochs=100,\n    learning_rate=1e-3,\n    optimizer=\"adamw\",\n    scheduler=\"cosine\",\n)\n\n# Create data configuration\ndata_config = DataConfig(\n    dataset=\"cifar10\",\n    train_split=\"train\",\n    val_split=\"test\",\n    augmentation=True,\n)\n</code></pre>"},{"location":"configs/#schema-reference","title":"Schema Reference","text":""},{"location":"configs/#base-schema","title":"Base Schema","text":"<p>Foundation configuration classes.</p> <pre><code>from artifex.configs.schema import BaseConfig\n\nclass BaseConfig:\n    \"\"\"Base configuration with common fields.\"\"\"\n    name: str\n    seed: int = 42\n    dtype: str = \"float32\"\n    debug: bool = False\n</code></pre> <p> Base Schema</p>"},{"location":"configs/#training-schema","title":"Training Schema","text":"<p>Training-specific configurations.</p> <pre><code>from artifex.configs.schema import TrainingConfig\n\nconfig = TrainingConfig(\n    batch_size=128,\n    num_epochs=100,\n    learning_rate=1e-3,\n    weight_decay=0.01,\n    gradient_clip=1.0,\n    mixed_precision=True,\n    gradient_accumulation_steps=4,\n)\n</code></pre> <p> Training Schema</p>"},{"location":"configs/#data-schema","title":"Data Schema","text":"<p>Data loading and preprocessing configurations.</p> <pre><code>from artifex.configs.schema import DataConfig\n\nconfig = DataConfig(\n    dataset=\"imagenet\",\n    root=\"/path/to/data\",\n    batch_size=256,\n    num_workers=8,\n    pin_memory=True,\n    prefetch_factor=2,\n)\n</code></pre> <p> Data Schema</p>"},{"location":"configs/#distributed-schema","title":"Distributed Schema","text":"<p>Multi-device training configurations.</p> <pre><code>from artifex.configs.schema import DistributedConfig\n\nconfig = DistributedConfig(\n    strategy=\"data_parallel\",\n    num_devices=4,\n    mesh_shape=(2, 2),\n    axis_names=(\"data\", \"model\"),\n)\n</code></pre> <p> Distributed Schema</p>"},{"location":"configs/#inference-schema","title":"Inference Schema","text":"<p>Inference and serving configurations.</p> <pre><code>from artifex.configs.schema import InferenceConfig\n\nconfig = InferenceConfig(\n    batch_size=1,\n    max_length=512,\n    temperature=1.0,\n    top_k=50,\n    top_p=0.9,\n)\n</code></pre> <p> Inference Schema</p>"},{"location":"configs/#hyperparameter-schema","title":"Hyperparameter Schema","text":"<p>Hyperparameter search configurations.</p> <pre><code>from artifex.configs.schema import HyperparamConfig\n\nconfig = HyperparamConfig(\n    search_space={\n        \"learning_rate\": {\"type\": \"log_uniform\", \"min\": 1e-5, \"max\": 1e-2},\n        \"batch_size\": {\"type\": \"choice\", \"values\": [32, 64, 128, 256]},\n    },\n    num_trials=100,\n    metric=\"val_loss\",\n    direction=\"minimize\",\n)\n</code></pre> <p> Hyperparameter Schema</p>"},{"location":"configs/#extensions-schema","title":"Extensions Schema","text":"<p>Extension-specific configurations.</p> <pre><code>from artifex.configs.schema import ExtensionsConfig\n\nconfig = ExtensionsConfig(\n    protein={\n        \"max_length\": 256,\n        \"include_structure\": True,\n    },\n    geometric={\n        \"equivariant\": True,\n        \"rotation_order\": 2,\n    },\n)\n</code></pre> <p> Extensions Schema</p>"},{"location":"configs/#configuration-utilities","title":"Configuration Utilities","text":""},{"location":"configs/#config-loader","title":"Config Loader","text":"<p>Load configurations from various sources.</p> <pre><code>from artifex.configs.utils import ConfigLoader\n\nloader = ConfigLoader()\n\n# Load from file\nconfig = loader.load(\"config.yaml\")\n\n# Load from dict\nconfig = loader.from_dict({\"batch_size\": 128})\n\n# Load with environment variable substitution\nconfig = loader.load(\"config.yaml\", resolve_env=True)\n</code></pre> <p> Config Loader</p>"},{"location":"configs/#validation","title":"Validation","text":"<p>Validate configurations against schemas.</p> <pre><code>from artifex.configs.utils import validate_config\n\n# Validate against schema\nerrors = validate_config(config, schema=\"training\")\n\nif errors:\n    for error in errors:\n        print(f\"Validation error: {error}\")\n</code></pre> <p> Validation</p>"},{"location":"configs/#conversion","title":"Conversion","text":"<p>Convert between configuration formats.</p> <pre><code>from artifex.configs.utils import convert_config\n\n# YAML to JSON\nconvert_config(\"config.yaml\", \"config.json\", format=\"json\")\n\n# To Python dict\nconfig_dict = convert_config(\"config.yaml\", format=\"dict\")\n\n# To Pydantic model\nconfig_model = convert_config(\"config.yaml\", format=\"pydantic\")\n</code></pre> <p> Conversion</p>"},{"location":"configs/#merge","title":"Merge","text":"<p>Merge multiple configurations.</p> <pre><code>from artifex.configs.utils import merge_configs\n\n# Merge configs with override priority\nmerged = merge_configs(\n    base_config,\n    override_config,\n    strategy=\"deep\",  # Deep merge nested dicts\n)\n\n# Merge from files\nmerged = merge_configs(\n    \"base.yaml\",\n    \"experiment.yaml\",\n    \"local.yaml\",  # Highest priority\n)\n</code></pre> <p> Merge</p>"},{"location":"configs/#templates","title":"Templates","text":"<p>Use pre-built configuration templates.</p> <pre><code>from artifex.configs.utils import get_template\n\n# Get VAE training template\ntemplate = get_template(\"vae_training\")\n\n# Customize template\ntemplate.model.latent_dim = 64\ntemplate.training.learning_rate = 1e-4\n</code></pre> <p> Templates</p>"},{"location":"configs/#io","title":"I/O","text":"<p>Configuration file operations.</p> <pre><code>from artifex.configs.utils import save_config, load_config\n\n# Save configuration\nsave_config(config, \"experiment_config.yaml\")\n\n# Load with comments preserved\nconfig = load_config(\"config.yaml\", preserve_comments=True)\n</code></pre> <p> I/O</p>"},{"location":"configs/#error-handling","title":"Error Handling","text":"<p>Configuration error handling utilities.</p> <pre><code>from artifex.configs.utils import ConfigError, handle_config_error\n\ntry:\n    config = load_config(\"config.yaml\")\nexcept ConfigError as e:\n    handle_config_error(e, show_suggestions=True)\n</code></pre> <p> Error Handling</p>"},{"location":"configs/#cli-configuration","title":"CLI Configuration","text":"<p>Command-line interface configuration.</p> <pre><code>from artifex.configs import CLIConfig\n\ncli_config = CLIConfig(\n    verbose=True,\n    log_level=\"INFO\",\n    output_dir=\"./outputs\",\n)\n</code></pre> <p> CLI Config</p>"},{"location":"configs/#extension-configuration","title":"Extension Configuration","text":"<p>Configuration for domain extensions.</p> <pre><code>from artifex.configs import ExtensionConfig\n\nextension_config = ExtensionConfig(\n    name=\"protein\",\n    enabled=True,\n    settings={\n        \"use_esm\": True,\n        \"structure_prediction\": True,\n    },\n)\n</code></pre> <p> Extension Config</p>"},{"location":"configs/#module-reference","title":"Module Reference","text":"Category Modules Schema base, data, distributed, extensions, hyperparam, inference, training Utils config_loader, conversion, error_handling, io, merge, templates, validation Other cli, extension_config"},{"location":"configs/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Guide - Complete configuration guide</li> <li>Training Guide - Using configs in training</li> <li>Core Configuration - Core configuration system</li> </ul>"},{"location":"configs/base/","title":"Base","text":"<p>Module: <code>configs.schema.base</code></p> <p>Source: <code>configs/schema/base.py</code></p>"},{"location":"configs/base/#classes","title":"Classes","text":""},{"location":"configs/base/#experimentconfig","title":"ExperimentConfig","text":"<pre><code>class ExperimentConfig\n</code></pre>"},{"location":"configs/base/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 0</li> <li>Imports: 2</li> </ul>"},{"location":"configs/cli/","title":"Cli","text":"<p>Module: <code>configs.cli</code></p> <p>Source: <code>configs/cli.py</code></p>"},{"location":"configs/cli/#overview","title":"Overview","text":"<p>Fixed CLI implementation with consistent imports and consolidated parsers.</p>"},{"location":"configs/cli/#functions","title":"Functions","text":""},{"location":"configs/cli/#create_parser","title":"create_parser","text":"<pre><code>def create_parser()\n</code></pre>"},{"location":"configs/cli/#handle_create_wrapper","title":"handle_create_wrapper","text":"<pre><code>def handle_create_wrapper()\n</code></pre>"},{"location":"configs/cli/#handle_get","title":"handle_get","text":"<pre><code>def handle_get()\n</code></pre>"},{"location":"configs/cli/#handle_list","title":"handle_list","text":"<pre><code>def handle_list()\n</code></pre>"},{"location":"configs/cli/#handle_show_wrapper","title":"handle_show_wrapper","text":"<pre><code>def handle_show_wrapper()\n</code></pre>"},{"location":"configs/cli/#handle_validate_wrapper","title":"handle_validate_wrapper","text":"<pre><code>def handle_validate_wrapper()\n</code></pre>"},{"location":"configs/cli/#handle_version_wrapper","title":"handle_version_wrapper","text":"<pre><code>def handle_version_wrapper()\n</code></pre>"},{"location":"configs/cli/#main","title":"main","text":"<pre><code>def main()\n</code></pre>"},{"location":"configs/cli/#register_config","title":"register_config","text":"<pre><code>def register_config()\n</code></pre>"},{"location":"configs/cli/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 9</li> <li>Imports: 14</li> </ul>"},{"location":"configs/config_loader/","title":"Config Loader","text":"<p>Module: <code>configs.utils.config_loader</code></p> <p>Source: <code>configs/utils/config_loader.py</code></p>"},{"location":"configs/config_loader/#overview","title":"Overview","text":"<p>Utilities for loading and processing configuration files.</p>"},{"location":"configs/config_loader/#functions","title":"Functions","text":""},{"location":"configs/config_loader/#create_config_from_yaml","title":"create_config_from_yaml","text":"<pre><code>def create_config_from_yaml()\n</code></pre>"},{"location":"configs/config_loader/#get_config_path","title":"get_config_path","text":"<pre><code>def get_config_path()\n</code></pre>"},{"location":"configs/config_loader/#get_data_config","title":"get_data_config","text":"<pre><code>def get_data_config()\n</code></pre>"},{"location":"configs/config_loader/#get_inference_config","title":"get_inference_config","text":"<pre><code>def get_inference_config()\n</code></pre>"},{"location":"configs/config_loader/#get_model_config","title":"get_model_config","text":"<pre><code>def get_model_config()\n</code></pre>"},{"location":"configs/config_loader/#get_training_config","title":"get_training_config","text":"<pre><code>def get_training_config()\n</code></pre>"},{"location":"configs/config_loader/#load_experiment_config","title":"load_experiment_config","text":"<pre><code>def load_experiment_config()\n</code></pre>"},{"location":"configs/config_loader/#load_yaml_config","title":"load_yaml_config","text":"<pre><code>def load_yaml_config()\n</code></pre>"},{"location":"configs/config_loader/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 8</li> <li>Imports: 7</li> </ul>"},{"location":"configs/conversion/","title":"Conversion","text":"<p>Module: <code>configs.utils.conversion</code></p> <p>Source: <code>configs/utils/conversion.py</code></p>"},{"location":"configs/conversion/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"configs/data/","title":"Data","text":"<p>Module: <code>configs.schema.data</code></p> <p>Source: <code>configs/schema/data.py</code></p>"},{"location":"configs/data/#classes","title":"Classes","text":""},{"location":"configs/data/#dataconfig","title":"DataConfig","text":"<pre><code>class DataConfig\n</code></pre>"},{"location":"configs/data/#datasetconfig","title":"DatasetConfig","text":"<pre><code>class DatasetConfig\n</code></pre>"},{"location":"configs/data/#proteindatasetconfig","title":"ProteinDatasetConfig","text":"<pre><code>class ProteinDatasetConfig\n</code></pre>"},{"location":"configs/data/#functions","title":"Functions","text":""},{"location":"configs/data/#validate_data_path","title":"validate_data_path","text":"<pre><code>def validate_data_path()\n</code></pre>"},{"location":"configs/data/#validate_max_seq_length","title":"validate_max_seq_length","text":"<pre><code>def validate_max_seq_length()\n</code></pre>"},{"location":"configs/data/#validate_positive_int","title":"validate_positive_int","text":"<pre><code>def validate_positive_int()\n</code></pre>"},{"location":"configs/data/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 3</li> <li>Imports: 2</li> </ul>"},{"location":"configs/distributed/","title":"Distributed","text":"<p>Module: <code>configs.schema.distributed</code></p> <p>Source: <code>configs/schema/distributed.py</code></p>"},{"location":"configs/distributed/#overview","title":"Overview","text":"<p>Enhanced distributed training configuration schema with improved validation.</p>"},{"location":"configs/distributed/#classes","title":"Classes","text":""},{"location":"configs/distributed/#distributedbackend","title":"DistributedBackend","text":"<pre><code>class DistributedBackend\n</code></pre>"},{"location":"configs/distributed/#distributedconfig","title":"DistributedConfig","text":"<pre><code>class DistributedConfig\n</code></pre>"},{"location":"configs/distributed/#functions","title":"Functions","text":""},{"location":"configs/distributed/#get_data_parallel_size","title":"get_data_parallel_size","text":"<pre><code>def get_data_parallel_size()\n</code></pre>"},{"location":"configs/distributed/#get_mesh_config","title":"get_mesh_config","text":"<pre><code>def get_mesh_config()\n</code></pre>"},{"location":"configs/distributed/#is_local_main_process","title":"is_local_main_process","text":"<pre><code>def is_local_main_process()\n</code></pre>"},{"location":"configs/distributed/#is_main_process","title":"is_main_process","text":"<pre><code>def is_main_process()\n</code></pre>"},{"location":"configs/distributed/#validate_distributed_consistency","title":"validate_distributed_consistency","text":"<pre><code>def validate_distributed_consistency()\n</code></pre>"},{"location":"configs/distributed/#validate_port","title":"validate_port","text":"<pre><code>def validate_port()\n</code></pre>"},{"location":"configs/distributed/#validate_positive_int","title":"validate_positive_int","text":"<pre><code>def validate_positive_int()\n</code></pre>"},{"location":"configs/distributed/#validate_rank_non_negative","title":"validate_rank_non_negative","text":"<pre><code>def validate_rank_non_negative()\n</code></pre>"},{"location":"configs/distributed/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 8</li> <li>Imports: 4</li> </ul>"},{"location":"configs/dit./","title":"Dit","text":"<p>Module: <code>configs.schema.models.dit.</code></p> <p>Source: <code>configs/schema/models/dit..py</code></p>"},{"location":"configs/dit./#overview","title":"Overview","text":"<p>Configuration schema for Diffusion Transformer models.</p> <p>This should be placed in: src/artifex/configs/schema/models/dit.py</p>"},{"location":"configs/dit./#classes","title":"Classes","text":""},{"location":"configs/dit./#ditconfig","title":"DiTConfig","text":"<pre><code>class DiTConfig\n</code></pre>"},{"location":"configs/dit./#ditsizepresets","title":"DiTSizePresets","text":"<pre><code>class DiTSizePresets\n</code></pre>"},{"location":"configs/dit./#functions","title":"Functions","text":""},{"location":"configs/dit./#dit_b","title":"DiT_B","text":"<pre><code>def DiT_B()\n</code></pre>"},{"location":"configs/dit./#dit_l","title":"DiT_L","text":"<pre><code>def DiT_L()\n</code></pre>"},{"location":"configs/dit./#dit_s","title":"DiT_S","text":"<pre><code>def DiT_S()\n</code></pre>"},{"location":"configs/dit./#dit_xl","title":"DiT_XL","text":"<pre><code>def DiT_XL()\n</code></pre>"},{"location":"configs/dit./#create_dit_config","title":"create_dit_config","text":"<pre><code>def create_dit_config()\n</code></pre>"},{"location":"configs/dit./#dit_config_to_model_config","title":"dit_config_to_model_config","text":"<pre><code>def dit_config_to_model_config()\n</code></pre>"},{"location":"configs/dit./#to_model_config","title":"to_model_config","text":"<pre><code>def to_model_config()\n</code></pre>"},{"location":"configs/dit./#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 7</li> <li>Imports: 4</li> </ul>"},{"location":"configs/error_handling/","title":"Error Handling","text":"<p>Module: <code>configs.utils.error_handling</code></p> <p>Source: <code>configs/utils/error_handling.py</code></p>"},{"location":"configs/error_handling/#overview","title":"Overview","text":"<p>Utilities for handling configuration errors with clear context and user-friendly messages.</p>"},{"location":"configs/error_handling/#classes","title":"Classes","text":""},{"location":"configs/error_handling/#configerror","title":"ConfigError","text":"<pre><code>class ConfigError\n</code></pre>"},{"location":"configs/error_handling/#configloaderror","title":"ConfigLoadError","text":"<pre><code>class ConfigLoadError\n</code></pre>"},{"location":"configs/error_handling/#confignotfounderror","title":"ConfigNotFoundError","text":"<pre><code>class ConfigNotFoundError\n</code></pre>"},{"location":"configs/error_handling/#configvalidationerror","title":"ConfigValidationError","text":"<pre><code>class ConfigValidationError\n</code></pre>"},{"location":"configs/error_handling/#functions","title":"Functions","text":""},{"location":"configs/error_handling/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"configs/error_handling/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"configs/error_handling/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"configs/error_handling/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"configs/error_handling/#format_config_error","title":"format_config_error","text":"<pre><code>def format_config_error()\n</code></pre>"},{"location":"configs/error_handling/#format_validation_error","title":"format_validation_error","text":"<pre><code>def format_validation_error()\n</code></pre>"},{"location":"configs/error_handling/#safe_load_config","title":"safe_load_config","text":"<pre><code>def safe_load_config()\n</code></pre>"},{"location":"configs/error_handling/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 7</li> <li>Imports: 4</li> </ul>"},{"location":"configs/extension_config/","title":"Extension Config","text":"<p>Module: <code>configs.extension_config</code></p> <p>Source: <code>configs/extension_config.py</code></p>"},{"location":"configs/extension_config/#overview","title":"Overview","text":"<p>Configuration schema for model extensions.</p> <p>This module defines configuration schemas for various extension types that can be attached to generative models.</p>"},{"location":"configs/extension_config/#classes","title":"Classes","text":""},{"location":"configs/extension_config/#bondangleconfig","title":"BondAngleConfig","text":"<pre><code>class BondAngleConfig\n</code></pre>"},{"location":"configs/extension_config/#bondlengthconfig","title":"BondLengthConfig","text":"<pre><code>class BondLengthConfig\n</code></pre>"},{"location":"configs/extension_config/#extensionconfig","title":"ExtensionConfig","text":"<pre><code>class ExtensionConfig\n</code></pre>"},{"location":"configs/extension_config/#extensionsconfig","title":"ExtensionsConfig","text":"<pre><code>class ExtensionsConfig\n</code></pre>"},{"location":"configs/extension_config/#proteinextensionsconfig","title":"ProteinExtensionsConfig","text":"<pre><code>class ProteinExtensionsConfig\n</code></pre>"},{"location":"configs/extension_config/#proteinmixinconfig","title":"ProteinMixinConfig","text":"<pre><code>class ProteinMixinConfig\n</code></pre>"},{"location":"configs/extension_config/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 6</li> <li>Functions: 0</li> <li>Imports: 2</li> </ul>"},{"location":"configs/extensions/","title":"Extensions","text":"<p>Module: <code>configs.schema.extensions</code></p> <p>Source: <code>configs/schema/extensions.py</code></p>"},{"location":"configs/extensions/#overview","title":"Overview","text":"<p>Schema for extension configurations.</p> <p>This module defines configuration schemas for model extensions.</p>"},{"location":"configs/extensions/#classes","title":"Classes","text":""},{"location":"configs/extensions/#constraintextensionconfig","title":"ConstraintExtensionConfig","text":"<pre><code>class ConstraintExtensionConfig\n</code></pre>"},{"location":"configs/extensions/#extensionconfig","title":"ExtensionConfig","text":"<pre><code>class ExtensionConfig\n</code></pre>"},{"location":"configs/extensions/#extensionsconfig","title":"ExtensionsConfig","text":"<pre><code>class ExtensionsConfig\n</code></pre>"},{"location":"configs/extensions/#proteinbackboneconstraintconfig","title":"ProteinBackboneConstraintConfig","text":"<pre><code>class ProteinBackboneConstraintConfig\n</code></pre>"},{"location":"configs/extensions/#proteinextensionconfig","title":"ProteinExtensionConfig","text":"<pre><code>class ProteinExtensionConfig\n</code></pre>"},{"location":"configs/extensions/#proteinmixinconfig","title":"ProteinMixinConfig","text":"<pre><code>class ProteinMixinConfig\n</code></pre>"},{"location":"configs/extensions/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 6</li> <li>Functions: 0</li> <li>Imports: 2</li> </ul>"},{"location":"configs/hyperparam/","title":"Hyperparam","text":"<p>Module: <code>configs.schema.hyperparam</code></p> <p>Source: <code>configs/schema/hyperparam.py</code></p>"},{"location":"configs/hyperparam/#overview","title":"Overview","text":"<p>Configuration schema for hyperparameter search.</p>"},{"location":"configs/hyperparam/#classes","title":"Classes","text":""},{"location":"configs/hyperparam/#categoricaldistribution","title":"CategoricalDistribution","text":"<pre><code>class CategoricalDistribution\n</code></pre>"},{"location":"configs/hyperparam/#choicedistribution","title":"ChoiceDistribution","text":"<pre><code>class ChoiceDistribution\n</code></pre>"},{"location":"configs/hyperparam/#hyperparamsearchconfig","title":"HyperparamSearchConfig","text":"<pre><code>class HyperparamSearchConfig\n</code></pre>"},{"location":"configs/hyperparam/#loguniformdistribution","title":"LogUniformDistribution","text":"<pre><code>class LogUniformDistribution\n</code></pre>"},{"location":"configs/hyperparam/#parameterdistribution","title":"ParameterDistribution","text":"<pre><code>class ParameterDistribution\n</code></pre>"},{"location":"configs/hyperparam/#searchtype","title":"SearchType","text":"<pre><code>class SearchType\n</code></pre>"},{"location":"configs/hyperparam/#uniformdistribution","title":"UniformDistribution","text":"<pre><code>class UniformDistribution\n</code></pre>"},{"location":"configs/hyperparam/#functions","title":"Functions","text":""},{"location":"configs/hyperparam/#validate_categories","title":"validate_categories","text":"<pre><code>def validate_categories()\n</code></pre>"},{"location":"configs/hyperparam/#validate_choices","title":"validate_choices","text":"<pre><code>def validate_choices()\n</code></pre>"},{"location":"configs/hyperparam/#validate_high","title":"validate_high","text":"<pre><code>def validate_high()\n</code></pre>"},{"location":"configs/hyperparam/#validate_low","title":"validate_low","text":"<pre><code>def validate_low()\n</code></pre>"},{"location":"configs/hyperparam/#validate_max_parallel","title":"validate_max_parallel","text":"<pre><code>def validate_max_parallel()\n</code></pre>"},{"location":"configs/hyperparam/#validate_positive_int","title":"validate_positive_int","text":"<pre><code>def validate_positive_int()\n</code></pre>"},{"location":"configs/hyperparam/#validate_q","title":"validate_q","text":"<pre><code>def validate_q()\n</code></pre>"},{"location":"configs/hyperparam/#validate_range","title":"validate_range","text":"<pre><code>def validate_range()\n</code></pre>"},{"location":"configs/hyperparam/#validate_search_space","title":"validate_search_space","text":"<pre><code>def validate_search_space()\n</code></pre>"},{"location":"configs/hyperparam/#validate_weights","title":"validate_weights","text":"<pre><code>def validate_weights()\n</code></pre>"},{"location":"configs/hyperparam/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 7</li> <li>Functions: 10</li> <li>Imports: 4</li> </ul>"},{"location":"configs/inference/","title":"Inference","text":"<p>Module: <code>configs.schema.inference</code></p> <p>Source: <code>configs/schema/inference.py</code></p>"},{"location":"configs/inference/#overview","title":"Overview","text":"<p>Inference configuration schemas.</p> <p>This module defines configuration schemas for model inference.</p>"},{"location":"configs/inference/#classes","title":"Classes","text":""},{"location":"configs/inference/#diffusioninferenceconfig","title":"DiffusionInferenceConfig","text":"<pre><code>class DiffusionInferenceConfig\n</code></pre>"},{"location":"configs/inference/#inferenceconfig","title":"InferenceConfig","text":"<pre><code>class InferenceConfig\n</code></pre>"},{"location":"configs/inference/#proteindiffusioninferenceconfig","title":"ProteinDiffusionInferenceConfig","text":"<pre><code>class ProteinDiffusionInferenceConfig\n</code></pre>"},{"location":"configs/inference/#functions","title":"Functions","text":""},{"location":"configs/inference/#validate_device","title":"validate_device","text":"<pre><code>def validate_device()\n</code></pre>"},{"location":"configs/inference/#validate_positive_float","title":"validate_positive_float","text":"<pre><code>def validate_positive_float()\n</code></pre>"},{"location":"configs/inference/#validate_positive_int","title":"validate_positive_int","text":"<pre><code>def validate_positive_int()\n</code></pre>"},{"location":"configs/inference/#validate_positive_int_1","title":"validate_positive_int","text":"<pre><code>def validate_positive_int()\n</code></pre>"},{"location":"configs/inference/#validate_target_seq_length","title":"validate_target_seq_length","text":"<pre><code>def validate_target_seq_length()\n</code></pre>"},{"location":"configs/inference/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 5</li> <li>Imports: 2</li> </ul>"},{"location":"configs/io/","title":"Io","text":"<p>Module: <code>configs.utils.io</code></p> <p>Source: <code>configs/utils/io.py</code></p>"},{"location":"configs/io/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"configs/merge/","title":"Merge","text":"<p>Module: <code>configs.utils.merge</code></p> <p>Source: <code>configs/utils/merge.py</code></p>"},{"location":"configs/merge/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"configs/templates/","title":"Templates","text":"<p>Module: <code>configs.utils.templates</code></p> <p>Source: <code>configs/utils/templates.py</code></p>"},{"location":"configs/templates/#overview","title":"Overview","text":"<p>Configuration template system and testing framework.</p>"},{"location":"configs/templates/#classes","title":"Classes","text":""},{"location":"configs/templates/#testconfigurationsystem","title":"TestConfigurationSystem","text":"<pre><code>class TestConfigurationSystem\n</code></pre>"},{"location":"configs/templates/#functions","title":"Functions","text":""},{"location":"configs/templates/#example_usage","title":"example_usage","text":"<pre><code>def example_usage()\n</code></pre>"},{"location":"configs/templates/#test_config_inheritance_and_overrides","title":"test_config_inheritance_and_overrides","text":"<pre><code>def test_config_inheritance_and_overrides()\n</code></pre>"},{"location":"configs/templates/#test_distributed_config_validation","title":"test_distributed_config_validation","text":"<pre><code>def test_distributed_config_validation()\n</code></pre>"},{"location":"configs/templates/#test_error_handling_and_suggestions","title":"test_error_handling_and_suggestions","text":"<pre><code>def test_error_handling_and_suggestions()\n</code></pre>"},{"location":"configs/templates/#test_full_experiment_loading","title":"test_full_experiment_loading","text":"<pre><code>def test_full_experiment_loading()\n</code></pre>"},{"location":"configs/templates/#test_performance_optimizations","title":"test_performance_optimizations","text":"<pre><code>def test_performance_optimizations()\n</code></pre>"},{"location":"configs/templates/#test_point_cloud_diffusion_config_validation","title":"test_point_cloud_diffusion_config_validation","text":"<pre><code>def test_point_cloud_diffusion_config_validation()\n</code></pre>"},{"location":"configs/templates/#test_template_system","title":"test_template_system","text":"<pre><code>def test_template_system()\n</code></pre>"},{"location":"configs/templates/#test_training_config_validation","title":"test_training_config_validation","text":"<pre><code>def test_training_config_validation()\n</code></pre>"},{"location":"configs/templates/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 9</li> <li>Imports: 5</li> </ul>"},{"location":"configs/training/","title":"Training","text":"<p>Module: <code>configs.schema.training</code></p> <p>Source: <code>configs/schema/training.py</code></p>"},{"location":"configs/training/#classes","title":"Classes","text":""},{"location":"configs/training/#optimizerconfig","title":"OptimizerConfig","text":"<pre><code>class OptimizerConfig\n</code></pre>"},{"location":"configs/training/#schedulerconfig","title":"SchedulerConfig","text":"<pre><code>class SchedulerConfig\n</code></pre>"},{"location":"configs/training/#trainingconfig","title":"TrainingConfig","text":"<pre><code>class TrainingConfig\n</code></pre>"},{"location":"configs/training/#functions","title":"Functions","text":""},{"location":"configs/training/#validate_beta","title":"validate_beta","text":"<pre><code>def validate_beta()\n</code></pre>"},{"location":"configs/training/#validate_grad_clip_norm","title":"validate_grad_clip_norm","text":"<pre><code>def validate_grad_clip_norm()\n</code></pre>"},{"location":"configs/training/#validate_max_steps","title":"validate_max_steps","text":"<pre><code>def validate_max_steps()\n</code></pre>"},{"location":"configs/training/#validate_positive_float","title":"validate_positive_float","text":"<pre><code>def validate_positive_float()\n</code></pre>"},{"location":"configs/training/#validate_positive_int","title":"validate_positive_int","text":"<pre><code>def validate_positive_int()\n</code></pre>"},{"location":"configs/training/#validate_ratio","title":"validate_ratio","text":"<pre><code>def validate_ratio()\n</code></pre>"},{"location":"configs/training/#validate_warmup_steps","title":"validate_warmup_steps","text":"<pre><code>def validate_warmup_steps()\n</code></pre>"},{"location":"configs/training/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 7</li> <li>Imports: 3</li> </ul>"},{"location":"configs/validation/","title":"Validation","text":"<p>Module: <code>configs.utils.validation</code></p> <p>Source: <code>configs/utils/validation.py</code></p>"},{"location":"configs/validation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"core/","title":"Core Components","text":"<p>The core module provides foundational abstractions, protocols, and utilities for building generative models in Artifex.</p>"},{"location":"core/#overview","title":"Overview","text":"<ul> <li> <p> Configuration</p> <p>Unified dataclass-based configuration system for all models</p> </li> <li> <p> Loss Functions</p> <p>Composable losses for reconstruction, adversarial, divergence, and more</p> </li> <li> <p> Distributions</p> <p>Probability distributions for latent spaces and sampling</p> </li> <li> <p> Sampling</p> <p>MCMC, ODE/SDE solvers, ancestral sampling, and BlackJAX integration</p> </li> </ul>"},{"location":"core/#configuration-system","title":"Configuration System","text":"<p>Type-safe, validated configuration for all model components.</p> <pre><code>from artifex.generative_models.core.configuration import (\n    VAEConfig,\n    EncoderConfig,\n    DecoderConfig,\n)\n\nencoder = EncoderConfig(\n    name=\"encoder\",\n    input_shape=(28, 28, 1),\n    latent_dim=32,\n    hidden_dims=(256, 128),\n)\n\nconfig = VAEConfig(\n    name=\"my_vae\",\n    encoder=encoder,\n    decoder=decoder,\n)\n</code></pre> <p> Configuration Reference</p>"},{"location":"core/#configuration-modules","title":"Configuration Modules","text":"Module Description unified Unified configuration classes environment Environment configuration gan GAN-specific configs validation Config validation migrate_configs Config migration tools"},{"location":"core/#loss-functions","title":"Loss Functions","text":"<p>Composable loss functions for training generative models.</p>"},{"location":"core/#reconstruction-losses","title":"Reconstruction Losses","text":"<pre><code>from artifex.generative_models.core.losses import (\n    mse_loss,\n    binary_cross_entropy,\n    gaussian_nll,\n)\n\n# Mean squared error\nloss = mse_loss(prediction, target)\n\n# Binary cross entropy for images\nloss = binary_cross_entropy(prediction, target)\n</code></pre> <p> Reconstruction Losses</p>"},{"location":"core/#adversarial-losses","title":"Adversarial Losses","text":"<pre><code>from artifex.generative_models.core.losses import (\n    discriminator_loss,\n    generator_loss,\n    wasserstein_loss,\n    gradient_penalty,\n)\n\n# Standard GAN loss\nd_loss = discriminator_loss(real_scores, fake_scores)\ng_loss = generator_loss(fake_scores)\n\n# WGAN with gradient penalty\ngp = gradient_penalty(discriminator, real, fake)\n</code></pre> <p> Adversarial Losses</p>"},{"location":"core/#divergence-losses","title":"Divergence Losses","text":"<pre><code>from artifex.generative_models.core.losses import (\n    kl_divergence,\n    kl_divergence_gaussian,\n    mmd_loss,\n)\n\n# KL divergence for VAE\nkl_loss = kl_divergence_gaussian(mean, log_var)\n\n# Maximum Mean Discrepancy\nmmd = mmd_loss(samples1, samples2)\n</code></pre> <p> Divergence Losses</p>"},{"location":"core/#perceptual-losses","title":"Perceptual Losses","text":"<pre><code>from artifex.generative_models.core.losses import (\n    perceptual_loss,\n    lpips_loss,\n)\n\n# VGG-based perceptual loss\nloss = perceptual_loss(generated, target, feature_extractor)\n</code></pre> <p> Perceptual Losses</p>"},{"location":"core/#composable-losses","title":"Composable Losses","text":"<pre><code>from artifex.generative_models.core.losses import CompositeLoss\n\nloss_fn = CompositeLoss(\n    losses=[\n        (\"reconstruction\", mse_loss, 1.0),\n        (\"kl\", kl_divergence_gaussian, 0.1),\n        (\"perceptual\", perceptual_loss, 0.01),\n    ]\n)\n\ntotal_loss, loss_dict = loss_fn(model_outputs, targets)\n</code></pre> <p> Composable Losses</p>"},{"location":"core/#distributions","title":"Distributions","text":"<p>Probability distributions for generative modeling.</p> <pre><code>from artifex.generative_models.core.distributions import (\n    Normal,\n    Categorical,\n    MixtureDiagonal,\n)\n\n# Gaussian distribution\ndist = Normal(mean, std)\nsamples = dist.sample(key, shape=(100,))\nlog_prob = dist.log_prob(samples)\n\n# Mixture of Gaussians\nmixture = MixtureDiagonal(means, stds, weights)\n</code></pre>"},{"location":"core/#distribution-modules","title":"Distribution Modules","text":"Module Description base Base distribution class continuous Normal, Laplace, etc. discrete Categorical, Bernoulli mixture Mixture distributions transformations Distribution transforms"},{"location":"core/#sampling-methods","title":"Sampling Methods","text":"<p>Advanced sampling algorithms for generative models.</p>"},{"location":"core/#ancestral-sampling","title":"Ancestral Sampling","text":"<pre><code>from artifex.generative_models.core.sampling import ancestral_sample\n\nsamples = ancestral_sample(\n    model=diffusion_model,\n    shape=(batch_size, *image_shape),\n    num_steps=1000,\n    key=prng_key,\n)\n</code></pre> <p> Ancestral Sampling</p>"},{"location":"core/#mcmc-sampling","title":"MCMC Sampling","text":"<pre><code>from artifex.generative_models.core.sampling import (\n    langevin_dynamics,\n    hmc_sample,\n)\n\n# Langevin dynamics for EBM\nsamples = langevin_dynamics(\n    energy_fn=model.energy,\n    initial=noise,\n    step_size=0.01,\n    num_steps=100,\n)\n</code></pre> <p> MCMC Sampling</p>"},{"location":"core/#blackjax-integration","title":"BlackJAX Integration","text":"<pre><code>from artifex.generative_models.core.sampling import BlackJAXSampler\n\nsampler = BlackJAXSampler(\n    algorithm=\"nuts\",\n    step_size=0.1,\n    num_warmup=500,\n)\n\nsamples = sampler.sample(\n    log_prob_fn=model.log_prob,\n    initial_state=initial,\n    num_samples=1000,\n)\n</code></pre> <p> BlackJAX Samplers</p>"},{"location":"core/#odesde-solvers","title":"ODE/SDE Solvers","text":"<pre><code>from artifex.generative_models.core.sampling import (\n    ode_solver,\n    sde_solver,\n)\n\n# Probability flow ODE\nsamples = ode_solver(\n    drift_fn=model.drift,\n    initial=noise,\n    t_span=(1.0, 0.0),\n)\n\n# Reverse-time SDE\nsamples = sde_solver(\n    drift_fn=model.drift,\n    diffusion_fn=model.diffusion,\n    initial=noise,\n)\n</code></pre> <p> ODE Solvers |  SDE Solvers</p>"},{"location":"core/#metrics","title":"Metrics","text":"<p>Evaluation metrics for generative models.</p>"},{"location":"core/#image-metrics","title":"Image Metrics","text":"<pre><code>from artifex.generative_models.core.metrics import (\n    compute_fid,\n    compute_inception_score,\n)\n\nfid = compute_fid(real_images, generated_images)\nis_score = compute_inception_score(generated_images)\n</code></pre> <p> FID |  Inception Score</p>"},{"location":"core/#statistical-metrics","title":"Statistical Metrics","text":"<pre><code>from artifex.generative_models.core.metrics import (\n    precision_recall,\n    coverage,\n    density,\n)\n\npr = precision_recall(real_features, generated_features)\n</code></pre> <p> Precision/Recall</p>"},{"location":"core/#layers","title":"Layers","text":"<p>Reusable neural network layers.</p>"},{"location":"core/#attention","title":"Attention","text":"<pre><code>from artifex.generative_models.core.layers import (\n    MultiHeadAttention,\n    FlashAttention,\n    CrossAttention,\n)\n\nattention = MultiHeadAttention(\n    num_heads=8,\n    head_dim=64,\n    rngs=rngs,\n)\n</code></pre> <p> Flash Attention</p>"},{"location":"core/#positional-encoding","title":"Positional Encoding","text":"<pre><code>from artifex.generative_models.core.layers import (\n    SinusoidalPositionalEncoding,\n    LearnedPositionalEncoding,\n    RotaryPositionalEncoding,\n)\n</code></pre> <p> Positional Encoding</p>"},{"location":"core/#transformer-blocks","title":"Transformer Blocks","text":"<pre><code>from artifex.generative_models.core.layers import (\n    TransformerBlock,\n    TransformerEncoder,\n    TransformerDecoder,\n)\n</code></pre> <p> Transformers</p>"},{"location":"core/#resnet-blocks","title":"ResNet Blocks","text":"<pre><code>from artifex.generative_models.core.layers import (\n    ResNetBlock,\n    ResNetBlockV2,\n)\n</code></pre> <p> ResNet</p>"},{"location":"core/#device-management","title":"Device Management","text":"<p>Hardware-aware device management.</p> <pre><code>from artifex.generative_models.core import DeviceManager\n\ndevice_manager = DeviceManager()\ndevice = device_manager.get_device()\n\n# Check GPU availability\nif device_manager.has_gpu():\n    print(f\"Using GPU: {device_manager.gpu_info()}\")\n</code></pre> <p> Device Manager</p>"},{"location":"core/#protocols","title":"Protocols","text":"<p>Protocol interfaces for type-safe implementations.</p> <pre><code>from artifex.generative_models.core.protocols import (\n    GenerativeModel,\n    Encoder,\n    Decoder,\n    LossFunction,\n)\n</code></pre>"},{"location":"core/#protocol-modules","title":"Protocol Modules","text":"Module Description benchmarks Benchmark protocols configuration Config protocols evaluation Evaluation protocols metrics Metric protocols"},{"location":"core/#module-reference","title":"Module Reference","text":"Category Modules Configuration unified, environment, gan, validation, migrate_configs Losses adversarial, base, composable, divergence, geometric, perceptual, reconstruction, regularization Distributions base, continuous, discrete, mixture, transformations Sampling ancestral, base, blackjax_samplers, diffusion, mcmc, ode, sde Metrics base, distance, fid, inception_score, information, precision_recall, quality, statistical Layers causal, flash_attention, positional, residual, resnet, transformers Core adapters, base, checkpointing, device_manager, interfaces, logging, parallelism, performance, types"},{"location":"core/#related-documentation","title":"Related Documentation","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Configuration Guide - Configuration system guide</li> <li>Loss Functions Guide - Loss function details</li> </ul>"},{"location":"core/adapters/","title":"Adapters","text":"<p>Module: <code>generative_models.core.adapters</code></p> <p>Source: <code>generative_models/core/adapters.py</code></p>"},{"location":"core/adapters/#overview","title":"Overview","text":"<p>Model adapter classes for different architectures.</p> <p>This module provides standardized interfaces for scaling different model architectures with consistent APIs and optimization strategies.</p> <p>All implementations follow JAX/Flax NNX best practices and provide hardware-aware optimization for different model types.</p>"},{"location":"core/adapters/#classes","title":"Classes","text":""},{"location":"core/adapters/#diffusionadapter","title":"DiffusionAdapter","text":"<pre><code>class DiffusionAdapter\n</code></pre>"},{"location":"core/adapters/#energyadapter","title":"EnergyAdapter","text":"<pre><code>class EnergyAdapter\n</code></pre>"},{"location":"core/adapters/#modeladapter","title":"ModelAdapter","text":"<pre><code>class ModelAdapter\n</code></pre>"},{"location":"core/adapters/#modelspecs","title":"ModelSpecs","text":"<pre><code>class ModelSpecs\n</code></pre>"},{"location":"core/adapters/#transformeradapter","title":"TransformerAdapter","text":"<pre><code>class TransformerAdapter\n</code></pre>"},{"location":"core/adapters/#functions","title":"Functions","text":""},{"location":"core/adapters/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/adapters/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/adapters/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/adapters/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/adapters/#apply_sharding","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"core/adapters/#apply_sharding_1","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"core/adapters/#apply_sharding_2","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"core/adapters/#apply_sharding_3","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"core/adapters/#create_diffusion_adapter","title":"create_diffusion_adapter","text":"<pre><code>def create_diffusion_adapter()\n</code></pre>"},{"location":"core/adapters/#create_energy_adapter","title":"create_energy_adapter","text":"<pre><code>def create_energy_adapter()\n</code></pre>"},{"location":"core/adapters/#create_transformer_adapter","title":"create_transformer_adapter","text":"<pre><code>def create_transformer_adapter()\n</code></pre>"},{"location":"core/adapters/#estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>def estimate_memory_usage()\n</code></pre>"},{"location":"core/adapters/#estimate_memory_usage_1","title":"estimate_memory_usage","text":"<pre><code>def estimate_memory_usage()\n</code></pre>"},{"location":"core/adapters/#estimate_memory_usage_2","title":"estimate_memory_usage","text":"<pre><code>def estimate_memory_usage()\n</code></pre>"},{"location":"core/adapters/#estimate_memory_usage_3","title":"estimate_memory_usage","text":"<pre><code>def estimate_memory_usage()\n</code></pre>"},{"location":"core/adapters/#get_model_specs","title":"get_model_specs","text":"<pre><code>def get_model_specs()\n</code></pre>"},{"location":"core/adapters/#get_model_specs_1","title":"get_model_specs","text":"<pre><code>def get_model_specs()\n</code></pre>"},{"location":"core/adapters/#get_model_specs_2","title":"get_model_specs","text":"<pre><code>def get_model_specs()\n</code></pre>"},{"location":"core/adapters/#get_model_specs_3","title":"get_model_specs","text":"<pre><code>def get_model_specs()\n</code></pre>"},{"location":"core/adapters/#get_optimal_batch_size","title":"get_optimal_batch_size","text":"<pre><code>def get_optimal_batch_size()\n</code></pre>"},{"location":"core/adapters/#get_optimal_batch_size_1","title":"get_optimal_batch_size","text":"<pre><code>def get_optimal_batch_size()\n</code></pre>"},{"location":"core/adapters/#get_optimal_batch_size_2","title":"get_optimal_batch_size","text":"<pre><code>def get_optimal_batch_size()\n</code></pre>"},{"location":"core/adapters/#get_optimal_batch_size_3","title":"get_optimal_batch_size","text":"<pre><code>def get_optimal_batch_size()\n</code></pre>"},{"location":"core/adapters/#get_performance_characteristics","title":"get_performance_characteristics","text":"<pre><code>def get_performance_characteristics()\n</code></pre>"},{"location":"core/adapters/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 5</li> <li>Functions: 24</li> <li>Imports: 6</li> </ul>"},{"location":"core/adversarial/","title":"Adversarial","text":"<p>Module: <code>generative_models.core.losses.adversarial</code></p> <p>Source: <code>generative_models/core/losses/adversarial.py</code></p>"},{"location":"core/adversarial/#overview","title":"Overview","text":"<p>Adversarial loss functions module.</p> <p>This module provides loss functions commonly used in Generative Adversarial Networks (GANs) and other adversarial training approaches.</p>"},{"location":"core/adversarial/#functions","title":"Functions","text":""},{"location":"core/adversarial/#hinge_discriminator_loss","title":"hinge_discriminator_loss","text":"<pre><code>def hinge_discriminator_loss()\n</code></pre>"},{"location":"core/adversarial/#hinge_generator_loss","title":"hinge_generator_loss","text":"<pre><code>def hinge_generator_loss()\n</code></pre>"},{"location":"core/adversarial/#least_squares_discriminator_loss","title":"least_squares_discriminator_loss","text":"<pre><code>def least_squares_discriminator_loss()\n</code></pre>"},{"location":"core/adversarial/#least_squares_generator_loss","title":"least_squares_generator_loss","text":"<pre><code>def least_squares_generator_loss()\n</code></pre>"},{"location":"core/adversarial/#vanilla_discriminator_loss","title":"vanilla_discriminator_loss","text":"<pre><code>def vanilla_discriminator_loss()\n</code></pre>"},{"location":"core/adversarial/#vanilla_generator_loss","title":"vanilla_generator_loss","text":"<pre><code>def vanilla_generator_loss()\n</code></pre>"},{"location":"core/adversarial/#wasserstein_discriminator_loss","title":"wasserstein_discriminator_loss","text":"<pre><code>def wasserstein_discriminator_loss()\n</code></pre>"},{"location":"core/adversarial/#wasserstein_generator_loss","title":"wasserstein_generator_loss","text":"<pre><code>def wasserstein_generator_loss()\n</code></pre>"},{"location":"core/adversarial/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 8</li> <li>Imports: 3</li> </ul>"},{"location":"core/ancestral/","title":"Ancestral","text":"<p>Module: <code>generative_models.core.sampling.ancestral</code></p> <p>Source: <code>generative_models/core/sampling/ancestral.py</code></p>"},{"location":"core/ancestral/#overview","title":"Overview","text":"<p>Ancestral sampling utilities.</p> <p>This module provides utilities for ancestral sampling from probabilistic distributions.</p>"},{"location":"core/ancestral/#functions","title":"Functions","text":""},{"location":"core/ancestral/#ancestral_sampling","title":"ancestral_sampling","text":"<pre><code>def ancestral_sampling()\n</code></pre>"},{"location":"core/ancestral/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 1</li> <li>Imports: 4</li> </ul>"},{"location":"core/architecture/","title":"Architecture Overview","text":"<p>Artifex follows a modular, protocol-based architecture designed for research experimentation and production deployment.</p>"},{"location":"core/architecture/#high-level-structure","title":"High-Level Structure","text":"<pre><code>artifex/\n\u251c\u2500\u2500 benchmarks/             # Evaluation framework and metrics\n\u251c\u2500\u2500 cli/                    # Command-line interface\n\u251c\u2500\u2500 configs/                # Configuration schemas and defaults\n\u251c\u2500\u2500 data/                   # Data loading and preprocessing\n\u251c\u2500\u2500 generative_models/\n\u2502   \u251c\u2500\u2500 core/               # Core abstractions\n\u2502   \u2502   \u251c\u2500\u2500 protocols/      # Type-safe interfaces\n\u2502   \u2502   \u251c\u2500\u2500 configuration/  # Frozen dataclass configs\n\u2502   \u2502   \u251c\u2500\u2500 losses/         # Modular loss functions\n\u2502   \u2502   \u251c\u2500\u2500 sampling/       # Sampling strategies\n\u2502   \u2502   \u251c\u2500\u2500 distributions/  # Probability distributions\n\u2502   \u2502   \u2514\u2500\u2500 evaluation/     # Metrics and benchmarks\n\u2502   \u251c\u2500\u2500 models/             # Model implementations\n\u2502   \u2502   \u251c\u2500\u2500 vae/            # VAE variants\n\u2502   \u2502   \u251c\u2500\u2500 gan/            # GAN variants\n\u2502   \u2502   \u251c\u2500\u2500 diffusion/      # Diffusion models\n\u2502   \u2502   \u251c\u2500\u2500 flow/           # Normalizing flows\n\u2502   \u2502   \u251c\u2500\u2500 energy/         # Energy-based models\n\u2502   \u2502   \u251c\u2500\u2500 autoregressive/ # Autoregressive models\n\u2502   \u2502   \u251c\u2500\u2500 geometric/      # Geometric models\n\u2502   \u2502   \u251c\u2500\u2500 audio/          # Audio models (WaveNet)\n\u2502   \u2502   \u2514\u2500\u2500 backbones/      # Shared architectures\n\u2502   \u251c\u2500\u2500 modalities/         # Multi-modal support\n\u2502   \u251c\u2500\u2500 training/           # Training infrastructure\n\u2502   \u251c\u2500\u2500 inference/          # Generation and serving\n\u2502   \u251c\u2500\u2500 factory/            # Model creation\n\u2502   \u251c\u2500\u2500 extensions/         # Domain-specific extensions\n\u2502   \u251c\u2500\u2500 fine_tuning/        # LoRA, adapters, RL\n\u2502   \u251c\u2500\u2500 scaling/            # Distributed training\n\u2502   \u2514\u2500\u2500 zoo/                # Pre-configured models\n\u251c\u2500\u2500 utils/                  # Shared utilities\n\u2514\u2500\u2500 visualization/          # Plotting and visualization\n</code></pre>"},{"location":"core/architecture/#core-design-patterns","title":"Core Design Patterns","text":""},{"location":"core/architecture/#protocol-based-interfaces","title":"Protocol-Based Interfaces","text":"<p>All major components implement Python Protocols for type-safe interfaces:</p> <pre><code>from typing import Protocol\n\nclass GenerativeModel(Protocol):\n    \"\"\"Protocol for all generative models.\"\"\"\n\n    def generate(self, n_samples: int, *, rngs: nnx.Rngs) -&gt; jax.Array:\n        \"\"\"Generate samples from the model.\"\"\"\n        ...\n\n    def loss(self, batch: jax.Array, *, rngs: nnx.Rngs) -&gt; LossOutput:\n        \"\"\"Compute loss for a batch.\"\"\"\n        ...\n</code></pre> <p>Benefits:</p> <ul> <li>Clear contracts between components</li> <li>Type checking at development time</li> <li>Easy to swap implementations</li> <li>Facilitates testing with mocks</li> </ul>"},{"location":"core/architecture/#frozen-dataclass-configuration","title":"Frozen Dataclass Configuration","text":"<p>All models use immutable configuration objects:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass VAEConfig:\n    name: str\n    encoder: EncoderConfig\n    decoder: DecoderConfig\n    kl_weight: float = 1.0\n</code></pre> <p>Benefits:</p> <ul> <li>Immutable during training</li> <li>Automatic validation</li> <li>Serializable for reproducibility</li> <li>IDE support with autocomplete</li> </ul>"},{"location":"core/architecture/#factory-pattern","title":"Factory Pattern","text":"<p>Models are created through factories for consistent initialization:</p> <pre><code>from artifex.generative_models.factory import create_model\n\nmodel = create_model(config, rngs=rngs)\n</code></pre> <p>The factory:</p> <ol> <li>Validates configuration</li> <li>Selects appropriate model class</li> <li>Initializes with proper RNG management</li> <li>Returns fully configured model</li> </ol>"},{"location":"core/architecture/#component-details","title":"Component Details","text":""},{"location":"core/architecture/#core-generative_modelscore","title":"Core (<code>generative_models/core/</code>)","text":"<ul> <li>protocols/: Interface definitions for models, trainers, data loaders</li> <li>configuration/: Frozen dataclass configs for all model types</li> <li>losses/: Composable loss functions (MSE, adversarial, perceptual)</li> <li>sampling/: MCMC, ancestral, ODE/SDE samplers</li> <li>distributions/: Probability distributions for generative modeling</li> <li>evaluation/: Metrics and benchmarks</li> </ul>"},{"location":"core/architecture/#models-generative_modelsmodels","title":"Models (<code>generative_models/models/</code>)","text":"<p>Each model type has its own subdirectory:</p> <ul> <li>vae/: VAE, Beta-VAE, VQ-VAE, Conditional VAE</li> <li>gan/: DCGAN, WGAN, StyleGAN, PatchGAN</li> <li>diffusion/: DDPM, DDIM, DiT, Score-based</li> <li>flow/: RealNVP, Glow, MAF, IAF, NSF</li> <li>energy/: Energy-based models with MCMC</li> <li>autoregressive/: PixelCNN, WaveNet, Transformers</li> <li>geometric/: Point clouds, meshes, voxels</li> <li>audio/: WaveNet and audio generation</li> <li>backbones/: UNet, ResNet, Transformer blocks</li> </ul>"},{"location":"core/architecture/#modalities-generative_modelsmodalities","title":"Modalities (<code>generative_models/modalities/</code>)","text":"<p>Each modality provides:</p> <ul> <li>datasets.py: Data loading and preprocessing</li> <li>evaluation.py: Modality-specific metrics</li> <li>representations.py: Feature representations</li> </ul> <p>Supported modalities:</p> <ul> <li>Image, Text, Audio</li> <li>Protein, Tabular, Timeseries</li> <li>Geometric (point clouds, meshes)</li> <li>Multimodal</li> </ul>"},{"location":"core/architecture/#training-generative_modelstraining","title":"Training (<code>generative_models/training/</code>)","text":"<ul> <li>Training loops and optimization</li> <li>Logging and checkpointing</li> <li>Learning rate scheduling</li> <li>Multi-GPU coordination</li> </ul>"},{"location":"core/architecture/#factory-generative_modelsfactory","title":"Factory (<code>generative_models/factory/</code>)","text":"<ul> <li>Model creation from configurations</li> <li>Automatic class selection</li> <li>Validation and error handling</li> </ul>"},{"location":"core/architecture/#data-flow","title":"Data Flow","text":""},{"location":"core/architecture/#training-flow","title":"Training Flow","text":"<pre><code>Configuration \u2192 Factory \u2192 Model\n                           \u2193\nData Loader \u2192 Batch \u2192 Model.loss() \u2192 Optimizer \u2192 Updated Parameters\n                           \u2193\n                      Metrics Logger\n</code></pre>"},{"location":"core/architecture/#generation-flow","title":"Generation Flow","text":"<pre><code>Configuration \u2192 Factory \u2192 Model\n                           \u2193\nLatent Space \u2192 Model.generate() \u2192 Samples \u2192 Post-processing\n</code></pre>"},{"location":"core/architecture/#extension-points","title":"Extension Points","text":""},{"location":"core/architecture/#adding-new-models","title":"Adding New Models","text":"<ol> <li>Define configuration in <code>core/configuration/</code></li> <li>Implement model in appropriate <code>models/</code> subdirectory</li> <li>Register in factory</li> <li>Add tests mirroring source structure</li> </ol>"},{"location":"core/architecture/#adding-new-modalities","title":"Adding New Modalities","text":"<ol> <li>Create directory in <code>modalities/</code></li> <li>Implement datasets, evaluation, representations</li> <li>Register in modality factory</li> <li>Add comprehensive tests</li> </ol>"},{"location":"core/architecture/#custom-losses","title":"Custom Losses","text":"<pre><code>from artifex.generative_models.core.losses import CompositeLoss, WeightedLoss\n\ncustom_loss = CompositeLoss([\n    WeightedLoss(mse_loss, weight=1.0, name=\"reconstruction\"),\n    WeightedLoss(custom_fn, weight=0.1, name=\"custom\"),\n])\n</code></pre>"},{"location":"core/architecture/#hardware-management","title":"Hardware Management","text":""},{"location":"core/architecture/#device-manager","title":"Device Manager","text":"<pre><code>from artifex.generative_models.core import DeviceManager\n\ndevice_manager = DeviceManager()\ndevice = device_manager.get_device()  # Auto-selects GPU/CPU\n</code></pre> <p>Handles:</p> <ul> <li>GPU/CPU detection and fallback</li> <li>Memory management</li> <li>Multi-device coordination</li> </ul>"},{"location":"core/architecture/#see-also","title":"See Also","text":"<ul> <li>Design Philosophy - Guiding principles</li> <li>Core Concepts - Getting started guide</li> <li>Testing Guide - Testing practices</li> </ul>"},{"location":"core/base/","title":"Base","text":"<p>Module: <code>generative_models.core.sampling.base</code></p> <p>Source: <code>generative_models/core/sampling/base.py</code></p>"},{"location":"core/base/#overview","title":"Overview","text":"<p>Base classes for sampling algorithms.</p>"},{"location":"core/base/#classes","title":"Classes","text":""},{"location":"core/base/#samplingalgorithm","title":"SamplingAlgorithm","text":"<pre><code>class SamplingAlgorithm\n</code></pre>"},{"location":"core/base/#functions","title":"Functions","text":""},{"location":"core/base/#init","title":"init","text":"<pre><code>def init()\n</code></pre>"},{"location":"core/base/#step","title":"step","text":"<pre><code>def step()\n</code></pre>"},{"location":"core/base/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 2</li> <li>Imports: 3</li> </ul>"},{"location":"core/benchmarks/","title":"Benchmarks","text":"<p>Module: <code>generative_models.core.protocols.benchmarks</code></p> <p>Source: <code>generative_models/core/protocols/benchmarks.py</code></p>"},{"location":"core/benchmarks/#overview","title":"Overview","text":"<p>Benchmark protocol definitions for artifex.generative_models.core.</p>"},{"location":"core/benchmarks/#classes","title":"Classes","text":""},{"location":"core/benchmarks/#benchmarkbase","title":"BenchmarkBase","text":"<pre><code>class BenchmarkBase\n</code></pre>"},{"location":"core/benchmarks/#benchmarkwithvalidation","title":"BenchmarkWithValidation","text":"<pre><code>class BenchmarkWithValidation\n</code></pre>"},{"location":"core/benchmarks/#functions","title":"Functions","text":""},{"location":"core/benchmarks/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/benchmarks/#get_benchmark_info","title":"get_benchmark_info","text":"<pre><code>def get_benchmark_info()\n</code></pre>"},{"location":"core/benchmarks/#get_performance_targets","title":"get_performance_targets","text":"<pre><code>def get_performance_targets()\n</code></pre>"},{"location":"core/benchmarks/#register","title":"register","text":"<pre><code>def register()\n</code></pre>"},{"location":"core/benchmarks/#run_evaluation","title":"run_evaluation","text":"<pre><code>def run_evaluation()\n</code></pre>"},{"location":"core/benchmarks/#run_training","title":"run_training","text":"<pre><code>def run_training()\n</code></pre>"},{"location":"core/benchmarks/#validate_performance","title":"validate_performance","text":"<pre><code>def validate_performance()\n</code></pre>"},{"location":"core/benchmarks/#validate_targets_achieved","title":"validate_targets_achieved","text":"<pre><code>def validate_targets_achieved()\n</code></pre>"},{"location":"core/benchmarks/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 8</li> <li>Imports: 4</li> </ul>"},{"location":"core/blackjax_samplers/","title":"Blackjax Samplers","text":"<p>Module: <code>generative_models.core.sampling.blackjax_samplers</code></p> <p>Source: <code>generative_models/core/sampling/blackjax_samplers.py</code></p>"},{"location":"core/blackjax_samplers/#overview","title":"Overview","text":"<p>BlackJAX integration module.</p> <p>This module provides integration with BlackJAX, a library of samplers for JAX. It allows using BlackJAX's advanced MCMC samplers with our distribution framework.</p>"},{"location":"core/blackjax_samplers/#classes","title":"Classes","text":""},{"location":"core/blackjax_samplers/#blackjaxhmc","title":"BlackJAXHMC","text":"<pre><code>class BlackJAXHMC\n</code></pre>"},{"location":"core/blackjax_samplers/#blackjaxmala","title":"BlackJAXMALA","text":"<pre><code>class BlackJAXMALA\n</code></pre>"},{"location":"core/blackjax_samplers/#blackjaxnuts","title":"BlackJAXNUTS","text":"<pre><code>class BlackJAXNUTS\n</code></pre>"},{"location":"core/blackjax_samplers/#blackjaxsamplerstate","title":"BlackJAXSamplerState","text":"<pre><code>class BlackJAXSamplerState\n</code></pre>"},{"location":"core/blackjax_samplers/#functions","title":"Functions","text":""},{"location":"core/blackjax_samplers/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/blackjax_samplers/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/blackjax_samplers/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/blackjax_samplers/#ensure_scalar_log_prob","title":"ensure_scalar_log_prob","text":"<pre><code>def ensure_scalar_log_prob()\n</code></pre>"},{"location":"core/blackjax_samplers/#hmc_sampling","title":"hmc_sampling","text":"<pre><code>def hmc_sampling()\n</code></pre>"},{"location":"core/blackjax_samplers/#init_3","title":"init","text":"<pre><code>def init()\n</code></pre>"},{"location":"core/blackjax_samplers/#init_4","title":"init","text":"<pre><code>def init()\n</code></pre>"},{"location":"core/blackjax_samplers/#init_5","title":"init","text":"<pre><code>def init()\n</code></pre>"},{"location":"core/blackjax_samplers/#mala_sampling","title":"mala_sampling","text":"<pre><code>def mala_sampling()\n</code></pre>"},{"location":"core/blackjax_samplers/#nuts_sampling","title":"nuts_sampling","text":"<pre><code>def nuts_sampling()\n</code></pre>"},{"location":"core/blackjax_samplers/#scalar_log_prob","title":"scalar_log_prob","text":"<pre><code>def scalar_log_prob()\n</code></pre>"},{"location":"core/blackjax_samplers/#step","title":"step","text":"<pre><code>def step()\n</code></pre>"},{"location":"core/blackjax_samplers/#step_1","title":"step","text":"<pre><code>def step()\n</code></pre>"},{"location":"core/blackjax_samplers/#step_2","title":"step","text":"<pre><code>def step()\n</code></pre>"},{"location":"core/blackjax_samplers/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 14</li> <li>Imports: 7</li> </ul>"},{"location":"core/causal/","title":"Causal","text":"<p>Module: <code>generative_models.core.layers.causal</code></p> <p>Source: <code>generative_models/core/layers/causal.py</code></p>"},{"location":"core/causal/#overview","title":"Overview","text":"<p>Causal utilities for autoregressive models.</p> <p>This module provides utility functions commonly used in autoregressive models, particularly for creating causal masks and shifting sequences.</p>"},{"location":"core/causal/#functions","title":"Functions","text":""},{"location":"core/causal/#apply_causal_mask","title":"apply_causal_mask","text":"<pre><code>def apply_causal_mask()\n</code></pre>"},{"location":"core/causal/#create_attention_mask","title":"create_attention_mask","text":"<pre><code>def create_attention_mask()\n</code></pre>"},{"location":"core/causal/#create_causal_mask","title":"create_causal_mask","text":"<pre><code>def create_causal_mask()\n</code></pre>"},{"location":"core/causal/#shift_right","title":"shift_right","text":"<pre><code>def shift_right()\n</code></pre>"},{"location":"core/causal/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 4</li> <li>Imports: 2</li> </ul>"},{"location":"core/checkpointing/","title":"Checkpointing","text":"<p>Module: <code>generative_models.core.checkpointing</code></p> <p>Source: <code>generative_models/core/checkpointing.py</code></p>"},{"location":"core/checkpointing/#overview","title":"Overview","text":"<p>Checkpointing utilities for saving and loading model state using Orbax. Provides functions for basic model checkpointing, optimizer state checkpointing, checkpoint validation, and corruption recovery.</p>"},{"location":"core/checkpointing/#functions","title":"Functions","text":""},{"location":"core/checkpointing/#setup_checkpoint_manager","title":"setup_checkpoint_manager","text":"<pre><code>def setup_checkpoint_manager(base_dir: str) -&gt; tuple[ocp.CheckpointManager, str]\n</code></pre> <p>Setup Orbax checkpoint manager.</p> <p>Parameters:</p> <ul> <li><code>base_dir</code>: Directory path for checkpoints</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (CheckpointManager, absolute_path)</li> </ul>"},{"location":"core/checkpointing/#save_checkpoint","title":"save_checkpoint","text":"<pre><code>def save_checkpoint(\n    checkpoint_manager: ocp.CheckpointManager,\n    model: nnx.Module,\n    step: int,\n) -&gt; ocp.CheckpointManager\n</code></pre> <p>Save model checkpoint using Orbax.</p> <p>Parameters:</p> <ul> <li><code>checkpoint_manager</code>: The Orbax CheckpointManager instance</li> <li><code>model</code>: The NNX model to save</li> <li><code>step</code>: The step number for this checkpoint</li> </ul> <p>Returns:</p> <ul> <li>The checkpoint manager (for chaining)</li> </ul>"},{"location":"core/checkpointing/#load_checkpoint","title":"load_checkpoint","text":"<pre><code>def load_checkpoint(\n    checkpoint_manager: ocp.CheckpointManager,\n    target_model_template: Optional[nnx.Module | nnx.GraphDef] = None,\n    step: Optional[int] = None,\n) -&gt; tuple[Optional[Any], Optional[int]]\n</code></pre> <p>Load model checkpoint using Orbax.</p> <p>Parameters:</p> <ul> <li><code>checkpoint_manager</code>: The Orbax CheckpointManager instance</li> <li><code>target_model_template</code>: Optional model template for restoration</li> <li><code>step</code>: Specific step to restore (None = latest)</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (restored_model_or_state, step) or (None, None) if not found</li> </ul>"},{"location":"core/checkpointing/#save_checkpoint_with_optimizer","title":"save_checkpoint_with_optimizer","text":"<pre><code>def save_checkpoint_with_optimizer(\n    checkpoint_manager: ocp.CheckpointManager,\n    model: nnx.Module,\n    optimizer: nnx.Optimizer,\n    step: int,\n) -&gt; ocp.CheckpointManager\n</code></pre> <p>Save both model and optimizer state to checkpoint.</p> <p>Parameters:</p> <ul> <li><code>checkpoint_manager</code>: The Orbax CheckpointManager instance</li> <li><code>model</code>: The NNX model to save</li> <li><code>optimizer</code>: The NNX Optimizer to save</li> <li><code>step</code>: The step number for this checkpoint</li> </ul> <p>Returns:</p> <ul> <li>The checkpoint manager (for chaining)</li> </ul>"},{"location":"core/checkpointing/#load_checkpoint_with_optimizer","title":"load_checkpoint_with_optimizer","text":"<pre><code>def load_checkpoint_with_optimizer(\n    checkpoint_manager: ocp.CheckpointManager,\n    model_template: nnx.Module,\n    optimizer_template: nnx.Optimizer,\n    step: Optional[int] = None,\n) -&gt; tuple[Optional[nnx.Module], Optional[nnx.Optimizer], Optional[int]]\n</code></pre> <p>Load both model and optimizer state from checkpoint.</p> <p>Parameters:</p> <ul> <li><code>checkpoint_manager</code>: The Orbax CheckpointManager instance</li> <li><code>model_template</code>: Model with same structure as saved model</li> <li><code>optimizer_template</code>: Optimizer with same structure as saved</li> <li><code>step</code>: Specific step to restore (None = latest)</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (model, optimizer, step) or (None, None, None) if not found</li> </ul>"},{"location":"core/checkpointing/#validate_checkpoint","title":"validate_checkpoint","text":"<pre><code>def validate_checkpoint(\n    checkpoint_manager: ocp.CheckpointManager,\n    model: nnx.Module,\n    step: int,\n    validation_data: Any,\n    tolerance: float = 1e-5,\n) -&gt; bool\n</code></pre> <p>Validate that a checkpoint loads correctly and produces consistent outputs.</p> <p>Parameters:</p> <ul> <li><code>checkpoint_manager</code>: The Orbax CheckpointManager instance</li> <li><code>model</code>: The current model whose state was saved</li> <li><code>step</code>: The step number to validate</li> <li><code>validation_data</code>: Input data to test model outputs</li> <li><code>tolerance</code>: Maximum allowed difference between outputs</li> </ul> <p>Returns:</p> <ul> <li>True if checkpoint is valid, False otherwise</li> </ul>"},{"location":"core/checkpointing/#recover_from_corruption","title":"recover_from_corruption","text":"<pre><code>def recover_from_corruption(\n    checkpoint_dir: str,\n    model_template: nnx.Module,\n) -&gt; tuple[Optional[nnx.Module], Optional[int]]\n</code></pre> <p>Attempt to recover from corrupted checkpoints.</p> <p>Tries loading checkpoints from newest to oldest until one succeeds.</p> <p>Parameters:</p> <ul> <li><code>checkpoint_dir</code>: Directory containing checkpoints</li> <li><code>model_template</code>: Model with same structure as saved model</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (recovered_model, step) or (None, None) if recovery failed</li> </ul>"},{"location":"core/checkpointing/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 7</li> <li>Imports: 6</li> </ul>"},{"location":"core/composable/","title":"Composable","text":"<p>Module: <code>generative_models.core.losses.composable</code></p> <p>Source: <code>generative_models/core/losses/composable.py</code></p>"},{"location":"core/composable/#overview","title":"Overview","text":"<p>Composable loss function framework using Flax NNX.</p>"},{"location":"core/composable/#classes","title":"Classes","text":""},{"location":"core/composable/#compositeloss","title":"CompositeLoss","text":"<pre><code>class CompositeLoss\n</code></pre>"},{"location":"core/composable/#loss","title":"Loss","text":"<pre><code>class Loss\n</code></pre>"},{"location":"core/composable/#scheduledloss","title":"ScheduledLoss","text":"<pre><code>class ScheduledLoss\n</code></pre>"},{"location":"core/composable/#weightedloss","title":"WeightedLoss","text":"<pre><code>class WeightedLoss\n</code></pre>"},{"location":"core/composable/#functions","title":"Functions","text":""},{"location":"core/composable/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/composable/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/composable/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/composable/#call_3","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/composable/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/composable/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/composable/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/composable/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/composable/#create_loss_suite","title":"create_loss_suite","text":"<pre><code>def create_loss_suite()\n</code></pre>"},{"location":"core/composable/#create_weighted_loss","title":"create_weighted_loss","text":"<pre><code>def create_weighted_loss()\n</code></pre>"},{"location":"core/composable/#weighted_loss_fn","title":"weighted_loss_fn","text":"<pre><code>def weighted_loss_fn()\n</code></pre>"},{"location":"core/composable/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 11</li> <li>Imports: 4</li> </ul>"},{"location":"core/config_commands/","title":"Config Commands","text":"<p>Module: <code>generative_models.core.cli.config_commands</code></p> <p>Source: <code>generative_models/core/cli/config_commands.py</code></p>"},{"location":"core/config_commands/#overview","title":"Overview","text":"<p>Reusable CLI functions for configuration management.</p>"},{"location":"core/config_commands/#functions","title":"Functions","text":""},{"location":"core/config_commands/#apply_overrides","title":"apply_overrides","text":"<pre><code>def apply_overrides()\n</code></pre>"},{"location":"core/config_commands/#create_config","title":"create_config","text":"<pre><code>def create_config()\n</code></pre>"},{"location":"core/config_commands/#diff_config","title":"diff_config","text":"<pre><code>def diff_config()\n</code></pre>"},{"location":"core/config_commands/#get_config","title":"get_config","text":"<pre><code>def get_config()\n</code></pre>"},{"location":"core/config_commands/#get_config_registry","title":"get_config_registry","text":"<pre><code>def get_config_registry()\n</code></pre>"},{"location":"core/config_commands/#list_configs","title":"list_configs","text":"<pre><code>def list_configs()\n</code></pre>"},{"location":"core/config_commands/#parse_override","title":"parse_override","text":"<pre><code>def parse_override()\n</code></pre>"},{"location":"core/config_commands/#show_config","title":"show_config","text":"<pre><code>def show_config()\n</code></pre>"},{"location":"core/config_commands/#validate_config_file","title":"validate_config_file","text":"<pre><code>def validate_config_file()\n</code></pre>"},{"location":"core/config_commands/#version_config","title":"version_config","text":"<pre><code>def version_config()\n</code></pre>"},{"location":"core/config_commands/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 10</li> <li>Imports: 7</li> </ul>"},{"location":"core/configuration/","title":"Configuration","text":"<p>Module: <code>generative_models.core.protocols.configuration</code></p> <p>Source: <code>generative_models/core/protocols/configuration.py</code></p>"},{"location":"core/configuration/#overview","title":"Overview","text":"<p>Configuration protocol definitions for artifex.generative_models.core.</p>"},{"location":"core/configuration/#classes","title":"Classes","text":""},{"location":"core/configuration/#baseconfig","title":"BaseConfig","text":"<pre><code>class BaseConfig\n</code></pre>"},{"location":"core/configuration/#basemodalityconfig","title":"BaseModalityConfig","text":"<pre><code>class BaseModalityConfig\n</code></pre>"},{"location":"core/configuration/#configtemplate","title":"ConfigTemplate","text":"<pre><code>class ConfigTemplate\n</code></pre>"},{"location":"core/configuration/#functions","title":"Functions","text":""},{"location":"core/configuration/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/configuration/#from_dict","title":"from_dict","text":"<pre><code>def from_dict()\n</code></pre>"},{"location":"core/configuration/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"core/configuration/#to_dict","title":"to_dict","text":"<pre><code>def to_dict()\n</code></pre>"},{"location":"core/configuration/#update","title":"update","text":"<pre><code>def update()\n</code></pre>"},{"location":"core/configuration/#validate","title":"validate","text":"<pre><code>def validate()\n</code></pre>"},{"location":"core/configuration/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 6</li> <li>Imports: 3</li> </ul>"},{"location":"core/continuous/","title":"Continuous","text":"<p>Module: <code>generative_models.core.distributions.continuous</code></p> <p>Source: <code>generative_models/core/distributions/continuous.py</code></p>"},{"location":"core/continuous/#classes","title":"Classes","text":""},{"location":"core/continuous/#beta","title":"Beta","text":"<pre><code>class Beta\n</code></pre>"},{"location":"core/continuous/#normal","title":"Normal","text":"<pre><code>class Normal\n</code></pre>"},{"location":"core/continuous/#functions","title":"Functions","text":""},{"location":"core/continuous/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/continuous/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/continuous/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/continuous/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/continuous/#entropy","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/continuous/#kl_divergence","title":"kl_divergence","text":"<pre><code>def kl_divergence()\n</code></pre>"},{"location":"core/continuous/#kl_divergence_1","title":"kl_divergence","text":"<pre><code>def kl_divergence()\n</code></pre>"},{"location":"core/continuous/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/continuous/#log_prob_1","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/continuous/#mean","title":"mean","text":"<pre><code>def mean()\n</code></pre>"},{"location":"core/continuous/#mode","title":"mode","text":"<pre><code>def mode()\n</code></pre>"},{"location":"core/continuous/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/continuous/#sample_1","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/continuous/#variance","title":"variance","text":"<pre><code>def variance()\n</code></pre>"},{"location":"core/continuous/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 14</li> <li>Imports: 5</li> </ul>"},{"location":"core/device_manager/","title":"Device Manager","text":"<p>Module: <code>generative_models.core.device_manager</code></p> <p>Source: <code>generative_models/core/device_manager.py</code></p>"},{"location":"core/device_manager/#overview","title":"Overview","text":"<p>Foundation-first device management system for Artifex.</p> <p>This module provides a comprehensive, type-safe device management architecture that prioritizes clean design and robust error handling over backward compatibility.</p>"},{"location":"core/device_manager/#classes","title":"Classes","text":""},{"location":"core/device_manager/#cudadetector","title":"CUDADetector","text":"<pre><code>class CUDADetector\n</code></pre>"},{"location":"core/device_manager/#devicecapabilities","title":"DeviceCapabilities","text":"<pre><code>class DeviceCapabilities\n</code></pre>"},{"location":"core/device_manager/#deviceconfiguration","title":"DeviceConfiguration","text":"<pre><code>class DeviceConfiguration\n</code></pre>"},{"location":"core/device_manager/#devicedetector","title":"DeviceDetector","text":"<pre><code>class DeviceDetector\n</code></pre>"},{"location":"core/device_manager/#devicemanager","title":"DeviceManager","text":"<pre><code>class DeviceManager\n</code></pre>"},{"location":"core/device_manager/#devicetype","title":"DeviceType","text":"<pre><code>class DeviceType\n</code></pre>"},{"location":"core/device_manager/#jaxdevicemanager","title":"JAXDeviceManager","text":"<pre><code>class JAXDeviceManager\n</code></pre>"},{"location":"core/device_manager/#memorystrategy","title":"MemoryStrategy","text":"<pre><code>class MemoryStrategy\n</code></pre>"},{"location":"core/device_manager/#functions","title":"Functions","text":""},{"location":"core/device_manager/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_manager/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_manager/#configure_for_generative_models","title":"configure_for_generative_models","text":"<pre><code>def configure_for_generative_models()\n</code></pre>"},{"location":"core/device_manager/#cpu_devices","title":"cpu_devices","text":"<pre><code>def cpu_devices()\n</code></pre>"},{"location":"core/device_manager/#detect_capabilities","title":"detect_capabilities","text":"<pre><code>def detect_capabilities()\n</code></pre>"},{"location":"core/device_manager/#detect_capabilities_1","title":"detect_capabilities","text":"<pre><code>def detect_capabilities()\n</code></pre>"},{"location":"core/device_manager/#device_count","title":"device_count","text":"<pre><code>def device_count()\n</code></pre>"},{"location":"core/device_manager/#devices","title":"devices","text":"<pre><code>def devices()\n</code></pre>"},{"location":"core/device_manager/#distribute_data","title":"distribute_data","text":"<pre><code>def distribute_data()\n</code></pre>"},{"location":"core/device_manager/#get_default_device","title":"get_default_device","text":"<pre><code>def get_default_device()\n</code></pre>"},{"location":"core/device_manager/#get_default_device_1","title":"get_default_device","text":"<pre><code>def get_default_device()\n</code></pre>"},{"location":"core/device_manager/#get_device_info","title":"get_device_info","text":"<pre><code>def get_device_info()\n</code></pre>"},{"location":"core/device_manager/#get_device_manager","title":"get_device_manager","text":"<pre><code>def get_device_manager()\n</code></pre>"},{"location":"core/device_manager/#gpu_count","title":"gpu_count","text":"<pre><code>def gpu_count()\n</code></pre>"},{"location":"core/device_manager/#gpu_devices","title":"gpu_devices","text":"<pre><code>def gpu_devices()\n</code></pre>"},{"location":"core/device_manager/#has_gpu","title":"has_gpu","text":"<pre><code>def has_gpu()\n</code></pre>"},{"location":"core/device_manager/#has_gpu_1","title":"has_gpu","text":"<pre><code>def has_gpu()\n</code></pre>"},{"location":"core/device_manager/#optimize_for_model_size","title":"optimize_for_model_size","text":"<pre><code>def optimize_for_model_size()\n</code></pre>"},{"location":"core/device_manager/#print_device_info","title":"print_device_info","text":"<pre><code>def print_device_info()\n</code></pre>"},{"location":"core/device_manager/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 8</li> <li>Functions: 19</li> <li>Imports: 8</li> </ul>"},{"location":"core/device_testing/","title":"Device Testing","text":"<p>Module: <code>generative_models.core.device_testing</code></p> <p>Source: <code>generative_models/core/device_testing.py</code></p>"},{"location":"core/device_testing/#overview","title":"Overview","text":"<p>Foundation-first device testing framework for Artifex.</p> <p>This module provides comprehensive testing capabilities for device management, following test-driven development principles and prioritizing strong design.</p>"},{"location":"core/device_testing/#classes","title":"Classes","text":""},{"location":"core/device_testing/#basiccomputationtest","title":"BasicComputationTest","text":"<pre><code>class BasicComputationTest\n</code></pre>"},{"location":"core/device_testing/#devicetest","title":"DeviceTest","text":"<pre><code>class DeviceTest\n</code></pre>"},{"location":"core/device_testing/#devicetestrunner","title":"DeviceTestRunner","text":"<pre><code>class DeviceTestRunner\n</code></pre>"},{"location":"core/device_testing/#generativemodeltest","title":"GenerativeModelTest","text":"<pre><code>class GenerativeModelTest\n</code></pre>"},{"location":"core/device_testing/#matrixmultiplicationtest","title":"MatrixMultiplicationTest","text":"<pre><code>class MatrixMultiplicationTest\n</code></pre>"},{"location":"core/device_testing/#memorystresstest","title":"MemoryStressTest","text":"<pre><code>class MemoryStressTest\n</code></pre>"},{"location":"core/device_testing/#neuralnetworktest","title":"NeuralNetworkTest","text":"<pre><code>class NeuralNetworkTest\n</code></pre>"},{"location":"core/device_testing/#testmlp","title":"TestMLP","text":"<pre><code>class TestMLP\n</code></pre>"},{"location":"core/device_testing/#testresult","title":"TestResult","text":"<pre><code>class TestResult\n</code></pre>"},{"location":"core/device_testing/#testseverity","title":"TestSeverity","text":"<pre><code>class TestSeverity\n</code></pre>"},{"location":"core/device_testing/#testsuite","title":"TestSuite","text":"<pre><code>class TestSuite\n</code></pre>"},{"location":"core/device_testing/#functions","title":"Functions","text":""},{"location":"core/device_testing/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/device_testing/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_testing/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_testing/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_testing/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_testing/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_testing/#init_5","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_testing/#init_6","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_testing/#init_7","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/device_testing/#add_noise","title":"add_noise","text":"<pre><code>def add_noise()\n</code></pre>"},{"location":"core/device_testing/#add_test","title":"add_test","text":"<pre><code>def add_test()\n</code></pre>"},{"location":"core/device_testing/#attention_like_op","title":"attention_like_op","text":"<pre><code>def attention_like_op()\n</code></pre>"},{"location":"core/device_testing/#critical_failures","title":"critical_failures","text":"<pre><code>def critical_failures()\n</code></pre>"},{"location":"core/device_testing/#failed_tests","title":"failed_tests","text":"<pre><code>def failed_tests()\n</code></pre>"},{"location":"core/device_testing/#is_healthy","title":"is_healthy","text":"<pre><code>def is_healthy()\n</code></pre>"},{"location":"core/device_testing/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"core/device_testing/#passed_tests","title":"passed_tests","text":"<pre><code>def passed_tests()\n</code></pre>"},{"location":"core/device_testing/#print_test_results","title":"print_test_results","text":"<pre><code>def print_test_results()\n</code></pre>"},{"location":"core/device_testing/#run","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"core/device_testing/#run_1","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"core/device_testing/#run_2","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"core/device_testing/#run_3","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"core/device_testing/#run_4","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"core/device_testing/#run_5","title":"run","text":"<pre><code>def run()\n</code></pre>"},{"location":"core/device_testing/#run_all_tests","title":"run_all_tests","text":"<pre><code>def run_all_tests()\n</code></pre>"},{"location":"core/device_testing/#run_critical_tests_only","title":"run_critical_tests_only","text":"<pre><code>def run_critical_tests_only()\n</code></pre>"},{"location":"core/device_testing/#run_device_tests","title":"run_device_tests","text":"<pre><code>def run_device_tests()\n</code></pre>"},{"location":"core/device_testing/#severity_icon","title":"severity_icon","text":"<pre><code>def severity_icon()\n</code></pre>"},{"location":"core/device_testing/#status_icon","title":"status_icon","text":"<pre><code>def status_icon()\n</code></pre>"},{"location":"core/device_testing/#success_rate","title":"success_rate","text":"<pre><code>def success_rate()\n</code></pre>"},{"location":"core/device_testing/#test_func","title":"test_func","text":"<pre><code>def test_func()\n</code></pre>"},{"location":"core/device_testing/#test_func_1","title":"test_func","text":"<pre><code>def test_func()\n</code></pre>"},{"location":"core/device_testing/#test_func_2","title":"test_func","text":"<pre><code>def test_func()\n</code></pre>"},{"location":"core/device_testing/#test_func_3","title":"test_func","text":"<pre><code>def test_func()\n</code></pre>"},{"location":"core/device_testing/#test_func_4","title":"test_func","text":"<pre><code>def test_func()\n</code></pre>"},{"location":"core/device_testing/#total_tests","title":"total_tests","text":"<pre><code>def total_tests()\n</code></pre>"},{"location":"core/device_testing/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 11</li> <li>Functions: 36</li> <li>Imports: 11</li> </ul>"},{"location":"core/diffusion/","title":"Diffusion","text":"<p>Module: <code>generative_models.core.sampling.diffusion</code></p> <p>Source: <code>generative_models/core/sampling/diffusion.py</code></p>"},{"location":"core/diffusion/#overview","title":"Overview","text":"<p>Diffusion-based sampling algorithms.</p>"},{"location":"core/diffusion/#classes","title":"Classes","text":""},{"location":"core/diffusion/#diffusionsampler","title":"DiffusionSampler","text":"<pre><code>class DiffusionSampler\n</code></pre>"},{"location":"core/diffusion/#functions","title":"Functions","text":""},{"location":"core/diffusion/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/diffusion/#init_1","title":"init","text":"<pre><code>def init()\n</code></pre>"},{"location":"core/diffusion/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/diffusion/#step","title":"step","text":"<pre><code>def step()\n</code></pre>"},{"location":"core/diffusion/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 4</li> <li>Imports: 4</li> </ul>"},{"location":"core/discrete/","title":"Discrete","text":"<p>Module: <code>generative_models.core.distributions.discrete</code></p> <p>Source: <code>generative_models/core/distributions/discrete.py</code></p>"},{"location":"core/discrete/#classes","title":"Classes","text":""},{"location":"core/discrete/#bernoulli","title":"Bernoulli","text":"<pre><code>class Bernoulli\n</code></pre>"},{"location":"core/discrete/#categorical","title":"Categorical","text":"<pre><code>class Categorical\n</code></pre>"},{"location":"core/discrete/#onehotcategorical","title":"OneHotCategorical","text":"<pre><code>class OneHotCategorical\n</code></pre>"},{"location":"core/discrete/#functions","title":"Functions","text":""},{"location":"core/discrete/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/discrete/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/discrete/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/discrete/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/discrete/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/discrete/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/discrete/#entropy","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/discrete/#entropy_1","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/discrete/#entropy_2","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/discrete/#kl_divergence","title":"kl_divergence","text":"<pre><code>def kl_divergence()\n</code></pre>"},{"location":"core/discrete/#kl_divergence_1","title":"kl_divergence","text":"<pre><code>def kl_divergence()\n</code></pre>"},{"location":"core/discrete/#kl_divergence_2","title":"kl_divergence","text":"<pre><code>def kl_divergence()\n</code></pre>"},{"location":"core/discrete/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/discrete/#log_prob_1","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/discrete/#log_prob_2","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/discrete/#mode","title":"mode","text":"<pre><code>def mode()\n</code></pre>"},{"location":"core/discrete/#mode_1","title":"mode","text":"<pre><code>def mode()\n</code></pre>"},{"location":"core/discrete/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/discrete/#sample_1","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/discrete/#sample_2","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/discrete/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 20</li> <li>Imports: 5</li> </ul>"},{"location":"core/distance/","title":"Distance","text":"<p>Module: <code>generative_models.core.evaluation.metrics.distance</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/distance.py</code></p>"},{"location":"core/distance/#overview","title":"Overview","text":"<p>Distance-based metrics for evaluation across modalities.</p>"},{"location":"core/distance/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 2</li> </ul>"},{"location":"core/divergence/","title":"Divergence","text":"<p>Module: <code>generative_models.core.losses.divergence</code></p> <p>Source: <code>generative_models/core/losses/divergence.py</code></p>"},{"location":"core/divergence/#overview","title":"Overview","text":"<p>Divergence losses module.</p> <p>This module provides loss functions based on divergence measures between probability distributions, commonly used in generative models like VAEs and GANs.</p>"},{"location":"core/divergence/#functions","title":"Functions","text":""},{"location":"core/divergence/#compute_kernel_matrix","title":"compute_kernel_matrix","text":"<pre><code>def compute_kernel_matrix()\n</code></pre>"},{"location":"core/divergence/#energy_distance","title":"energy_distance","text":"<pre><code>def energy_distance()\n</code></pre>"},{"location":"core/divergence/#js_divergence","title":"js_divergence","text":"<pre><code>def js_divergence()\n</code></pre>"},{"location":"core/divergence/#kl_divergence","title":"kl_divergence","text":"<pre><code>def kl_divergence()\n</code></pre>"},{"location":"core/divergence/#maximum_mean_discrepancy","title":"maximum_mean_discrepancy","text":"<pre><code>def maximum_mean_discrepancy()\n</code></pre>"},{"location":"core/divergence/#pairwise_distance","title":"pairwise_distance","text":"<pre><code>def pairwise_distance()\n</code></pre>"},{"location":"core/divergence/#reverse_kl_divergence","title":"reverse_kl_divergence","text":"<pre><code>def reverse_kl_divergence()\n</code></pre>"},{"location":"core/divergence/#wasserstein_distance","title":"wasserstein_distance","text":"<pre><code>def wasserstein_distance()\n</code></pre>"},{"location":"core/divergence/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 8</li> <li>Imports: 4</li> </ul>"},{"location":"core/environment/","title":"Environment","text":"<p>Module: <code>generative_models.core.configuration.environment</code></p> <p>Source: <code>generative_models/core/configuration/environment.py</code></p>"},{"location":"core/environment/#overview","title":"Overview","text":"<p>Environment-specific configuration utilities for artifex.generative_models.core.</p>"},{"location":"core/environment/#functions","title":"Functions","text":""},{"location":"core/environment/#apply_env_overrides","title":"apply_env_overrides","text":"<pre><code>def apply_env_overrides()\n</code></pre>"},{"location":"core/environment/#deep_update","title":"deep_update","text":"<pre><code>def deep_update()\n</code></pre>"},{"location":"core/environment/#detect_environment","title":"detect_environment","text":"<pre><code>def detect_environment()\n</code></pre>"},{"location":"core/environment/#get_env_config_path","title":"get_env_config_path","text":"<pre><code>def get_env_config_path()\n</code></pre>"},{"location":"core/environment/#get_env_name","title":"get_env_name","text":"<pre><code>def get_env_name()\n</code></pre>"},{"location":"core/environment/#load_env_config","title":"load_env_config","text":"<pre><code>def load_env_config()\n</code></pre>"},{"location":"core/environment/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 6</li> <li>Imports: 4</li> </ul>"},{"location":"core/evaluation/","title":"Evaluation","text":"<p>Module: <code>generative_models.core.protocols.evaluation</code></p> <p>Source: <code>generative_models/core/protocols/evaluation.py</code></p>"},{"location":"core/evaluation/#overview","title":"Overview","text":"<p>Protocols for evaluation of generative models.</p>"},{"location":"core/evaluation/#classes","title":"Classes","text":""},{"location":"core/evaluation/#batchabledatasetprotocol","title":"BatchableDatasetProtocol","text":"<pre><code>class BatchableDatasetProtocol\n</code></pre>"},{"location":"core/evaluation/#datasetprotocol","title":"DatasetProtocol","text":"<pre><code>class DatasetProtocol\n</code></pre>"},{"location":"core/evaluation/#modelprotocol","title":"ModelProtocol","text":"<pre><code>class ModelProtocol\n</code></pre>"},{"location":"core/evaluation/#functions","title":"Functions","text":""},{"location":"core/evaluation/#get_batch","title":"get_batch","text":"<pre><code>def get_batch()\n</code></pre>"},{"location":"core/evaluation/#model_name","title":"model_name","text":"<pre><code>def model_name()\n</code></pre>"},{"location":"core/evaluation/#predict","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"core/evaluation/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/evaluation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 4</li> <li>Imports: 3</li> </ul>"},{"location":"core/fid/","title":"Fid","text":"<p>Module: <code>generative_models.core.evaluation.metrics.image.fid</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/image/fid.py</code></p>"},{"location":"core/fid/#overview","title":"Overview","text":"<p>Fr\u00e9chet Inception Distance (FID) implementation using JAX and NNX.</p> <p>FID is a metric that calculates the distance between feature vectors of real and generated images. It uses the Inception-v3 network to extract features and then computes the Fr\u00e9chet distance between distributions.</p>"},{"location":"core/fid/#classes","title":"Classes","text":""},{"location":"core/fid/#frechetinceptiondistance","title":"FrechetInceptionDistance","text":"<pre><code>class FrechetInceptionDistance\n</code></pre>"},{"location":"core/fid/#functions","title":"Functions","text":""},{"location":"core/fid/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/fid/#calculate_frechet_distance","title":"calculate_frechet_distance","text":"<pre><code>def calculate_frechet_distance()\n</code></pre>"},{"location":"core/fid/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"core/fid/#feature_extractor","title":"feature_extractor","text":"<pre><code>def feature_extractor()\n</code></pre>"},{"location":"core/fid/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 4</li> <li>Imports: 6</li> </ul>"},{"location":"core/flash_attention/","title":"Flash Attention","text":"<p>Module: <code>generative_models.core.layers.flash_attention</code></p> <p>Source: <code>generative_models/core/layers/flash_attention.py</code></p>"},{"location":"core/flash_attention/#overview","title":"Overview","text":"<p>Flash Attention implementation for Flax NNX with kvax optimizations.</p> <p>This module provides a Flash Attention implementation designed to serve as a drop-in replacement for Flax NNX's MultiHeadAttention with performance improvements and additional features.</p> <p>Based on:</p> <ul> <li>Flash Attention paper: https://arxiv.org/abs/2205.14135</li> <li>Flash Attention 2: https://arxiv.org/abs/2307.08691</li> <li>kvax implementation: https://github.com/nebius/kvax</li> </ul>"},{"location":"core/flash_attention/#classes","title":"Classes","text":""},{"location":"core/flash_attention/#attentionbackend","title":"AttentionBackend","text":"<pre><code>class AttentionBackend\n</code></pre>"},{"location":"core/flash_attention/#attentionmask","title":"AttentionMask","text":"<pre><code>class AttentionMask\n</code></pre>"},{"location":"core/flash_attention/#flashattentionconfig","title":"FlashAttentionConfig","text":"<pre><code>class FlashAttentionConfig\n</code></pre>"},{"location":"core/flash_attention/#flashmultiheadattention","title":"FlashMultiHeadAttention","text":"<pre><code>class FlashMultiHeadAttention\n</code></pre>"},{"location":"core/flash_attention/#functions","title":"Functions","text":""},{"location":"core/flash_attention/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/flash_attention/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/flash_attention/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/flash_attention/#create_attention_mask","title":"create_attention_mask","text":"<pre><code>def create_attention_mask()\n</code></pre>"},{"location":"core/flash_attention/#flash_attention_forward_kernel","title":"flash_attention_forward_kernel","text":"<pre><code>def flash_attention_forward_kernel()\n</code></pre>"},{"location":"core/flash_attention/#flash_attention_triton","title":"flash_attention_triton","text":"<pre><code>def flash_attention_triton()\n</code></pre>"},{"location":"core/flash_attention/#init_cache","title":"init_cache","text":"<pre><code>def init_cache()\n</code></pre>"},{"location":"core/flash_attention/#make_causal_mask","title":"make_causal_mask","text":"<pre><code>def make_causal_mask()\n</code></pre>"},{"location":"core/flash_attention/#make_segment_mask","title":"make_segment_mask","text":"<pre><code>def make_segment_mask()\n</code></pre>"},{"location":"core/flash_attention/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 9</li> <li>Imports: 20</li> </ul>"},{"location":"core/gan/","title":"Gan","text":"<p>Module: <code>generative_models.core.configuration.gan</code></p> <p>Source: <code>generative_models/core/configuration/gan.py</code></p>"},{"location":"core/gan/#overview","title":"Overview","text":"<p>GAN-specific configurations for the unified configuration system.</p> <p>This module provides configuration classes for various GAN architectures that extend the base ModelConfig.</p>"},{"location":"core/gan/#classes","title":"Classes","text":""},{"location":"core/gan/#conditionalganconfiguration","title":"ConditionalGANConfiguration","text":"<pre><code>class ConditionalGANConfiguration\n</code></pre>"},{"location":"core/gan/#cycleganconfiguration","title":"CycleGANConfiguration","text":"<pre><code>class CycleGANConfiguration\n</code></pre>"},{"location":"core/gan/#dcganconfiguration","title":"DCGANConfiguration","text":"<pre><code>class DCGANConfiguration\n</code></pre>"},{"location":"core/gan/#ganconfiguration","title":"GANConfiguration","text":"<pre><code>class GANConfiguration\n</code></pre>"},{"location":"core/gan/#lsganconfiguration","title":"LSGANConfiguration","text":"<pre><code>class LSGANConfiguration\n</code></pre>"},{"location":"core/gan/#wganconfiguration","title":"WGANConfiguration","text":"<pre><code>class WGANConfiguration\n</code></pre>"},{"location":"core/gan/#functions","title":"Functions","text":""},{"location":"core/gan/#create_gan_config_from_model_config","title":"create_gan_config_from_model_config","text":"<pre><code>def create_gan_config_from_model_config()\n</code></pre>"},{"location":"core/gan/#image_shape","title":"image_shape","text":"<pre><code>def image_shape()\n</code></pre>"},{"location":"core/gan/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 6</li> <li>Functions: 2</li> <li>Imports: 2</li> </ul>"},{"location":"core/geometric/","title":"Geometric","text":"<p>Module: <code>generative_models.core.losses.geometric</code></p> <p>Source: <code>generative_models/core/losses/geometric.py</code></p>"},{"location":"core/geometric/#overview","title":"Overview","text":"<p>Loss functions for geometric generative models.</p>"},{"location":"core/geometric/#classes","title":"Classes","text":""},{"location":"core/geometric/#meshloss","title":"MeshLoss","text":"<pre><code>class MeshLoss\n</code></pre>"},{"location":"core/geometric/#functions","title":"Functions","text":""},{"location":"core/geometric/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/geometric/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/geometric/#binary_cross_entropy","title":"binary_cross_entropy","text":"<pre><code>def binary_cross_entropy()\n</code></pre>"},{"location":"core/geometric/#chamfer_distance","title":"chamfer_distance","text":"<pre><code>def chamfer_distance()\n</code></pre>"},{"location":"core/geometric/#dice_loss","title":"dice_loss","text":"<pre><code>def dice_loss()\n</code></pre>"},{"location":"core/geometric/#earth_mover_distance","title":"earth_mover_distance","text":"<pre><code>def earth_mover_distance()\n</code></pre>"},{"location":"core/geometric/#focal_loss","title":"focal_loss","text":"<pre><code>def focal_loss()\n</code></pre>"},{"location":"core/geometric/#get_mesh_loss","title":"get_mesh_loss","text":"<pre><code>def get_mesh_loss()\n</code></pre>"},{"location":"core/geometric/#get_point_cloud_loss","title":"get_point_cloud_loss","text":"<pre><code>def get_point_cloud_loss()\n</code></pre>"},{"location":"core/geometric/#get_voxel_loss","title":"get_voxel_loss","text":"<pre><code>def get_voxel_loss()\n</code></pre>"},{"location":"core/geometric/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 10</li> <li>Imports: 4</li> </ul>"},{"location":"core/inception_score/","title":"Inception Score","text":"<p>Module: <code>generative_models.core.evaluation.metrics.image.inception_score</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/image/inception_score.py</code></p>"},{"location":"core/inception_score/#overview","title":"Overview","text":"<p>Inception Score implementation using JAX and NNX.</p> <p>The Inception Score (IS) is a metric for evaluating generative models, particularly GANs, based on two criteria:</p> <ol> <li>Quality: Generated images should contain clear, recognizable objects</li> <li>Diversity: The model should generate diverse images across classes</li> </ol>"},{"location":"core/inception_score/#classes","title":"Classes","text":""},{"location":"core/inception_score/#inceptionscore","title":"InceptionScore","text":"<pre><code>class InceptionScore\n</code></pre>"},{"location":"core/inception_score/#functions","title":"Functions","text":""},{"location":"core/inception_score/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/inception_score/#calculate_kl_divergence","title":"calculate_kl_divergence","text":"<pre><code>def calculate_kl_divergence()\n</code></pre>"},{"location":"core/inception_score/#calculate_score","title":"calculate_score","text":"<pre><code>def calculate_score()\n</code></pre>"},{"location":"core/inception_score/#classifier","title":"classifier","text":"<pre><code>def classifier()\n</code></pre>"},{"location":"core/inception_score/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"core/inception_score/#get_predictions","title":"get_predictions","text":"<pre><code>def get_predictions()\n</code></pre>"},{"location":"core/inception_score/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 6</li> <li>Imports: 5</li> </ul>"},{"location":"core/information/","title":"Information","text":"<p>Module: <code>generative_models.core.evaluation.metrics.information</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/information.py</code></p>"},{"location":"core/information/#overview","title":"Overview","text":"<p>Information-theoretic metrics for evaluation across modalities.</p>"},{"location":"core/information/#functions","title":"Functions","text":""},{"location":"core/information/#compute_average_log_probability","title":"compute_average_log_probability","text":"<pre><code>def compute_average_log_probability()\n</code></pre>"},{"location":"core/information/#compute_perplexity","title":"compute_perplexity","text":"<pre><code>def compute_perplexity()\n</code></pre>"},{"location":"core/information/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 2</li> <li>Imports: 1</li> </ul>"},{"location":"core/interfaces/","title":"Interfaces","text":"<p>Module: <code>generative_models.core.interfaces</code></p> <p>Source: <code>generative_models/core/interfaces.py</code></p>"},{"location":"core/interfaces/#overview","title":"Overview","text":"<p>Interfaces module.</p> <p>This module provides common interfaces and abstract classes that are used across the generative models package. It helps avoid circular imports between modules.</p>"},{"location":"core/interfaces/#classes","title":"Classes","text":""},{"location":"core/interfaces/#distribution","title":"Distribution","text":"<pre><code>class Distribution\n</code></pre>"},{"location":"core/interfaces/#functions","title":"Functions","text":""},{"location":"core/interfaces/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/interfaces/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/interfaces/#chunked_vmap","title":"chunked_vmap","text":"<pre><code>def chunked_vmap()\n</code></pre>"},{"location":"core/interfaces/#entropy","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/interfaces/#kl_divergence","title":"kl_divergence","text":"<pre><code>def kl_divergence()\n</code></pre>"},{"location":"core/interfaces/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/interfaces/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/interfaces/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 7</li> <li>Imports: 3</li> </ul>"},{"location":"core/jax_config/","title":"Jax Config","text":"<p>Module: <code>generative_models.core.jax_config</code></p> <p>Source: <code>generative_models/core/jax_config.py</code></p>"},{"location":"core/jax_config/#overview","title":"Overview","text":"<p>JAX configuration optimization for maximum performance.</p>"},{"location":"core/jax_config/#functions","title":"Functions","text":""},{"location":"core/jax_config/#configure_jax","title":"configure_jax","text":"<pre><code>def configure_jax()\n</code></pre>"},{"location":"core/jax_config/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 1</li> <li>Imports: 5</li> </ul>"},{"location":"core/logging/","title":"Logging","text":"<p>Module: <code>generative_models.core.logging</code></p> <p>Source: <code>generative_models/core/logging.py</code></p>"},{"location":"core/logging/#overview","title":"Overview","text":"<p>Logging utilities.</p> <p>This module re-exports the logging functionality from the utils/logging package for better organization and easier imports.</p>"},{"location":"core/logging/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 1</li> </ul>"},{"location":"core/mcmc/","title":"Mcmc","text":"<p>Module: <code>generative_models.core.sampling.mcmc</code></p> <p>Source: <code>generative_models/core/sampling/mcmc.py</code></p>"},{"location":"core/mcmc/#overview","title":"Overview","text":"<p>MCMC sampling utilities.</p> <p>This module provides utilities for Markov Chain Monte Carlo (MCMC) sampling. Specifically, we implement a basic Metropolis-Hastings algorithm for sampling from distributions defined by their (unnormalized) log probability functions.</p>"},{"location":"core/mcmc/#functions","title":"Functions","text":""},{"location":"core/mcmc/#mcmc_sampling","title":"mcmc_sampling","text":"<pre><code>def mcmc_sampling()\n</code></pre>"},{"location":"core/mcmc/#metropolis_step","title":"metropolis_step","text":"<pre><code>def metropolis_step()\n</code></pre>"},{"location":"core/mcmc/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 2</li> <li>Imports: 5</li> </ul>"},{"location":"core/metric_ops/","title":"Metric Ops","text":"<p>Module: <code>generative_models.core.evaluation.metrics.metric_ops</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/metric_ops.py</code></p>"},{"location":"core/metric_ops/#overview","title":"Overview","text":"<p>JAX-compatible operations for metrics computation.</p> <p>This module provides JAX implementations of common algorithms needed for metrics, replacing numpy and sklearn dependencies to ensure compatibility with NNX modules.</p>"},{"location":"core/metric_ops/#functions","title":"Functions","text":""},{"location":"core/metric_ops/#bincount","title":"bincount","text":"<pre><code>def bincount()\n</code></pre>"},{"location":"core/metric_ops/#compute_cdf","title":"compute_cdf","text":"<pre><code>def compute_cdf()\n</code></pre>"},{"location":"core/metric_ops/#compute_ks_distance","title":"compute_ks_distance","text":"<pre><code>def compute_ks_distance()\n</code></pre>"},{"location":"core/metric_ops/#corrcoef","title":"corrcoef","text":"<pre><code>def corrcoef()\n</code></pre>"},{"location":"core/metric_ops/#matrix_sqrtm","title":"matrix_sqrtm","text":"<pre><code>def matrix_sqrtm()\n</code></pre>"},{"location":"core/metric_ops/#nearest_neighbors","title":"nearest_neighbors","text":"<pre><code>def nearest_neighbors()\n</code></pre>"},{"location":"core/metric_ops/#pairwise_distances","title":"pairwise_distances","text":"<pre><code>def pairwise_distances()\n</code></pre>"},{"location":"core/metric_ops/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 7</li> <li>Imports: 2</li> </ul>"},{"location":"core/metrics/","title":"Metrics","text":"<p>Artifex provides a comprehensive metrics system for evaluating generative models. The metrics framework is built on Flax NNX and follows JAX-compatible patterns for efficient computation.</p>"},{"location":"core/metrics/#overview","title":"Overview","text":"<ul> <li> <p> Image Metrics</p> <p>FID, Inception Score, and feature-based metrics for image quality</p> <p> Image Metrics</p> </li> <li> <p> Text Metrics</p> <p>Perplexity and sequence-based metrics for language models</p> <p> Text Metrics</p> </li> <li> <p> Distribution Metrics</p> <p>Statistical comparison between real and generated distributions</p> <p> Distribution Metrics</p> </li> <li> <p> Metric Pipeline</p> <p>Compose and orchestrate metrics across modalities</p> <p> Evaluation Pipeline</p> </li> </ul>"},{"location":"core/metrics/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.core.evaluation.metrics import (\n    FrechetInceptionDistance,\n    InceptionScore,\n    Perplexity,\n    MetricsRegistry,\n    EvaluationPipeline,\n)\nfrom flax import nnx\n\n# Create a metric instance\nkey = jax.random.key(42)\nfid_metric = FrechetInceptionDistance(\n    batch_size=32,\n    rngs=nnx.Rngs(params=key),\n)\n\n# Compute FID between real and generated images\nresults = fid_metric.compute(real_images, generated_images)\nprint(f\"FID: {results['fid']:.2f}\")\n</code></pre>"},{"location":"core/metrics/#base-classes","title":"Base Classes","text":"<p>All metrics inherit from base classes that provide consistent interfaces and JAX compatibility.</p>"},{"location":"core/metrics/#metricmodule","title":"MetricModule","text":"<p>The foundation class for all metrics, providing the basic compute interface.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics.base import MetricModule\nfrom flax import nnx\n\nclass MyMetric(MetricModule):\n    \"\"\"Custom metric implementation.\"\"\"\n\n    def __init__(\n        self,\n        name: str = \"my_metric\",\n        batch_size: int = 32,\n        *,\n        rngs: nnx.Rngs | None = None,\n    ):\n        super().__init__(name=name, batch_size=batch_size, rngs=rngs)\n\n    def compute(self, predictions, targets) -&gt; dict[str, float]:\n        \"\"\"Compute the metric value.\n\n        Returns:\n            Dictionary with metric name and value\n        \"\"\"\n        value = ...  # Your computation\n        return {self.name: float(value)}\n</code></pre>"},{"location":"core/metrics/#featurebasedmetric","title":"FeatureBasedMetric","text":"<p>For metrics requiring feature extraction (e.g., FID, Inception Score).</p> <pre><code>from artifex.generative_models.core.evaluation.metrics.base import FeatureBasedMetric\n\nclass MyFeatureMetric(FeatureBasedMetric):\n    def __init__(\n        self,\n        feature_extractor=None,\n        batch_size: int = 32,\n        *,\n        rngs: nnx.Rngs | None = None,\n    ):\n        super().__init__(\n            name=\"my_feature_metric\",\n            feature_extractor=feature_extractor,\n            batch_size=batch_size,\n            rngs=rngs,\n        )\n\n    def compute(self, real_data, generated_data) -&gt; dict[str, float]:\n        # Extract features using batch processing\n        real_features = self.extract_features(real_data)\n        gen_features = self.extract_features(generated_data)\n\n        # Compare features\n        score = ...\n        return {self.name: float(score)}\n</code></pre>"},{"location":"core/metrics/#distributionmetric","title":"DistributionMetric","text":"<p>For metrics comparing statistical distributions.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics.base import DistributionMetric\n\nclass MyDistributionMetric(DistributionMetric):\n    def compute(self, real_data, generated_data) -&gt; dict[str, float]:\n        # Compute statistics for each distribution\n        real_stats = self.compute_statistics(real_data)\n        gen_stats = self.compute_statistics(generated_data)\n\n        # Returns: {\"mean\": ..., \"covariance\": ..., \"std\": ...}\n        distance = self._compare_distributions(real_stats, gen_stats)\n        return {self.name: float(distance)}\n</code></pre>"},{"location":"core/metrics/#sequencemetric","title":"SequenceMetric","text":"<p>For metrics operating on sequences (text, time series).</p> <pre><code>from artifex.generative_models.core.evaluation.metrics.base import SequenceMetric\n\nclass MySequenceMetric(SequenceMetric):\n    def compute(self, sequences, masks=None) -&gt; dict[str, float]:\n        # Process sequences with optional masking\n        processed = self.process_sequences(sequences, masks)\n\n        score = ...\n        return {self.name: float(score)}\n</code></pre>"},{"location":"core/metrics/#image-metrics","title":"Image Metrics","text":""},{"location":"core/metrics/#fid-frechet-inception-distance","title":"FID (Fr\u00e9chet Inception Distance)","text":"<p>Measures the distance between feature distributions of real and generated images.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics import FrechetInceptionDistance\n\nfid = FrechetInceptionDistance(\n    feature_extractor=None,  # Uses default Inception-v3\n    batch_size=32,\n    rngs=nnx.Rngs(params=key),\n)\n\n# Compute FID\nresults = fid.compute(real_images, generated_images)\nprint(f\"FID: {results['fid']:.2f}\")  # Lower is better\n</code></pre> <p>Interpretation:</p> FID Range Quality &lt; 10 Excellent 10-50 Good 50-100 Moderate &gt; 100 Poor <p>How FID Works:</p> <ol> <li>Extract features from both real and generated images using Inception-v3</li> <li>Fit multivariate Gaussians to both feature sets</li> <li>Compute Fr\u00e9chet distance between the two Gaussians</li> </ol> <pre><code># The Fr\u00e9chet distance formula:\n# FID = ||\u03bc_r - \u03bc_g||\u00b2 + Tr(\u03a3_r + \u03a3_g - 2(\u03a3_r \u03a3_g)^\u00bd)\n</code></pre>"},{"location":"core/metrics/#inception-score-is","title":"Inception Score (IS)","text":"<p>Measures both quality and diversity of generated images.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics import InceptionScore\n\nis_metric = InceptionScore(\n    classifier=None,  # Uses default Inception classifier\n    batch_size=32,\n    splits=10,  # Number of splits for computing variance\n    rngs=nnx.Rngs(params=key),\n)\n\n# Compute IS\nresults = is_metric.compute(generated_images)\nprint(f\"IS: {results['is']:.2f}\")  # Higher is better\n</code></pre> <p>Interpretation:</p> <ul> <li>Higher IS indicates better quality and diversity</li> <li>Real CIFAR-10: ~11.24</li> <li>Real ImageNet: ~100+</li> </ul>"},{"location":"core/metrics/#precision-and-recall","title":"Precision and Recall","text":"<p>Measures coverage (recall) and quality (precision) separately.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics import PrecisionRecall\n\npr_metric = PrecisionRecall(\n    k=3,  # k-nearest neighbors\n    rngs=nnx.Rngs(params=key),\n)\n\nresults = pr_metric.compute(real_images, generated_images)\nprint(f\"Precision: {results['precision']:.3f}\")  # Quality\nprint(f\"Recall: {results['recall']:.3f}\")        # Coverage\n</code></pre>"},{"location":"core/metrics/#text-metrics","title":"Text Metrics","text":""},{"location":"core/metrics/#perplexity","title":"Perplexity","text":"<p>Measures how well a model predicts text sequences.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics import Perplexity\n\nppl_metric = Perplexity(\n    model=language_model,\n    batch_size=32,\n    rngs=nnx.Rngs(params=key),\n)\n\nresults = ppl_metric.compute(text_sequences)\nprint(f\"Perplexity: {results['perplexity']:.2f}\")  # Lower is better\n</code></pre> <p>Interpretation:</p> <ul> <li>Lower perplexity indicates better prediction</li> <li>Typical values: 10-100 depending on dataset complexity</li> </ul>"},{"location":"core/metrics/#distribution-metrics","title":"Distribution Metrics","text":""},{"location":"core/metrics/#statistical-comparison","title":"Statistical Comparison","text":"<p>Compare distributions using mean and covariance statistics.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics.base import DistributionMetric\n\n# Compute statistics\nstats = DistributionMetric.compute_statistics(features)\n# Returns:\n# {\n#     \"mean\": jax.Array,        # Feature means\n#     \"covariance\": jax.Array,  # Covariance matrix\n#     \"std\": jax.Array,         # Standard deviations\n# }\n</code></pre>"},{"location":"core/metrics/#metrics-registry","title":"Metrics Registry","text":"<p>The registry provides centralized management of metrics.</p>"},{"location":"core/metrics/#registering-metrics","title":"Registering Metrics","text":"<pre><code>from artifex.generative_models.core.evaluation.metrics import MetricsRegistry\n\nregistry = MetricsRegistry()\n\n# Register a custom metric\ndef my_custom_metric(predictions, targets):\n    error = jnp.mean((predictions - targets) ** 2)\n    return {\"my_metric\": float(error)}\n\nregistry.register_metric_computer(\"my_metric\", my_custom_metric)\n</code></pre>"},{"location":"core/metrics/#using-the-registry","title":"Using the Registry","text":"<pre><code># List available metrics\navailable = registry.list_available_metrics()\nprint(f\"Available metrics: {available}\")\n\n# Check if metric exists\nif registry.has_metric(\"mse\"):\n    results = registry.compute_metrics(\"mse\", predictions, targets)\n    print(f\"MSE: {results['mse']:.4f}\")\n</code></pre>"},{"location":"core/metrics/#built-in-metrics","title":"Built-in Metrics","text":"<p>The registry comes with standard metrics pre-registered:</p> Metric Description <code>accuracy</code> Classification accuracy <code>mse</code> Mean Squared Error <code>mae</code> Mean Absolute Error"},{"location":"core/metrics/#evaluation-pipeline","title":"Evaluation Pipeline","text":"<p>The evaluation pipeline orchestrates metrics across multiple modalities.</p>"},{"location":"core/metrics/#basic-usage","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.core.evaluation.metrics import EvaluationPipeline\nfrom artifex.generative_models.core.configuration import EvaluationConfig\n\n# Configure evaluation\nconfig = EvaluationConfig(\n    metrics=[\"image:fid\", \"image:is\", \"text:perplexity\"],\n    metric_params={\n        \"fid\": {\"batch_size\": 64},\n        \"is\": {\"splits\": 10},\n    },\n)\n\n# Create pipeline\npipeline = EvaluationPipeline(config, rngs=nnx.Rngs(params=key))\n\n# Run evaluation\ndata = {\n    \"image\": {\"real\": real_images, \"generated\": gen_images},\n    \"text\": {\"real\": real_text, \"generated\": gen_text},\n}\n\nresults = pipeline.evaluate(data)\n# Returns: {\"image\": {\"fid\": ..., \"is\": ...}, \"text\": {\"perplexity\": ...}}\n</code></pre>"},{"location":"core/metrics/#metric-composer","title":"Metric Composer","text":"<p>Compose and aggregate metrics with custom rules.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics import MetricComposer\n\nconfig = EvaluationConfig(\n    metrics=[\"image:fid\", \"image:is\"],\n    metric_params={\n        \"composition_rules\": {\n            \"quality_score\": {\n                \"weights\": {\"fid\": -0.5, \"is\": 0.5},  # FID negative (lower is better)\n                \"normalization\": \"min_max\",\n            },\n        },\n    },\n)\n\ncomposer = MetricComposer(config, rngs=nnx.Rngs(params=key))\n\n# Compose metrics into single score\ncomposed = composer.compose({\"fid\": 25.0, \"is\": 8.5})\nprint(f\"Quality Score: {composed['quality_score']:.3f}\")\n</code></pre>"},{"location":"core/metrics/#cross-modality-aggregation","title":"Cross-Modality Aggregation","text":"<pre><code># Aggregate results across modalities\nconfig = EvaluationConfig(\n    metrics=[\"image:fid\", \"text:perplexity\"],\n    metric_params={\n        \"composer_settings\": {\n            \"aggregation_strategy\": \"weighted_average\",\n            \"modality_weights\": {\"image\": 0.6, \"text\": 0.4},\n        },\n    },\n)\n\ncomposer = MetricComposer(config, rngs=nnx.Rngs(params=key))\n\nmodality_results = {\n    \"image\": {\"fid\": 25.0},\n    \"text\": {\"perplexity\": 45.0},\n}\n\naggregated = composer.aggregate_modalities(modality_results)\nprint(f\"Cross-modality Score: {aggregated['cross_modality_score']:.3f}\")\n</code></pre>"},{"location":"core/metrics/#modality-metrics-manager","title":"Modality Metrics Manager","text":"<p>Select appropriate metrics based on modality and quality requirements.</p> <pre><code>from artifex.generative_models.core.evaluation.metrics import ModalityMetrics\n\nconfig = EvaluationConfig(\n    metrics=[\"image:fid\", \"image:is\", \"text:bleu\"],\n    metric_params={\n        \"quality_levels\": {\n            \"fast\": [\"fid\"],\n            \"standard\": [\"fid\", \"is\"],\n            \"comprehensive\": [\"fid\", \"is\", \"lpips\", \"precision_recall\"],\n        },\n    },\n)\n\nmanager = ModalityMetrics(config, rngs=nnx.Rngs(params=key))\n\n# Get supported modalities\nmodalities = manager.get_supported_modalities()\nprint(f\"Supported: {modalities}\")\n\n# Select metrics for quality level\nmetrics = manager.select_metrics(\"image\", quality_level=\"standard\")\nprint(f\"Selected metrics: {metrics}\")  # [\"fid\", \"is\"]\n</code></pre>"},{"location":"core/metrics/#creating-custom-metrics","title":"Creating Custom Metrics","text":""},{"location":"core/metrics/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li>Choose the appropriate base class:</li> <li><code>MetricModule</code>: General metrics</li> <li><code>FeatureBasedMetric</code>: Metrics requiring feature extraction</li> <li><code>DistributionMetric</code>: Distribution comparison metrics</li> <li> <p><code>SequenceMetric</code>: Sequence-based metrics</p> </li> <li> <p>Implement the <code>compute</code> method:</p> </li> </ol> <pre><code>from artifex.generative_models.core.evaluation.metrics.base import MetricModule\nimport jax.numpy as jnp\n\nclass SSIMMetric(MetricModule):\n    \"\"\"Structural Similarity Index Metric.\"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 11,\n        *,\n        rngs: nnx.Rngs | None = None,\n    ):\n        super().__init__(name=\"ssim\", batch_size=32, rngs=rngs)\n        self.window_size = window_size\n\n    def compute(\n        self,\n        image1: jax.Array,\n        image2: jax.Array,\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute SSIM between two images.\n\n        Args:\n            image1: First image batch\n            image2: Second image batch\n\n        Returns:\n            Dictionary with SSIM score\n        \"\"\"\n        # SSIM computation logic\n        ssim_value = self._compute_ssim(image1, image2)\n        return {self.name: float(ssim_value)}\n\n    def _compute_ssim(self, img1, img2):\n        # Implementation details...\n        pass\n</code></pre> <ol> <li>Register with the registry (optional):</li> </ol> <pre><code>registry = MetricsRegistry()\nregistry.register_metric_computer(\n    \"ssim\",\n    lambda img1, img2: SSIMMetric().compute(img1, img2)\n)\n</code></pre>"},{"location":"core/metrics/#best-practices","title":"Best Practices","text":"<p>DO</p> <ul> <li>Use appropriate metrics for each modality</li> <li>Report multiple metrics for comprehensive evaluation</li> <li>Include confidence intervals with multiple runs</li> <li>Use consistent sample sizes for fair comparison</li> </ul> <p>DON'T</p> <ul> <li>Don't cherry-pick metrics that favor your model</li> <li>Don't use incompatible feature extractors for comparison</li> <li>Don't ignore statistical significance</li> <li>Don't compare metrics computed on different datasets</li> </ul>"},{"location":"core/metrics/#summary","title":"Summary","text":"<p>The Artifex metrics system provides:</p> <ul> <li>Base Classes: <code>MetricModule</code>, <code>FeatureBasedMetric</code>, <code>DistributionMetric</code>, <code>SequenceMetric</code></li> <li>Image Metrics: FID, Inception Score, Precision/Recall</li> <li>Text Metrics: Perplexity and sequence metrics</li> <li>Registry: Centralized metric management and discovery</li> <li>Pipeline: Multi-modal evaluation orchestration</li> <li>Composition: Metric aggregation and cross-modality scoring</li> </ul> <p>Use the evaluation pipeline for comprehensive model assessment across modalities.</p>"},{"location":"core/migrate_configs/","title":"Migrate Configs","text":"<p>Module: <code>generative_models.core.configuration.migrate_configs</code></p> <p>Source: <code>generative_models/core/configuration/migrate_configs.py</code></p>"},{"location":"core/migrate_configs/#overview","title":"Overview","text":"<p>Script to migrate all configurations in the codebase to the new unified system.</p> <p>This script will:</p> <ol> <li>Find all files using old configuration patterns</li> <li>Create new configuration objects</li> <li>Update the code to use the new system</li> <li>Generate migration reports</li> </ol>"},{"location":"core/migrate_configs/#classes","title":"Classes","text":""},{"location":"core/migrate_configs/#configmigrator","title":"ConfigMigrator","text":"<pre><code>class ConfigMigrator\n</code></pre>"},{"location":"core/migrate_configs/#functions","title":"Functions","text":""},{"location":"core/migrate_configs/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/migrate_configs/#create_migration_plan","title":"create_migration_plan","text":"<pre><code>def create_migration_plan()\n</code></pre>"},{"location":"core/migrate_configs/#find_config_patterns","title":"find_config_patterns","text":"<pre><code>def find_config_patterns()\n</code></pre>"},{"location":"core/migrate_configs/#generate_config_templates","title":"generate_config_templates","text":"<pre><code>def generate_config_templates()\n</code></pre>"},{"location":"core/migrate_configs/#generate_migration_code","title":"generate_migration_code","text":"<pre><code>def generate_migration_code()\n</code></pre>"},{"location":"core/migrate_configs/#main","title":"main","text":"<pre><code>def main()\n</code></pre>"},{"location":"core/migrate_configs/#migrate_metric_classes","title":"migrate_metric_classes","text":"<pre><code>def migrate_metric_classes()\n</code></pre>"},{"location":"core/migrate_configs/#migrate_model_factories","title":"migrate_model_factories","text":"<pre><code>def migrate_model_factories()\n</code></pre>"},{"location":"core/migrate_configs/#visit_dict","title":"visit_Dict","text":"<pre><code>def visit_Dict()\n</code></pre>"},{"location":"core/migrate_configs/#visit_functiondef","title":"visit_FunctionDef","text":"<pre><code>def visit_FunctionDef()\n</code></pre>"},{"location":"core/migrate_configs/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 10</li> <li>Imports: 5</li> </ul>"},{"location":"core/mixture/","title":"Mixture","text":"<p>Module: <code>generative_models.core.distributions.mixture</code></p> <p>Source: <code>generative_models/core/distributions/mixture.py</code></p>"},{"location":"core/mixture/#overview","title":"Overview","text":"<p>Mixture distributions.</p> <p>This module implements mixture distributions that combine multiple distributions with discrete probability weights.</p>"},{"location":"core/mixture/#classes","title":"Classes","text":""},{"location":"core/mixture/#mixture_1","title":"Mixture","text":"<pre><code>class Mixture\n</code></pre>"},{"location":"core/mixture/#mixtureofgaussians","title":"MixtureOfGaussians","text":"<pre><code>class MixtureOfGaussians\n</code></pre>"},{"location":"core/mixture/#functions","title":"Functions","text":""},{"location":"core/mixture/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/mixture/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/mixture/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/mixture/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/mixture/#entropy","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/mixture/#entropy_1","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/mixture/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/mixture/#log_prob_1","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/mixture/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/mixture/#sample_1","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/mixture/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 10</li> <li>Imports: 5</li> </ul>"},{"location":"core/ode/","title":"Ode","text":"<p>Module: <code>generative_models.core.sampling.ode</code></p> <p>Source: <code>generative_models/core/sampling/ode.py</code></p>"},{"location":"core/ode/#overview","title":"Overview","text":"<p>ODE-based sampling utilities.</p> <p>This module provides utilities for sampling using ordinary differential equations (ODEs). ODE-based sampling is useful for score-based generative models like diffusion models.</p>"},{"location":"core/ode/#functions","title":"Functions","text":""},{"location":"core/ode/#euler_step","title":"euler_step","text":"<pre><code>def euler_step()\n</code></pre>"},{"location":"core/ode/#ode_sampling","title":"ode_sampling","text":"<pre><code>def ode_sampling()\n</code></pre>"},{"location":"core/ode/#rk4_step","title":"rk4_step","text":"<pre><code>def rk4_step()\n</code></pre>"},{"location":"core/ode/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 3</li> <li>Imports: 3</li> </ul>"},{"location":"core/parallelism/","title":"Parallelism","text":"<p>Module: <code>generative_models.core.parallelism</code></p> <p>Source: <code>generative_models/core/parallelism.py</code></p>"},{"location":"core/parallelism/#overview","title":"Overview","text":"<p>Utilities for distributed and parallel training in JAX.</p> <p>This module provides functions for setting up device meshes, sharding strategies, and other utilities for distributed training of large models.</p>"},{"location":"core/parallelism/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"core/perceptual/","title":"Perceptual","text":"<p>Module: <code>generative_models.core.losses.perceptual</code></p> <p>Source: <code>generative_models/core/losses/perceptual.py</code></p>"},{"location":"core/perceptual/#overview","title":"Overview","text":"<p>Perceptual losses module.</p> <p>This module provides loss functions that compare features extracted from neural networks, rather than direct pixel-wise comparisons. These losses are especially useful for image generation tasks.</p>"},{"location":"core/perceptual/#classes","title":"Classes","text":""},{"location":"core/perceptual/#perceptualloss","title":"PerceptualLoss","text":"<pre><code>class PerceptualLoss\n</code></pre>"},{"location":"core/perceptual/#functions","title":"Functions","text":""},{"location":"core/perceptual/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/perceptual/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/perceptual/#compute_contextual_loss_single","title":"compute_contextual_loss_single","text":"<pre><code>def compute_contextual_loss_single()\n</code></pre>"},{"location":"core/perceptual/#contextual_loss","title":"contextual_loss","text":"<pre><code>def contextual_loss()\n</code></pre>"},{"location":"core/perceptual/#create_vgg_perceptual_loss","title":"create_vgg_perceptual_loss","text":"<pre><code>def create_vgg_perceptual_loss()\n</code></pre>"},{"location":"core/perceptual/#feature_reconstruction_loss","title":"feature_reconstruction_loss","text":"<pre><code>def feature_reconstruction_loss()\n</code></pre>"},{"location":"core/perceptual/#style_loss","title":"style_loss","text":"<pre><code>def style_loss()\n</code></pre>"},{"location":"core/perceptual/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 7</li> <li>Imports: 6</li> </ul>"},{"location":"core/performance/","title":"Performance","text":"<p>Module: <code>generative_models.core.performance</code></p> <p>Source: <code>generative_models/core/performance.py</code></p>"},{"location":"core/performance/#overview","title":"Overview","text":"<p>Performance infrastructure for roofline analysis and optimization.</p> <p>This module provides core performance analysis capabilities including:</p> <ul> <li>Hardware detection and specification</li> <li>Roofline model analysis for performance estimation</li> <li>FLOP counting and arithmetic intensity calculation</li> <li>JAX function profiling and benchmarking</li> </ul> <p>All implementations follow JAX/Flax NNX best practices and avoid numpy usage within any performance-critical code paths.</p>"},{"location":"core/performance/#classes","title":"Classes","text":""},{"location":"core/performance/#hardwaredetector","title":"HardwareDetector","text":"<pre><code>class HardwareDetector\n</code></pre>"},{"location":"core/performance/#hardwarespecs","title":"HardwareSpecs","text":"<pre><code>class HardwareSpecs\n</code></pre>"},{"location":"core/performance/#performanceestimator","title":"PerformanceEstimator","text":"<pre><code>class PerformanceEstimator\n</code></pre>"},{"location":"core/performance/#rooflinemetrics","title":"RooflineMetrics","text":"<pre><code>class RooflineMetrics\n</code></pre>"},{"location":"core/performance/#functions","title":"Functions","text":""},{"location":"core/performance/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/performance/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/performance/#analyze_roofline","title":"analyze_roofline","text":"<pre><code>def analyze_roofline()\n</code></pre>"},{"location":"core/performance/#benchmark_operation","title":"benchmark_operation","text":"<pre><code>def benchmark_operation()\n</code></pre>"},{"location":"core/performance/#calculate_arithmetic_intensity","title":"calculate_arithmetic_intensity","text":"<pre><code>def calculate_arithmetic_intensity()\n</code></pre>"},{"location":"core/performance/#detect_hardware","title":"detect_hardware","text":"<pre><code>def detect_hardware()\n</code></pre>"},{"location":"core/performance/#estimate_flops_attention","title":"estimate_flops_attention","text":"<pre><code>def estimate_flops_attention()\n</code></pre>"},{"location":"core/performance/#estimate_flops_linear","title":"estimate_flops_linear","text":"<pre><code>def estimate_flops_linear()\n</code></pre>"},{"location":"core/performance/#estimate_memory_usage","title":"estimate_memory_usage","text":"<pre><code>def estimate_memory_usage()\n</code></pre>"},{"location":"core/performance/#estimate_transformer_layer_performance","title":"estimate_transformer_layer_performance","text":"<pre><code>def estimate_transformer_layer_performance()\n</code></pre>"},{"location":"core/performance/#get_batch_size_recommendation","title":"get_batch_size_recommendation","text":"<pre><code>def get_batch_size_recommendation()\n</code></pre>"},{"location":"core/performance/#get_critical_batch_size","title":"get_critical_batch_size","text":"<pre><code>def get_critical_batch_size()\n</code></pre>"},{"location":"core/performance/#get_optimal_batch_size","title":"get_optimal_batch_size","text":"<pre><code>def get_optimal_batch_size()\n</code></pre>"},{"location":"core/performance/#is_batch_size_optimal","title":"is_batch_size_optimal","text":"<pre><code>def is_batch_size_optimal()\n</code></pre>"},{"location":"core/performance/#profile_jax_function","title":"profile_jax_function","text":"<pre><code>def profile_jax_function()\n</code></pre>"},{"location":"core/performance/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 15</li> <li>Imports: 4</li> </ul>"},{"location":"core/perplexity/","title":"Perplexity","text":"<p>Module: <code>generative_models.core.evaluation.metrics.text.perplexity</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/text/perplexity.py</code></p>"},{"location":"core/perplexity/#overview","title":"Overview","text":"<p>Perplexity metric implementation using JAX and NNX.</p> <p>Perplexity is a common evaluation metric for language models that measures how well a probability model predicts a sample. Lower perplexity indicates better prediction performance.</p>"},{"location":"core/perplexity/#classes","title":"Classes","text":""},{"location":"core/perplexity/#perplexity_1","title":"Perplexity","text":"<pre><code>class Perplexity\n</code></pre>"},{"location":"core/perplexity/#functions","title":"Functions","text":""},{"location":"core/perplexity/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/perplexity/#calculate_perplexity","title":"calculate_perplexity","text":"<pre><code>def calculate_perplexity()\n</code></pre>"},{"location":"core/perplexity/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"core/perplexity/#compute_log_probs","title":"compute_log_probs","text":"<pre><code>def compute_log_probs()\n</code></pre>"},{"location":"core/perplexity/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 4</li> <li>Imports: 5</li> </ul>"},{"location":"core/pipeline/","title":"Pipeline","text":"<p>Module: <code>generative_models.core.evaluation.metrics.pipeline</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/pipeline.py</code></p>"},{"location":"core/pipeline/#overview","title":"Overview","text":"<p>Evaluation pipeline and metric composition for artifex.generative_models.core.evaluation.</p>"},{"location":"core/pipeline/#classes","title":"Classes","text":""},{"location":"core/pipeline/#evaluationpipeline","title":"EvaluationPipeline","text":"<pre><code>class EvaluationPipeline\n</code></pre>"},{"location":"core/pipeline/#metriccomposer","title":"MetricComposer","text":"<pre><code>class MetricComposer\n</code></pre>"},{"location":"core/pipeline/#modalitymetrics","title":"ModalityMetrics","text":"<pre><code>class ModalityMetrics\n</code></pre>"},{"location":"core/pipeline/#functions","title":"Functions","text":""},{"location":"core/pipeline/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/pipeline/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/pipeline/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/pipeline/#aggregate_modalities","title":"aggregate_modalities","text":"<pre><code>def aggregate_modalities()\n</code></pre>"},{"location":"core/pipeline/#compose","title":"compose","text":"<pre><code>def compose()\n</code></pre>"},{"location":"core/pipeline/#evaluate","title":"evaluate","text":"<pre><code>def evaluate()\n</code></pre>"},{"location":"core/pipeline/#get_supported_modalities","title":"get_supported_modalities","text":"<pre><code>def get_supported_modalities()\n</code></pre>"},{"location":"core/pipeline/#select_metrics","title":"select_metrics","text":"<pre><code>def select_metrics()\n</code></pre>"},{"location":"core/pipeline/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 8</li> <li>Imports: 7</li> </ul>"},{"location":"core/positional/","title":"Positional","text":"<p>Module: <code>generative_models.core.layers.positional</code></p> <p>Source: <code>generative_models/core/layers/positional.py</code></p>"},{"location":"core/positional/#overview","title":"Overview","text":"<p>Positional encoding implementations for transformers - CORRECTED VERSION.</p> <p>This module provides various positional encoding implementations that can be used with transformer models using Flax NNX.</p>"},{"location":"core/positional/#classes","title":"Classes","text":""},{"location":"core/positional/#learnedpositionalencoding","title":"LearnedPositionalEncoding","text":"<pre><code>class LearnedPositionalEncoding\n</code></pre>"},{"location":"core/positional/#positionalencoding","title":"PositionalEncoding","text":"<pre><code>class PositionalEncoding\n</code></pre>"},{"location":"core/positional/#rotarypositionalencoding","title":"RotaryPositionalEncoding","text":"<pre><code>class RotaryPositionalEncoding\n</code></pre>"},{"location":"core/positional/#sinusoidalpositionalencoding","title":"SinusoidalPositionalEncoding","text":"<pre><code>class SinusoidalPositionalEncoding\n</code></pre>"},{"location":"core/positional/#functions","title":"Functions","text":""},{"location":"core/positional/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/positional/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/positional/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/positional/#call_3","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/positional/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/positional/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/positional/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/positional/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/positional/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 8</li> <li>Imports: 4</li> </ul>"},{"location":"core/precision_recall/","title":"Precision Recall","text":"<p>Module: <code>generative_models.core.evaluation.metrics.general.precision_recall</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/general/precision_recall.py</code></p>"},{"location":"core/precision_recall/#overview","title":"Overview","text":"<p>Precision and Recall metrics for generative models using JAX.</p> <p>This module implements the improved Precision and Recall metrics for evaluating generative models as proposed in \"Improved Precision and Recall Metric for Assessing Generative Models\" (Kynk\u00e4\u00e4nniemi et al., 2019).</p> <p>All implementations use JAX for compatibility with NNX modules.</p>"},{"location":"core/precision_recall/#classes","title":"Classes","text":""},{"location":"core/precision_recall/#densityprecisionrecall","title":"DensityPrecisionRecall","text":"<pre><code>class DensityPrecisionRecall\n</code></pre>"},{"location":"core/precision_recall/#precisionrecall","title":"PrecisionRecall","text":"<pre><code>class PrecisionRecall\n</code></pre>"},{"location":"core/precision_recall/#functions","title":"Functions","text":""},{"location":"core/precision_recall/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/precision_recall/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/precision_recall/#compute","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"core/precision_recall/#compute_1","title":"compute","text":"<pre><code>def compute()\n</code></pre>"},{"location":"core/precision_recall/#compute_f1_score","title":"compute_f1_score","text":"<pre><code>def compute_f1_score()\n</code></pre>"},{"location":"core/precision_recall/#compute_improved_precision_recall","title":"compute_improved_precision_recall","text":"<pre><code>def compute_improved_precision_recall()\n</code></pre>"},{"location":"core/precision_recall/#compute_manifold_radii","title":"compute_manifold_radii","text":"<pre><code>def compute_manifold_radii()\n</code></pre>"},{"location":"core/precision_recall/#compute_precision","title":"compute_precision","text":"<pre><code>def compute_precision()\n</code></pre>"},{"location":"core/precision_recall/#compute_recall","title":"compute_recall","text":"<pre><code>def compute_recall()\n</code></pre>"},{"location":"core/precision_recall/#density_precision_recall","title":"density_precision_recall","text":"<pre><code>def density_precision_recall()\n</code></pre>"},{"location":"core/precision_recall/#estimate_densities","title":"estimate_densities","text":"<pre><code>def estimate_densities()\n</code></pre>"},{"location":"core/precision_recall/#precision_recall","title":"precision_recall","text":"<pre><code>def precision_recall()\n</code></pre>"},{"location":"core/precision_recall/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 12</li> <li>Imports: 6</li> </ul>"},{"location":"core/quality/","title":"Quality","text":"<p>Module: <code>generative_models.core.evaluation.metrics.quality</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/quality.py</code></p>"},{"location":"core/quality/#overview","title":"Overview","text":"<p>Quality metrics for evaluation across modalities.</p>"},{"location":"core/quality/#functions","title":"Functions","text":""},{"location":"core/quality/#calculate_fid_score","title":"calculate_fid_score","text":"<pre><code>def calculate_fid_score()\n</code></pre>"},{"location":"core/quality/#calculate_fid_statistics","title":"calculate_fid_statistics","text":"<pre><code>def calculate_fid_statistics()\n</code></pre>"},{"location":"core/quality/#calculate_frechet_distance","title":"calculate_frechet_distance","text":"<pre><code>def calculate_frechet_distance()\n</code></pre>"},{"location":"core/quality/#compute_lpips_distance","title":"compute_lpips_distance","text":"<pre><code>def compute_lpips_distance()\n</code></pre>"},{"location":"core/quality/#compute_mel_cepstral_distortion","title":"compute_mel_cepstral_distortion","text":"<pre><code>def compute_mel_cepstral_distortion()\n</code></pre>"},{"location":"core/quality/#compute_spectral_convergence","title":"compute_spectral_convergence","text":"<pre><code>def compute_spectral_convergence()\n</code></pre>"},{"location":"core/quality/#gradient_magnitude","title":"gradient_magnitude","text":"<pre><code>def gradient_magnitude()\n</code></pre>"},{"location":"core/quality/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 7</li> <li>Imports: 3</li> </ul>"},{"location":"core/reconstruction/","title":"Reconstruction","text":"<p>Module: <code>generative_models.core.losses.reconstruction</code></p> <p>Source: <code>generative_models/core/losses/reconstruction.py</code></p>"},{"location":"core/reconstruction/#overview","title":"Overview","text":"<p>Reconstruction losses module.</p> <p>This module provides loss functions for direct comparison between model outputs and target values, typically used for reconstruction tasks in autoencoders, image-to-image translation, and other generative models.</p>"},{"location":"core/reconstruction/#functions","title":"Functions","text":""},{"location":"core/reconstruction/#charbonnier_loss","title":"charbonnier_loss","text":"<pre><code>def charbonnier_loss()\n</code></pre>"},{"location":"core/reconstruction/#huber_loss","title":"huber_loss","text":"<pre><code>def huber_loss()\n</code></pre>"},{"location":"core/reconstruction/#mae_loss","title":"mae_loss","text":"<pre><code>def mae_loss()\n</code></pre>"},{"location":"core/reconstruction/#mse_loss","title":"mse_loss","text":"<pre><code>def mse_loss()\n</code></pre>"},{"location":"core/reconstruction/#psnr_loss","title":"psnr_loss","text":"<pre><code>def psnr_loss()\n</code></pre>"},{"location":"core/reconstruction/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 5</li> <li>Imports: 3</li> </ul>"},{"location":"core/registry/","title":"Registry","text":"<p>Module: <code>generative_models.core.evaluation.metrics.registry</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/registry.py</code></p>"},{"location":"core/registry/#overview","title":"Overview","text":"<p>Metrics registry for artifex.generative_models.core.evaluation.</p>"},{"location":"core/registry/#classes","title":"Classes","text":""},{"location":"core/registry/#metricsregistry","title":"MetricsRegistry","text":"<pre><code>class MetricsRegistry\n</code></pre>"},{"location":"core/registry/#functions","title":"Functions","text":""},{"location":"core/registry/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/registry/#accuracy_metric","title":"accuracy_metric","text":"<pre><code>def accuracy_metric()\n</code></pre>"},{"location":"core/registry/#compute_metrics","title":"compute_metrics","text":"<pre><code>def compute_metrics()\n</code></pre>"},{"location":"core/registry/#has_metric","title":"has_metric","text":"<pre><code>def has_metric()\n</code></pre>"},{"location":"core/registry/#list_available_metrics","title":"list_available_metrics","text":"<pre><code>def list_available_metrics()\n</code></pre>"},{"location":"core/registry/#mae_metric","title":"mae_metric","text":"<pre><code>def mae_metric()\n</code></pre>"},{"location":"core/registry/#mse_metric","title":"mse_metric","text":"<pre><code>def mse_metric()\n</code></pre>"},{"location":"core/registry/#register_metric_computer","title":"register_metric_computer","text":"<pre><code>def register_metric_computer()\n</code></pre>"},{"location":"core/registry/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 8</li> <li>Imports: 3</li> </ul>"},{"location":"core/regularization/","title":"Regularization","text":"<p>Module: <code>generative_models.core.losses.regularization</code></p> <p>Source: <code>generative_models/core/losses/regularization.py</code></p>"},{"location":"core/regularization/#overview","title":"Overview","text":"<p>Regularization losses module.</p> <p>This module provides regularization terms that can be added to other losses to improve model stability, generalization, and prevent overfitting. All functions are JAX-compatible and work with NNX modules.</p>"},{"location":"core/regularization/#classes","title":"Classes","text":""},{"location":"core/regularization/#dropoutregularization","title":"DropoutRegularization","text":"<pre><code>class DropoutRegularization\n</code></pre>"},{"location":"core/regularization/#spectralnormregularization","title":"SpectralNormRegularization","text":"<pre><code>class SpectralNormRegularization\n</code></pre>"},{"location":"core/regularization/#functions","title":"Functions","text":""},{"location":"core/regularization/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/regularization/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/regularization/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/regularization/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/regularization/#disc_interpolated","title":"disc_interpolated","text":"<pre><code>def disc_interpolated()\n</code></pre>"},{"location":"core/regularization/#exclude_bias_predicate","title":"exclude_bias_predicate","text":"<pre><code>def exclude_bias_predicate()\n</code></pre>"},{"location":"core/regularization/#exclude_norm_predicate","title":"exclude_norm_predicate","text":"<pre><code>def exclude_norm_predicate()\n</code></pre>"},{"location":"core/regularization/#gradient_penalty","title":"gradient_penalty","text":"<pre><code>def gradient_penalty()\n</code></pre>"},{"location":"core/regularization/#l1_regularization","title":"l1_regularization","text":"<pre><code>def l1_regularization()\n</code></pre>"},{"location":"core/regularization/#l2_regularization","title":"l2_regularization","text":"<pre><code>def l2_regularization()\n</code></pre>"},{"location":"core/regularization/#only_conv_predicate","title":"only_conv_predicate","text":"<pre><code>def only_conv_predicate()\n</code></pre>"},{"location":"core/regularization/#orthogonal_regularization","title":"orthogonal_regularization","text":"<pre><code>def orthogonal_regularization()\n</code></pre>"},{"location":"core/regularization/#spectral_norm_regularization","title":"spectral_norm_regularization","text":"<pre><code>def spectral_norm_regularization()\n</code></pre>"},{"location":"core/regularization/#total_variation_loss","title":"total_variation_loss","text":"<pre><code>def total_variation_loss()\n</code></pre>"},{"location":"core/regularization/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 14</li> <li>Imports: 5</li> </ul>"},{"location":"core/residual/","title":"Residual","text":"<p>Module: <code>generative_models.core.layers.residual</code></p> <p>Source: <code>generative_models/core/layers/residual.py</code></p>"},{"location":"core/residual/#overview","title":"Overview","text":"<p>Residual block implementations for generative models.</p> <p>This module provides flexible residual block implementations that can be used across different model architectures, supporting both 1D and 2D convolutions.</p>"},{"location":"core/residual/#classes","title":"Classes","text":""},{"location":"core/residual/#baseresidualblock","title":"BaseResidualBlock","text":"<pre><code>class BaseResidualBlock\n</code></pre>"},{"location":"core/residual/#conv1dresidualblock","title":"Conv1DResidualBlock","text":"<pre><code>class Conv1DResidualBlock\n</code></pre>"},{"location":"core/residual/#conv2dresidualblock","title":"Conv2DResidualBlock","text":"<pre><code>class Conv2DResidualBlock\n</code></pre>"},{"location":"core/residual/#maskedconv2dresidualblock","title":"MaskedConv2DResidualBlock","text":"<pre><code>class MaskedConv2DResidualBlock\n</code></pre>"},{"location":"core/residual/#functions","title":"Functions","text":""},{"location":"core/residual/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/residual/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/residual/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/residual/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/residual/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/residual/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/residual/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/residual/#create_residual_block","title":"create_residual_block","text":"<pre><code>def create_residual_block()\n</code></pre>"},{"location":"core/residual/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 8</li> <li>Imports: 5</li> </ul>"},{"location":"core/resnet/","title":"Resnet","text":"<p>Module: <code>generative_models.core.layers.resnet</code></p> <p>Source: <code>generative_models/core/layers/resnet.py</code></p>"},{"location":"core/resnet/#overview","title":"Overview","text":"<p>ResNet block implementations for JAX/Flax models.</p> <p>This module provides ResNet and Bottleneck block implementations using the latest Flax NNX API.</p>"},{"location":"core/resnet/#classes","title":"Classes","text":""},{"location":"core/resnet/#bottleneckblock","title":"BottleneckBlock","text":"<pre><code>class BottleneckBlock\n</code></pre>"},{"location":"core/resnet/#resnetblock","title":"ResNetBlock","text":"<pre><code>class ResNetBlock\n</code></pre>"},{"location":"core/resnet/#functions","title":"Functions","text":""},{"location":"core/resnet/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/resnet/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/resnet/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/resnet/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/resnet/#create_resnet_block","title":"create_resnet_block","text":"<pre><code>def create_resnet_block()\n</code></pre>"},{"location":"core/resnet/#create_resnet_stage","title":"create_resnet_stage","text":"<pre><code>def create_resnet_stage()\n</code></pre>"},{"location":"core/resnet/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 6</li> <li>Imports: 3</li> </ul>"},{"location":"core/runner/","title":"Runner","text":"<p>Module: <code>generative_models.core.evaluation.benchmarks.runner</code></p> <p>Source: <code>generative_models/core/evaluation/benchmarks/runner.py</code></p>"},{"location":"core/runner/#overview","title":"Overview","text":"<p>Benchmark runner implementations for artifex.generative_models.core.evaluation.</p>"},{"location":"core/runner/#classes","title":"Classes","text":""},{"location":"core/runner/#benchmarkrunner","title":"BenchmarkRunner","text":"<pre><code>class BenchmarkRunner\n</code></pre>"},{"location":"core/runner/#performancetracker","title":"PerformanceTracker","text":"<pre><code>class PerformanceTracker\n</code></pre>"},{"location":"core/runner/#functions","title":"Functions","text":""},{"location":"core/runner/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/runner/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/runner/#check_target_achievement","title":"check_target_achievement","text":"<pre><code>def check_target_achievement()\n</code></pre>"},{"location":"core/runner/#clear_history","title":"clear_history","text":"<pre><code>def clear_history()\n</code></pre>"},{"location":"core/runner/#compare_performance","title":"compare_performance","text":"<pre><code>def compare_performance()\n</code></pre>"},{"location":"core/runner/#get_current_performance","title":"get_current_performance","text":"<pre><code>def get_current_performance()\n</code></pre>"},{"location":"core/runner/#get_latest_results","title":"get_latest_results","text":"<pre><code>def get_latest_results()\n</code></pre>"},{"location":"core/runner/#get_performance_summary","title":"get_performance_summary","text":"<pre><code>def get_performance_summary()\n</code></pre>"},{"location":"core/runner/#get_run_count","title":"get_run_count","text":"<pre><code>def get_run_count()\n</code></pre>"},{"location":"core/runner/#run_full_benchmark","title":"run_full_benchmark","text":"<pre><code>def run_full_benchmark()\n</code></pre>"},{"location":"core/runner/#track_metrics","title":"track_metrics","text":"<pre><code>def track_metrics()\n</code></pre>"},{"location":"core/runner/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 11</li> <li>Imports: 5</li> </ul>"},{"location":"core/sde/","title":"Sde","text":"<p>Module: <code>generative_models.core.sampling.sde</code></p> <p>Source: <code>generative_models/core/sampling/sde.py</code></p>"},{"location":"core/sde/#overview","title":"Overview","text":"<p>SDE-based sampling utilities.</p> <p>This module provides utilities for sampling using stochastic differential equations (SDEs). SDE-based sampling is useful for score-based generative models and stochastic processes like Brownian motion.</p>"},{"location":"core/sde/#functions","title":"Functions","text":""},{"location":"core/sde/#euler_maruyama_step","title":"euler_maruyama_step","text":"<pre><code>def euler_maruyama_step()\n</code></pre>"},{"location":"core/sde/#milstein_step","title":"milstein_step","text":"<pre><code>def milstein_step()\n</code></pre>"},{"location":"core/sde/#sde_sampling","title":"sde_sampling","text":"<pre><code>def sde_sampling()\n</code></pre>"},{"location":"core/sde/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 3</li> <li>Imports: 4</li> </ul>"},{"location":"core/statistical/","title":"Statistical","text":"<p>Module: <code>generative_models.core.evaluation.metrics.statistical</code></p> <p>Source: <code>generative_models/core/evaluation/metrics/statistical.py</code></p>"},{"location":"core/statistical/#overview","title":"Overview","text":"<p>Statistical metrics for evaluation across modalities.</p>"},{"location":"core/statistical/#functions","title":"Functions","text":""},{"location":"core/statistical/#compute_chi2_statistic","title":"compute_chi2_statistic","text":"<pre><code>def compute_chi2_statistic()\n</code></pre>"},{"location":"core/statistical/#compute_correlation_preservation","title":"compute_correlation_preservation","text":"<pre><code>def compute_correlation_preservation()\n</code></pre>"},{"location":"core/statistical/#compute_correlation_preservation_internal","title":"compute_correlation_preservation_internal","text":"<pre><code>def compute_correlation_preservation_internal()\n</code></pre>"},{"location":"core/statistical/#compute_ks_distance","title":"compute_ks_distance","text":"<pre><code>def compute_ks_distance()\n</code></pre>"},{"location":"core/statistical/#compute_ks_distance_internal","title":"compute_ks_distance_internal","text":"<pre><code>def compute_ks_distance_internal()\n</code></pre>"},{"location":"core/statistical/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 5</li> <li>Imports: 2</li> </ul>"},{"location":"core/templates/","title":"Templates","text":"<p>Module: <code>generative_models.core.configuration.management.templates</code></p> <p>Source: <code>generative_models/core/configuration/management/templates.py</code></p>"},{"location":"core/templates/#overview","title":"Overview","text":"<p>Configuration template system for artifex.generative_models.core.</p>"},{"location":"core/templates/#classes","title":"Classes","text":""},{"location":"core/templates/#configtemplatemanager","title":"ConfigTemplateManager","text":"<pre><code>class ConfigTemplateManager\n</code></pre>"},{"location":"core/templates/#trainingconfigtemplate","title":"TrainingConfigTemplate","text":"<pre><code>class TrainingConfigTemplate\n</code></pre>"},{"location":"core/templates/#functions","title":"Functions","text":""},{"location":"core/templates/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/templates/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"core/templates/#generate_config","title":"generate_config","text":"<pre><code>def generate_config()\n</code></pre>"},{"location":"core/templates/#get_template","title":"get_template","text":"<pre><code>def get_template()\n</code></pre>"},{"location":"core/templates/#list_templates","title":"list_templates","text":"<pre><code>def list_templates()\n</code></pre>"},{"location":"core/templates/#register_template","title":"register_template","text":"<pre><code>def register_template()\n</code></pre>"},{"location":"core/templates/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 6</li> <li>Imports: 5</li> </ul>"},{"location":"core/transformations/","title":"Transformations","text":"<p>Module: <code>generative_models.core.distributions.transformations</code></p> <p>Source: <code>generative_models/core/distributions/transformations.py</code></p>"},{"location":"core/transformations/#overview","title":"Overview","text":"<p>Distribution transformations.</p> <p>This module provides transformations for distributions that enable operations like shifting, scaling, and more complex bijective transformations.</p>"},{"location":"core/transformations/#classes","title":"Classes","text":""},{"location":"core/transformations/#affinetransform","title":"AffineTransform","text":"<pre><code>class AffineTransform\n</code></pre>"},{"location":"core/transformations/#transformeddistribution","title":"TransformedDistribution","text":"<pre><code>class TransformedDistribution\n</code></pre>"},{"location":"core/transformations/#functions","title":"Functions","text":""},{"location":"core/transformations/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/transformations/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/transformations/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/transformations/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/transformations/#entropy","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/transformations/#entropy_1","title":"entropy","text":"<pre><code>def entropy()\n</code></pre>"},{"location":"core/transformations/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/transformations/#log_prob_1","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"core/transformations/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/transformations/#sample_1","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"core/transformations/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 10</li> <li>Imports: 4</li> </ul>"},{"location":"core/transformers/","title":"Transformers","text":"<p>Module: <code>generative_models.core.layers.transformers</code></p> <p>Source: <code>generative_models/core/layers/transformers.py</code></p>"},{"location":"core/transformers/#overview","title":"Overview","text":"<p>Transformer implementation with flexible positional encodings - FINAL CORRECTED VERSION.</p> <p>This module provides transformer encoder and decoder implementations that support various types of positional encodings using the latest Flax NNX API.</p>"},{"location":"core/transformers/#classes","title":"Classes","text":""},{"location":"core/transformers/#feedforwardnetwork","title":"FeedForwardNetwork","text":"<pre><code>class FeedForwardNetwork\n</code></pre>"},{"location":"core/transformers/#transformerdecoder","title":"TransformerDecoder","text":"<pre><code>class TransformerDecoder\n</code></pre>"},{"location":"core/transformers/#transformerdecoderblock","title":"TransformerDecoderBlock","text":"<pre><code>class TransformerDecoderBlock\n</code></pre>"},{"location":"core/transformers/#transformerencoder","title":"TransformerEncoder","text":"<pre><code>class TransformerEncoder\n</code></pre>"},{"location":"core/transformers/#transformerencoderblock","title":"TransformerEncoderBlock","text":"<pre><code>class TransformerEncoderBlock\n</code></pre>"},{"location":"core/transformers/#functions","title":"Functions","text":""},{"location":"core/transformers/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/transformers/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/transformers/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/transformers/#call_3","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/transformers/#call_4","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"core/transformers/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/transformers/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/transformers/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/transformers/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/transformers/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/transformers/#create_attention_mask","title":"create_attention_mask","text":"<pre><code>def create_attention_mask()\n</code></pre>"},{"location":"core/transformers/#create_transformer","title":"create_transformer","text":"<pre><code>def create_transformer()\n</code></pre>"},{"location":"core/transformers/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 5</li> <li>Functions: 12</li> <li>Imports: 6</li> </ul>"},{"location":"core/types/","title":"Types","text":"<p>Module: <code>generative_models.core.types</code></p> <p>Source: <code>generative_models/core/types.py</code></p>"},{"location":"core/types/#overview","title":"Overview","text":"<p>Type definitions.</p>"},{"location":"core/types/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"core/unified/","title":"Unified","text":"<p>Module: <code>generative_models.core.configuration.unified</code></p> <p>Source: <code>generative_models/core/configuration/unified.py</code></p>"},{"location":"core/unified/#overview","title":"Overview","text":"<p>Unified configuration system for artifex generative models.</p> <p>This module provides a centralized configuration management system that replaces the fragmented configuration approaches across the codebase.</p> <p>Key Features:</p> <ol> <li>Type-safe configuration with Pydantic validation</li> <li>Hierarchical configuration inheritance</li> <li>Centralized registry for all configuration types</li> <li>Consistent validation and serialization</li> <li>Easy extension mechanism for new modalities/models</li> </ol>"},{"location":"core/unified/#classes","title":"Classes","text":""},{"location":"core/unified/#baseconfiguration","title":"BaseConfiguration","text":"<pre><code>class BaseConfiguration\n</code></pre>"},{"location":"core/unified/#config","title":"Config","text":"<pre><code>class Config\n</code></pre>"},{"location":"core/unified/#configurationregistry","title":"ConfigurationRegistry","text":"<pre><code>class ConfigurationRegistry\n</code></pre>"},{"location":"core/unified/#configurationtype","title":"ConfigurationType","text":"<pre><code>class ConfigurationType\n</code></pre>"},{"location":"core/unified/#dataconfig","title":"DataConfig","text":"<pre><code>class DataConfig\n</code></pre>"},{"location":"core/unified/#evaluationconfig","title":"EvaluationConfig","text":"<pre><code>class EvaluationConfig\n</code></pre>"},{"location":"core/unified/#experimentconfig","title":"ExperimentConfig","text":"<pre><code>class ExperimentConfig\n</code></pre>"},{"location":"core/unified/#modalityconfiguration","title":"ModalityConfiguration","text":"<pre><code>class ModalityConfiguration\n</code></pre>"},{"location":"core/unified/#modelconfig","title":"ModelConfig","text":"<pre><code>class ModelConfig\n</code></pre>"},{"location":"core/unified/#optimizerconfig","title":"OptimizerConfig","text":"<pre><code>class OptimizerConfig\n</code></pre>"},{"location":"core/unified/#schedulerconfig","title":"SchedulerConfig","text":"<pre><code>class SchedulerConfig\n</code></pre>"},{"location":"core/unified/#trainingconfig","title":"TrainingConfig","text":"<pre><code>class TrainingConfig\n</code></pre>"},{"location":"core/unified/#functions","title":"Functions","text":""},{"location":"core/unified/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/unified/#create_from_template","title":"create_from_template","text":"<pre><code>def create_from_template()\n</code></pre>"},{"location":"core/unified/#create_from_template_1","title":"create_from_template","text":"<pre><code>def create_from_template()\n</code></pre>"},{"location":"core/unified/#from_yaml","title":"from_yaml","text":"<pre><code>def from_yaml()\n</code></pre>"},{"location":"core/unified/#get","title":"get","text":"<pre><code>def get()\n</code></pre>"},{"location":"core/unified/#get_config","title":"get_config","text":"<pre><code>def get_config()\n</code></pre>"},{"location":"core/unified/#list_configs","title":"list_configs","text":"<pre><code>def list_configs()\n</code></pre>"},{"location":"core/unified/#list_configs_1","title":"list_configs","text":"<pre><code>def list_configs()\n</code></pre>"},{"location":"core/unified/#load_from_directory","title":"load_from_directory","text":"<pre><code>def load_from_directory()\n</code></pre>"},{"location":"core/unified/#merge","title":"merge","text":"<pre><code>def merge()\n</code></pre>"},{"location":"core/unified/#register","title":"register","text":"<pre><code>def register()\n</code></pre>"},{"location":"core/unified/#register_config","title":"register_config","text":"<pre><code>def register_config()\n</code></pre>"},{"location":"core/unified/#register_template","title":"register_template","text":"<pre><code>def register_template()\n</code></pre>"},{"location":"core/unified/#resolve_configs","title":"resolve_configs","text":"<pre><code>def resolve_configs()\n</code></pre>"},{"location":"core/unified/#to_yaml","title":"to_yaml","text":"<pre><code>def to_yaml()\n</code></pre>"},{"location":"core/unified/#validate_activation","title":"validate_activation","text":"<pre><code>def validate_activation()\n</code></pre>"},{"location":"core/unified/#validate_compatibility","title":"validate_compatibility","text":"<pre><code>def validate_compatibility()\n</code></pre>"},{"location":"core/unified/#validate_optimizer_type","title":"validate_optimizer_type","text":"<pre><code>def validate_optimizer_type()\n</code></pre>"},{"location":"core/unified/#validate_scheduler_type","title":"validate_scheduler_type","text":"<pre><code>def validate_scheduler_type()\n</code></pre>"},{"location":"core/unified/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 12</li> <li>Functions: 19</li> <li>Imports: 6</li> </ul>"},{"location":"core/validation/","title":"Validation","text":"<p>Module: <code>generative_models.core.configuration.validation</code></p> <p>Source: <code>generative_models/core/configuration/validation.py</code></p>"},{"location":"core/validation/#overview","title":"Overview","text":"<p>Centralized configuration validation utilities.</p> <p>This module provides shared validation functions to eliminate duplication across the codebase and ensure consistent error handling.</p>"},{"location":"core/validation/#functions","title":"Functions","text":""},{"location":"core/validation/#ensure_model_configuration_compatibility","title":"ensure_model_configuration_compatibility","text":"<pre><code>def ensure_model_configuration_compatibility()\n</code></pre>"},{"location":"core/validation/#validate_base_configuration","title":"validate_base_configuration","text":"<pre><code>def validate_base_configuration()\n</code></pre>"},{"location":"core/validation/#validate_configuration_type","title":"validate_configuration_type","text":"<pre><code>def validate_configuration_type()\n</code></pre>"},{"location":"core/validation/#validate_model_configuration","title":"validate_model_configuration","text":"<pre><code>def validate_model_configuration()\n</code></pre>"},{"location":"core/validation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 4</li> <li>Imports: 2</li> </ul>"},{"location":"core/versioning/","title":"Versioning","text":"<p>Module: <code>generative_models.core.configuration.management.versioning</code></p> <p>Source: <code>generative_models/core/configuration/management/versioning.py</code></p>"},{"location":"core/versioning/#overview","title":"Overview","text":"<p>Configuration versioning and management for artifex.generative_models.core.</p>"},{"location":"core/versioning/#classes","title":"Classes","text":""},{"location":"core/versioning/#configversion","title":"ConfigVersion","text":"<pre><code>class ConfigVersion\n</code></pre>"},{"location":"core/versioning/#configversionregistry","title":"ConfigVersionRegistry","text":"<pre><code>class ConfigVersionRegistry\n</code></pre>"},{"location":"core/versioning/#datetimeencoder","title":"DateTimeEncoder","text":"<pre><code>class DateTimeEncoder\n</code></pre>"},{"location":"core/versioning/#functions","title":"Functions","text":""},{"location":"core/versioning/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/versioning/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"core/versioning/#compute_config_hash","title":"compute_config_hash","text":"<pre><code>def compute_config_hash()\n</code></pre>"},{"location":"core/versioning/#default","title":"default","text":"<pre><code>def default()\n</code></pre>"},{"location":"core/versioning/#get_by_hash","title":"get_by_hash","text":"<pre><code>def get_by_hash()\n</code></pre>"},{"location":"core/versioning/#get_by_version","title":"get_by_version","text":"<pre><code>def get_by_version()\n</code></pre>"},{"location":"core/versioning/#hash","title":"hash","text":"<pre><code>def hash()\n</code></pre>"},{"location":"core/versioning/#list_versions","title":"list_versions","text":"<pre><code>def list_versions()\n</code></pre>"},{"location":"core/versioning/#register","title":"register","text":"<pre><code>def register()\n</code></pre>"},{"location":"core/versioning/#save","title":"save","text":"<pre><code>def save()\n</code></pre>"},{"location":"core/versioning/#to_dict","title":"to_dict","text":"<pre><code>def to_dict()\n</code></pre>"},{"location":"core/versioning/#version_id","title":"version_id","text":"<pre><code>def version_id()\n</code></pre>"},{"location":"core/versioning/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 12</li> <li>Imports: 7</li> </ul>"},{"location":"data/","title":"Data Processing","text":"<p>Comprehensive data loading, preprocessing, and augmentation pipelines for training generative models across multiple modalities.</p>"},{"location":"data/#overview","title":"Overview","text":"<ul> <li> <p> Multi-Modal Support</p> <p>Image, text, audio, video, protein, and multimodal datasets</p> </li> <li> <p> Standard Datasets</p> <p>MNIST, CIFAR, ImageNet, FFHQ, LibriSpeech, and more</p> </li> <li> <p> Augmentation</p> <p>Modality-specific data augmentation pipelines</p> </li> <li> <p> Streaming</p> <p>WebDataset, TFRecord, and remote data loading</p> </li> </ul>"},{"location":"data/#quick-start","title":"Quick Start","text":""},{"location":"data/#loading-standard-datasets","title":"Loading Standard Datasets","text":"<pre><code>from artifex.data import load_dataset\n\n# Load MNIST\ntrain_data, test_data = load_dataset(\n    \"mnist\",\n    batch_size=128,\n    split=(\"train\", \"test\"),\n)\n\n# Load CIFAR-10\ntrain_data = load_dataset(\n    \"cifar10\",\n    batch_size=64,\n    augment=True,\n)\n</code></pre>"},{"location":"data/#custom-datasets","title":"Custom Datasets","text":"<pre><code>from artifex.data import ImageDataset\n\ndataset = ImageDataset(\n    root=\"/path/to/images\",\n    image_size=(256, 256),\n    normalize=True,\n)\n</code></pre>"},{"location":"data/#image-datasets","title":"Image Datasets","text":""},{"location":"data/#standard-datasets","title":"Standard Datasets","text":"Dataset Description Size CIFAR-10/100 32x32 natural images 60K/60K ImageNet 1000-class natural images 1.2M FFHQ High-quality face images 70K Custom Image Load from directory Variable"},{"location":"data/#loading-images","title":"Loading Images","text":"<pre><code>from artifex.data.image import CIFARDataset, FFHQDataset\n\n# CIFAR-10\ncifar = CIFARDataset(\n    root=\"./data\",\n    train=True,\n    download=True,\n)\n\n# FFHQ\nffhq = FFHQDataset(\n    root=\"/path/to/ffhq\",\n    resolution=256,\n)\n</code></pre>"},{"location":"data/#image-augmentation","title":"Image Augmentation","text":"<pre><code>from artifex.data.augmentation import ImageAugmentation\n\naugment = ImageAugmentation(\n    random_flip=True,\n    random_crop=True,\n    color_jitter=0.1,\n    random_rotation=15,\n)\n\naugmented = augment(images, key=prng_key)\n</code></pre> <p> Image Augmentation</p>"},{"location":"data/#text-datasets","title":"Text Datasets","text":""},{"location":"data/#standard-datasets_1","title":"Standard Datasets","text":"Dataset Description Size Wikipedia Wikipedia articles 6M+ articles BookCorpus Book text corpus 11K books Custom Text Load from files Variable"},{"location":"data/#loading-text","title":"Loading Text","text":"<pre><code>from artifex.data.text import WikipediaDataset\n\nwiki = WikipediaDataset(\n    language=\"en\",\n    max_length=512,\n)\n</code></pre>"},{"location":"data/#tokenizers","title":"Tokenizers","text":"<pre><code>from artifex.data.tokenizers import BPETokenizer, SentencePieceTokenizer\n\n# BPE Tokenizer\ntokenizer = BPETokenizer(vocab_size=32000)\ntokenizer.fit(corpus)\ntokens = tokenizer.encode(text)\n\n# SentencePiece\ntokenizer = SentencePieceTokenizer(model_path=\"model.spm\")\n</code></pre> <p> BPE Tokenizer |  SentencePiece</p>"},{"location":"data/#audio-datasets","title":"Audio Datasets","text":""},{"location":"data/#standard-datasets_2","title":"Standard Datasets","text":"Dataset Description Hours LibriSpeech Read English speech 1000h VCTK Multi-speaker speech 44h Custom Audio Load from files Variable"},{"location":"data/#loading-audio","title":"Loading Audio","text":"<pre><code>from artifex.data.audio import LibriSpeechDataset\n\nlibrispeech = LibriSpeechDataset(\n    root=\"./data\",\n    subset=\"train-clean-100\",\n    sample_rate=16000,\n)\n</code></pre>"},{"location":"data/#audio-preprocessing","title":"Audio Preprocessing","text":"<pre><code>from artifex.data.preprocessing import AudioPreprocessor\n\npreprocessor = AudioPreprocessor(\n    sample_rate=16000,\n    n_mels=80,\n    n_fft=1024,\n    hop_length=256,\n)\n\nmel_spec = preprocessor.to_mel_spectrogram(audio)\n</code></pre> <p> Audio Preprocessing</p>"},{"location":"data/#video-datasets","title":"Video Datasets","text":""},{"location":"data/#standard-datasets_3","title":"Standard Datasets","text":"Dataset Description Size UCF101 Action recognition 13K clips Custom Video Load from files Variable"},{"location":"data/#loading-video","title":"Loading Video","text":"<pre><code>from artifex.data.video import UCF101Dataset\n\nucf = UCF101Dataset(\n    root=\"/path/to/ucf101\",\n    frames_per_clip=16,\n    frame_rate=5,\n)\n</code></pre> <p> Video Datasets</p>"},{"location":"data/#multimodal-datasets","title":"Multimodal Datasets","text":""},{"location":"data/#standard-datasets_4","title":"Standard Datasets","text":"Dataset Description Modalities COCO Image + captions Image, Text Custom Multimodal Custom pairs Variable"},{"location":"data/#loading-multimodal-data","title":"Loading Multimodal Data","text":"<pre><code>from artifex.data.multimodal import COCODataset\n\ncoco = COCODataset(\n    root=\"/path/to/coco\",\n    split=\"train2017\",\n    include_captions=True,\n)\n\nfor batch in coco:\n    images = batch[\"image\"]\n    captions = batch[\"caption\"]\n</code></pre> <p> COCO Dataset</p>"},{"location":"data/#protein-datasets","title":"Protein Datasets","text":"<pre><code>from artifex.data.protein import ProteinDataset\n\nprotein_data = ProteinDataset(\n    pdb_dir=\"/path/to/structures\",\n    max_length=256,\n    include_sequence=True,\n    include_structure=True,\n)\n</code></pre> <p> Protein Dataset</p>"},{"location":"data/#data-pipeline","title":"Data Pipeline","text":""},{"location":"data/#pipeline-api","title":"Pipeline API","text":"<pre><code>from artifex.data import DataPipeline\n\npipeline = DataPipeline()\npipeline.add_step(\"load\", loader)\npipeline.add_step(\"preprocess\", preprocessor)\npipeline.add_step(\"augment\", augmentation)\npipeline.add_step(\"batch\", batcher)\n\nfor batch in pipeline(data):\n    # Process batch\n    pass\n</code></pre> <p> Pipeline Reference</p>"},{"location":"data/#collators","title":"Collators","text":"<pre><code>from artifex.data import DynamicBatchCollator\n\ncollator = DynamicBatchCollator(\n    pad_token_id=0,\n    max_length=512,\n)\n</code></pre> <p> Collators</p>"},{"location":"data/#samplers","title":"Samplers","text":"<pre><code>from artifex.data import DistributedSampler, WeightedSampler\n\n# Distributed training\nsampler = DistributedSampler(\n    dataset=dataset,\n    num_replicas=4,\n    rank=0,\n)\n\n# Weighted sampling\nsampler = WeightedSampler(\n    weights=class_weights,\n    num_samples=len(dataset),\n)\n</code></pre> <p> Samplers</p>"},{"location":"data/#streaming-data","title":"Streaming Data","text":""},{"location":"data/#webdataset","title":"WebDataset","text":"<pre><code>from artifex.data.streaming import WebDatasetLoader\n\nloader = WebDatasetLoader(\n    urls=\"s3://bucket/data-{000..099}.tar\",\n    batch_size=64,\n    shuffle=True,\n)\n</code></pre> <p> WebDataset</p>"},{"location":"data/#tfrecord","title":"TFRecord","text":"<pre><code>from artifex.data.streaming import TFRecordLoader\n\nloader = TFRecordLoader(\n    pattern=\"/path/to/data-*.tfrecord\",\n    features={\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"label\": tf.io.FixedLenFeature([], tf.int64),\n    },\n)\n</code></pre> <p> TFRecord</p>"},{"location":"data/#remote-loading","title":"Remote Loading","text":"<pre><code>from artifex.data.streaming import RemoteLoader\n\nloader = RemoteLoader(\n    endpoint=\"https://api.example.com/data\",\n    cache_dir=\"./cache\",\n)\n</code></pre> <p> Remote Loading</p>"},{"location":"data/#module-reference","title":"Module Reference","text":"Category Modules Image cifar, custom_image, ffhq, imagenet Text bookcorpus, custom_text, wikipedia Audio custom_audio, librispeech, vctk Video custom_video, ucf101 Multimodal coco, custom_multimodal Protein dataset Tokenizers bpe, character, sentencepiece, word Augmentation audio, image, text, video Preprocessing audio, base, image, text, video Loaders audio, base, collators, image, pipeline, protein_dataset, registry, samplers, structured, text, video Streaming remote, tfrecord, webdataset"},{"location":"data/#related-documentation","title":"Related Documentation","text":"<ul> <li>Data Loading Guide - Complete data loading guide</li> <li>Image Modality - Image-specific features</li> <li>Text Modality - Text-specific features</li> <li>Audio Modality - Audio-specific features</li> </ul>"},{"location":"data/audio/","title":"Audio","text":"<p>Module: <code>data.preprocessing.audio</code></p> <p>Source: <code>data/preprocessing/audio.py</code></p>"},{"location":"data/audio/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/base/","title":"Base","text":"<p>Module: <code>data.preprocessing.base</code></p> <p>Source: <code>data/preprocessing/base.py</code></p>"},{"location":"data/base/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/bookcorpus/","title":"Bookcorpus","text":"<p>Module: <code>data.datasets.text.bookcorpus</code></p> <p>Source: <code>data/datasets/text/bookcorpus.py</code></p>"},{"location":"data/bookcorpus/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/bpe/","title":"Bpe","text":"<p>Module: <code>data.tokenizers.bpe</code></p> <p>Source: <code>data/tokenizers/bpe.py</code></p>"},{"location":"data/bpe/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/character/","title":"Character","text":"<p>Module: <code>data.tokenizers.character</code></p> <p>Source: <code>data/tokenizers/character.py</code></p>"},{"location":"data/character/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/cifar/","title":"Cifar","text":"<p>Module: <code>data.datasets.image.cifar</code></p> <p>Source: <code>data/datasets/image/cifar.py</code></p>"},{"location":"data/cifar/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/coco/","title":"Coco","text":"<p>Module: <code>data.datasets.multimodal.coco</code></p> <p>Source: <code>data/datasets/multimodal/coco.py</code></p>"},{"location":"data/coco/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/collators/","title":"Collators","text":"<p>Module: <code>data.collators</code></p> <p>Source: <code>data/collators.py</code></p>"},{"location":"data/collators/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/custom_audio/","title":"Custom Audio","text":"<p>Module: <code>data.datasets.audio.custom_audio</code></p> <p>Source: <code>data/datasets/audio/custom_audio.py</code></p>"},{"location":"data/custom_audio/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/custom_image/","title":"Custom Image","text":"<p>Module: <code>data.datasets.image.custom_image</code></p> <p>Source: <code>data/datasets/image/custom_image.py</code></p>"},{"location":"data/custom_image/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/custom_multimodal/","title":"Custom Multimodal","text":"<p>Module: <code>data.datasets.multimodal.custom_multimodal</code></p> <p>Source: <code>data/datasets/multimodal/custom_multimodal.py</code></p>"},{"location":"data/custom_multimodal/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/custom_text/","title":"Custom Text","text":"<p>Module: <code>data.datasets.text.custom_text</code></p> <p>Source: <code>data/datasets/text/custom_text.py</code></p>"},{"location":"data/custom_text/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/custom_video/","title":"Custom Video","text":"<p>Module: <code>data.datasets.video.custom_video</code></p> <p>Source: <code>data/datasets/video/custom_video.py</code></p>"},{"location":"data/custom_video/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/dataset/","title":"Dataset","text":"<p>Module: <code>data.protein.dataset</code></p> <p>Source: <code>data/protein/dataset.py</code></p>"},{"location":"data/dataset/#overview","title":"Overview","text":"<p>Protein dataset implementation.</p>"},{"location":"data/dataset/#classes","title":"Classes","text":""},{"location":"data/dataset/#proteindataset","title":"ProteinDataset","text":"<pre><code>class ProteinDataset\n</code></pre>"},{"location":"data/dataset/#functions","title":"Functions","text":""},{"location":"data/dataset/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"data/dataset/#create_synthetic_data","title":"create_synthetic_data","text":"<pre><code>def create_synthetic_data()\n</code></pre>"},{"location":"data/dataset/#get_batch","title":"get_batch","text":"<pre><code>def get_batch()\n</code></pre>"},{"location":"data/dataset/#pdb_to_protein_example","title":"pdb_to_protein_example","text":"<pre><code>def pdb_to_protein_example()\n</code></pre>"},{"location":"data/dataset/#save_synthetic_data","title":"save_synthetic_data","text":"<pre><code>def save_synthetic_data()\n</code></pre>"},{"location":"data/dataset/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 5</li> <li>Imports: 7</li> </ul>"},{"location":"data/ffhq/","title":"Ffhq","text":"<p>Module: <code>data.datasets.image.ffhq</code></p> <p>Source: <code>data/datasets/image/ffhq.py</code></p>"},{"location":"data/ffhq/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/image/","title":"Image","text":"<p>Module: <code>data.preprocessing.image</code></p> <p>Source: <code>data/preprocessing/image.py</code></p>"},{"location":"data/image/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/imagenet/","title":"Imagenet","text":"<p>Module: <code>data.datasets.image.imagenet</code></p> <p>Source: <code>data/datasets/image/imagenet.py</code></p>"},{"location":"data/imagenet/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/librispeech/","title":"Librispeech","text":"<p>Module: <code>data.datasets.audio.librispeech</code></p> <p>Source: <code>data/datasets/audio/librispeech.py</code></p>"},{"location":"data/librispeech/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/pipeline/","title":"Pipeline","text":"<p>Module: <code>data.pipeline</code></p> <p>Source: <code>data/pipeline.py</code></p>"},{"location":"data/pipeline/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/protein_dataset/","title":"Protein Dataset","text":"<p>Module: <code>data.protein_dataset</code></p> <p>Source: <code>data/protein_dataset.py</code></p>"},{"location":"data/protein_dataset/#overview","title":"Overview","text":"<p>Protein dataset implementation for diffusion models.</p> <p>This module provides dataset classes for loading, processing, and batching protein structure data for diffusion models.</p>"},{"location":"data/protein_dataset/#classes","title":"Classes","text":""},{"location":"data/protein_dataset/#proteindataset","title":"ProteinDataset","text":"<pre><code>class ProteinDataset\n</code></pre>"},{"location":"data/protein_dataset/#proteinstructure","title":"ProteinStructure","text":"<pre><code>class ProteinStructure\n</code></pre>"},{"location":"data/protein_dataset/#functions","title":"Functions","text":""},{"location":"data/protein_dataset/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"data/protein_dataset/#collate_batch","title":"collate_batch","text":"<pre><code>def collate_batch()\n</code></pre>"},{"location":"data/protein_dataset/#create_synthetic_protein_dataset","title":"create_synthetic_protein_dataset","text":"<pre><code>def create_synthetic_protein_dataset()\n</code></pre>"},{"location":"data/protein_dataset/#from_numpy","title":"from_numpy","text":"<pre><code>def from_numpy()\n</code></pre>"},{"location":"data/protein_dataset/#get_batch","title":"get_batch","text":"<pre><code>def get_batch()\n</code></pre>"},{"location":"data/protein_dataset/#get_statistics","title":"get_statistics","text":"<pre><code>def get_statistics()\n</code></pre>"},{"location":"data/protein_dataset/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 6</li> <li>Imports: 4</li> </ul>"},{"location":"data/registry/","title":"Registry","text":"<p>Module: <code>data.registry</code></p> <p>Source: <code>data/registry.py</code></p>"},{"location":"data/registry/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/remote/","title":"Remote","text":"<p>Module: <code>data.streaming.remote</code></p> <p>Source: <code>data/streaming/remote.py</code></p>"},{"location":"data/remote/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/samplers/","title":"Samplers","text":"<p>Module: <code>data.samplers</code></p> <p>Source: <code>data/samplers.py</code></p>"},{"location":"data/samplers/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/sentencepiece/","title":"Sentencepiece","text":"<p>Module: <code>data.tokenizers.sentencepiece</code></p> <p>Source: <code>data/tokenizers/sentencepiece.py</code></p>"},{"location":"data/sentencepiece/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/structured/","title":"Structured","text":"<p>Module: <code>data.loaders.structured</code></p> <p>Source: <code>data/loaders/structured.py</code></p>"},{"location":"data/structured/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/text/","title":"Text","text":"<p>Module: <code>data.preprocessing.text</code></p> <p>Source: <code>data/preprocessing/text.py</code></p>"},{"location":"data/text/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/tfrecord/","title":"Tfrecord","text":"<p>Module: <code>data.streaming.tfrecord</code></p> <p>Source: <code>data/streaming/tfrecord.py</code></p>"},{"location":"data/tfrecord/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/ucf101/","title":"Ucf101","text":"<p>Module: <code>data.datasets.video.ucf101</code></p> <p>Source: <code>data/datasets/video/ucf101.py</code></p>"},{"location":"data/ucf101/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/vctk/","title":"Vctk","text":"<p>Module: <code>data.datasets.audio.vctk</code></p> <p>Source: <code>data/datasets/audio/vctk.py</code></p>"},{"location":"data/vctk/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/video/","title":"Video","text":"<p>Module: <code>data.preprocessing.video</code></p> <p>Source: <code>data/preprocessing/video.py</code></p>"},{"location":"data/video/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/webdataset/","title":"Webdataset","text":"<p>Module: <code>data.streaming.webdataset</code></p> <p>Source: <code>data/streaming/webdataset.py</code></p>"},{"location":"data/webdataset/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/wikipedia/","title":"Wikipedia","text":"<p>Module: <code>data.datasets.text.wikipedia</code></p> <p>Source: <code>data/datasets/text/wikipedia.py</code></p>"},{"location":"data/wikipedia/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"data/word/","title":"Word","text":"<p>Module: <code>data.tokenizers.word</code></p> <p>Source: <code>data/tokenizers/word.py</code></p>"},{"location":"data/word/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"development/blackjax-testing/","title":"BlackJAX Integration Tests","text":"<p>The BlackJAX integration tests are slow and are disabled by default, but can be easily enabled.</p>"},{"location":"development/blackjax-testing/#disabling-blackjax-tests","title":"Disabling BlackJAX Tests","text":"<p>BlackJAX tests are disabled by default. To run with the tests disabled:</p> <pre><code># Tests will run with BlackJAX tests disabled by default (no need to set anything)\npytest tests/\n\n# Explicitly disable BlackJAX tests\nSKIP_BLACKJAX_TESTS=1 pytest tests/\n</code></pre>"},{"location":"development/blackjax-testing/#enabling-blackjax-tests","title":"Enabling BlackJAX Tests","text":"<p>To enable and run the BlackJAX tests:</p> <pre><code># Method 1: Set ENABLE_BLACKJAX_TESTS to any non-empty value (preferred method)\nENABLE_BLACKJAX_TESTS=1 pytest tests/\n\n# Method 2: Set SKIP_BLACKJAX_TESTS to empty string (legacy method)\nSKIP_BLACKJAX_TESTS=\"\" pytest tests/\n\n# Method 3: Use the pytest marker to run ONLY BlackJAX tests\npytest tests/ -m blackjax\n</code></pre>"},{"location":"development/blackjax-testing/#blackjax-test-helper-script","title":"BlackJAX Test Helper Script","text":"<p>The easiest way to manage BlackJAX tests is to use the helper script:</p> <pre><code># Show current status of BlackJAX tests\n./scripts/blackjax_test_helper.py --status\n\n# Enable BlackJAX tests\n./scripts/blackjax_test_helper.py --enable\n\n# Disable BlackJAX tests\n./scripts/blackjax_test_helper.py --disable\n\n# Enable tests and run them immediately\n./scripts/blackjax_test_helper.py --enable --run\n\n# Run only BlackJAX tests in parallel (parallel is default)\n./scripts/blackjax_test_helper.py --enable --run --only-blackjax\n\n# Run only BlackJAX tests without parallel execution\n./scripts/blackjax_test_helper.py --enable --run --only-blackjax --no-parallel\n</code></pre>"},{"location":"development/blackjax-testing/#making-blackjax-tests-run-by-default","title":"Making BlackJAX Tests Run By Default","text":"<p>To make BlackJAX tests run by default in CI/CD or for all developers, you can:</p> <ol> <li>Add to CI/CD pipeline file:</li> </ol> <pre><code>env:\n  ENABLE_BLACKJAX_TESTS: \"1\"\n</code></pre> <ol> <li>For shell rc files (.bashrc, .zshrc, etc.):</li> </ol> <pre><code>export ENABLE_BLACKJAX_TESTS=1\n</code></pre> <ol> <li>For project .env file (if used with python-dotenv):</li> </ol> <pre><code>ENABLE_BLACKJAX_TESTS=1\n</code></pre> <ol> <li>Reset the default in code:    You can edit the <code>should_skip_blackjax_tests()</code> function in the sampling module's <code>__init__.py</code>    to change the default behavior.</li> </ol>"},{"location":"development/blackjax-testing/#selective-test-runs","title":"Selective Test Runs","text":"<p>You can be more selective about which tests to run:</p> <pre><code># Run only the BlackJAX sampler tests\npytest tests/artifex/generative_models/core/sampling/test_blackjax_samplers.py -m blackjax\n\n# Run all tests except BlackJAX tests\npytest tests/ -m \"not blackjax\"\n</code></pre>"},{"location":"development/blackjax-testing/#using-the-helper-script","title":"Using the Helper Script","text":"<p>The <code>scripts/run_tests.sh</code> helper script makes it easy to run tests with or without BlackJAX:</p> <pre><code># Run without BlackJAX tests (default)\n./scripts/run_tests.sh --fast\n\n# Run with BlackJAX tests\n./scripts/run_tests.sh --all\n\n# Run only BlackJAX tests\n./scripts/run_tests.sh --only-blackjax\n\n# With verbose output\n./scripts/run_tests.sh --all -v\n\n# With coverage report\n./scripts/run_tests.sh --all -c\n\n# Disable parallel execution (parallel is default)\n./scripts/run_tests.sh --all --no-parallel\n\n# Use classic dots output instead of progress bar\n./scripts/run_tests.sh --all --no-progress\n</code></pre>"},{"location":"development/blackjax-testing/#parallel-testing-with-blackjax","title":"Parallel Testing with BlackJAX","text":"<p>Tests run in parallel by default using pytest-xdist, which is especially beneficial for BlackJAX tests:</p> <pre><code># Run all tests including BlackJAX tests (parallel is default)\nENABLE_BLACKJAX_TESTS=1 pytest tests/\n\n# Run with specific number of processes\nENABLE_BLACKJAX_TESTS=1 pytest tests/ -n4\n\n# Disable parallel execution if needed\nENABLE_BLACKJAX_TESTS=1 pytest tests/ -o console_output_style=progress\n\n# Using the main test script\n./test.py all --enable-blackjax      # Parallel by default\n./test.py all --enable-blackjax --no-parallel  # Disable parallel execution\n</code></pre> <p>When running BlackJAX tests in parallel, each process gets its own JAX runtime, which can help isolate the tests and prevent interference between them.</p>"},{"location":"development/blackjax-testing/#test-display-options","title":"Test Display Options","text":"<p>The tests now use a progress bar display by default instead of the traditional dots:</p> <pre><code># Use the default progress bar\npytest tests/\n\n# Switch to count-style output\npytest tests/ -o console_output_style=count\n\n# Use classic dots display\npytest tests/ -o console_output_style=classic\n\n# With the test script\n./test.py all --output-style=count\n./test.py all --no-progress-bar      # Use classic dots\n</code></pre>"},{"location":"development/blackjax-testing/#cicd-configuration","title":"CI/CD Configuration","text":"<p>For CI/CD pipelines, it's recommended to include a dedicated job for BlackJAX tests that can run less frequently:</p> <pre><code>jobs:\n  fast_tests:\n    # Run regular tests with BlackJAX tests disabled (default)\n    run: pytest tests/\n\n  blackjax_tests:\n    # Only run this on scheduled jobs or specific branches\n    if: github.event_name == 'schedule' || github.ref == 'refs/heads/main'\n    run: ENABLE_BLACKJAX_TESTS=1 pytest tests/ -m blackjax\n</code></pre>"},{"location":"development/blackjax-testing/#technical-details","title":"Technical Details","text":"<p>The BlackJAX tests are marked using both:</p> <ol> <li>A custom pytest marker (<code>@pytest.mark.blackjax</code>)</li> <li>A conditional skip decorator that checks the <code>SKIP_BLACKJAX_TESTS</code> and <code>ENABLE_BLACKJAX_TESTS</code> environment variables</li> </ol> <p>See the sampling module's <code>__init__.py</code> and <code>test_blackjax_samplers.py</code> for implementation details.</p>"},{"location":"development/philosophy/","title":"Design Philosophy","text":"<p>Artifex is designed with a clear set of guiding principles that inform every architectural and implementation decision.</p>"},{"location":"development/philosophy/#core-principles","title":"Core Principles","text":""},{"location":"development/philosophy/#research-first-design","title":"Research-First Design","text":"<p>Artifex prioritizes research workflows and experimentation:</p> <ul> <li>Modularity: Components can be easily swapped and combined for experimentation</li> <li>Clarity: Implementations favor readability over clever optimizations</li> <li>Extensibility: Simple to add new models, losses, and domain-specific functionality</li> <li>Reproducibility: Deterministic execution with clear configuration management</li> </ul>"},{"location":"development/philosophy/#modern-stack","title":"Modern Stack","text":"<p>Built on the latest JAX ecosystem:</p> <ul> <li>JAX Native: Leverages JAX's functional programming paradigm for composable transformations</li> <li>Flax NNX: Uses the modern object-oriented API (not Linen) for neural network definitions</li> <li>Optax: Standard optimization library for gradient-based learning</li> <li>BlackJAX: MCMC sampling for energy-based models</li> </ul>"},{"location":"development/philosophy/#type-safety-and-protocols","title":"Type Safety and Protocols","text":"<p>Strong typing throughout:</p> <ul> <li>Protocol-Based Design: All major components implement Python Protocols for type-safe interfaces</li> <li>Frozen Dataclass Configs: Immutable configuration objects with automatic validation</li> <li>Full Type Annotations: Comprehensive type hints for IDE support and static analysis</li> </ul>"},{"location":"development/philosophy/#configuration-philosophy","title":"Configuration Philosophy","text":""},{"location":"development/philosophy/#why-frozen-dataclasses","title":"Why Frozen Dataclasses?","text":"<p>Artifex uses frozen dataclass configurations instead of dictionaries or mutable config objects:</p> <pre><code>from artifex.generative_models.core.configuration import VAEConfig, EncoderConfig\n\n# Immutable, validated configuration\nencoder_config = EncoderConfig(\n    name=\"encoder\",\n    input_shape=(28, 28, 1),\n    latent_dim=64,\n    hidden_dims=(256, 128),  # Tuple, not list\n    activation=\"relu\",\n)\n</code></pre> <p>Benefits:</p> <ol> <li>Immutability: Prevents accidental modification during training</li> <li>Validation: Automatic type checking and value validation at construction</li> <li>Serialization: Easy JSON/YAML export for reproducibility</li> <li>IDE Support: Full autocomplete and type checking</li> <li>Nested Composition: Complex models built from simple, composable configs</li> </ol>"},{"location":"development/philosophy/#configuration-composition","title":"Configuration Composition","text":"<p>Models use nested configurations for clear separation of concerns:</p> <pre><code>config = VAEConfig(\n    name=\"my_vae\",\n    encoder=encoder_config,  # Nested encoder config\n    decoder=decoder_config,  # Nested decoder config\n    kl_weight=1.0,\n)\n</code></pre>"},{"location":"development/philosophy/#module-design","title":"Module Design","text":""},{"location":"development/philosophy/#factory-pattern","title":"Factory Pattern","text":"<p>All models are created through factories for consistent initialization:</p> <pre><code>from artifex.generative_models.factory import create_model\n\nmodel = create_model(config, rngs=rngs)\n</code></pre> <p>Why factories?</p> <ul> <li>Consistent validation before instantiation</li> <li>Proper RNG management</li> <li>Easy to swap models for experimentation</li> <li>Clear error messages for misconfiguration</li> </ul>"},{"location":"development/philosophy/#rng-management","title":"RNG Management","text":"<p>Flax NNX requires explicit RNG streams:</p> <pre><code>from flax import nnx\n\nrngs = nnx.Rngs(params=42, dropout=43, sample=44)\nmodel = create_model(config, rngs=rngs)\n</code></pre> <p>Different streams serve different purposes:</p> <ul> <li><code>params</code>: Parameter initialization</li> <li><code>dropout</code>: Dropout randomness</li> <li><code>sample</code>: Generative sampling</li> </ul>"},{"location":"development/philosophy/#testing-philosophy","title":"Testing Philosophy","text":""},{"location":"development/philosophy/#test-driven-development","title":"Test-Driven Development","text":"<ul> <li>Write tests first, then implement functionality</li> <li>Tests define expected behavior, not current implementation</li> <li>Never modify tests to accommodate flawed implementations</li> <li>Minimum 80% coverage for new code</li> </ul>"},{"location":"development/philosophy/#test-organization","title":"Test Organization","text":"<p>Tests mirror source structure:</p> <ul> <li><code>tests/standalone/</code>: Isolated component tests</li> <li><code>tests/artifex/</code>: Integrated system tests</li> <li>GPU tests marked with <code>@pytest.mark.gpu</code></li> </ul>"},{"location":"development/philosophy/#what-we-dont-do","title":"What We Don't Do","text":""},{"location":"development/philosophy/#no-backward-compatibility-hacks","title":"No Backward Compatibility Hacks","text":"<ul> <li>Breaking changes are acceptable for better foundations</li> <li>No deprecated parameter forwarding</li> <li>No compatibility shims</li> <li>Clean removal of unused code</li> </ul>"},{"location":"development/philosophy/#no-over-engineering","title":"No Over-Engineering","text":"<ul> <li>Only make changes directly requested or clearly necessary</li> <li>Don't add features, refactor code, or make \"improvements\" beyond what was asked</li> <li>Don't add error handling for scenarios that can't happen</li> <li>Don't create abstractions for one-time operations</li> </ul>"},{"location":"development/philosophy/#framework-constraints","title":"Framework Constraints","text":""},{"location":"development/philosophy/#flax-nnx-only","title":"Flax NNX Only","text":"<p>Artifex exclusively uses Flax NNX:</p> <ul> <li>NEVER use Flax Linen</li> <li>NEVER use PyTorch or TensorFlow</li> <li>Always call <code>super().__init__()</code> in module constructors</li> <li>Use <code>nnx.gelu</code>, <code>nnx.relu</code> not <code>jax.nn.gelu</code>, <code>jax.nn.relu</code></li> </ul>"},{"location":"development/philosophy/#jax-compatibility","title":"JAX Compatibility","text":"<p>Inside NNX modules:</p> <ul> <li>Use <code>jax.numpy</code> not <code>numpy</code></li> <li>Use <code>jax.scipy</code> not <code>scipy</code></li> <li>No numpy-based packages (scipy, sklearn)</li> </ul>"},{"location":"development/philosophy/#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Architecture overview</li> <li>Quickstart Guide - Practical getting started</li> </ul>"},{"location":"development/roadmap/","title":"Roadmap","text":"<p>This document outlines the development status and planned improvements for Artifex.</p>"},{"location":"development/roadmap/#current-status-major-refactoring","title":"Current Status: Major Refactoring","text":"<p>Artifex is undergoing a significant architectural refactoring. The library is being restructured for better modularity, maintainability, and API consistency.</p> <p>Note: During this refactoring period:</p> <ul> <li>APIs may change without deprecation warnings</li> <li>Some tests may fail or be skipped</li> <li>Documentation may not reflect current implementation</li> </ul>"},{"location":"development/roadmap/#components-under-refactoring","title":"Components Under Refactoring","text":"<ul> <li>Core Model Implementations</li> <li>VAE Family: VAE, Beta-VAE, VQ-VAE, Conditional VAE</li> <li>GAN Family: DCGAN, WGAN, StyleGAN, CycleGAN, PatchGAN</li> <li>Diffusion Models: DDPM, DDIM, Score-based, DiT, Latent Diffusion</li> <li>Normalizing Flows: RealNVP, Glow, MAF, IAF, Neural Spline Flows</li> <li>Energy-Based Models: Langevin dynamics, MCMC sampling</li> <li>Autoregressive Models: PixelCNN, WaveNet, Transformer-based</li> <li> <p>Geometric Models: Point clouds, meshes, SE(3) molecular flows</p> </li> <li> <p>Multi-Modal Support</p> </li> <li>Image: Convolutional architectures, quality metrics</li> <li>Text: Tokenization, language modeling</li> <li>Audio: Spectral processing, WaveNet</li> <li>Protein: Structure generation with physical constraints</li> <li>Tabular: Mixed data types</li> <li>Timeseries: Sequential patterns</li> <li>Molecular: Chemical structure generation</li> <li> <p>Geometric: Point clouds, meshes, voxels</p> </li> <li> <p>Infrastructure</p> </li> <li>Unified frozen dataclass configuration system</li> <li>Protocol-based architecture</li> <li>Factory pattern for model creation</li> <li>GPU/CPU device management</li> <li>Composable loss functions</li> </ul>"},{"location":"development/roadmap/#in-progress","title":"In Progress","text":"<ul> <li>Test Suite Restructuring</li> <li>Reorganizing test hierarchy</li> <li>Improving test isolation</li> <li>Updating test fixtures</li> <li> <p>Coverage reporting improvements</p> </li> <li> <p>API Stabilization</p> </li> <li>Consistent method signatures</li> <li>Clear public/private boundaries</li> <li>Improved error messages</li> <li> <p>Type annotation completion</p> </li> <li> <p>Documentation Updates</p> </li> <li>Syncing docs with code changes</li> <li>Updating code examples</li> <li>API reference refresh</li> </ul>"},{"location":"development/roadmap/#planned-features","title":"Planned Features","text":"<ul> <li>Performance Optimizations</li> <li>JIT compilation improvements</li> <li>Memory-efficient attention</li> <li>Gradient checkpointing</li> <li> <p>Mixed precision training</p> </li> <li> <p>Additional Model Variants</p> </li> <li>Consistency models</li> <li>Flow matching</li> <li>Rectified flows</li> <li> <p>Additional transformer architectures</p> </li> <li> <p>Extended Modality Support</p> </li> <li>Video generation</li> <li>3D scene understanding</li> <li> <p>Multi-modal alignment</p> </li> <li> <p>Training Improvements</p> </li> <li>Advanced learning rate schedules</li> <li>Automatic hyperparameter tuning</li> <li> <p>Training visualization dashboard</p> </li> <li> <p>Scaling Features</p> </li> <li>Multi-GPU training support</li> <li>TPU compatibility</li> <li>Gradient accumulation</li> <li>Memory optimization</li> </ul>"},{"location":"development/roadmap/#version-milestones","title":"Version Milestones","text":""},{"location":"development/roadmap/#v01x-current-refactoring","title":"v0.1.x (Current - Refactoring)","text":"<p>Focus: Architecture refactoring and API stabilization</p> <ul> <li>Restructuring codebase for better modularity</li> <li>Stabilizing public APIs</li> <li>Improving test coverage and reliability</li> <li>Updating documentation</li> </ul>"},{"location":"development/roadmap/#v02x-planned","title":"v0.2.x (Planned)","text":"<p>Focus: Stability and performance</p> <ul> <li>Stable, documented APIs</li> <li>Multi-GPU support</li> <li>Memory optimizations</li> <li>Extended benchmark suite</li> <li>Performance profiling tools</li> </ul>"},{"location":"development/roadmap/#v03x-planned","title":"v0.3.x (Planned)","text":"<p>Focus: Advanced features</p> <ul> <li>Additional model architectures</li> <li>Video and multi-modal support</li> <li>Advanced fine-tuning methods</li> <li>Production deployment tools</li> </ul>"},{"location":"development/roadmap/#contributing","title":"Contributing","text":"<p>We welcome contributions in all areas. Priority areas during the refactoring period:</p> <ol> <li>Testing: Helping stabilize and expand test coverage</li> <li>Documentation: Keeping docs in sync with code changes</li> <li>Bug Reports: Reporting issues encountered during refactoring</li> <li>Code Review: Reviewing refactoring PRs</li> </ol> <p>See the GitHub Issues for current tasks and feature requests.</p>"},{"location":"development/roadmap/#see-also","title":"See Also","text":"<ul> <li>Design Philosophy - Development principles</li> <li>Testing Guide - How to run and write tests</li> </ul>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>Artifex follows test-driven development practices with comprehensive test coverage.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-commands","title":"Basic Test Commands","text":"<pre><code># Run all tests\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/path/to/test_file.py -xvs\n\n# Run single test\nuv run pytest tests/path/to/test_file.py::TestClass::test_method -xvs\n\n# Run with coverage\nuv run pytest --cov=src/artifex --cov-report=html\n\n# Run all tests with JSON report\nuv run pytest -vv --json-report --json-report-file=temp/test-results.json \\\n    --json-report-indent=2 --json-report-verbosity=2 \\\n    --cov=src/ --cov-report=json:temp/coverage.json --cov-report=term-missing\n</code></pre>"},{"location":"development/testing/#gpu-aware-testing","title":"GPU-Aware Testing","text":"<pre><code># Use smart test runner for automatic GPU/CPU routing\n./scripts/smart_test_runner.sh tests/ -v\n\n# Run GPU-specific tests only (requires CUDA)\nuv run pytest -m gpu\n\n# Run expensive BlackJAX tests (optional)\nuv run pytest -m blackjax\n</code></pre>"},{"location":"development/testing/#test-organization","title":"Test Organization","text":""},{"location":"development/testing/#directory-structure","title":"Directory Structure","text":"<p>Tests mirror the source structure:</p> <pre><code>tests/\n\u251c\u2500\u2500 standalone/          # Isolated component tests\n\u2502   \u2514\u2500\u2500 artifex/\n\u2502       \u2514\u2500\u2500 generative_models/\n\u2502           \u251c\u2500\u2500 models/\n\u2502           \u2502   \u251c\u2500\u2500 vae/\n\u2502           \u2502   \u251c\u2500\u2500 gan/\n\u2502           \u2502   \u2514\u2500\u2500 ...\n\u2502           \u2514\u2500\u2500 core/\n\u2514\u2500\u2500 artifex/            # Integrated system tests\n    \u2514\u2500\u2500 generative_models/\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":"<ul> <li>Unit tests: Test individual functions and classes</li> <li>Integration tests: Test component interactions</li> <li>GPU tests: Marked with <code>@pytest.mark.gpu</code>, automatically skipped on CPU-only systems</li> <li>BlackJAX tests: Marked with <code>@pytest.mark.blackjax</code>, expensive MCMC tests</li> </ul>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#basic-test-structure","title":"Basic Test Structure","text":"<pre><code>import pytest\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration import VAEConfig, EncoderConfig\nfrom artifex.generative_models.factory import create_model\n\n\nclass TestVAE:\n    \"\"\"Test suite for VAE model.\"\"\"\n\n    def test_forward_pass(self):\n        \"\"\"Test basic forward pass.\"\"\"\n        rngs = nnx.Rngs(params=42, dropout=42, sample=42)\n\n        encoder_config = EncoderConfig(\n            name=\"encoder\",\n            input_shape=(28, 28, 1),\n            latent_dim=32,\n            hidden_dims=(64, 32),\n            activation=\"relu\",\n        )\n\n        config = VAEConfig(\n            name=\"test_vae\",\n            encoder=encoder_config,\n            decoder=decoder_config,\n            kl_weight=1.0,\n        )\n\n        model = create_model(config, rngs=rngs)\n        x = jax.random.normal(jax.random.key(0), (4, 28, 28, 1))\n\n        outputs = model(x, rngs=rngs)\n\n        assert outputs.reconstruction.shape == x.shape\n        assert outputs.z.shape == (4, 32)\n</code></pre>"},{"location":"development/testing/#gpu-test-marking","title":"GPU Test Marking","text":"<pre><code>import pytest\n\n@pytest.mark.gpu\ndef test_gpu_operation():\n    \"\"\"Test that requires GPU.\"\"\"\n    # This test is skipped on CPU-only systems\n    ...\n</code></pre>"},{"location":"development/testing/#using-device-fixtures","title":"Using Device Fixtures","text":"<pre><code>def test_with_device(device):\n    \"\"\"Test with automatic GPU/CPU selection.\"\"\"\n    # 'device' fixture automatically selects GPU if available\n    ...\n</code></pre>"},{"location":"development/testing/#test-best-practices","title":"Test Best Practices","text":""},{"location":"development/testing/#dos","title":"Do's","text":"<ol> <li>Test behavior, not implementation: Focus on what the code does, not how</li> <li>Use specific assertions: Compare specific properties, not entire objects</li> <li>Provide meaningful test names: Describe what's being tested</li> <li>Clean up GPU memory: Use <code>device_manager.cleanup()</code> when needed</li> <li>Use fixtures for common setup: Avoid duplicating initialization code</li> </ol>"},{"location":"development/testing/#donts","title":"Don'ts","text":"<ol> <li>Don't compare complex objects with <code>==</code>: Use specific property comparisons</li> <li>Don't modify tests to accommodate bugs: Fix the implementation instead</li> <li>Don't skip tests without documentation: Mark with reason</li> <li>Don't use random seeds without purpose: Be explicit about reproducibility</li> </ol>"},{"location":"development/testing/#assertion-patterns","title":"Assertion Patterns","text":"<pre><code># CORRECT: Compare specific properties\nassert model.config.name == \"expected_name\"\nassert outputs.reconstruction.shape == expected_shape\nassert jnp.allclose(outputs.loss, expected_loss, rtol=1e-5)\n\n# WRONG: Compare entire objects\nassert model.config == expected_config  # May fail unexpectedly\nassert outputs == expected_outputs      # Too broad\n</code></pre>"},{"location":"development/testing/#code-coverage","title":"Code Coverage","text":""},{"location":"development/testing/#running-coverage-reports","title":"Running Coverage Reports","text":"<pre><code># Generate HTML coverage report\nuv run pytest --cov=src/artifex --cov-report=html\n\n# View report\nopen htmlcov/index.html\n</code></pre>"},{"location":"development/testing/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>Minimum 80% coverage for new code</li> <li>Critical paths should have 100% coverage</li> <li>Document any intentionally uncovered code</li> </ul>"},{"location":"development/testing/#pre-commit-testing","title":"Pre-Commit Testing","text":"<p>Before committing, ensure tests pass:</p> <pre><code># Run all quality checks\nuv run pre-commit run --all-files\n\n# Quick test run\nuv run pytest tests/ -x  # Stop on first failure\n</code></pre>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"development/testing/#verbose-output","title":"Verbose Output","text":"<pre><code># Maximum verbosity\nuv run pytest tests/path/to/test.py -xvs --tb=long\n\n# Show local variables on failure\nuv run pytest --tb=long --showlocals\n</code></pre>"},{"location":"development/testing/#running-single-tests","title":"Running Single Tests","text":"<pre><code># Run specific test method\nuv run pytest tests/path/to/test.py::TestClass::test_method -xvs\n\n# Run tests matching pattern\nuv run pytest -k \"vae and forward\" -xvs\n</code></pre>"},{"location":"development/testing/#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Architecture overview</li> <li>Design Philosophy - Testing philosophy</li> </ul>"},{"location":"examples/","title":"All Examples","text":""},{"location":"examples/#quick-start","title":"Quick Start","text":"VAE on MNIST Beginner <p>Train a Variational Autoencoder on MNIST digits</p> VAE Image MNIST \u23f1 10 min GAN on MNIST Beginner <p>Train a Generative Adversarial Network on MNIST</p> GAN DCGAN MNIST \u23f1 15 min Diffusion on MNIST Beginner <p>Train a DDPM diffusion model on MNIST digits</p> Diffusion DDPM MNIST \u23f1 15 min Diffusion MNIST Training Beginner <p>Complete training workflow for diffusion models on MNIST</p> Diffusion Training MNIST \u23f1 20 min Flow on MNIST Beginner <p>Train a normalizing flow model on MNIST</p> Flow RealNVP MNIST \u23f1 10 min"},{"location":"examples/#diffusion-models","title":"Diffusion Models","text":"Simple Diffusion Intermediate <p>Introduction to diffusion models with DDPM</p> DDPM Denoising Image \u23f1 20 min DiT Demo Advanced <p>Scalable diffusion with Diffusion Transformers</p> DiT Transformer Scalable \u23f1 30 min"},{"location":"examples/#advanced-tutorials","title":"Advanced Tutorials","text":"Advanced Training Pipeline Intermediate <p>Production-ready training with optimizers, schedulers, and checkpointing</p> Training Optimizer Scheduler Checkpointing \u23f1 2 min Advanced VAE Advanced <p>\u03b2-VAE, VQ-VAE, and disentanglement techniques</p> \u03b2-VAE VQ-VAE Disentanglement \u23f1 45 min Advanced GAN Advanced <p>WGAN, StyleGAN, and progressive training</p> WGAN StyleGAN Progressive \u23f1 45 min Advanced Diffusion Advanced <p>DDIM, latent diffusion, and guidance techniques</p> DDIM Latent Guidance \u23f1 50 min Advanced Flow Advanced <p>Neural spline flows, MAF, and IAF architectures</p> Spline MAF IAF \u23f1 40 min"},{"location":"examples/#specialized-models","title":"Specialized Models","text":"Energy-Based Model Advanced <p>EBM training with MCMC and Langevin sampling</p> EBM MCMC Langevin \u23f1 25 min Audio Generation Intermediate <p>Generate audio with generative models</p> Audio WaveNet Spectrogram \u23f1 30 min Simple Text Generation Beginner <p>Character-level text generation with temperature sampling</p> Text Character-Level RNN Temperature \u23f1 5 min Simple Image-Text Multimodal Intermediate <p>Multimodal learning with image and text encoders</p> Multimodal Image Text Retrieval \u23f1 10 min Geometric Benchmark Advanced <p>Geometric generative model benchmarking</p> 3D Point Cloud Mesh \u23f1 45 min"},{"location":"examples/#geometric-models","title":"Geometric Models","text":"Geometric Models Demo Beginner <p>Quick reference for point clouds, meshes, and voxels</p> Point Cloud Mesh Voxel \u23f1 5 min Geometric Losses Demo Intermediate <p>Loss functions for point clouds, meshes, and voxels</p> Loss Functions Point Cloud Mesh Voxel \u23f1 15 min Simple Point Cloud Beginner <p>Generate and visualize 3D point clouds with transformers</p> Point Cloud Transformer 3D \u23f1 10 min Geometric Benchmark Advanced <p>Comprehensive evaluation on geometric tasks</p> Benchmark ShapeNet Metrics \u23f1 45 min"},{"location":"examples/#protein-modeling","title":"Protein Modeling","text":"Protein Diffusion Advanced <p>Generate 3D protein structures with diffusion models</p> Diffusion 3D Generation Constraints Visualization \u23f1 5 min Protein Tech Validation Beginner <p>Validate your environment for protein diffusion modeling</p> Validation Setup JAX \u23f1 5 min Protein Extensions Intermediate <p>Add domain-specific constraints with protein extensions</p> Extensions Constraints Bonds \u23f1 10 min Protein Model Extension Intermediate <p>Combine point cloud models with protein extensions</p> Protein Extensions Point Cloud \u23f1 10 min Protein Model with Modality Intermediate <p>Using the modality architecture for protein models</p> Protein Modality Factory \u23f1 10 min Protein Point Cloud Intermediate <p>Point cloud modeling with geometric constraints</p> Protein Point Cloud Constraints \u23f1 15 min Protein Extensions Intermediate <p>Using protein extensions with configuration system</p> Extensions Config Protein \u23f1 10 min Protein-Ligand Benchmark Advanced <p>Protein-ligand binding site generation</p> Protein SE(3) Equivariant \u23f1 60 min"},{"location":"examples/#framework-techniques","title":"Framework &amp; Techniques","text":"BlackJAX Integration Intermediate <p>MCMC sampling with BlackJAX: HMC, NUTS, and MALA algorithms</p> MCMC HMC NUTS BlackJAX \u23f1 15 min BlackJAX Sampling Examples Advanced <p>Compare HMC, MALA, NUTS samplers and direct BlackJAX API usage</p> MCMC Comparison Performance \u23f1 20 min BlackJAX Integration Examples Advanced <p>Direct API vs functional API: progress bars, JIT compilation, and performance</p> Integration Performance JIT \u23f1 25 min Loss Functions Intermediate <p>Comprehensive guide to Artifex loss functions</p> Losses KL Adversarial \u23f1 20 min Framework Features Intermediate <p>Explore Artifex's architectural patterns</p> Architecture Patterns Framework \u23f1 30 min \u03b2-VAE Benchmark Advanced <p>Compare \u03b2-VAE configurations and disentanglement</p> \u03b2-VAE Benchmark Tuning \u23f1 40 min"},{"location":"examples/#reference-tables","title":"Reference Tables","text":"\ud83d\udcca Browse by Category \ud83d\ude80 Getting Started <p>New to Artifex? Start with the beginner examples on MNIST.</p> \ud83d\udcd6 Learn More <p>Check the Examples Overview for detailed guidance.</p> \ud83e\udd1d Contribute <p>Share your examples! See the Contributing Guide.</p>"},{"location":"examples/#by-model-type","title":"By Model Type","text":"Model Examples Level VAE Basic, Advanced, \u03b2-VAE Benchmark \u2b50 - \u2b50\u2b50\u2b50 GAN Basic, Advanced \u2b50 - \u2b50\u2b50\u2b50 Diffusion Basic, Training, Simple, DiT, Advanced \u2b50 - \u2b50\u2b50\u2b50 Flow Basic, Advanced \u2b50 - \u2b50\u2b50\u2b50 EBM Simple EBM \u2b50\u2b50\u2b50 Text Simple Text Generation \u2b50 Multimodal Image-Text \u2b50\u2b50 Protein Modality, Point Cloud, Extensions, Ligand \u2b50\u2b50 - \u2b50\u2b50\u2b50 Geometric Benchmark \u2b50\u2b50\u2b50"},{"location":"examples/#by-dataset","title":"By Dataset","text":"Dataset Examples MNIST VAE \u00b7 GAN \u00b7 Diffusion \u00b7 Diffusion Training \u00b7 Flow Text Simple Text Generation Multimodal Image-Text Audio Audio Generation Protein/3D Modality \u00b7 Point Cloud \u00b7 Extensions \u00b7 Ligand \u00b7 Geometric"},{"location":"examples/#by-topic","title":"By Topic","text":"<p>Disentanglement \u00b7 Advanced VAE \u00b7 \u03b2-VAE Benchmark</p> <p>MCMC/Sampling \u00b7 BlackJAX Integration \u00b7 BlackJAX Sampling Examples \u00b7 BlackJAX Integration Examples \u00b7 EBM</p> <p>Transformers \u00b7 DiT Demo</p> <p>Equivariance \u00b7 Protein-Ligand \u00b7 Geometric</p> <p>Benchmarking \u00b7 \u03b2-VAE \u00b7 Protein-Ligand \u00b7 Geometric</p> <p>Configuration \u00b7 Protein Extensions \u00b7 Framework Features</p> <p>Point Clouds \u00b7 Protein Point Cloud \u00b7 Geometric</p> <p>Loss Functions \u00b7 Loss Examples</p> <p>Text Generation \u00b7 Simple Text Generation</p> <p>Multimodal Learning \u00b7 Image-Text</p>"},{"location":"examples/overview/","title":"Artifex Framework Examples","text":"<p>This directory contains comprehensive examples demonstrating how to use the Artifex framework for generative modeling with JAX/Flax.</p>"},{"location":"examples/overview/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"examples/overview/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Artifex with all dependencies\nuv sync --all-extras\n\n# For CPU-only execution (recommended for testing)\nexport JAX_PLATFORMS=cpu\n</code></pre>"},{"location":"examples/overview/#running-examples","title":"Running Examples","text":"<pre><code># Basic examples\npython examples/generative_models/diffusion/simple_diffusion_example.py\npython examples/generative_models/vae/vae_mnist.py\npython examples/generative_models/gan/simple_gan.py\n\n# Framework features demonstration\npython examples/generative_models/framework_features_demo.py\n</code></pre>"},{"location":"examples/overview/#directory-structure","title":"\ud83d\udcc1 Directory Structure","text":"<pre><code>examples/\n\u251c\u2500\u2500 README.md                              # This file\n\u251c\u2500\u2500 generative_models/                     # All generative modeling examples\n\u2502   \u251c\u2500\u2500 audio/                            # Audio generation examples\n\u2502   \u251c\u2500\u2500 config/                           # Configuration examples\n\u2502   \u251c\u2500\u2500 deployments/                      # Model deployment examples\n\u2502   \u251c\u2500\u2500 diffusion/                        # Diffusion model examples\n\u2502   \u2502   \u2514\u2500\u2500 simple_diffusion_example.py   # Basic diffusion implementation\n\u2502   \u251c\u2500\u2500 distributed/                      # Distributed training examples\n\u2502   \u251c\u2500\u2500 energy/                            # Energy-Based Model examples\n\u2502   \u2502   \u2514\u2500\u2500 simple_ebm_example.py         # Basic EBM implementation\n\u2502   \u251c\u2500\u2500 geometric/                        # Geometric modeling examples\n\u2502   \u2502   \u251c\u2500\u2500 geometric_benchmark_demo.py   # Benchmark suite\n\u2502   \u2502   \u251c\u2500\u2500 geometric_losses_demo.py      # Specialized loss functions\n\u2502   \u2502   \u251c\u2500\u2500 geometric_models_demo.py      # Model architectures\n\u2502   \u2502   \u2514\u2500\u2500 simple_point_cloud_example.py # Point cloud processing\n\u2502   \u251c\u2500\u2500 image/                            # Image generation examples\n\u2502   \u2502   \u251c\u2500\u2500 diffusion/                    # Image diffusion models\n\u2502   \u2502   \u251c\u2500\u2500 gan/                          # GAN models\n\u2502   \u2502   \u2514\u2500\u2500 vae/                          # VAE models\n\u2502   \u251c\u2500\u2500 multimodal/                       # Multi-modal examples\n\u2502   \u251c\u2500\u2500 optimization/                     # Optimization examples\n\u2502   \u251c\u2500\u2500 protein/                          # Protein modeling examples\n\u2502   \u2502   \u251c\u2500\u2500 protein_diffusion_example.py  # Comprehensive protein diffusion\n\u2502   \u2502   \u251c\u2500\u2500 protein_diffusion_tech_validation.py # Technical validation\n\u2502   \u2502   \u251c\u2500\u2500 protein_extensions_example.py # Extension mechanisms\n\u2502   \u2502   \u251c\u2500\u2500 protein_extensions_with_config.py # Config-driven modeling\n\u2502   \u2502   \u251c\u2500\u2500 protein_ligand_benchmark_demo.py # Protein-ligand interactions\n\u2502   \u2502   \u251c\u2500\u2500 protein_model_extension.py    # Model extensions\n\u2502   \u2502   \u251c\u2500\u2500 protein_model_with_modality.py # Multi-modal protein modeling\n\u2502   \u2502   \u2514\u2500\u2500 protein_point_cloud_example.py # Point cloud representations\n\u2502   \u251c\u2500\u2500 sampling/                         # Advanced sampling techniques\n\u2502   \u2502   \u2514\u2500\u2500 blackjax_example.py          # MCMC sampling with BlackJAX\n\u2502   \u251c\u2500\u2500 text/                             # Text generation examples\n\u2502   \u251c\u2500\u2500 vae/                              # VAE examples\n\u2502   \u2502   \u2514\u2500\u2500 multi_beta_vae_benchmark_demo.py # \u03b2-VAE benchmarking\n\u2502   \u2514\u2500\u2500 loss_examples.py                  # Comprehensive loss functions\n\u251c\u2500\u2500 utils/                                 # Utility modules\n\u2502   \u2514\u2500\u2500 demo_utils.py                     # Shared helper functions\n\u2514\u2500\u2500 tests/                                 # Example-specific tests\n</code></pre>"},{"location":"examples/overview/#featured-examples","title":"\ud83c\udfaf Featured Examples","text":""},{"location":"examples/overview/#protein-modeling","title":"Protein Modeling","text":""},{"location":"examples/overview/#generative_modelsproteinprotein_diffusion_examplepy","title":"<code>generative_models/protein/protein_diffusion_example.py</code>","text":"<p>Comprehensive protein diffusion modeling example - Our flagship example showcasing both high-level API and direct model approaches for protein structure generation.</p> <ul> <li>High-level API with extensions: Demonstrates protein-specific extensions and constraints</li> <li>Direct model creation: Shows how to create and manipulate models directly</li> <li>Visualization &amp; quality assessment: Complete pipeline from generation to analysis</li> <li>Size matching fixes: Robust handling of different structure sizes</li> </ul>"},{"location":"examples/overview/#generative_modelsproteinprotein_extensions_with_configpy","title":"<code>generative_models/protein/protein_extensions_with_config.py</code>","text":"<p>Configuration-driven protein modeling with extension system.</p>"},{"location":"examples/overview/#generative_modelsproteinprotein_ligand_benchmark_demopy","title":"<code>generative_models/protein/protein_ligand_benchmark_demo.py</code>","text":"<p>Benchmark demonstrations for protein-ligand interaction modeling.</p>"},{"location":"examples/overview/#generative_modelsproteinprotein_point_cloud_examplepy","title":"<code>generative_models/protein/protein_point_cloud_example.py</code>","text":"<p>Point cloud representations for protein structure modeling.</p>"},{"location":"examples/overview/#geometric-modeling","title":"Geometric Modeling","text":""},{"location":"examples/overview/#generative_modelsgeometricgeometric_benchmark_demopy","title":"<code>generative_models/geometric/geometric_benchmark_demo.py</code>","text":"<p>Comprehensive benchmark suite for geometric generative models.</p>"},{"location":"examples/overview/#generative_modelsgeometricgeometric_losses_demopy","title":"<code>generative_models/geometric/geometric_losses_demo.py</code>","text":"<p>Specialized loss functions for geometric data.</p>"},{"location":"examples/overview/#generative_modelsgeometricgeometric_models_demopy","title":"<code>generative_models/geometric/geometric_models_demo.py</code>","text":"<p>Various geometric model architectures and applications.</p>"},{"location":"examples/overview/#generative_modelsgeometricsimple_point_cloud_examplepy","title":"<code>generative_models/geometric/simple_point_cloud_example.py</code>","text":"<p>Basic point cloud generation and processing.</p>"},{"location":"examples/overview/#advanced-sampling","title":"Advanced Sampling","text":""},{"location":"examples/overview/#generative_modelssamplingblackjax_examplepy","title":"<code>generative_models/sampling/blackjax_example.py</code>","text":"<p>MCMC sampling integration - Advanced sampling techniques using BlackJAX.</p> <ul> <li>HMC (Hamiltonian Monte Carlo)</li> <li>NUTS (No-U-Turn Sampler)</li> <li>MALA (Metropolis-Adjusted Langevin Algorithm)</li> </ul>"},{"location":"examples/overview/#energy-based-models","title":"Energy-Based Models","text":""},{"location":"examples/overview/#generative_modelsenergysimple_ebm_examplepy","title":"<code>generative_models/energy/simple_ebm_example.py</code>","text":"<p>Energy-Based Model implementation - Comprehensive example of EBMs with MCMC sampling.</p> <ul> <li>Energy function computation</li> <li>Langevin dynamics sampling</li> <li>Contrastive divergence training</li> <li>Persistent contrastive divergence with sample buffers</li> <li>Deep EBM architectures</li> </ul>"},{"location":"examples/overview/#diffusion-models","title":"Diffusion Models","text":""},{"location":"examples/overview/#generative_modelsdiffusionsimple_diffusion_examplepy","title":"<code>generative_models/diffusion/simple_diffusion_example.py</code>","text":"<p>Basic diffusion model implementation - Simple example demonstrating diffusion model fundamentals.</p> <ul> <li>SimpleDiffusionModel creation and configuration</li> <li>Noise schedule setup (beta schedules)</li> <li>Sample generation from noise</li> <li>Visualization of generated samples</li> </ul>"},{"location":"examples/overview/#generative_modelsdiffusiondit_demopy","title":"<code>generative_models/diffusion/dit_demo.py</code>","text":"<p>Diffusion Transformer (DiT) demonstration - Advanced diffusion model using transformer architecture.</p> <ul> <li>DiffusionTransformer backbone architecture</li> <li>DiT model sizes (S, B, L, XL configurations)</li> <li>Conditional generation with classifier-free guidance (CFG)</li> <li>Patch-based image processing</li> <li>Performance benchmarking across model sizes</li> <li>Sample generation and visualization</li> </ul>"},{"location":"examples/overview/#vae-models","title":"VAE Models","text":""},{"location":"examples/overview/#generative_modelsvaemulti_beta_vae_benchmark_demopy","title":"<code>generative_models/vae/multi_beta_vae_benchmark_demo.py</code>","text":"<p>Multi-scale \u03b2-VAE benchmarking and evaluation.</p>"},{"location":"examples/overview/#image-generation","title":"Image Generation","text":"<p>The <code>generative_models/image/</code> directory contains comprehensive examples for:</p> <ul> <li>Diffusion models: DDPM, LDM for various datasets</li> <li>GANs: StyleGAN, CycleGAN implementations</li> <li>VAEs: VQ-VAE, standard VAE for image generation</li> </ul>"},{"location":"examples/overview/#text-generation","title":"Text Generation","text":"<p>The <code>generative_models/text/</code> directory includes:</p> <ul> <li>Transformers: Language models and fine-tuning</li> <li>Compression: VQ-VAE for text</li> </ul>"},{"location":"examples/overview/#quick-start_1","title":"\ud83d\ude80 Quick Start","text":""},{"location":"examples/overview/#basic-usage","title":"Basic Usage","text":"<pre><code># Run a simple diffusion example\npython examples/generative_models/diffusion/simple_diffusion_example.py\n\n# Run Energy-Based Model example\npython examples/generative_models/energy/simple_ebm_example.py\n\n# Run protein diffusion (comprehensive)\npython examples/generative_models/protein/protein_diffusion_example.py\n\n# Run BlackJAX sampling\npython examples/generative_models/sampling/blackjax_example.py\n\n# Run geometric benchmarks\npython examples/generative_models/geometric/geometric_benchmark_demo.py\n</code></pre>"},{"location":"examples/overview/#prerequisites_1","title":"Prerequisites","text":"<p>Make sure you have activated the Artifex environment:</p> <pre><code>source ./activate.sh\n</code></pre>"},{"location":"examples/overview/#gpu-support","title":"GPU Support","text":"<p>Most examples are optimized for GPU usage and will automatically use CUDA when available:</p> <pre><code>import jax\nprint(f\"Backend: {jax.default_backend()}\")  # Should show 'gpu' if CUDA is available\n</code></pre>"},{"location":"examples/overview/#example-categories","title":"\ud83d\udd27 Example Categories","text":""},{"location":"examples/overview/#protein-modeling_1","title":"\ud83e\uddec Protein Modeling","text":"<ul> <li>Structure generation and prediction</li> <li>Physical constraints and validation</li> <li>Multi-modal protein representations</li> <li>Benchmarking and evaluation</li> <li>Located in: <code>generative_models/protein/</code></li> </ul>"},{"location":"examples/overview/#geometric-modeling_1","title":"\ud83d\udcd0 Geometric Modeling","text":"<ul> <li>Point cloud generation</li> <li>3D shape modeling</li> <li>Geometric loss functions</li> <li>Benchmark suites</li> <li>Located in: <code>generative_models/geometric/</code></li> </ul>"},{"location":"examples/overview/#advanced-sampling_1","title":"\ud83c\udfb2 Advanced Sampling","text":"<ul> <li>MCMC methods (HMC, NUTS, MALA)</li> <li>Custom sampling strategies</li> <li>Convergence analysis</li> <li>Located in: <code>generative_models/sampling/</code></li> </ul>"},{"location":"examples/overview/#energy-based-models_1","title":"\u26a1 Energy-Based Models","text":"<ul> <li>Energy function learning</li> <li>MCMC sampling techniques</li> <li>Contrastive divergence training</li> <li>Persistent sample buffers</li> <li>Located in: <code>generative_models/energy/</code></li> </ul>"},{"location":"examples/overview/#diffusion-models_1","title":"\ud83c\udf0a Diffusion Models","text":"<ul> <li>DDPM implementations</li> <li>Custom noise schedules</li> <li>Conditional generation</li> <li>Located in: <code>generative_models/diffusion/</code></li> </ul>"},{"location":"examples/overview/#image-generation_1","title":"\ud83d\uddbc\ufe0f Image Generation","text":"<ul> <li>Diffusion models for images</li> <li>GAN architectures</li> <li>VAE variants</li> <li>Located in: <code>generative_models/image/</code></li> </ul>"},{"location":"examples/overview/#text-generation_1","title":"\ud83d\udcdd Text Generation","text":"<ul> <li>Transformer models</li> <li>Text compression</li> <li>Language modeling</li> <li>Located in: <code>generative_models/text/</code></li> </ul>"},{"location":"examples/overview/#benchmarking","title":"\ud83d\udcca Benchmarking","text":"<ul> <li>Performance evaluation</li> <li>Model comparison</li> <li>Metrics computation</li> <li>Distributed across relevant directories</li> </ul>"},{"location":"examples/overview/#usage-tips","title":"\ud83d\udca1 Usage Tips","text":""},{"location":"examples/overview/#running-examples_1","title":"Running Examples","text":"<ol> <li>Environment Setup:</li> </ol> <pre><code>source ./activate.sh  # Activate Artifex environment\n</code></pre> <ol> <li>Navigate to Category:</li> </ol> <pre><code>cd examples/generative_models/protein/  # For protein examples\ncd examples/generative_models/geometric/  # For geometric examples\n# etc.\n</code></pre> <ol> <li>Run Example:</li> </ol> <pre><code>python protein_diffusion_example.py\n</code></pre>"},{"location":"examples/overview/#gpu-optimization","title":"GPU Optimization","text":"<p>Examples automatically detect and use GPU when available. Check with:</p> <pre><code>import jax\nprint(jax.devices())  # Should show CUDA devices\n</code></pre>"},{"location":"examples/overview/#modifying-examples","title":"Modifying Examples","text":"<p>All examples are designed to be educational and modifiable:</p> <ul> <li>Configuration: Most examples have clear configuration sections at the top</li> <li>Modularity: Functions are well-separated for easy customization</li> <li>Documentation: Comprehensive docstrings explain each component</li> </ul>"},{"location":"examples/overview/#integration-with-artifex","title":"Integration with Artifex","text":"<p>Examples demonstrate integration with Artifex's core components:</p> <ul> <li>Models: <code>artifex.generative_models.models.*</code></li> <li>Factory: <code>artifex.generative_models.factory.*</code></li> <li>Config: <code>artifex.generative_models.core.configuration.*</code></li> <li>Modalities: <code>artifex.generative_models.modalities.*</code></li> <li>Extensions: <code>artifex.generative_models.extensions.*</code></li> </ul>"},{"location":"examples/overview/#testing-examples","title":"\ud83e\uddea Testing Examples","text":"<p>To verify examples work correctly:</p> <pre><code># Test imports\npython -c \"\nfrom examples.generative_models.diffusion import simple_diffusion_example\nfrom examples.generative_models.protein import protein_diffusion_example\nprint('\u2705 Basic imports work')\n\"\n\n# Run specific examples\npython examples/generative_models/diffusion/simple_diffusion_example.py\npython examples/generative_models/protein/protein_diffusion_example.py\n</code></pre>"},{"location":"examples/overview/#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Main Documentation: See the main README.md in the repository root</li> <li>API Reference: Check <code>docs/</code> directory</li> <li>Configuration: See <code>src/artifex/generative_models/core/configuration/</code></li> <li>Factory System: See <code>src/artifex/generative_models/factory/</code></li> </ul>"},{"location":"examples/overview/#best-practices","title":"\ud83d\udcdd Best Practices","text":""},{"location":"examples/overview/#configuration-management","title":"Configuration Management","text":"<p>Always use the unified configuration system:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    ModelConfig,\n    OptimizerConfig,\n    TrainingConfig\n)\n\n# Never use raw dictionaries\n# \u274c Wrong\nconfig = {\"latent_dim\": 32, \"beta\": 1.0}\n\n# \u2705 Correct\nconfig = ModelConfig(\n    name=\"my_model\",\n    model_class=\"...\",\n    parameters={\"latent_dim\": 32, \"beta\": 1.0}\n)\n</code></pre>"},{"location":"examples/overview/#factory-pattern","title":"Factory Pattern","text":"<p>Always use the factory system for model creation:</p> <pre><code>from artifex.generative_models.factory import create_model\n\n# \u274c Wrong: Direct instantiation\nmodel = VAE(config)\n\n# \u2705 Correct: Factory pattern\nmodel = create_model(config, rngs=rngs)\n</code></pre>"},{"location":"examples/overview/#rng-management","title":"RNG Management","text":"<p>Proper random number generator handling:</p> <pre><code># Create RNGs\nkey = jax.random.key(seed)\nrngs = nnx.Rngs(params=key, dropout=key)\n\n# In training loops, generate keys outside loss functions\ntrain_key, step_key = jax.random.split(train_key)\nz = jax.random.normal(step_key, shape)\n\ndef loss_fn(model):\n    # Use pre-generated z, don't call RNG here\n    output = model(z)\n    ...\n</code></pre>"},{"location":"examples/overview/#loss-composition","title":"Loss Composition","text":"<p>Use the composable loss system:</p> <pre><code>from artifex.generative_models.core.losses import (\n    CompositeLoss,\n    WeightedLoss\n)\n\nloss = CompositeLoss([\n    WeightedLoss(mse_loss, weight=1.0, name=\"reconstruction\"),\n    WeightedLoss(kl_divergence, weight=0.5, name=\"kl\")\n], return_components=True)\n\ntotal_loss, components = loss(predictions, targets)\n</code></pre>"},{"location":"examples/overview/#adding-new-examples","title":"\ud83d\udcc4 Adding New Examples","text":"<p>When creating new examples, use this template:</p> <pre><code>#!/usr/bin/env python\n\"\"\"Brief description of what this example demonstrates.\n\nThis example shows:\n- Feature 1\n- Feature 2\n\"\"\"\n\nimport jax\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration import ModelConfig\nfrom artifex.generative_models.factory import create_model\n\ndef main():\n    \"\"\"Run the example.\"\"\"\n    print(\"=\" * 60)\n    print(\"Example Name\")\n    print(\"=\" * 60)\n\n    # Configuration\n    config = ModelConfig(...)\n\n    # Create model\n    rngs = nnx.Rngs(params=jax.random.key(42))\n    model = create_model(config, rngs=rngs)\n\n    # Demonstrate functionality\n    ...\n\n    print(\"Example completed successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Checklist for new examples:</p> <ol> <li>Place in appropriate category directory</li> <li>Use configuration objects (not raw dicts)</li> <li>Include docstrings documenting purpose</li> <li>Test both CPU/GPU compatibility</li> <li>Update documentation as needed</li> </ol>"},{"location":"examples/overview/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"examples/overview/#common-issues","title":"Common Issues","text":"<ol> <li>Import Errors: Make sure you've activated the environment with <code>source ./activate.sh</code></li> <li>CUDA Issues: Run <code>python scripts/verify_gpu_setup.py</code> to diagnose GPU problems</li> <li>Memory Issues: Try reducing batch sizes in example configurations</li> <li>Module Not Found: Ensure you're running from the artifex root directory</li> </ol>"},{"location":"examples/overview/#getting-help","title":"Getting Help","text":"<ul> <li>Check example docstrings for parameter explanations</li> <li>Review the main Artifex documentation</li> <li>Look at similar examples in the same category for reference patterns</li> <li>Check the imports at the top of each example for required dependencies</li> </ul> <p>Note: Examples are continuously updated to demonstrate the latest Artifex features. The organized structure makes it easier to find relevant examples for your use case.</p>"},{"location":"examples/advanced/advanced-ar/","title":"Advanced Autoregressive Models","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on advanced autoregressive implementations.</p>"},{"location":"examples/advanced/advanced-ar/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Advanced autoregressive architectures</li> <li>Transformer-based sequence modeling</li> <li>Parallel training techniques</li> <li>Long-context generation</li> </ul>"},{"location":"examples/advanced/advanced-ar/#related-documentation","title":"Related Documentation","text":"<ul> <li>Autoregressive Concepts</li> <li>Autoregressive Guide</li> <li>Autoregressive API</li> </ul>"},{"location":"examples/advanced/advanced-diffusion/","title":"Advanced Diffusion Examples","text":"<p>This guide demonstrates advanced diffusion model techniques using Artifex and Flax NNX, including classifier-free guidance, conditioning patterns, custom noise schedules, and latent diffusion.</p>"},{"location":"examples/advanced/advanced-diffusion/#overview","title":"Overview","text":"<ul> <li> <p> Classifier-Free Guidance</p> <p>Improve sample quality without a separate classifier</p> <p> CFG</p> </li> <li> <p> Conditioning Patterns</p> <p>Conditional generation with text, class, or image inputs</p> <p> Conditioning</p> </li> <li> <p> Custom Noise Schedules</p> <p>Design noise schedules for different data types</p> <p> Noise Schedules</p> </li> <li> <p> Latent Diffusion</p> <p>Efficient diffusion in compressed latent space</p> <p> Latent Diffusion</p> </li> </ul>"},{"location":"examples/advanced/advanced-diffusion/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Artifex with all dependencies\nuv pip install \"artifex[cuda]\"  # With GPU support\n# or\nuv pip install artifex  # CPU only\n</code></pre> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nfrom artifex.generative_models.core import DeviceManager\nfrom artifex.generative_models.models.diffusion import DiffusionModel\n</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<p>Classifier-free guidance (CFG) improves sample quality by jointly training conditional and unconditional models, eliminating the need for a separate classifier.</p>"},{"location":"examples/advanced/advanced-diffusion/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[Conditional Model] --&gt; C[Weighted Combination]\n    B[Unconditional Model] --&gt; C\n    C --&gt; D[Guided Prediction]\n\n    style A fill:#e3f2fd\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n    style D fill:#e8f5e9</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#cfg-implementation","title":"CFG Implementation","text":"<pre><code>from flax import nnx\nimport jax.numpy as jnp\n\nclass ClassifierFreeGuidanceUNet(nnx.Module):\n    \"\"\"U-Net with classifier-free guidance support.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int = 3,\n        model_channels: int = 128,\n        num_classes: int = 10,\n        dropout_prob: float = 0.1,  # For unconditional training\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.model_channels = model_channels\n        self.num_classes = num_classes\n        self.dropout_prob = dropout_prob\n\n        # Time embedding\n        self.time_embed = nnx.Sequential(\n            nnx.Linear(model_channels, model_channels * 4, rngs=rngs),\n            nnx.silu,\n            nnx.Linear(model_channels * 4, model_channels * 4, rngs=rngs),\n        )\n\n        # Class embedding (with null token for unconditional)\n        self.class_embed = nnx.Embed(\n            num_embeddings=num_classes + 1,  # +1 for null class\n            features=model_channels * 4,\n            rngs=rngs,\n        )\n\n        # U-Net architecture\n        self.input_conv = nnx.Conv(\n            in_features=in_channels,\n            out_features=model_channels,\n            kernel_size=(3, 3),\n            padding=1,\n            rngs=rngs,\n        )\n\n        # Encoder blocks\n        encoder_blocks = []\n        channels = [model_channels, model_channels * 2, model_channels * 4]\n\n        for i, out_ch in enumerate(channels):\n            in_ch = model_channels if i == 0 else channels[i - 1]\n            block = self._make_resnet_block(in_ch, out_ch, rngs=rngs)\n            encoder_blocks.append(block)\n        self.encoder_blocks = nnx.List(encoder_blocks)\n\n        # Middle block\n        self.middle_block = self._make_resnet_block(\n            channels[-1],\n            channels[-1],\n            rngs=rngs,\n        )\n\n        # Decoder blocks\n        decoder_blocks = []\n        for i, out_ch in enumerate(reversed(channels[:-1]) + [model_channels]):\n            in_ch = channels[-(i + 1)]\n            block = self._make_resnet_block(in_ch * 2, out_ch, rngs=rngs)  # *2 for skip connections\n            decoder_blocks.append(block)\n        self.decoder_blocks = nnx.List(decoder_blocks)\n\n        # Output\n        self.output_conv = nnx.Sequential(\n            nnx.GroupNorm(num_groups=32, rngs=rngs),\n            nnx.silu,\n            nnx.Conv(\n                in_features=model_channels,\n                out_features=in_channels,\n                kernel_size=(3, 3),\n                padding=1,\n                rngs=rngs,\n            ),\n        )\n\n    def _make_resnet_block(\n        self,\n        in_channels: int,\n        out_channels: int,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; nnx.Module:\n        \"\"\"Create a ResNet-style block with time and class conditioning.\"\"\"\n\n        class ResNetBlock(nnx.Module):\n            def __init__(self, in_ch, out_ch, *, rngs):\n                super().__init__()\n\n                self.norm1 = nnx.GroupNorm(num_groups=32, rngs=rngs)\n                self.conv1 = nnx.Conv(\n                    in_features=in_ch,\n                    out_features=out_ch,\n                    kernel_size=(3, 3),\n                    padding=1,\n                    rngs=rngs,\n                )\n\n                self.norm2 = nnx.GroupNorm(num_groups=32, rngs=rngs)\n                self.conv2 = nnx.Conv(\n                    in_features=out_ch,\n                    out_features=out_ch,\n                    kernel_size=(3, 3),\n                    padding=1,\n                    rngs=rngs,\n                )\n\n                # Conditioning projection\n                self.cond_proj = nnx.Linear(\n                    in_features=self.model_channels * 4,\n                    out_features=out_ch,\n                    rngs=rngs,\n                )\n\n                # Skip connection\n                if in_ch != out_ch:\n                    self.skip = nnx.Conv(\n                        in_features=in_ch,\n                        out_features=out_ch,\n                        kernel_size=(1, 1),\n                        rngs=rngs,\n                    )\n                else:\n                    self.skip = None\n\n            def __call__(self, x, cond_embed):\n                h = self.norm1(x)\n                h = nnx.silu(h)\n                h = self.conv1(h)\n\n                # Add conditioning\n                cond_proj = self.cond_proj(cond_embed)[:, None, None, :]\n                h = h + cond_proj\n\n                h = self.norm2(h)\n                h = nnx.silu(h)\n                h = self.conv2(h)\n\n                # Skip connection\n                if self.skip is not None:\n                    x = self.skip(x)\n\n                return h + x\n\n        return ResNetBlock(in_channels, out_channels, rngs=rngs)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        t: jax.Array,\n        class_labels: jax.Array,\n        *,\n        deterministic: bool = False,\n        rngs: nnx.Rngs | None = None,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Forward pass with classifier-free guidance support.\n\n        Args:\n            x: Noisy input [batch, H, W, channels]\n            t: Timesteps [batch]\n            class_labels: Class labels [batch], use num_classes for unconditional\n            deterministic: If True, don't apply dropout\n            rngs: Random number generators\n\n        Returns:\n            Predicted noise [batch, H, W, channels]\n        \"\"\"\n        # Random dropout for classifier-free guidance training\n        if not deterministic and rngs is not None and \"dropout\" in rngs:\n            # Randomly replace class labels with null class\n            dropout_mask = jax.random.bernoulli(\n                rngs.dropout(),\n                self.dropout_prob,\n                shape=class_labels.shape,\n            )\n            class_labels = jnp.where(dropout_mask, self.num_classes, class_labels)\n\n        # Time embedding\n        t_embed = self.time_embed(self._timestep_embedding(t))\n\n        # Class embedding\n        c_embed = self.class_embed(class_labels)\n\n        # Combined conditioning\n        cond_embed = t_embed + c_embed\n\n        # U-Net forward pass\n        h = self.input_conv(x)\n\n        # Encoder with skip connections\n        skip_connections = []\n        for block in self.encoder_blocks:\n            h = block(h, cond_embed)\n            skip_connections.append(h)\n\n            # Downsample\n            batch, height, width, channels = h.shape\n            h = jax.image.resize(h, (batch, height // 2, width // 2, channels), method=\"bilinear\")\n\n        # Middle\n        h = self.middle_block(h, cond_embed)\n\n        # Decoder with skip connections\n        for i, block in enumerate(self.decoder_blocks):\n            # Upsample\n            batch, height, width, channels = h.shape\n            h = jax.image.resize(h, (batch, height * 2, width * 2, channels), method=\"nearest\")\n\n            # Concatenate skip connection\n            skip = skip_connections[-(i + 1)]\n            h = jnp.concatenate([h, skip], axis=-1)\n\n            h = block(h, cond_embed)\n\n        # Output\n        return self.output_conv(h)\n\n    def _timestep_embedding(self, timesteps: jax.Array, dim: int = None) -&gt; jax.Array:\n        \"\"\"Create sinusoidal timestep embeddings.\"\"\"\n        if dim is None:\n            dim = self.model_channels\n\n        half_dim = dim // 2\n        emb = jnp.log(10000) / (half_dim - 1)\n        emb = jnp.exp(jnp.arange(half_dim) * -emb)\n        emb = timesteps[:, None] * emb[None, :]\n        emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis=-1)\n\n        return emb\n\n\ndef sample_with_cfg(\n    model: ClassifierFreeGuidanceUNet,\n    class_labels: jax.Array,\n    guidance_scale: float = 7.5,\n    num_steps: int = 50,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; jax.Array:\n    \"\"\"\n    Sample with classifier-free guidance.\n\n    Args:\n        model: Trained U-Net model\n        class_labels: Desired class labels [batch]\n        guidance_scale: Guidance strength (1.0 = no guidance, &gt;1.0 = stronger guidance)\n        num_steps: Number of diffusion steps\n        rngs: Random number generators\n\n    Returns:\n        Generated samples [batch, H, W, channels]\n    \"\"\"\n    batch_size = class_labels.shape[0]\n    image_size = 32  # Adjust based on your model\n\n    # Start from random noise\n    x = jax.random.normal(rngs.sample(), (batch_size, image_size, image_size, 3))\n\n    # Diffusion timesteps\n    timesteps = jnp.linspace(1000, 0, num_steps, dtype=jnp.int32)\n\n    for t in timesteps:\n        t_batch = jnp.full((batch_size,), t)\n\n        # Conditional prediction\n        noise_pred_cond = model(\n            x,\n            t_batch,\n            class_labels,\n            deterministic=True,\n        )\n\n        # Unconditional prediction (null class)\n        noise_pred_uncond = model(\n            x,\n            t_batch,\n            jnp.full_like(class_labels, model.num_classes),  # null class\n            deterministic=True,\n        )\n\n        # Classifier-free guidance\n        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n\n        # DDIM update step\n        alpha_t = 1.0 - t / 1000.0\n        alpha_t_prev = 1.0 - (t - 1000 / num_steps) / 1000.0\n\n        # Predict x0\n        x0_pred = (x - jnp.sqrt(1 - alpha_t) * noise_pred) / jnp.sqrt(alpha_t)\n        x0_pred = jnp.clip(x0_pred, -1, 1)\n\n        # Update x\n        x = jnp.sqrt(alpha_t_prev) * x0_pred + jnp.sqrt(1 - alpha_t_prev) * noise_pred\n\n    return x\n</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#training-with-cfg","title":"Training with CFG","text":"<pre><code>def train_with_cfg(\n    model: ClassifierFreeGuidanceUNet,\n    train_data: jnp.ndarray,\n    train_labels: jnp.ndarray,\n    num_epochs: int = 100,\n    cfg_dropout: float = 0.1,\n):\n    \"\"\"Train model with classifier-free guidance.\"\"\"\n\n    rngs = nnx.Rngs(42)\n    optimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n    for epoch in range(num_epochs):\n        for batch_idx in range(0, len(train_data), 32):\n            batch_images = train_data[batch_idx:batch_idx + 32]\n            batch_labels = train_labels[batch_idx:batch_idx + 32]\n\n            # Random timesteps\n            t = jax.random.randint(rngs.params(), (batch_images.shape[0],), 0, 1000)\n\n            # Add noise\n            noise = jax.random.normal(rngs.params(), batch_images.shape)\n            alpha_t = 1.0 - t[:, None, None, None] / 1000.0\n            noisy_images = jnp.sqrt(alpha_t) * batch_images + jnp.sqrt(1 - alpha_t) * noise\n\n            def loss_fn(model):\n                # Predict noise with CFG dropout\n                noise_pred = model(\n                    noisy_images,\n                    t,\n                    batch_labels,\n                    deterministic=False,\n                    rngs=rngs,\n                )\n                # MSE loss\n                return jnp.mean((noise_pred - noise) ** 2)\n\n            # Compute loss and gradients, then update\n            loss, grads = nnx.value_and_grad(loss_fn)(model)\n            optimizer.update(model, grads)\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}\")\n\n            # Sample with different guidance scales\n            for scale in [1.0, 3.0, 7.5]:\n                samples = sample_with_cfg(\n                    model,\n                    jnp.array([0, 1, 2, 3]),  # Sample different classes\n                    guidance_scale=scale,\n                    rngs=rngs,\n                )\n                print(f\"  Guidance scale {scale}: samples generated\")\n\n    return model\n</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#conditioning-patterns","title":"Conditioning Patterns","text":"<p>Diffusion models can be conditioned on various inputs including text, class labels, or images.</p>"},{"location":"examples/advanced/advanced-diffusion/#text-conditioning","title":"Text Conditioning","text":"<pre><code>class TextConditionedDiffusion(nnx.Module):\n    \"\"\"Diffusion model with text conditioning.\"\"\"\n\n    def __init__(\n        self,\n        text_encoder_dim: int = 512,\n        text_max_length: int = 77,\n        cross_attention_dim: int = 768,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.text_encoder_dim = text_encoder_dim\n        self.cross_attention_dim = cross_attention_dim\n\n        # Text encoder (simplified - in practice use CLIP or T5)\n        self.text_encoder = nnx.Sequential(\n            nnx.Embed(\n                num_embeddings=50000,  # Vocabulary size\n                features=text_encoder_dim,\n                rngs=rngs,\n            ),\n            # Positional encoding would go here\n            nnx.Linear(text_encoder_dim, cross_attention_dim, rngs=rngs),\n        )\n\n        # U-Net with cross-attention\n        self.unet = self._build_unet_with_cross_attention(rngs)\n\n    def _build_unet_with_cross_attention(self, rngs: nnx.Rngs) -&gt; nnx.Module:\n        \"\"\"Build U-Net with cross-attention layers for text conditioning.\"\"\"\n\n        class CrossAttentionBlock(nnx.Module):\n            def __init__(self, dim, context_dim, num_heads=8, *, rngs):\n                super().__init__()\n\n                self.norm1 = nnx.GroupNorm(num_groups=32, rngs=rngs)\n                self.norm2 = nnx.LayerNorm(dim, rngs=rngs)\n\n                # Self-attention\n                self.self_attn = nnx.MultiHeadAttention(\n                    num_heads=num_heads,\n                    in_features=dim,\n                    decode=False,\n                    rngs=rngs,\n                )\n\n                # Cross-attention\n                self.cross_attn = nnx.MultiHeadAttention(\n                    num_heads=num_heads,\n                    in_features=dim,\n                    qkv_features=context_dim,\n                    decode=False,\n                    rngs=rngs,\n                )\n\n                # Feed-forward\n                self.ff = nnx.Sequential(\n                    nnx.Linear(dim, dim * 4, rngs=rngs),\n                    nnx.gelu,\n                    nnx.Linear(dim * 4, dim, rngs=rngs),\n                )\n\n            def __call__(self, x, context):\n                \"\"\"\n                Args:\n                    x: Image features [batch, H, W, dim]\n                    context: Text features [batch, seq_len, context_dim]\n                \"\"\"\n                batch, h, w, c = x.shape\n\n                # Reshape to sequence\n                x_seq = x.reshape(batch, h * w, c)\n\n                # Self-attention\n                x_norm = self.norm1(x_seq)\n                x_seq = x_seq + self.self_attn(x_norm)\n\n                # Cross-attention with text\n                x_norm = self.norm2(x_seq)\n                x_seq = x_seq + self.cross_attn(x_norm, context)\n\n                # Feed-forward\n                x_seq = x_seq + self.ff(self.norm2(x_seq))\n\n                # Reshape back\n                return x_seq.reshape(batch, h, w, c)\n\n        # Return U-Net architecture with cross-attention blocks\n        # (simplified for example)\n        return CrossAttentionBlock(\n            dim=512,\n            context_dim=self.cross_attention_dim,\n            rngs=rngs,\n        )\n\n    def __call__(\n        self,\n        x: jax.Array,\n        t: jax.Array,\n        text_tokens: jax.Array,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Forward pass with text conditioning.\n\n        Args:\n            x: Noisy images [batch, H, W, channels]\n            t: Timesteps [batch]\n            text_tokens: Text token indices [batch, seq_len]\n\n        Returns:\n            Predicted noise [batch, H, W, channels]\n        \"\"\"\n        # Encode text\n        text_features = self.text_encoder(text_tokens)\n\n        # Denoise with cross-attention to text\n        return self.unet(x, text_features)\n\n\ndef sample_from_text_prompt(\n    model: TextConditionedDiffusion,\n    text_prompt: str,\n    tokenizer,  # Your tokenizer\n    num_steps: int = 50,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; jax.Array:\n    \"\"\"Generate image from text prompt.\"\"\"\n\n    # Tokenize text\n    text_tokens = tokenizer(text_prompt)\n\n    # Start from noise\n    x = jax.random.normal(rngs.sample(), (1, 64, 64, 3))\n\n    # Diffusion sampling\n    for t in jnp.linspace(1000, 0, num_steps):\n        t_batch = jnp.array([t])\n        noise_pred = model(x, t_batch, text_tokens)\n\n        # Update x (DDIM or DDPM)\n        alpha_t = 1.0 - t / 1000.0\n        x = (x - jnp.sqrt(1 - alpha_t) * noise_pred) / jnp.sqrt(alpha_t)\n\n    return x\n</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#image-conditioning-controlnet-style","title":"Image Conditioning (ControlNet-style)","text":"<pre><code>class ControlNetConditioner(nnx.Module):\n    \"\"\"ControlNet-style conditioning for spatial control.\"\"\"\n\n    def __init__(\n        self,\n        base_channels: int = 128,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        # Condition encoder (processes control image)\n        self.condition_encoder = nnx.Sequential(\n            nnx.Conv(3, base_channels, kernel_size=(3, 3), padding=1, rngs=rngs),\n            nnx.silu,\n            nnx.Conv(base_channels, base_channels * 2, kernel_size=(3, 3), strides=(2, 2), padding=1, rngs=rngs),\n            nnx.silu,\n            nnx.Conv(base_channels * 2, base_channels * 4, kernel_size=(3, 3), strides=(2, 2), padding=1, rngs=rngs),\n        )\n\n        # Zero-initialized projection (ControlNet key insight)\n        self.zero_conv = nnx.Conv(\n            base_channels * 4,\n            base_channels * 4,\n            kernel_size=(1, 1),\n            kernel_init=nnx.initializers.zeros,\n            rngs=rngs,\n        )\n\n    def __call__(self, control_image: jax.Array) -&gt; jax.Array:\n        \"\"\"\n        Encode control image to conditioning features.\n\n        Args:\n            control_image: Control input (edges, depth, pose, etc.) [batch, H, W, 3]\n\n        Returns:\n            Conditioning features [batch, H', W', channels]\n        \"\"\"\n        features = self.condition_encoder(control_image)\n        return self.zero_conv(features)\n\n\nclass ControlledDiffusion(nnx.Module):\n    \"\"\"Diffusion with ControlNet-style spatial conditioning.\"\"\"\n\n    def __init__(\n        self,\n        base_channels: int = 128,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        # Base diffusion model\n        self.base_model = ClassifierFreeGuidanceUNet(\n            model_channels=base_channels,\n            rngs=rngs,\n        )\n\n        # Control network\n        self.control_net = ControlNetConditioner(\n            base_channels=base_channels,\n            rngs=rngs,\n        )\n\n    def __call__(\n        self,\n        x: jax.Array,\n        t: jax.Array,\n        class_labels: jax.Array,\n        control_image: jax.Array,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Forward with spatial control.\n\n        Args:\n            x: Noisy images [batch, H, W, 3]\n            t: Timesteps [batch]\n            class_labels: Class labels [batch]\n            control_image: Control image (edges, depth, etc.) [batch, H, W, 3]\n\n        Returns:\n            Predicted noise [batch, H, W, 3]\n        \"\"\"\n        # Get control features\n        control_features = self.control_net(control_image)\n\n        # Base prediction\n        base_pred = self.base_model(x, t, class_labels, deterministic=True)\n\n        # Add control (injected at multiple resolutions in practice)\n        # This is simplified - real ControlNet injects at multiple layers\n        return base_pred + control_features\n\n\ndef sample_with_control(\n    model: ControlledDiffusion,\n    control_image: jax.Array,\n    class_label: int,\n    num_steps: int = 50,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; jax.Array:\n    \"\"\"\n    Generate image guided by control image.\n\n    Args:\n        model: Trained model with ControlNet\n        control_image: Control input (canny edges, depth map, etc.)\n        class_label: Target class\n        num_steps: Diffusion steps\n        rngs: RNG state\n\n    Returns:\n        Generated image matching control structure\n    \"\"\"\n    batch_size = control_image.shape[0]\n\n    # Start from noise\n    x = jax.random.normal(rngs.sample(), control_image.shape)\n\n    class_labels = jnp.full((batch_size,), class_label)\n\n    # Denoising loop\n    for t in jnp.linspace(1000, 0, num_steps):\n        t_batch = jnp.full((batch_size,), t)\n\n        noise_pred = model(x, t_batch, class_labels, control_image)\n\n        # DDIM update\n        alpha_t = 1.0 - t / 1000.0\n        alpha_t_prev = 1.0 - (t - 1000 / num_steps) / 1000.0\n\n        x0_pred = (x - jnp.sqrt(1 - alpha_t) * noise_pred) / jnp.sqrt(alpha_t)\n        x0_pred = jnp.clip(x0_pred, -1, 1)\n\n        x = jnp.sqrt(alpha_t_prev) * x0_pred + jnp.sqrt(1 - alpha_t_prev) * noise_pred\n\n    return x\n</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#custom-noise-schedules","title":"Custom Noise Schedules","text":"<p>Different noise schedules can improve quality for specific data types.</p>"},{"location":"examples/advanced/advanced-diffusion/#common-schedules","title":"Common Schedules","text":"<pre><code>def linear_beta_schedule(num_timesteps: int = 1000) -&gt; jax.Array:\n    \"\"\"Linear noise schedule (original DDPM).\"\"\"\n    beta_start = 0.0001\n    beta_end = 0.02\n    return jnp.linspace(beta_start, beta_end, num_timesteps)\n\n\ndef cosine_beta_schedule(num_timesteps: int = 1000, s: float = 0.008) -&gt; jax.Array:\n    \"\"\"\n    Cosine noise schedule from \"Improved Denoising Diffusion Probabilistic Models\".\n\n    Better for high-resolution images.\n    \"\"\"\n    steps = num_timesteps + 1\n    x = jnp.linspace(0, num_timesteps, steps)\n\n    alphas_cumprod = jnp.cos(((x / num_timesteps) + s) / (1 + s) * jnp.pi * 0.5) ** 2\n    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n\n    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n    return jnp.clip(betas, 0.0001, 0.9999)\n\n\ndef sigmoid_beta_schedule(num_timesteps: int = 1000, start: float = -3, end: float = 3) -&gt; jax.Array:\n    \"\"\"Sigmoid schedule for smoother transitions.\"\"\"\n    betas = jnp.linspace(start, end, num_timesteps)\n    betas = nnx.sigmoid(betas)\n    betas = (betas - betas.min()) / (betas.max() - betas.min())\n    return betas * 0.02 + 0.0001\n\n\ndef custom_schedule_for_audio(num_timesteps: int = 1000) -&gt; jax.Array:\n    \"\"\"Custom schedule optimized for audio spectrograms.\"\"\"\n    # More noise at higher frequencies\n    t = jnp.linspace(0, 1, num_timesteps)\n    betas = 0.0001 + 0.02 * (t ** 2)  # Quadratic increase\n    return betas\n\n\nclass CustomScheduleDiffusion(nnx.Module):\n    \"\"\"Diffusion model with custom noise schedule.\"\"\"\n\n    def __init__(\n        self,\n        schedule_type: str = \"cosine\",\n        num_timesteps: int = 1000,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.num_timesteps = num_timesteps\n\n        # Select schedule\n        if schedule_type == \"linear\":\n            betas = linear_beta_schedule(num_timesteps)\n        elif schedule_type == \"cosine\":\n            betas = cosine_beta_schedule(num_timesteps)\n        elif schedule_type == \"sigmoid\":\n            betas = sigmoid_beta_schedule(num_timesteps)\n        elif schedule_type == \"audio\":\n            betas = custom_schedule_for_audio(num_timesteps)\n        else:\n            raise ValueError(f\"Unknown schedule: {schedule_type}\")\n\n        # Pre-compute alpha values\n        self.betas = betas\n        self.alphas = 1.0 - betas\n        self.alphas_cumprod = jnp.cumprod(self.alphas)\n        self.alphas_cumprod_prev = jnp.concatenate([jnp.array([1.0]), self.alphas_cumprod[:-1]])\n\n        # Coefficients for sampling\n        self.sqrt_alphas_cumprod = jnp.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = jnp.sqrt(1.0 - self.alphas_cumprod)\n\n        # Denoising model\n        self.model = ClassifierFreeGuidanceUNet(rngs=rngs)\n\n    def add_noise(self, x0: jax.Array, t: jax.Array, *, rngs: nnx.Rngs) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"\n        Add noise according to schedule.\n\n        Args:\n            x0: Clean images [batch, H, W, C]\n            t: Timestep indices [batch]\n            rngs: Random generators\n\n        Returns:\n            Tuple of (noisy images, noise)\n        \"\"\"\n        noise = jax.random.normal(rngs.sample(), x0.shape)\n\n        sqrt_alpha_t = self.sqrt_alphas_cumprod[t][:, None, None, None]\n        sqrt_one_minus_alpha_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n\n        noisy = sqrt_alpha_t * x0 + sqrt_one_minus_alpha_t * noise\n\n        return noisy, noise\n\n    def sample(\n        self,\n        shape: tuple,\n        class_labels: jax.Array,\n        guidance_scale: float = 1.0,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample using the custom schedule.\"\"\"\n\n        # Start from noise\n        x = jax.random.normal(rngs.sample(), shape)\n\n        # Reverse diffusion\n        for t in range(self.num_timesteps - 1, -1, -1):\n            t_batch = jnp.full((shape[0],), t)\n\n            # Predict noise\n            noise_pred_cond = self.model(x, t_batch, class_labels, deterministic=True)\n            noise_pred_uncond = self.model(\n                x,\n                t_batch,\n                jnp.full_like(class_labels, self.model.num_classes),\n                deterministic=True,\n            )\n\n            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n\n            # Denoise step with custom schedule\n            alpha_t = self.alphas_cumprod[t]\n            alpha_t_prev = self.alphas_cumprod_prev[t]\n            beta_t = self.betas[t]\n\n            # Predict x0\n            x0_pred = (x - jnp.sqrt(1 - alpha_t) * noise_pred) / jnp.sqrt(alpha_t)\n            x0_pred = jnp.clip(x0_pred, -1, 1)\n\n            # Sample x_{t-1}\n            if t &gt; 0:\n                noise = jax.random.normal(rngs.sample(), x.shape)\n            else:\n                noise = jnp.zeros_like(x)\n\n            x = (\n                jnp.sqrt(alpha_t_prev) * x0_pred\n                + jnp.sqrt(1 - alpha_t_prev - beta_t ** 2) * noise_pred\n                + beta_t * noise\n            )\n\n        return x\n</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#schedule-comparison","title":"Schedule Comparison","text":"<pre><code>def compare_schedules():\n    \"\"\"Visualize different noise schedules.\"\"\"\n    import matplotlib.pyplot as plt\n\n    num_steps = 1000\n    schedules = {\n        \"Linear\": linear_beta_schedule(num_steps),\n        \"Cosine\": cosine_beta_schedule(num_steps),\n        \"Sigmoid\": sigmoid_beta_schedule(num_steps),\n        \"Audio\": custom_schedule_for_audio(num_steps),\n    }\n\n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    axes = axes.flatten()\n\n    for idx, (name, betas) in enumerate(schedules.items()):\n        alphas_cumprod = jnp.cumprod(1 - betas)\n\n        ax = axes[idx]\n        ax.plot(betas, label=\"Beta\", alpha=0.7)\n        ax.plot(alphas_cumprod, label=\"Alpha_cumprod\", alpha=0.7)\n        ax.set_title(f\"{name} Schedule\")\n        ax.set_xlabel(\"Timestep\")\n        ax.set_ylabel(\"Value\")\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(\"schedule_comparison.png\")\n    print(\"Schedule comparison saved to schedule_comparison.png\")\n</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#latent-diffusion","title":"Latent Diffusion","text":"<p>Latent diffusion operates in a compressed latent space, significantly reducing computational cost while maintaining quality.</p>"},{"location":"examples/advanced/advanced-diffusion/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    A[Input Image] --&gt; B[VAE Encoder]\n    B --&gt; C[Latent Space]\n    C --&gt; D[Diffusion Model]\n    D --&gt; E[Denoised Latent]\n    E --&gt; F[VAE Decoder]\n    F --&gt; G[Output Image]\n\n    style C fill:#e8f5e9\n    style D fill:#e3f2fd</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#latent-diffusion-implementation","title":"Latent Diffusion Implementation","text":"<pre><code>class LatentDiffusionModel(nnx.Module):\n    \"\"\"Latent Diffusion Model (LDM) operating in compressed space.\"\"\"\n\n    def __init__(\n        self,\n        vae_latent_dim: int = 4,\n        latent_size: int = 32,  # Compressed spatial size\n        condition_dim: int = 768,  # For text/class conditioning\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.vae_latent_dim = vae_latent_dim\n        self.latent_size = latent_size\n\n        # VAE for compression (pre-trained and frozen)\n        self.vae = self._build_vae(rngs)\n\n        # Diffusion model in latent space\n        self.diffusion_model = ClassifierFreeGuidanceUNet(\n            in_channels=vae_latent_dim,\n            model_channels=256,\n            rngs=rngs,\n        )\n\n        # Condition encoder (text, class, etc.)\n        self.condition_encoder = nnx.Sequential(\n            nnx.Embed(num_embeddings=1000, features=condition_dim, rngs=rngs),\n            nnx.Linear(condition_dim, condition_dim, rngs=rngs),\n        )\n\n    def _build_vae(self, rngs: nnx.Rngs) -&gt; nnx.Module:\n        \"\"\"Build VAE for latent compression.\"\"\"\n\n        class SimpleVAE(nnx.Module):\n            def __init__(self, latent_dim, *, rngs):\n                super().__init__()\n\n                # Encoder\n                self.encoder = nnx.Sequential(\n                    nnx.Conv(3, 128, kernel_size=(4, 4), strides=(2, 2), padding=1, rngs=rngs),\n                    nnx.relu,\n                    nnx.Conv(128, 256, kernel_size=(4, 4), strides=(2, 2), padding=1, rngs=rngs),\n                    nnx.relu,\n                    nnx.Conv(256, latent_dim, kernel_size=(4, 4), strides=(2, 2), padding=1, rngs=rngs),\n                )\n\n                # Decoder\n                self.decoder = nnx.Sequential(\n                    nnx.ConvTranspose(latent_dim, 256, kernel_size=(4, 4), strides=(2, 2), padding=1, rngs=rngs),\n                    nnx.relu,\n                    nnx.ConvTranspose(256, 128, kernel_size=(4, 4), strides=(2, 2), padding=1, rngs=rngs),\n                    nnx.relu,\n                    nnx.ConvTranspose(128, 3, kernel_size=(4, 4), strides=(2, 2), padding=1, rngs=rngs),\n                    nnx.tanh,\n                )\n\n            def encode(self, x):\n                return self.encoder(x)\n\n            def decode(self, z):\n                return self.decoder(z)\n\n        return SimpleVAE(self.vae_latent_dim, rngs=rngs)\n\n    def encode_to_latent(self, images: jax.Array) -&gt; jax.Array:\n        \"\"\"Encode images to latent space.\"\"\"\n        # Apply VAE encoder\n        latents = self.vae.encode(images)\n\n        # Scale latents (standard practice in LDM)\n        latents = latents * 0.18215\n\n        return latents\n\n    def decode_from_latent(self, latents: jax.Array) -&gt; jax.Array:\n        \"\"\"Decode latents to images.\"\"\"\n        # Unscale\n        latents = latents / 0.18215\n\n        # Apply VAE decoder\n        return self.vae.decode(latents)\n\n    def __call__(\n        self,\n        latents: jax.Array,\n        t: jax.Array,\n        condition: jax.Array,\n        *,\n        deterministic: bool = False,\n        rngs: nnx.Rngs | None = None,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Predict noise in latent space.\n\n        Args:\n            latents: Noisy latents [batch, H, W, vae_latent_dim]\n            t: Timesteps [batch]\n            condition: Conditioning (class labels, text tokens, etc.) [batch, ...]\n            deterministic: Training vs inference mode\n            rngs: Random generators\n\n        Returns:\n            Predicted noise in latent space\n        \"\"\"\n        # Encode condition\n        cond_embed = self.condition_encoder(condition)\n\n        # Predict noise in latent space\n        return self.diffusion_model(\n            latents,\n            t,\n            cond_embed,\n            deterministic=deterministic,\n            rngs=rngs,\n        )\n\n    def sample(\n        self,\n        condition: jax.Array,\n        num_steps: int = 50,\n        guidance_scale: float = 7.5,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Generate images in latent space.\n\n        Args:\n            condition: Conditioning input [batch, ...]\n            num_steps: Number of diffusion steps\n            guidance_scale: CFG scale\n            rngs: Random generators\n\n        Returns:\n            Generated images [batch, H, W, 3]\n        \"\"\"\n        batch_size = condition.shape[0]\n\n        # Start from random latent noise\n        latents = jax.random.normal(\n            rngs.sample(),\n            (batch_size, self.latent_size, self.latent_size, self.vae_latent_dim),\n        )\n\n        # Diffusion in latent space\n        for t in jnp.linspace(1000, 0, num_steps):\n            t_batch = jnp.full((batch_size,), t, dtype=jnp.int32)\n\n            # CFG\n            noise_pred_cond = self(\n                latents,\n                t_batch,\n                condition,\n                deterministic=True,\n            )\n\n            noise_pred_uncond = self(\n                latents,\n                t_batch,\n                jnp.zeros_like(condition),  # Null condition\n                deterministic=True,\n            )\n\n            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n\n            # DDIM update\n            alpha_t = 1.0 - t / 1000.0\n            alpha_t_prev = 1.0 - (t - 1000 / num_steps) / 1000.0\n\n            latents = jnp.sqrt(alpha_t_prev) * (\n                (latents - jnp.sqrt(1 - alpha_t) * noise_pred) / jnp.sqrt(alpha_t)\n            )\n\n        # Decode to images\n        images = self.decode_from_latent(latents)\n\n        return images\n\n\ndef train_latent_diffusion(\n    model: LatentDiffusionModel,\n    train_data: jnp.ndarray,\n    train_conditions: jnp.ndarray,\n    num_epochs: int = 100,\n):\n    \"\"\"Train latent diffusion model.\"\"\"\n\n    rngs = nnx.Rngs(42)\n    diffusion_model = model.diffusion_model\n    optimizer = nnx.Optimizer(diffusion_model, optax.adam(1e-4), wrt=nnx.Param)\n\n    # Pre-encode all images to latent space (saves computation)\n    print(\"Encoding dataset to latent space...\")\n    latent_data = model.encode_to_latent(train_data)\n\n    for epoch in range(num_epochs):\n        for batch_idx in range(0, len(latent_data), 32):\n            batch_latents = latent_data[batch_idx:batch_idx + 32]\n            batch_conditions = train_conditions[batch_idx:batch_idx + 32]\n\n            # Random timesteps\n            t = jax.random.randint(rngs.params(), (batch_latents.shape[0],), 0, 1000)\n\n            # Add noise in latent space\n            noise = jax.random.normal(rngs.params(), batch_latents.shape)\n            alpha_t = 1.0 - t[:, None, None, None] / 1000.0\n            noisy_latents = jnp.sqrt(alpha_t) * batch_latents + jnp.sqrt(1 - alpha_t) * noise\n\n            def loss_fn(diffusion_model):\n                # Predict noise\n                noise_pred = model(\n                    noisy_latents,\n                    t,\n                    batch_conditions,\n                    deterministic=False,\n                    rngs=rngs,\n                )\n                # Loss\n                return jnp.mean((noise_pred - noise) ** 2)\n\n            # Compute loss and gradients, then update\n            loss, grads = nnx.value_and_grad(loss_fn)(diffusion_model)\n            optimizer.update(diffusion_model, grads)\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}\")\n\n    return model\n</code></pre>"},{"location":"examples/advanced/advanced-diffusion/#best-practices","title":"Best Practices","text":"<p>DO</p> <ul> <li>Use classifier-free guidance for better sample quality</li> <li>Pre-train VAE separately for latent diffusion</li> <li>Use cosine schedule for high-resolution images</li> <li>Implement zero-initialized convolutions for ControlNet</li> <li>Monitor FID and CLIP scores during training</li> <li>Use mixed precision training for efficiency</li> </ul> <p>DON'T</p> <ul> <li>Don't use guidance scales &gt;10 (causes artifacts)</li> <li>Don't skip warm-up in training</li> <li>Don't forget to normalize conditioning embeddings</li> <li>Don't train latent diffusion and VAE jointly (unstable)</li> <li>Don't use too few sampling steps (&lt;20)</li> </ul>"},{"location":"examples/advanced/advanced-diffusion/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution Low quality with CFG Wrong guidance scale Try scales 3-7.5, visualize different scales Color shift Incorrect normalization Check data/latent scaling factors Blurry samples Too few steps Increase sampling steps (50-100) Training instability Schedule mismatch Use cosine schedule, adjust learning rate Control not working Zero-conv not used Initialize control layers to zero"},{"location":"examples/advanced/advanced-diffusion/#summary","title":"Summary","text":"<p>We covered four advanced diffusion techniques:</p> <ol> <li>Classifier-Free Guidance: Improved quality without separate classifier</li> <li>Conditioning Patterns: Text, class, and spatial control</li> <li>Custom Noise Schedules: Optimized schedules for different data types</li> <li>Latent Diffusion: Efficient generation in compressed space</li> </ol> <p>Key Takeaways:</p> <ul> <li>CFG significantly improves sample quality at minimal cost</li> <li>Custom schedules matter for specific domains</li> <li>Latent diffusion reduces memory/compute by 4-8x</li> <li>ControlNet enables precise spatial control</li> </ul>"},{"location":"examples/advanced/advanced-diffusion/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Diffusion Concepts</p> <p>Deep dive into diffusion theory</p> <p> Diffusion Explained</p> </li> <li> <p> Training Guide</p> <p>Scale diffusion training</p> <p> Training Guide</p> </li> <li> <p> Inference</p> <p>Optimize inference speed</p> <p> Inference Guide</p> </li> <li> <p> API Reference</p> <p>Complete diffusion API</p> <p> Diffusion API</p> </li> </ul>"},{"location":"examples/advanced/advanced-flow/","title":"Advanced Flow Examples","text":"<p>This guide demonstrates advanced normalizing flow architectures using Artifex and Flax NNX, including continuous normalizing flows (CNF), FFJORD, custom coupling flows, and conditional flows.</p>"},{"location":"examples/advanced/advanced-flow/#overview","title":"Overview","text":"<ul> <li> <p> Continuous Normalizing Flows</p> <p>Neural ODEs for flexible continuous-time transformations</p> <p> CNF</p> </li> <li> <p> FFJORD</p> <p>Free-form Jacobian of Reversible Dynamics with efficient trace estimation</p> <p> FFJORD</p> </li> <li> <p> Advanced Coupling Flows</p> <p>Custom coupling architectures with attention and residual connections</p> <p> Coupling Flows</p> </li> <li> <p> Conditional Flows</p> <p>Conditional generation with class, text, or image inputs</p> <p> Conditional Flows</p> </li> </ul>"},{"location":"examples/advanced/advanced-flow/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Artifex with all dependencies\nuv pip install \"artifex[cuda]\"  # With GPU support\n# or\nuv pip install artifex  # CPU only\n</code></pre> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nfrom artifex.generative_models.core import DeviceManager\nfrom artifex.generative_models.models.flow import FlowModel\n</code></pre>"},{"location":"examples/advanced/advanced-flow/#continuous-normalizing-flows-cnf","title":"Continuous Normalizing Flows (CNF)","text":"<p>CNFs use neural ODEs to learn continuous-time transformations, providing more flexibility than discrete flows.</p>"},{"location":"examples/advanced/advanced-flow/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[z ~ N(0,I)] --&gt; B[ODE Solver]\n    B --&gt; C[x = z_T]\n    C --&gt; D[Data]\n\n    style A fill:#e1f5ff\n    style B fill:#f3e5f5\n    style C fill:#e8f5e9\n    style D fill:#fff3e0</code></pre>"},{"location":"examples/advanced/advanced-flow/#cnf-implementation","title":"CNF Implementation","text":"<pre><code>from flax import nnx\nimport jax\nimport jax.numpy as jnp\nfrom functools import partial\n\nclass ContinuousNormalizingFlow(nnx.Module):\n    \"\"\"Continuous Normalizing Flow using Neural ODEs.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 128,\n        num_layers: int = 3,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        # Dynamics network f(z, t)\n        # Maps (z, t) -&gt; dz/dt\n        layers = []\n        for i in range(num_layers):\n            in_dim = input_dim + 1 if i == 0 else hidden_dim  # +1 for time\n            out_dim = input_dim if i == num_layers - 1 else hidden_dim\n\n            layers.append(nnx.Linear(in_dim, out_dim, rngs=rngs))\n\n            if i &lt; num_layers - 1:\n                layers.append(nnx.tanh)\n\n        self.dynamics_net = nnx.Sequential(*layers)\n\n    def dynamics(self, t: float, z: jax.Array) -&gt; jax.Array:\n        \"\"\"\n        Compute dz/dt at time t.\n\n        Args:\n            t: Current time (scalar)\n            z: Current state [batch, input_dim]\n\n        Returns:\n            Time derivative dz/dt [batch, input_dim]\n        \"\"\"\n        batch_size = z.shape[0]\n\n        # Concatenate time to input\n        t_expanded = jnp.full((batch_size, 1), t)\n        z_t = jnp.concatenate([z, t_expanded], axis=-1)\n\n        # Compute dynamics\n        return self.dynamics_net(z_t)\n\n    def forward(\n        self,\n        z0: jax.Array,\n        t_span: tuple[float, float] = (0.0, 1.0),\n        num_steps: int = 100,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Integrate from z0 at t=t0 to t=t1.\n\n        Args:\n            z0: Initial state [batch, input_dim]\n            t_span: Time interval (t0, t1)\n            num_steps: Number of integration steps\n\n        Returns:\n            Final state z1 [batch, input_dim]\n        \"\"\"\n        from jax.experimental.ode import odeint\n\n        # Time points\n        t_eval = jnp.linspace(t_span[0], t_span[1], num_steps)\n\n        # Solve ODE: dz/dt = f(z, t)\n        def ode_func(z, t):\n            return self.dynamics(t, z)\n\n        # Integrate (returns [num_steps, batch, input_dim])\n        z_trajectory = odeint(ode_func, z0, t_eval)\n\n        # Return final state\n        return z_trajectory[-1]\n\n    def inverse(\n        self,\n        z1: jax.Array,\n        t_span: tuple[float, float] = (1.0, 0.0),\n        num_steps: int = 100,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Integrate backwards from z1 at t=t1 to t=t0.\n\n        Args:\n            z1: Final state [batch, input_dim]\n            t_span: Time interval (t1, t0) - reversed\n            num_steps: Number of integration steps\n\n        Returns:\n            Initial state z0 [batch, input_dim]\n        \"\"\"\n        return self.forward(z1, t_span, num_steps)\n\n    def log_prob(\n        self,\n        x: jax.Array,\n        base_log_prob_fn,\n        num_steps: int = 100,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Compute log probability using instantaneous change of variables.\n\n        Args:\n            x: Data samples [batch, input_dim]\n            base_log_prob_fn: Log probability function for base distribution\n            num_steps: Integration steps\n\n        Returns:\n            Log probabilities [batch]\n        \"\"\"\n        from jax.experimental.ode import odeint\n\n        batch_size = x.shape[0]\n\n        # Integrate backwards to get z0 and log determinant\n        def augmented_dynamics(augmented_state, t):\n            z, _ = augmented_state\n            dz_dt = self.dynamics(t, z)\n\n            # Trace of Jacobian (Hutchinson's trace estimator in FFJORD)\n            # For exact computation (expensive):\n            def dynamics_fn(z_single):\n                return self.dynamics(t, z_single[None, :])[0]\n\n            jacobian = jax.jacfwd(dynamics_fn)(z)\n            trace = jnp.trace(jacobian)\n\n            return dz_dt, -trace  # Negative for inverse\n\n        # Initial augmented state\n        initial_state = (x, jnp.zeros(batch_size))\n\n        # Integrate\n        t_eval = jnp.linspace(1.0, 0.0, num_steps)\n\n        def ode_func(state, t):\n            return augmented_dynamics(state, t)\n\n        trajectory = odeint(ode_func, initial_state, t_eval)\n\n        z0, log_det_jacobian = trajectory[0], trajectory[1]\n\n        # Compute log probability\n        base_log_prob = base_log_prob_fn(z0)\n        return base_log_prob + log_det_jacobian\n\n\ndef train_cnf(\n    model: ContinuousNormalizingFlow,\n    train_data: jnp.ndarray,\n    num_epochs: int = 100,\n    batch_size: int = 128,\n):\n    \"\"\"Train continuous normalizing flow.\"\"\"\n\n    rngs = nnx.Rngs(42)\n    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n\n    # Base distribution (standard normal)\n    def base_log_prob(z):\n        return -0.5 * jnp.sum(z ** 2, axis=-1) - 0.5 * z.shape[-1] * jnp.log(2 * jnp.pi)\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        num_batches = 0\n\n        for batch_idx in range(0, len(train_data), batch_size):\n            batch = train_data[batch_idx:batch_idx + batch_size]\n\n            def loss_fn(model):\n                # Compute negative log likelihood\n                log_probs = model.log_prob(batch, base_log_prob, num_steps=50)\n                return -jnp.mean(log_probs)\n\n            # Compute loss and gradients, then update\n            loss, grads = nnx.value_and_grad(loss_fn)(model)\n            optimizer.update(model, grads)\n\n            epoch_loss += loss\n            num_batches += 1\n\n        avg_loss = epoch_loss / num_batches\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n    return model\n\n\ndef sample_from_cnf(\n    model: ContinuousNormalizingFlow,\n    num_samples: int,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; jax.Array:\n    \"\"\"Sample from learned distribution.\"\"\"\n\n    # Sample from base distribution\n    z0 = jax.random.normal(rngs.sample(), (num_samples, model.input_dim))\n\n    # Transform to data space\n    x = model.forward(z0, t_span=(0.0, 1.0), num_steps=100)\n\n    return x\n</code></pre>"},{"location":"examples/advanced/advanced-flow/#ffjord","title":"FFJORD","text":"<p>FFJORD (Free-Form Jacobian of Reversible Dynamics) uses Hutchinson's trace estimator for efficient computation of log determinants.</p>"},{"location":"examples/advanced/advanced-flow/#hutchinsons-trace-estimator","title":"Hutchinson's Trace Estimator","text":"<pre><code>class FFJORD(nnx.Module):\n    \"\"\"\n    FFJORD: Scalable Continuous Normalizing Flow.\n\n    Uses Hutchinson's trace estimator for O(1) memory complexity.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 64,\n        num_layers: int = 3,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n\n        # Time-conditioned dynamics network\n        self.dynamics_net = self._build_dynamics_net(\n            input_dim,\n            hidden_dim,\n            num_layers,\n            rngs,\n        )\n\n    def _build_dynamics_net(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        num_layers: int,\n        rngs: nnx.Rngs,\n    ) -&gt; nnx.Module:\n        \"\"\"Build dynamics network with time conditioning.\"\"\"\n\n        class TimeConditionedMLP(nnx.Module):\n            def __init__(self, in_dim, hidden, n_layers, *, rngs):\n                super().__init__()\n\n                layers = []\n                for i in range(n_layers):\n                    layer_in = in_dim + 1 if i == 0 else hidden  # +1 for time\n                    layer_out = in_dim if i == n_layers - 1 else hidden\n\n                    layers.append(nnx.Linear(layer_in, layer_out, rngs=rngs))\n\n                    if i &lt; n_layers - 1:\n                        layers.append(nnx.softplus)\n\n                self.net = nnx.Sequential(*layers)\n\n            def __call__(self, z, t):\n                batch_size = z.shape[0]\n                t_expanded = jnp.full((batch_size, 1), t)\n                z_t = jnp.concatenate([z, t_expanded], axis=-1)\n                return self.net(z_t)\n\n        return TimeConditionedMLP(input_dim, hidden_dim, num_layers, rngs=rngs)\n\n    def dynamics(self, t: float, z: jax.Array) -&gt; jax.Array:\n        \"\"\"Compute dz/dt.\"\"\"\n        return self.dynamics_net(z, t)\n\n    def divergence_approx(\n        self,\n        t: float,\n        z: jax.Array,\n        epsilon: jax.Array,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Approximate divergence using Hutchinson's trace estimator.\n\n        Tr(J) \u2248 E[\u03b5^T J \u03b5] where \u03b5 ~ N(0, I)\n\n        Args:\n            t: Time\n            z: State [batch, dim]\n            epsilon: Random vector [batch, dim]\n\n        Returns:\n            Trace estimate [batch]\n        \"\"\"\n\n        def dynamics_fn(z_single):\n            return self.dynamics(t, z_single[None, :])[0]\n\n        # Compute Jacobian-vector product efficiently\n        _, jvp_result = jax.jvp(dynamics_fn, (z,), (epsilon,))\n\n        # Hutchinson estimator: \u03b5^T J \u03b5\n        trace_estimate = jnp.sum(epsilon * jvp_result, axis=-1)\n\n        return trace_estimate\n\n    def ode_with_log_prob(\n        self,\n        z: jax.Array,\n        t_span: tuple[float, float] = (0.0, 1.0),\n        num_steps: int = 100,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"\n        Integrate ODE and compute log probability.\n\n        Args:\n            z: Initial/final state [batch, dim]\n            t_span: Time interval\n            num_steps: Integration steps\n            rngs: For trace estimation\n\n        Returns:\n            Tuple of (final_state, log_determinant)\n        \"\"\"\n        from jax.experimental.ode import odeint\n\n        batch_size = z.shape[0]\n\n        # Sample noise for trace estimation (reuse across time)\n        epsilon = jax.random.normal(rngs.sample(), (batch_size, self.input_dim))\n\n        def augmented_dynamics(augmented_state, t):\n            z_current, _ = augmented_state\n\n            # Dynamics\n            dz_dt = self.dynamics(t, z_current)\n\n            # Trace estimate\n            trace = self.divergence_approx(t, z_current, epsilon)\n\n            # For forward: positive trace, for inverse: negative\n            sign = 1.0 if t_span[1] &gt; t_span[0] else -1.0\n\n            return dz_dt, sign * trace\n\n        # Initial state\n        initial_augmented = (z, jnp.zeros(batch_size))\n\n        # Time points\n        t_eval = jnp.linspace(t_span[0], t_span[1], num_steps)\n\n        # Integrate\n        def ode_func(state, t):\n            return augmented_dynamics(state, t)\n\n        trajectory = odeint(ode_func, initial_augmented, t_eval)\n\n        # Extract final values\n        z_final = trajectory[0][-1]\n        log_det_jacobian = trajectory[1][-1]\n\n        return z_final, log_det_jacobian\n\n    def forward_and_log_det(\n        self,\n        z0: jax.Array,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Forward transformation with log determinant.\"\"\"\n        return self.ode_with_log_prob(z0, t_span=(0.0, 1.0), rngs=rngs)\n\n    def inverse_and_log_det(\n        self,\n        x: jax.Array,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Inverse transformation with log determinant.\"\"\"\n        z0, log_det = self.ode_with_log_prob(x, t_span=(1.0, 0.0), rngs=rngs)\n        return z0, -log_det  # Negate for inverse\n\n    def log_prob(\n        self,\n        x: jax.Array,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Compute log probability.\"\"\"\n\n        # Transform to base space\n        z0, log_det = self.inverse_and_log_det(x, rngs=rngs)\n\n        # Base distribution log prob (standard normal)\n        base_log_prob = -0.5 * jnp.sum(z0 ** 2, axis=-1) - 0.5 * self.input_dim * jnp.log(2 * jnp.pi)\n\n        return base_log_prob + log_det\n\n\ndef train_ffjord(\n    model: FFJORD,\n    train_data: jnp.ndarray,\n    num_epochs: int = 100,\n    batch_size: int = 128,\n):\n    \"\"\"Train FFJORD model.\"\"\"\n\n    rngs = nnx.Rngs(42)\n    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        num_batches = 0\n\n        for batch_idx in range(0, len(train_data), batch_size):\n            batch = train_data[batch_idx:batch_idx + batch_size]\n\n            def loss_fn(model):\n                # Compute negative log likelihood\n                log_probs = model.log_prob(batch, rngs=rngs)\n                return -jnp.mean(log_probs)\n\n            # Compute loss and gradients, then update\n            loss, grads = nnx.value_and_grad(loss_fn)(model)\n            optimizer.update(model, grads)\n\n            epoch_loss += loss\n            num_batches += 1\n\n        if epoch % 10 == 0:\n            avg_loss = epoch_loss / num_batches\n            print(f\"Epoch {epoch}/{num_epochs}, NLL: {avg_loss:.4f}\")\n\n    return model\n</code></pre>"},{"location":"examples/advanced/advanced-flow/#advanced-coupling-flows","title":"Advanced Coupling Flows","text":"<p>Custom coupling architectures with attention mechanisms and residual connections for improved expressiveness.</p>"},{"location":"examples/advanced/advanced-flow/#attention-coupling-layer","title":"Attention Coupling Layer","text":"<pre><code>class AttentionCouplingLayer(nnx.Module):\n    \"\"\"Coupling layer with self-attention in the transformation network.\"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        hidden_dim: int = 256,\n        num_heads: int = 4,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.features = features\n        self.hidden_dim = hidden_dim\n        self.split_dim = features // 2\n\n        # Transformation layers (applied manually for reshape flexibility)\n        self.linear1 = nnx.Linear(self.split_dim, hidden_dim, rngs=rngs)\n        self.attention = nnx.MultiHeadAttention(\n            num_heads=num_heads,\n            in_features=hidden_dim,\n            decode=False,\n            rngs=rngs,\n        )\n        self.linear2 = nnx.Linear(hidden_dim, self.split_dim * 2, rngs=rngs)\n\n    def _transform(self, x1: jax.Array) -&gt; jax.Array:\n        \"\"\"Apply transformation network with attention.\"\"\"\n        h = nnx.relu(self.linear1(x1))\n        # Reshape for attention: [batch, features] -&gt; [batch, 1, features]\n        h = h[:, None, :]\n        h = self.attention(h)\n        # Reshape back: [batch, 1, features] -&gt; [batch, features]\n        h = h[:, 0, :]\n        return self.linear2(h)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        reverse: bool = False,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"\n        Forward or inverse transformation.\n\n        Args:\n            x: Input [batch, features]\n            reverse: If True, compute inverse\n\n        Returns:\n            Tuple of (output, log_det_jacobian)\n        \"\"\"\n        # Split input\n        x1, x2 = jnp.split(x, [self.split_dim], axis=-1)\n\n        if not reverse:\n            # Forward: x2' = x2 * exp(s(x1)) + t(x1)\n            transform_params = self._transform(x1)\n            log_scale, shift = jnp.split(transform_params, 2, axis=-1)\n\n            # Bound log scale for stability\n            log_scale = jnp.tanh(log_scale)\n\n            x2_transformed = x2 * jnp.exp(log_scale) + shift\n\n            output = jnp.concatenate([x1, x2_transformed], axis=-1)\n            log_det = jnp.sum(log_scale, axis=-1)\n\n        else:\n            # Inverse: x2 = (x2' - t(x1)) / exp(s(x1))\n            transform_params = self._transform(x1)\n            log_scale, shift = jnp.split(transform_params, 2, axis=-1)\n\n            log_scale = jnp.tanh(log_scale)\n\n            x2_original = (x2 - shift) * jnp.exp(-log_scale)\n\n            output = jnp.concatenate([x1, x2_original], axis=-1)\n            log_det = -jnp.sum(log_scale, axis=-1)\n\n        return output, log_det\n\n\nclass ResidualCouplingFlow(nnx.Module):\n    \"\"\"Coupling flow with residual connections.\"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        hidden_dim: int = 256,\n        num_blocks: int = 3,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.features = features\n        self.split_dim = features // 2\n\n        # Residual transformation blocks\n        blocks = []\n        for _ in range(num_blocks):\n            block = nnx.Sequential(\n                nnx.Linear(self.split_dim, hidden_dim, rngs=rngs),\n                nnx.relu,\n                nnx.Linear(hidden_dim, hidden_dim, rngs=rngs),\n                nnx.relu,\n                nnx.Linear(hidden_dim, self.split_dim, rngs=rngs),\n            )\n            blocks.append(block)\n        self.blocks = nnx.List(blocks)\n\n        # Final projection\n        self.final_proj = nnx.Linear(self.split_dim, self.split_dim * 2, rngs=rngs)\n\n    def transform_network(self, x1: jax.Array) -&gt; jax.Array:\n        \"\"\"Apply residual blocks.\"\"\"\n        h = x1\n\n        for block in self.blocks:\n            h = h + block(h)  # Residual connection\n\n        return self.final_proj(h)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        reverse: bool = False,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Forward or inverse transformation.\"\"\"\n\n        x1, x2 = jnp.split(x, [self.split_dim], axis=-1)\n\n        # Get transformation parameters\n        params = self.transform_network(x1)\n        log_scale, shift = jnp.split(params, 2, axis=-1)\n\n        # Stabilize log scale\n        log_scale = 2.0 * jnp.tanh(log_scale / 2.0)\n\n        if not reverse:\n            x2_new = x2 * jnp.exp(log_scale) + shift\n            log_det = jnp.sum(log_scale, axis=-1)\n        else:\n            x2_new = (x2 - shift) * jnp.exp(-log_scale)\n            log_det = -jnp.sum(log_scale, axis=-1)\n\n        output = jnp.concatenate([x1, x2_new], axis=-1)\n        return output, log_det\n\n\nclass AdvancedCouplingFlow(nnx.Module):\n    \"\"\"Multi-scale coupling flow with attention and residual connections.\"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        num_layers: int = 8,\n        hidden_dim: int = 256,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.features = features\n        self.num_layers = num_layers\n\n        # Build coupling layers (stored in nnx.List)\n        layers = []\n        for i in range(num_layers):\n            if i % 3 == 0:\n                # Use attention every 3 layers\n                layer = AttentionCouplingLayer(\n                    features=features,\n                    hidden_dim=hidden_dim,\n                    rngs=rngs,\n                )\n            else:\n                # Use residual coupling\n                layer = ResidualCouplingFlow(\n                    features=features,\n                    hidden_dim=hidden_dim,\n                    num_blocks=2,\n                    rngs=rngs,\n                )\n            layers.append(layer)\n        self.coupling_layers = nnx.List(layers)\n\n        # Pre-compute permutation indices (static data)\n        self.permutations = []\n        for i in range(num_layers):\n            if i % 2 == 0:\n                perm = jnp.arange(features)[::-1]  # Reverse\n            else:\n                perm = jnp.roll(jnp.arange(features), features // 2)  # Roll\n            self.permutations.append(perm)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        reverse: bool = False,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Forward or inverse pass through all layers.\"\"\"\n\n        log_det_total = jnp.zeros(x.shape[0])\n\n        indices = range(self.num_layers) if not reverse else reversed(range(self.num_layers))\n\n        for i in indices:\n            # Apply coupling layer\n            x, log_det = self.coupling_layers[i](x, reverse=reverse)\n            log_det_total += log_det\n\n            # Apply permutation\n            x = x[:, self.permutations[i]]\n\n        return x, log_det_total\n\n    def log_prob(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Compute log probability.\"\"\"\n\n        # Transform to base space\n        z, log_det = self(x, reverse=True)\n\n        # Base distribution log prob\n        base_log_prob = -0.5 * jnp.sum(z ** 2, axis=-1) - 0.5 * self.features * jnp.log(2 * jnp.pi)\n\n        return base_log_prob + log_det\n\n    def sample(self, num_samples: int, *, rngs: nnx.Rngs) -&gt; jax.Array:\n        \"\"\"Sample from the flow.\"\"\"\n\n        # Sample from base\n        z = jax.random.normal(rngs.sample(), (num_samples, self.features))\n\n        # Transform to data space\n        x, _ = self(z, reverse=False)\n\n        return x\n</code></pre>"},{"location":"examples/advanced/advanced-flow/#conditional-flows","title":"Conditional Flows","text":"<p>Flows can be conditioned on additional information for controlled generation.</p>"},{"location":"examples/advanced/advanced-flow/#class-conditional-flow","title":"Class-Conditional Flow","text":"<pre><code>class ConditionalCouplingLayer(nnx.Module):\n    \"\"\"Coupling layer with class conditioning.\"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        num_classes: int,\n        hidden_dim: int = 256,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.features = features\n        self.split_dim = features // 2\n\n        # Class embedding\n        self.class_embedding = nnx.Embed(\n            num_embeddings=num_classes,\n            features=hidden_dim,\n            rngs=rngs,\n        )\n\n        # Conditioned transformation network\n        self.transform_net = nnx.Sequential(\n            nnx.Linear(self.split_dim + hidden_dim, hidden_dim * 2, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(hidden_dim * 2, hidden_dim, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(hidden_dim, self.split_dim * 2, rngs=rngs),\n        )\n\n    def __call__(\n        self,\n        x: jax.Array,\n        class_labels: jax.Array,\n        reverse: bool = False,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"\n        Conditional transformation.\n\n        Args:\n            x: Input [batch, features]\n            class_labels: Class indices [batch]\n            reverse: Forward or inverse\n\n        Returns:\n            Tuple of (output, log_det)\n        \"\"\"\n        # Split\n        x1, x2 = jnp.split(x, [self.split_dim], axis=-1)\n\n        # Embed class\n        class_embed = self.class_embedding(class_labels)\n\n        # Concatenate x1 and class embedding\n        x1_conditioned = jnp.concatenate([x1, class_embed], axis=-1)\n\n        # Get transformation parameters\n        params = self.transform_net(x1_conditioned)\n        log_scale, shift = jnp.split(params, 2, axis=-1)\n\n        log_scale = jnp.tanh(log_scale)\n\n        if not reverse:\n            x2_new = x2 * jnp.exp(log_scale) + shift\n            log_det = jnp.sum(log_scale, axis=-1)\n        else:\n            x2_new = (x2 - shift) * jnp.exp(-log_scale)\n            log_det = -jnp.sum(log_scale, axis=-1)\n\n        output = jnp.concatenate([x1, x2_new], axis=-1)\n        return output, log_det\n\n\nclass ConditionalNormalizingFlow(nnx.Module):\n    \"\"\"Full conditional normalizing flow model.\"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        num_classes: int,\n        num_layers: int = 8,\n        hidden_dim: int = 256,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.features = features\n        self.num_classes = num_classes\n\n        # Stack of conditional coupling layers\n        layers = []\n        for i in range(num_layers):\n            layer = ConditionalCouplingLayer(\n                features=features,\n                num_classes=num_classes,\n                hidden_dim=hidden_dim,\n                rngs=rngs,\n            )\n            layers.append(layer)\n        self.layers = nnx.List(layers)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        class_labels: jax.Array,\n        reverse: bool = False,\n    ) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Forward or inverse pass.\"\"\"\n\n        log_det_total = jnp.zeros(x.shape[0])\n\n        layers = self.layers if not reverse else reversed(self.layers)\n\n        for layer in layers:\n            x, log_det = layer(x, class_labels, reverse=reverse)\n            log_det_total += log_det\n\n        return x, log_det_total\n\n    def log_prob(self, x: jax.Array, class_labels: jax.Array) -&gt; jax.Array:\n        \"\"\"Compute conditional log probability.\"\"\"\n\n        z, log_det = self(x, class_labels, reverse=True)\n\n        base_log_prob = -0.5 * jnp.sum(z ** 2, axis=-1) - 0.5 * self.features * jnp.log(2 * jnp.pi)\n\n        return base_log_prob + log_det\n\n    def sample(\n        self,\n        num_samples: int,\n        class_labels: jax.Array,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample conditioned on classes.\"\"\"\n\n        z = jax.random.normal(rngs.sample(), (num_samples, self.features))\n\n        x, _ = self(z, class_labels, reverse=False)\n\n        return x\n\n\ndef train_conditional_flow(\n    model: ConditionalNormalizingFlow,\n    train_data: jnp.ndarray,\n    train_labels: jnp.ndarray,\n    num_epochs: int = 100,\n):\n    \"\"\"Train conditional flow.\"\"\"\n\n    rngs = nnx.Rngs(42)\n    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n\n    for epoch in range(num_epochs):\n        for batch_idx in range(0, len(train_data), 128):\n            batch = train_data[batch_idx:batch_idx + 128]\n            labels = train_labels[batch_idx:batch_idx + 128]\n\n            def loss_fn(model):\n                # Negative log likelihood\n                log_probs = model.log_prob(batch, labels)\n                return -jnp.mean(log_probs)\n\n            # Compute loss and gradients, then update\n            loss, grads = nnx.value_and_grad(loss_fn)(model)\n            optimizer.update(model, grads)\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}\")\n\n    return model\n\n\n# Generate samples for specific class\ndef generate_class_samples(\n    model: ConditionalNormalizingFlow,\n    class_id: int,\n    num_samples: int = 16,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; jax.Array:\n    \"\"\"Generate samples for a specific class.\"\"\"\n\n    class_labels = jnp.full(num_samples, class_id)\n    return model.sample(num_samples, class_labels, rngs=rngs)\n</code></pre>"},{"location":"examples/advanced/advanced-flow/#best-practices","title":"Best Practices","text":"<p>DO</p> <ul> <li>Use FFJORD for high-dimensional data (more efficient)</li> <li>Add residual connections in coupling networks</li> <li>Use attention for long-range dependencies</li> <li>Monitor both NLL and sample quality</li> <li>Use adaptive ODE solvers for CNF</li> <li>Implement gradient clipping for training stability</li> </ul> <p>DON'T</p> <ul> <li>Don't use too few ODE steps (&lt;20 for CNF)</li> <li>Don't forget to alternate coupling directions</li> <li>Don't use unbounded activation in scale networks</li> <li>Don't skip permutation layers between couplings</li> <li>Don't train with learning rates &gt;1e-3</li> <li>Don't use batch norm in flow transformations</li> </ul>"},{"location":"examples/advanced/advanced-flow/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution Training instability Unbounded scales Use tanh or bounded activations for log_scale Slow ODE integration Too many steps Use adaptive solvers, reduce steps Poor sample quality Insufficient coupling Add more layers, use attention NaN in training Exploding gradients Add gradient clipping, reduce learning rate High memory usage Full Jacobian computation Use FFJORD with Hutchinson estimator"},{"location":"examples/advanced/advanced-flow/#summary","title":"Summary","text":"<p>We covered four advanced normalizing flow techniques:</p> <ol> <li>Continuous Normalizing Flows: Flexible continuous-time transformations with Neural ODEs</li> <li>FFJORD: Efficient CNF with Hutchinson's trace estimator</li> <li>Advanced Coupling: Attention and residual connections for expressiveness</li> <li>Conditional Flows: Class or context-conditional generation</li> </ol> <p>Key Takeaways:</p> <ul> <li>CNF provides more flexibility than discrete flows</li> <li>FFJORD makes CNF scalable to high dimensions</li> <li>Attention and residual connections improve coupling flows</li> <li>Conditional flows enable controlled generation</li> </ul>"},{"location":"examples/advanced/advanced-flow/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Flow Concepts</p> <p>Deep dive into normalizing flow theory</p> <p> Flow Explained</p> </li> <li> <p> Training Guide</p> <p>Scale flow training efficiently</p> <p> Training Guide</p> </li> <li> <p> Benchmarks</p> <p>Evaluate flow models</p> <p> Evaluation</p> </li> <li> <p> API Reference</p> <p>Complete flow API documentation</p> <p> Flow API</p> </li> </ul>"},{"location":"examples/advanced/advanced-gan/","title":"Advanced GAN Examples","text":"<p>This guide demonstrates advanced GAN architectures and training techniques using Artifex and Flax NNX. We cover progressive training, style-based generation, conditional GANs, and Wasserstein GANs with gradient penalty.</p>"},{"location":"examples/advanced/advanced-gan/#overview","title":"Overview","text":"<ul> <li> <p> Progressive GAN</p> <p>Grow the network progressively during training for high-resolution image generation</p> <p> Progressive GAN</p> </li> <li> <p> StyleGAN Patterns</p> <p>Style-based generator architecture with adaptive instance normalization</p> <p> StyleGAN</p> </li> <li> <p> Conditional GAN</p> <p>Class-conditional generation with label embedding</p> <p> Conditional GAN</p> </li> <li> <p> Wasserstein GAN</p> <p>Improved training stability with Wasserstein distance and gradient penalty</p> <p> WGAN-GP</p> </li> </ul>"},{"location":"examples/advanced/advanced-gan/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Artifex with all dependencies\nuv pip install \"artifex[cuda]\"  # With GPU support\n# or\nuv pip install artifex  # CPU only\n</code></pre> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nfrom artifex.generative_models.core import DeviceManager\nfrom artifex.generative_models.models.gan import GAN\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#progressive-gan","title":"Progressive GAN","text":"<p>Progressive GANs gradually increase the resolution of generated images during training, starting from low resolution (e.g., 4\u00d74) and progressively adding layers to reach high resolution (e.g., 1024\u00d71024).</p>"},{"location":"examples/advanced/advanced-gan/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[4\u00d74] --&gt; B[8\u00d78]\n    B --&gt; C[16\u00d716]\n    C --&gt; D[32\u00d732]\n    D --&gt; E[64\u00d764]\n    E --&gt; F[128\u00d7128]\n\n    style A fill:#e1f5ff\n    style B fill:#b3e5fc\n    style C fill:#81d4fa\n    style D fill:#4fc3f7\n    style E fill:#29b6f6\n    style F fill:#039be5</code></pre>"},{"location":"examples/advanced/advanced-gan/#progressive-layers","title":"Progressive Layers","text":"<pre><code>from flax import nnx\nimport jax.numpy as jnp\n\nclass ProgressiveConvBlock(nnx.Module):\n    \"\"\"Convolutional block with progressive growth support.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        *,\n        rngs: nnx.Rngs,\n        use_pixelnorm: bool = True,\n    ):\n        super().__init__()\n\n        self.conv1 = nnx.Conv(\n            in_features=in_channels,\n            out_features=out_channels,\n            kernel_size=(3, 3),\n            padding=1,\n            rngs=rngs,\n        )\n\n        self.conv2 = nnx.Conv(\n            in_features=out_channels,\n            out_features=out_channels,\n            kernel_size=(3, 3),\n            padding=1,\n            rngs=rngs,\n        )\n\n        self.use_pixelnorm = use_pixelnorm\n\n    def pixel_norm(self, x: jax.Array, epsilon: float = 1e-8) -&gt; jax.Array:\n        \"\"\"Pixel-wise feature normalization.\"\"\"\n        return x / jnp.sqrt(jnp.mean(x ** 2, axis=-1, keepdims=True) + epsilon)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        x = self.conv1(x)\n        x = nnx.leaky_relu(x, negative_slope=0.2)\n        if self.use_pixelnorm:\n            x = self.pixel_norm(x)\n\n        x = self.conv2(x)\n        x = nnx.leaky_relu(x, negative_slope=0.2)\n        if self.use_pixelnorm:\n            x = self.pixel_norm(x)\n\n        return x\n\n\nclass ProgressiveGenerator(nnx.Module):\n    \"\"\"Generator with progressive growing.\"\"\"\n\n    def __init__(\n        self,\n        latent_dim: int,\n        max_channels: int = 512,\n        num_stages: int = 6,  # 4x4 to 128x128\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.num_stages = num_stages\n        self.current_stage = 0  # Start at 4x4\n\n        # Initial 4x4 block\n        self.initial = nnx.Sequential(\n            nnx.Linear(latent_dim, max_channels * 4 * 4, rngs=rngs),\n            # Reshape happens in forward pass\n        )\n\n        # Progressive blocks\n        blocks = []\n        for i in range(num_stages - 1):\n            in_ch = max_channels // (2 ** i)\n            out_ch = max_channels // (2 ** (i + 1))\n\n            block = ProgressiveConvBlock(\n                in_channels=in_ch,\n                out_channels=out_ch,\n                rngs=rngs,\n            )\n            blocks.append(block)\n        self.blocks = nnx.List(blocks)\n\n        # Output layers (to RGB) for each resolution\n        to_rgb_layers = []\n        for i in range(num_stages):\n            ch = max_channels // (2 ** i)\n            to_rgb = nnx.Conv(\n                in_features=ch,\n                out_features=3,\n                kernel_size=(1, 1),\n                rngs=rngs,\n            )\n            to_rgb_layers.append(to_rgb)\n        self.to_rgb_layers = nnx.List(to_rgb_layers)\n\n    def set_stage(self, stage: int):\n        \"\"\"Set the current training stage (resolution level).\"\"\"\n        self.current_stage = min(stage, self.num_stages - 1)\n\n    def __call__(\n        self,\n        z: jax.Array,\n        alpha: float = 1.0,  # Blending factor for smooth transition\n    ) -&gt; jax.Array:\n        \"\"\"\n        Generate images at the current resolution.\n\n        Args:\n            z: Latent vectors [batch, latent_dim]\n            alpha: Blending factor (0=previous resolution, 1=current resolution)\n\n        Returns:\n            Generated images [batch, H, W, 3]\n        \"\"\"\n        batch_size = z.shape[0]\n\n        # Initial 4x4 block\n        x = self.initial(z)\n        x = x.reshape(batch_size, 4, 4, -1)\n\n        # Progress through stages\n        for stage in range(self.current_stage):\n            # Upsample\n            h, w, c = x.shape[1:]\n            x = jax.image.resize(x, (batch_size, h * 2, w * 2, c), method=\"nearest\")\n\n            # Apply block\n            x = self.blocks[stage](x)\n\n        # Convert to RGB\n        if alpha &lt; 1.0 and self.current_stage &gt; 0:\n            # Blend with previous resolution\n            prev_rgb = self.to_rgb_layers[self.current_stage - 1](x)\n            h, w = prev_rgb.shape[1:3]\n            prev_rgb = jax.image.resize(prev_rgb, (batch_size, h * 2, w * 2, 3), method=\"nearest\")\n\n            curr_rgb = self.to_rgb_layers[self.current_stage](x)\n            output = alpha * curr_rgb + (1 - alpha) * prev_rgb\n        else:\n            output = self.to_rgb_layers[self.current_stage](x)\n\n        return nnx.tanh(output)\n\n\nclass ProgressiveDiscriminator(nnx.Module):\n    \"\"\"Discriminator with progressive growing.\"\"\"\n\n    def __init__(\n        self,\n        max_channels: int = 512,\n        num_stages: int = 6,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.num_stages = num_stages\n        self.current_stage = 0\n\n        # From RGB layers for each resolution\n        from_rgb_layers = []\n        for i in range(num_stages):\n            ch = max_channels // (2 ** (num_stages - 1 - i))\n            from_rgb = nnx.Conv(\n                in_features=3,\n                out_features=ch,\n                kernel_size=(1, 1),\n                rngs=rngs,\n            )\n            from_rgb_layers.append(from_rgb)\n        self.from_rgb_layers = nnx.List(from_rgb_layers)\n\n        # Progressive blocks\n        blocks = []\n        for i in range(num_stages - 1):\n            in_ch = max_channels // (2 ** (num_stages - 1 - i))\n            out_ch = max_channels // (2 ** (num_stages - 2 - i))\n\n            block = ProgressiveConvBlock(\n                in_channels=in_ch,\n                out_channels=out_ch,\n                rngs=rngs,\n                use_pixelnorm=False,\n            )\n            blocks.append(block)\n        self.blocks = nnx.List(blocks)\n\n        # Final block layers (applied manually to avoid Lambda)\n        self.final_conv1 = nnx.Conv(max_channels + 1, max_channels, kernel_size=(3, 3), padding=1, rngs=rngs)\n        self.final_conv2 = nnx.Conv(max_channels, max_channels, kernel_size=(4, 4), rngs=rngs)\n        self.final_fc = nnx.Linear(max_channels, 1, rngs=rngs)\n\n    def set_stage(self, stage: int):\n        \"\"\"Set the current training stage.\"\"\"\n        self.current_stage = min(stage, self.num_stages - 1)\n\n    def minibatch_std(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Add minibatch standard deviation as additional feature.\"\"\"\n        batch_std = jnp.std(x, axis=0, keepdims=True)\n        mean_std = jnp.mean(batch_std)\n\n        # Replicate across batch and spatial dimensions\n        batch_size, h, w, _ = x.shape\n        std_feature = jnp.ones((batch_size, h, w, 1)) * mean_std\n\n        return jnp.concatenate([x, std_feature], axis=-1)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        alpha: float = 1.0,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Discriminate images at current resolution.\n\n        Args:\n            x: Input images [batch, H, W, 3]\n            alpha: Blending factor for smooth transition\n\n        Returns:\n            Discriminator scores [batch, 1]\n        \"\"\"\n        batch_size = x.shape[0]\n\n        # Convert from RGB\n        if alpha &lt; 1.0 and self.current_stage &gt; 0:\n            # Blend with downsampled previous resolution\n            h, w = x.shape[1:3]\n            x_down = jax.image.resize(x, (batch_size, h // 2, w // 2, 3), method=\"bilinear\")\n\n            prev_features = self.from_rgb_layers[self.current_stage - 1](x_down)\n            prev_features = nnx.leaky_relu(prev_features, negative_slope=0.2)\n\n            curr_features = self.from_rgb_layers[self.current_stage](x)\n            curr_features = nnx.leaky_relu(curr_features, negative_slope=0.2)\n            curr_features = self.blocks[self.num_stages - 1 - self.current_stage](curr_features)\n\n            # Average pool current features\n            h, w, c = curr_features.shape[1:]\n            curr_features = jax.image.resize(curr_features, (batch_size, h // 2, w // 2, c), method=\"bilinear\")\n\n            features = alpha * curr_features + (1 - alpha) * prev_features\n        else:\n            features = self.from_rgb_layers[self.current_stage](x)\n            features = nnx.leaky_relu(features, negative_slope=0.2)\n\n        # Progress through blocks\n        for stage in range(self.num_stages - 1 - self.current_stage, self.num_stages - 1):\n            features = self.blocks[stage](features)\n\n            # Downsample\n            batch_size, h, w, c = features.shape\n            features = jax.image.resize(features, (batch_size, h // 2, w // 2, c), method=\"bilinear\")\n\n        # Add minibatch std and final layers\n        features = self.minibatch_std(features)\n\n        # Apply final layers manually\n        h = nnx.leaky_relu(self.final_conv1(features), negative_slope=0.2)\n        h = nnx.leaky_relu(self.final_conv2(h), negative_slope=0.2)\n        h = h.reshape(h.shape[0], -1)\n        output = self.final_fc(h)\n\n        return output\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#progressive-training-loop","title":"Progressive Training Loop","text":"<pre><code>def train_progressive_gan(\n    config: dict,\n    train_data: jnp.ndarray,\n    num_epochs_per_stage: int = 100,\n    transition_epochs: int = 100,\n):\n    \"\"\"Train Progressive GAN with gradual resolution increase.\"\"\"\n\n    # Initialize\n    device_manager = DeviceManager()\n    rngs = nnx.Rngs(config[\"seed\"])\n\n    # Create models\n    generator = ProgressiveGenerator(\n        latent_dim=config[\"latent_dim\"],\n        max_channels=512,\n        num_stages=6,\n        rngs=rngs,\n    )\n\n    discriminator = ProgressiveDiscriminator(\n        max_channels=512,\n        num_stages=6,\n        rngs=rngs,\n    )\n\n    # Optimizers with learning rate decay\n    lr_schedule = optax.exponential_decay(\n        init_value=0.001,\n        transition_steps=1000,\n        decay_rate=0.99,\n    )\n\n    g_optimizer = nnx.Optimizer(generator, optax.adam(lr_schedule), wrt=nnx.Param)\n    d_optimizer = nnx.Optimizer(discriminator, optax.adam(lr_schedule), wrt=nnx.Param)\n\n    # Training loop over stages\n    for stage in range(6):\n        print(f\"\\n{'='*50}\")\n        print(f\"Training Stage {stage} (Resolution: {4 * (2**stage)}x{4 * (2**stage)})\")\n        print(f\"{'='*50}\\n\")\n\n        generator.set_stage(stage)\n        discriminator.set_stage(stage)\n\n        # Transition phase (gradually blend in new layers)\n        for epoch in range(transition_epochs):\n            alpha = epoch / transition_epochs  # 0 -&gt; 1\n\n            for batch in get_batches_for_resolution(train_data, stage):\n                # Train discriminator\n                z = jax.random.normal(rngs.params(), (batch.shape[0], config[\"latent_dim\"]))\n                fake_images = generator(z, alpha=alpha)\n\n                real_scores = discriminator(batch, alpha=alpha)\n                fake_scores = discriminator(fake_images, alpha=alpha)\n\n                d_loss = -jnp.mean(real_scores) + jnp.mean(fake_scores)\n\n                # Gradient penalty\n                epsilon = jax.random.uniform(rngs.params(), (batch.shape[0], 1, 1, 1))\n                interpolated = epsilon * batch + (1 - epsilon) * fake_images\n\n                def d_interpolated(x):\n                    return discriminator(x, alpha=alpha)\n\n                gradients = jax.grad(lambda x: jnp.sum(d_interpolated(x)))(interpolated)\n                gradient_penalty = jnp.mean((jnp.sqrt(jnp.sum(gradients ** 2, axis=(1, 2, 3))) - 1) ** 2)\n\n                d_loss = d_loss + 10.0 * gradient_penalty\n\n                # Update discriminator\n                d_grads = jax.grad(lambda m: d_loss)(discriminator)\n                d_optimizer.update(discriminator, d_grads)\n\n                # Train generator\n                z = jax.random.normal(rngs.params(), (batch.shape[0], config[\"latent_dim\"]))\n                fake_images = generator(z, alpha=alpha)\n                fake_scores = discriminator(fake_images, alpha=alpha)\n\n                g_loss = -jnp.mean(fake_scores)\n\n                # Update generator\n                g_grads = jax.grad(lambda m: g_loss)(generator)\n                g_optimizer.update(generator, g_grads)\n\n            if epoch % 10 == 0:\n                print(f\"Stage {stage}, Transition Epoch {epoch}/{transition_epochs}, \"\n                      f\"Alpha: {alpha:.2f}, G Loss: {g_loss:.4f}, D Loss: {d_loss:.4f}\")\n\n        # Stabilization phase (train at full resolution)\n        print(f\"\\nStabilization phase for stage {stage}\")\n\n        for epoch in range(num_epochs_per_stage):\n            for batch in get_batches_for_resolution(train_data, stage):\n                # Train with alpha=1.0 (full new resolution)\n                # ... (similar to transition phase but with alpha=1.0)\n                pass\n\n            if epoch % 10 == 0:\n                print(f\"Stage {stage}, Stabilization Epoch {epoch}/{num_epochs_per_stage}\")\n\n    return generator, discriminator\n\n\ndef get_batches_for_resolution(data: jnp.ndarray, stage: int, batch_size: int = 32):\n    \"\"\"Get data batches at the appropriate resolution for the current stage.\"\"\"\n    target_size = 4 * (2 ** stage)\n\n    # Resize data to target resolution\n    batch_data = jax.image.resize(\n        data,\n        (data.shape[0], target_size, target_size, data.shape[-1]),\n        method=\"bilinear\"\n    )\n\n    # Yield batches\n    num_batches = len(batch_data) // batch_size\n    for i in range(num_batches):\n        yield batch_data[i * batch_size:(i + 1) * batch_size]\n</code></pre> <p>Progressive Training Tips</p> <ul> <li>Start with a small learning rate (0.001) and decay gradually</li> <li>Use equal number of steps for transition and stabilization</li> <li>Monitor FID score at each resolution</li> <li>Use pixel normalization in generator for stability</li> <li>Add minibatch standard deviation in discriminator</li> </ul>"},{"location":"examples/advanced/advanced-gan/#stylegan-patterns","title":"StyleGAN Patterns","text":"<p>StyleGAN introduces style-based generator architecture with adaptive instance normalization (AdaIN) for better control over generated images.</p>"},{"location":"examples/advanced/advanced-gan/#style-based-generator","title":"Style-Based Generator","text":"<pre><code>class AdaptiveInstanceNorm(nnx.Module):\n    \"\"\"Adaptive Instance Normalization for style injection.\"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        style_dim: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        # Affine transformation from style to scale and bias\n        self.style_scale = nnx.Linear(style_dim, num_features, rngs=rngs)\n        self.style_bias = nnx.Linear(style_dim, num_features, rngs=rngs)\n\n        self.epsilon = 1e-8\n\n    def __call__(self, x: jax.Array, style: jax.Array) -&gt; jax.Array:\n        \"\"\"\n        Apply AdaIN.\n\n        Args:\n            x: Feature maps [batch, H, W, channels]\n            style: Style vector [batch, style_dim]\n\n        Returns:\n            Normalized and styled features\n        \"\"\"\n        # Instance normalization\n        mean = jnp.mean(x, axis=(1, 2), keepdims=True)\n        var = jnp.var(x, axis=(1, 2), keepdims=True)\n        normalized = (x - mean) / jnp.sqrt(var + self.epsilon)\n\n        # Style modulation\n        scale = self.style_scale(style)[:, None, None, :]  # [batch, 1, 1, channels]\n        bias = self.style_bias(style)[:, None, None, :]\n\n        return normalized * (1 + scale) + bias\n\n\nclass StyleConvBlock(nnx.Module):\n    \"\"\"Convolutional block with style modulation.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        style_dim: int,\n        *,\n        rngs: nnx.Rngs,\n        upsample: bool = False,\n    ):\n        super().__init__()\n\n        self.upsample = upsample\n\n        self.conv = nnx.Conv(\n            in_features=in_channels,\n            out_features=out_channels,\n            kernel_size=(3, 3),\n            padding=1,\n            rngs=rngs,\n        )\n\n        self.adain = AdaptiveInstanceNorm(\n            num_features=out_channels,\n            style_dim=style_dim,\n            rngs=rngs,\n        )\n\n        # Noise injection\n        self.noise_weight = nnx.Param(jnp.zeros((1, 1, 1, out_channels)))\n\n    def __call__(\n        self,\n        x: jax.Array,\n        style: jax.Array,\n        *,\n        rngs: nnx.Rngs | None = None,\n    ) -&gt; jax.Array:\n        if self.upsample:\n            batch, h, w, c = x.shape\n            x = jax.image.resize(x, (batch, h * 2, w * 2, c), method=\"nearest\")\n\n        x = self.conv(x)\n\n        # Add noise\n        if rngs is not None and \"noise\" in rngs:\n            noise = jax.random.normal(rngs.noise(), x.shape)\n            x = x + self.noise_weight.value * noise\n\n        # Apply AdaIN\n        x = self.adain(x, style)\n        x = nnx.leaky_relu(x, negative_slope=0.2)\n\n        return x\n\n\nclass MappingNetwork(nnx.Module):\n    \"\"\"Mapping network to transform latent code to style vector.\"\"\"\n\n    def __init__(\n        self,\n        latent_dim: int,\n        style_dim: int,\n        num_layers: int = 8,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        layers = []\n        for i in range(num_layers):\n            in_dim = latent_dim if i == 0 else style_dim\n            layers.append(nnx.Linear(in_dim, style_dim, rngs=rngs))\n\n        self.layers = layers\n\n    def __call__(self, z: jax.Array) -&gt; jax.Array:\n        \"\"\"Map latent code to style vector.\"\"\"\n        w = z\n        for layer in self.layers:\n            w = layer(w)\n            w = nnx.leaky_relu(w, negative_slope=0.2)\n        return w\n\n\nclass StyleGANGenerator(nnx.Module):\n    \"\"\"StyleGAN-style generator with style modulation.\"\"\"\n\n    def __init__(\n        self,\n        latent_dim: int = 512,\n        style_dim: int = 512,\n        image_size: int = 256,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.style_dim = style_dim\n\n        # Mapping network\n        self.mapping = MappingNetwork(\n            latent_dim=latent_dim,\n            style_dim=style_dim,\n            num_layers=8,\n            rngs=rngs,\n        )\n\n        # Constant input\n        self.constant_input = nnx.Param(jnp.ones((1, 4, 4, 512)))\n\n        # Style blocks\n        num_blocks = int(jnp.log2(image_size)) - 1  # e.g., 6 for 256x256\n\n        style_blocks = []\n        in_channels = 512\n\n        for i in range(num_blocks):\n            out_channels = 512 // (2 ** min(i, 3))\n\n            block = StyleConvBlock(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                style_dim=style_dim,\n                rngs=rngs,\n                upsample=i &gt; 0,\n            )\n            style_blocks.append(block)\n            in_channels = out_channels\n        self.style_blocks = nnx.List(style_blocks)\n\n        # To RGB\n        self.to_rgb = nnx.Conv(\n            in_features=in_channels,\n            out_features=3,\n            kernel_size=(1, 1),\n            rngs=rngs,\n        )\n\n    def __call__(\n        self,\n        z: jax.Array,\n        *,\n        rngs: nnx.Rngs | None = None,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Generate images with style modulation.\n\n        Args:\n            z: Latent codes [batch, latent_dim]\n            rngs: Random number generators for noise injection\n\n        Returns:\n            Generated images [batch, H, W, 3]\n        \"\"\"\n        batch_size = z.shape[0]\n\n        # Map to style space\n        w = self.mapping(z)\n\n        # Start from constant input\n        x = jnp.tile(self.constant_input.value, (batch_size, 1, 1, 1))\n\n        # Apply style blocks\n        for block in self.style_blocks:\n            x = block(x, w, rngs=rngs)\n\n        # Convert to RGB\n        rgb = self.to_rgb(x)\n        return nnx.tanh(rgb)\n\n    def style_mixing(\n        self,\n        z1: jax.Array,\n        z2: jax.Array,\n        mix_layer: int,\n        *,\n        rngs: nnx.Rngs | None = None,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Generate images with style mixing.\n\n        Args:\n            z1: First latent codes [batch, latent_dim]\n            z2: Second latent codes [batch, latent_dim]\n            mix_layer: Layer index to switch from z1 to z2\n            rngs: Random number generators\n\n        Returns:\n            Generated images with mixed styles\n        \"\"\"\n        batch_size = z1.shape[0]\n\n        # Map both to style space\n        w1 = self.mapping(z1)\n        w2 = self.mapping(z2)\n\n        # Start from constant\n        x = jnp.tile(self.constant_input.value, (batch_size, 1, 1, 1))\n\n        # Apply blocks with style switching\n        for i, block in enumerate(self.style_blocks):\n            w = w1 if i &lt; mix_layer else w2\n            x = block(x, w, rngs=rngs)\n\n        rgb = self.to_rgb(x)\n        return nnx.tanh(rgb)\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#style-mixing-example","title":"Style Mixing Example","text":"<pre><code>def demonstrate_style_mixing():\n    \"\"\"Demonstrate style mixing for disentanglement visualization.\"\"\"\n\n    # Initialize generator\n    rngs = nnx.Rngs(42)\n    generator = StyleGANGenerator(\n        latent_dim=512,\n        style_dim=512,\n        image_size=256,\n        rngs=rngs,\n    )\n\n    # Generate latent codes\n    z1 = jax.random.normal(rngs.params(), (1, 512))\n    z2 = jax.random.normal(rngs.params(), (1, 512))\n\n    # Generate images with style mixing at different layers\n    results = []\n    for mix_layer in range(len(generator.style_blocks)):\n        mixed_image = generator.style_mixing(z1, z2, mix_layer, rngs=rngs)\n        results.append(mixed_image)\n\n    # Visualize: early layers affect coarse features, later layers affect fine details\n    import matplotlib.pyplot as plt\n\n    fig, axes = plt.subplots(1, len(results), figsize=(20, 3))\n    for i, img in enumerate(results):\n        axes[i].imshow((img[0] + 1) / 2)  # Denormalize\n        axes[i].set_title(f\"Mix at layer {i}\")\n        axes[i].axis(\"off\")\n\n    plt.tight_layout()\n    plt.savefig(\"style_mixing.png\")\n    print(\"Style mixing visualization saved to style_mixing.png\")\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#conditional-gan","title":"Conditional GAN","text":"<p>Conditional GANs extend standard GANs by conditioning generation on additional information like class labels.</p>"},{"location":"examples/advanced/advanced-gan/#conditional-architecture","title":"Conditional Architecture","text":"<pre><code>class ConditionalGenerator(nnx.Module):\n    \"\"\"Generator conditioned on class labels.\"\"\"\n\n    def __init__(\n        self,\n        latent_dim: int,\n        num_classes: int,\n        image_channels: int = 3,\n        hidden_dim: int = 256,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.num_classes = num_classes\n\n        # Label embedding\n        self.label_embedding = nnx.Embed(\n            num_embeddings=num_classes,\n            features=latent_dim,\n            rngs=rngs,\n        )\n\n        # Generator layers\n        self.fc = nnx.Linear(latent_dim * 2, hidden_dim * 8 * 7 * 7, rngs=rngs)\n\n        # Upsample blocks\n        self.deconv1 = nnx.ConvTranspose(\n            in_features=hidden_dim * 8,\n            out_features=hidden_dim * 4,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=1,\n            rngs=rngs,\n        )\n        self.bn1 = nnx.BatchNorm(num_features=hidden_dim * 4, rngs=rngs)\n\n        self.deconv2 = nnx.ConvTranspose(\n            in_features=hidden_dim * 4,\n            out_features=hidden_dim * 2,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=1,\n            rngs=rngs,\n        )\n        self.bn2 = nnx.BatchNorm(num_features=hidden_dim * 2, rngs=rngs)\n\n        self.final_conv = nnx.Conv(\n            in_features=hidden_dim * 2,\n            out_features=image_channels,\n            kernel_size=(3, 3),\n            padding=1,\n            rngs=rngs,\n        )\n\n        self.hidden_dim = hidden_dim\n\n    def __call__(\n        self,\n        z: jax.Array,\n        labels: jax.Array,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Generate images conditioned on labels.\n\n        Args:\n            z: Latent vectors [batch, latent_dim]\n            labels: Class labels [batch] (integer indices)\n\n        Returns:\n            Generated images [batch, H, W, channels]\n        \"\"\"\n        # Embed labels\n        label_embed = self.label_embedding(labels)\n\n        # Concatenate z and label embedding\n        combined = jnp.concatenate([z, label_embed], axis=-1)\n\n        # Forward pass\n        x = nnx.relu(self.fc(combined))\n        x = x.reshape(x.shape[0], 7, 7, self.hidden_dim * 8)\n\n        x = nnx.relu(self.bn1(self.deconv1(x)))\n        x = nnx.relu(self.bn2(self.deconv2(x)))\n\n        return nnx.tanh(self.final_conv(x))\n\n\nclass ConditionalDiscriminator(nnx.Module):\n    \"\"\"Discriminator conditioned on class labels.\"\"\"\n\n    def __init__(\n        self,\n        num_classes: int,\n        image_channels: int = 3,\n        hidden_dim: int = 256,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.num_classes = num_classes\n\n        # Label embedding (project to spatial dimensions)\n        self.label_embedding = nnx.Embed(\n            num_embeddings=num_classes,\n            features=28 * 28,  # Match image spatial size\n            rngs=rngs,\n        )\n\n        # Discriminator layers\n        self.conv1 = nnx.Conv(\n            in_features=image_channels + 1,\n            out_features=hidden_dim,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=1,\n            rngs=rngs,\n        )\n        self.conv2 = nnx.Conv(\n            in_features=hidden_dim,\n            out_features=hidden_dim * 2,\n            kernel_size=(4, 4),\n            strides=(2, 2),\n            padding=1,\n            rngs=rngs,\n        )\n        self.bn = nnx.BatchNorm(num_features=hidden_dim * 2, rngs=rngs)\n        self.fc = nnx.Linear(hidden_dim * 2 * 7 * 7, 1, rngs=rngs)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        labels: jax.Array,\n    ) -&gt; jax.Array:\n        \"\"\"\n        Discriminate images conditioned on labels.\n\n        Args:\n            x: Input images [batch, H, W, channels]\n            labels: Class labels [batch]\n\n        Returns:\n            Discriminator scores [batch, 1]\n        \"\"\"\n        batch_size = x.shape[0]\n\n        # Embed labels and reshape to spatial map\n        label_embed = self.label_embedding(labels)\n        label_map = label_embed.reshape(batch_size, 28, 28, 1)\n\n        # Concatenate image and label map\n        combined = jnp.concatenate([x, label_map], axis=-1)\n\n        # Forward pass\n        h = nnx.leaky_relu(self.conv1(combined), negative_slope=0.2)\n        h = nnx.leaky_relu(self.bn(self.conv2(h)), negative_slope=0.2)\n        h = h.reshape(batch_size, -1)\n        return self.fc(h)\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#training-conditional-gan","title":"Training Conditional GAN","text":"<pre><code>def train_conditional_gan(\n    train_data: jnp.ndarray,\n    train_labels: jnp.ndarray,\n    config: dict,\n    num_epochs: int = 100,\n):\n    \"\"\"Train conditional GAN on labeled data.\"\"\"\n\n    rngs = nnx.Rngs(config[\"seed\"])\n\n    # Initialize models\n    generator = ConditionalGenerator(\n        latent_dim=config[\"latent_dim\"],\n        num_classes=config[\"num_classes\"],\n        image_channels=config.get(\"image_channels\", 3),\n        rngs=rngs,\n    )\n\n    discriminator = ConditionalDiscriminator(\n        num_classes=config[\"num_classes\"],\n        image_channels=config.get(\"image_channels\", 3),\n        rngs=rngs,\n    )\n\n    # Optimizers (wrt=nnx.Param required in NNX 0.11.0+)\n    g_optimizer = nnx.Optimizer(generator, optax.adam(config[\"lr\"]), wrt=nnx.Param)\n    d_optimizer = nnx.Optimizer(discriminator, optax.adam(config[\"lr\"]), wrt=nnx.Param)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        for batch_idx in range(0, len(train_data), config[\"batch_size\"]):\n            real_images = train_data[batch_idx:batch_idx + config[\"batch_size\"]]\n            real_labels = train_labels[batch_idx:batch_idx + config[\"batch_size\"]]\n            batch_size = real_images.shape[0]\n\n            # Train discriminator\n            z = jax.random.normal(rngs.params(), (batch_size, config[\"latent_dim\"]))\n            fake_labels = jax.random.randint(rngs.params(), (batch_size,), 0, config[\"num_classes\"])\n\n            fake_images = generator(z, fake_labels)\n\n            real_scores = discriminator(real_images, real_labels)\n            fake_scores = discriminator(fake_images, fake_labels)\n\n            d_loss = -jnp.mean(jnp.log(nnx.sigmoid(real_scores) + 1e-8) +\n                              jnp.log(1 - nnx.sigmoid(fake_scores) + 1e-8))\n\n            d_grads = jax.grad(lambda m: d_loss)(discriminator)\n            d_optimizer.update(discriminator, d_grads)\n\n            # Train generator\n            z = jax.random.normal(rngs.params(), (batch_size, config[\"latent_dim\"]))\n            fake_labels = jax.random.randint(rngs.params(), (batch_size,), 0, config[\"num_classes\"])\n\n            fake_images = generator(z, fake_labels)\n            fake_scores = discriminator(fake_images, fake_labels)\n\n            g_loss = -jnp.mean(jnp.log(nnx.sigmoid(fake_scores) + 1e-8))\n\n            g_grads = jax.grad(lambda m: g_loss)(generator)\n            g_optimizer.update(generator, g_grads)\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}/{num_epochs}, G Loss: {g_loss:.4f}, D Loss: {d_loss:.4f}\")\n\n    return generator, discriminator\n\n\n# Generate specific classes\ndef generate_by_class(generator: ConditionalGenerator, class_id: int, num_samples: int = 16):\n    \"\"\"Generate samples for a specific class.\"\"\"\n    rngs = nnx.Rngs(42)\n    z = jax.random.normal(rngs.params(), (num_samples, generator.latent_dim))\n    labels = jnp.ones(num_samples, dtype=jnp.int32) * class_id\n\n    return generator(z, labels)\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#wasserstein-gan-with-gradient-penalty","title":"Wasserstein GAN with Gradient Penalty","text":"<p>WGAN-GP improves training stability using Wasserstein distance and gradient penalty instead of standard GAN loss.</p>"},{"location":"examples/advanced/advanced-gan/#wasserstein-loss-implementation","title":"Wasserstein Loss Implementation","text":"<pre><code>def wasserstein_discriminator_loss(\n    discriminator: nnx.Module,\n    real_images: jax.Array,\n    fake_images: jax.Array,\n    lambda_gp: float = 10.0,\n) -&gt; tuple[jax.Array, dict]:\n    \"\"\"\n    Compute Wasserstein discriminator loss with gradient penalty.\n\n    Args:\n        discriminator: Discriminator (critic) network\n        real_images: Real image batch\n        fake_images: Generated image batch\n        lambda_gp: Gradient penalty coefficient\n\n    Returns:\n        Tuple of (loss, metrics_dict)\n    \"\"\"\n    # Wasserstein distance\n    real_scores = discriminator(real_images)\n    fake_scores = discriminator(fake_images)\n\n    w_distance = jnp.mean(real_scores) - jnp.mean(fake_scores)\n\n    # Gradient penalty\n    batch_size = real_images.shape[0]\n    alpha = jax.random.uniform(jax.random.PRNGKey(0), (batch_size, 1, 1, 1))\n\n    interpolated = alpha * real_images + (1 - alpha) * fake_images\n\n    def critic_interpolated(x):\n        return jnp.sum(discriminator(x))\n\n    gradients = jax.grad(critic_interpolated)(interpolated)\n\n    # Compute gradient norm\n    gradient_norms = jnp.sqrt(jnp.sum(gradients ** 2, axis=(1, 2, 3)))\n    gradient_penalty = jnp.mean((gradient_norms - 1.0) ** 2)\n\n    # Total loss (minimize negative Wasserstein distance + GP)\n    loss = -w_distance + lambda_gp * gradient_penalty\n\n    metrics = {\n        \"w_distance\": w_distance,\n        \"gradient_penalty\": gradient_penalty,\n        \"real_scores\": jnp.mean(real_scores),\n        \"fake_scores\": jnp.mean(fake_scores),\n    }\n\n    return loss, metrics\n\n\ndef wasserstein_generator_loss(\n    generator: nnx.Module,\n    discriminator: nnx.Module,\n    z: jax.Array,\n    *,\n    rngs: nnx.Rngs | None = None,\n) -&gt; tuple[jax.Array, dict]:\n    \"\"\"\n    Compute Wasserstein generator loss.\n\n    Args:\n        generator: Generator network\n        discriminator: Discriminator (critic) network\n        z: Latent vectors\n        rngs: Random number generators\n\n    Returns:\n        Tuple of (loss, metrics_dict)\n    \"\"\"\n    fake_images = generator(z, rngs=rngs)\n    fake_scores = discriminator(fake_images)\n\n    # Generator wants to maximize discriminator score\n    # (minimize negative score)\n    loss = -jnp.mean(fake_scores)\n\n    metrics = {\n        \"fake_scores\": jnp.mean(fake_scores),\n    }\n\n    return loss, metrics\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#wgan-gp-training-loop","title":"WGAN-GP Training Loop","text":"<pre><code>def train_wgan_gp(\n    train_data: jnp.ndarray,\n    config: dict,\n    num_epochs: int = 100,\n    n_critic: int = 5,  # Train critic 5x per generator update\n):\n    \"\"\"\n    Train WGAN with gradient penalty.\n\n    Args:\n        train_data: Training images\n        config: Training configuration\n        num_epochs: Number of training epochs\n        n_critic: Number of critic updates per generator update\n    \"\"\"\n\n    rngs = nnx.Rngs(config[\"seed\"])\n\n    # Initialize models (use any generator/discriminator architecture)\n    generator = StyleGANGenerator(\n        latent_dim=config[\"latent_dim\"],\n        style_dim=config[\"style_dim\"],\n        image_size=config[\"image_size\"],\n        rngs=rngs,\n    )\n\n    # Note: For WGAN, discriminator is called \"critic\" and has no sigmoid\n    discriminator = ProgressiveDiscriminator(\n        max_channels=512,\n        num_stages=int(jnp.log2(config[\"image_size\"])) - 1,\n        rngs=rngs,\n    )\n\n    # RMSprop is recommended for WGAN (wrt=nnx.Param required in NNX 0.11.0+)\n    g_optimizer = nnx.Optimizer(generator, optax.rmsprop(config[\"lr\"]), wrt=nnx.Param)\n    d_optimizer = nnx.Optimizer(discriminator, optax.rmsprop(config[\"lr\"]), wrt=nnx.Param)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        for batch_idx in range(0, len(train_data), config[\"batch_size\"]):\n            real_images = train_data[batch_idx:batch_idx + config[\"batch_size\"]]\n            batch_size = real_images.shape[0]\n\n            # Train critic multiple times\n            for _ in range(n_critic):\n                z = jax.random.normal(rngs.params(), (batch_size, config[\"latent_dim\"]))\n                fake_images = generator(z, rngs=rngs)\n\n                d_loss, d_metrics = wasserstein_discriminator_loss(\n                    discriminator,\n                    real_images,\n                    fake_images,\n                    lambda_gp=config.get(\"lambda_gp\", 10.0),\n                )\n\n                d_grads = jax.grad(lambda m: d_loss)(discriminator)\n                d_optimizer.update(discriminator, d_grads)\n\n            # Train generator\n            z = jax.random.normal(rngs.params(), (batch_size, config[\"latent_dim\"]))\n            g_loss, g_metrics = wasserstein_generator_loss(\n                generator,\n                discriminator,\n                z,\n                rngs=rngs,\n            )\n\n            g_grads = jax.grad(lambda m: g_loss)(generator)\n            g_optimizer.update(generator, g_grads)\n\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}/{num_epochs}\")\n            print(f\"  G Loss: {g_loss:.4f}\")\n            print(f\"  D Loss: {d_loss:.4f}\")\n            print(f\"  W Distance: {d_metrics['w_distance']:.4f}\")\n            print(f\"  GP: {d_metrics['gradient_penalty']:.4f}\")\n\n    return generator, discriminator\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#common-patterns","title":"Common Patterns","text":""},{"location":"examples/advanced/advanced-gan/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<pre><code>def create_gan_schedule(\n    base_lr: float = 0.0002,\n    warmup_steps: int = 1000,\n    decay_steps: int = 10000,\n) -&gt; optax.Schedule:\n    \"\"\"Create learning rate schedule for GAN training.\"\"\"\n\n    warmup = optax.linear_schedule(\n        init_value=0.0,\n        end_value=base_lr,\n        transition_steps=warmup_steps,\n    )\n\n    decay = optax.exponential_decay(\n        init_value=base_lr,\n        transition_steps=decay_steps,\n        decay_rate=0.95,\n    )\n\n    return optax.join_schedules(\n        schedules=[warmup, decay],\n        boundaries=[warmup_steps],\n    )\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#gradient-accumulation","title":"Gradient Accumulation","text":"<pre><code>def train_with_gradient_accumulation(\n    generator: nnx.Module,\n    discriminator: nnx.Module,\n    data_loader,\n    accumulation_steps: int = 4,\n):\n    \"\"\"Train with gradient accumulation for larger effective batch size.\"\"\"\n\n    # wrt=nnx.Param required in NNX 0.11.0+\n    g_optimizer = nnx.Optimizer(generator, optax.adam(0.0002), wrt=nnx.Param)\n    d_optimizer = nnx.Optimizer(discriminator, optax.adam(0.0002), wrt=nnx.Param)\n\n    g_grads_accumulated = None\n    d_grads_accumulated = None\n\n    for step, batch in enumerate(data_loader):\n        # Compute gradients\n        z = jax.random.normal(jax.random.PRNGKey(step), (batch.shape[0], 512))\n        fake_images = generator(z)\n\n        d_grads = jax.grad(lambda m: wasserstein_discriminator_loss(m, batch, fake_images)[0])(discriminator)\n        g_grads = jax.grad(lambda m: wasserstein_generator_loss(m, discriminator, z)[0])(generator)\n\n        # Accumulate gradients\n        if g_grads_accumulated is None:\n            g_grads_accumulated = g_grads\n            d_grads_accumulated = d_grads\n        else:\n            g_grads_accumulated = jax.tree_map(lambda x, y: x + y, g_grads_accumulated, g_grads)\n            d_grads_accumulated = jax.tree_map(lambda x, y: x + y, d_grads_accumulated, d_grads)\n\n        # Update every N steps\n        if (step + 1) % accumulation_steps == 0:\n            # Average accumulated gradients\n            g_grads_avg = jax.tree_map(lambda x: x / accumulation_steps, g_grads_accumulated)\n            d_grads_avg = jax.tree_map(lambda x: x / accumulation_steps, d_grads_accumulated)\n\n            # Apply updates (NNX 0.11.0+ API)\n            g_optimizer.update(generator, g_grads_avg)\n            d_optimizer.update(discriminator, d_grads_avg)\n\n            # Reset accumulation\n            g_grads_accumulated = None\n            d_grads_accumulated = None\n</code></pre>"},{"location":"examples/advanced/advanced-gan/#best-practices","title":"Best Practices","text":"<p>DO</p> <ul> <li>Use Progressive training for high-resolution images (&gt;256px)</li> <li>Implement style mixing for better disentanglement</li> <li>Use WGAN-GP for stable training</li> <li>Monitor Wasserstein distance and gradient penalties</li> <li>Use appropriate learning rate schedules</li> <li>Apply spectral normalization in discriminator</li> <li>Use equalized learning rate in StyleGAN</li> </ul> <p>DON'T</p> <ul> <li>Don't use batch normalization in discriminator with small batches</li> <li>Don't skip gradient penalty in WGAN</li> <li>Don't use too large learning rates (&gt;0.001)</li> <li>Don't train generator more frequently than discriminator</li> <li>Don't forget to normalize pixel values correctly</li> <li>Don't use biases after batch norm layers</li> </ul>"},{"location":"examples/advanced/advanced-gan/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution Mode collapse Discriminator too strong Use WGAN-GP, reduce D learning rate Training instability Loss imbalance Use spectral normalization, gradient penalty Poor quality at high resolution Insufficient capacity Use progressive training, increase model size Style not disentangled Insufficient mixing Increase mixing regularization, check AdaIN Vanishing gradients Saturated discriminator Use WGAN or non-saturating loss"},{"location":"examples/advanced/advanced-gan/#summary","title":"Summary","text":"<p>We covered four advanced GAN architectures:</p> <ol> <li>Progressive GAN: Gradual resolution increase for stable high-res training</li> <li>StyleGAN: Style-based generation with AdaIN for better control</li> <li>Conditional GAN: Class-conditional generation</li> <li>WGAN-GP: Improved stability with Wasserstein distance</li> </ol> <p>Key Takeaways:</p> <ul> <li>Progressive training enables high-resolution generation</li> <li>Style modulation provides better disentanglement</li> <li>Gradient penalty stabilizes Wasserstein GAN training</li> <li>Proper architecture design is crucial for quality</li> </ul>"},{"location":"examples/advanced/advanced-gan/#next-steps","title":"Next Steps","text":"<ul> <li> <p> GAN Concepts</p> <p>Dive deeper into GAN theory</p> <p> GAN Explained</p> </li> <li> <p> Training Guide</p> <p>Learn distributed GAN training</p> <p> Training Guide</p> </li> <li> <p> Evaluation</p> <p>Evaluate GAN quality with FID, IS</p> <p> Benchmarks</p> </li> <li> <p> API Reference</p> <p>Complete GAN API documentation</p> <p> GAN API</p> </li> </ul>"},{"location":"examples/advanced/advanced-training/","title":"Advanced Training Pipeline","text":"<p>Level: Intermediate Runtime: ~2 minutes Format: Dual (.py script | .ipynb notebook)</p> <p>Production-ready training patterns including optimizer configuration, learning rate scheduling, metrics tracking, and checkpointing strategies.</p>"},{"location":"examples/advanced/advanced-training/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/advanced_training_example.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/advanced_training_example.ipynb</code></li> </ul>"},{"location":"examples/advanced/advanced-training/#quick-start","title":"Quick Start","text":"<pre><code># Run the Python script\npython examples/generative_models/advanced_training_example.py\n\n# Or open the Jupyter notebook\njupyter notebook examples/generative_models/advanced_training_example.ipynb\n</code></pre>"},{"location":"examples/advanced/advanced-training/#overview","title":"Overview","text":"<p>This example demonstrates how to build a complete, production-ready training pipeline using the Artifex framework. You'll learn essential patterns for training deep learning models including configuration management, optimization strategies, metrics tracking, and model checkpointing.</p>"},{"location":"examples/advanced/advanced-training/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this example, you will understand:</p> <ul> <li> How to implement a complete training pipeline with proper validation</li> <li> Optimizer and learning rate scheduler configuration</li> <li> Metrics tracking and visualization during training</li> <li> Checkpoint management and model persistence</li> <li> Best practices for training loop organization</li> </ul>"},{"location":"examples/advanced/advanced-training/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of neural network training</li> <li>Familiarity with JAX and Flax NNX</li> <li>Understanding of gradient descent and backpropagation</li> <li>Knowledge of learning rate scheduling concepts</li> </ul>"},{"location":"examples/advanced/advanced-training/#theory-and-key-concepts","title":"Theory and Key Concepts","text":""},{"location":"examples/advanced/advanced-training/#training-loop-components","title":"Training Loop Components","text":"<p>A production training loop requires several key components working together:</p> <ol> <li>Data Management: Efficient batching and shuffling strategies</li> <li>Optimization: Gradient computation and parameter updates</li> <li>Metrics Tracking: Monitor training and validation performance</li> <li>Checkpointing: Save model state for recovery and deployment</li> <li>Validation: Monitor generalization to unseen data</li> </ol>"},{"location":"examples/advanced/advanced-training/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Learning rate schedules improve training stability and convergence by adapting the learning rate during training:</p> <p>Warmup: Gradually increase learning rate from zero to avoid early instability Decay: Reduce learning rate as training progresses to enable fine-grained convergence Cosine Annealing: Smooth decrease following a cosine curve</p> <p>The formula for cosine decay is:</p> \\[ \\eta_t = \\eta_{\\text{min}} + \\frac{1}{2}(\\eta_{\\text{max}} - \\eta_{\\text{min}})\\left(1 + \\cos\\left(\\frac{t}{T}\\pi\\right)\\right) \\] <p>where \\(\\eta_t\\) is the learning rate at step \\(t\\), and \\(T\\) is the total number of steps.</p>"},{"location":"examples/advanced/advanced-training/#optimization-algorithms","title":"Optimization Algorithms","text":"<p>Adam (Adaptive Moment Estimation): Combines momentum and adaptive learning rates</p> \\[ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\] \\[ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\] \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{m_t}{\\sqrt{v_t} + \\epsilon} \\] <p>SGD with Momentum: Accelerates convergence by accumulating gradients</p> \\[ v_t = \\mu v_{t-1} + \\eta g_t \\] \\[ \\theta_{t+1} = \\theta_t - v_t \\]"},{"location":"examples/advanced/advanced-training/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/advanced/advanced-training/#1-configuration-setup","title":"1. Configuration Setup","text":"<p>The example uses frozen dataclass configuration objects for all settings:</p> <pre><code>from dataclasses import dataclass\n\n# Model configuration (simple frozen dataclass)\n@dataclass(frozen=True)\nclass ClassifierConfig:\n    \"\"\"Configuration for a simple classifier.\"\"\"\n    name: str = \"classifier\"\n    input_dim: int = 784\n    hidden_dims: tuple[int, ...] = (256, 128)  # Tuple for frozen dataclass\n    output_dim: int = 10\n    dropout_rate: float = 0.1\n\n# Training configuration using Artifex's ModelConfig\nfrom artifex.generative_models.core.configuration import ModelConfig\n\n# ModelConfig is a general-purpose training configuration\nconfig = ModelConfig(\n    name=\"advanced_training\",\n    batch_size=32,\n    num_epochs=10,\n    learning_rate=1e-3,\n    optimizer_type=\"adam\",\n    checkpoint_dir=\"./checkpoints/advanced_example\",\n    save_frequency=5,\n    # Scheduler settings\n    scheduler_type=\"cosine\",\n    warmup_steps=100,\n    total_steps=1000,\n    # Optimizer settings\n    beta1=0.9,\n    beta2=0.999,\n    weight_decay=1e-4,\n)\n\n# Create model config\nmodel_config = ClassifierConfig(\n    input_dim=784,\n    hidden_dims=(256, 128),  # Tuple for frozen dataclass\n    output_dim=10,\n    dropout_rate=0.1,\n)\n</code></pre> <p>This approach centralizes all hyperparameters using frozen dataclasses, making experiments reproducible and configuration management type-safe.</p>"},{"location":"examples/advanced/advanced-training/#2-data-loading","title":"2. Data Loading","text":"<p>The example implements a simple data loader with shuffling:</p> <pre><code>def create_data_loader(data, batch_size=32, shuffle=True):\n    \"\"\"Create a simple data loader.\"\"\"\n    x, y = data\n    num_samples = len(x)\n    indices = jnp.arange(num_samples)\n\n    if shuffle:\n        key = jax.random.key(np.random.randint(0, 10000))\n        indices = jax.random.permutation(key, indices)\n\n    for i in range(0, num_samples, batch_size):\n        batch_indices = indices[i : i + batch_size]\n        yield x[batch_indices], y[batch_indices]\n</code></pre> <p>In production, you would use more sophisticated data loading strategies like TensorFlow Datasets or PyTorch DataLoader equivalents.</p>"},{"location":"examples/advanced/advanced-training/#3-model-definition","title":"3. Model Definition","text":"<p>A simple classifier using Flax NNX demonstrates proper module patterns:</p> <pre><code>class SimpleClassifier(nnx.Module):\n    def __init__(self, input_dim, hidden_dims, num_classes, *, rngs: nnx.Rngs):\n        super().__init__()  # Always call this\n\n        layers = []\n        prev_dim = input_dim\n\n        # Build hidden layers\n        for hidden_dim in hidden_dims:\n            layers.append(nnx.Linear(prev_dim, hidden_dim, rngs=rngs))\n            layers.append(nnx.relu)\n            layers.append(nnx.Dropout(rate=0.1, rngs=rngs))\n            prev_dim = hidden_dim\n\n        # Output layer\n        layers.append(nnx.Linear(prev_dim, num_classes, rngs=rngs))\n        self.net = nnx.Sequential(*layers)\n\n    def __call__(self, x, *, training=False):\n        return self.net(x)\n</code></pre>"},{"location":"examples/advanced/advanced-training/#4-training-step","title":"4. Training Step","text":"<p>The core training step computes loss, gradients, and updates parameters:</p> <pre><code>def train_step(model, optimizer, batch_x, batch_y, loss_fn):\n    def compute_loss(model):\n        logits = model(batch_x, training=True)\n\n        # Cross-entropy loss\n        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch_y)\n        loss = jnp.mean(loss)\n\n        # Accuracy\n        predictions = jnp.argmax(logits, axis=-1)\n        accuracy = jnp.mean(predictions == batch_y)\n\n        return loss, accuracy\n\n    (loss, accuracy), grads = nnx.value_and_grad(compute_loss, has_aux=True)(model)\n    optimizer.update(model, grads)\n\n    return loss, accuracy\n</code></pre> <p>This pattern uses NNX's <code>value_and_grad</code> for efficient gradient computation with auxiliary outputs (accuracy).</p>"},{"location":"examples/advanced/advanced-training/#5-main-training-loop","title":"5. Main Training Loop","text":"<p>The main loop orchestrates all components:</p> <pre><code>for epoch in range(training_config.num_epochs):\n    # Training\n    train_loss = 0\n    train_acc = 0\n    num_train_batches = 0\n\n    train_loader = create_data_loader(\n        train_data, batch_size=training_config.batch_size, shuffle=True\n    )\n\n    for batch_x, batch_y in train_loader:\n        loss, acc = train_step(model, optimizer, batch_x, batch_y, None)\n        train_loss += loss\n        train_acc += acc\n        num_train_batches += 1\n\n    train_loss /= num_train_batches\n    train_acc /= num_train_batches\n\n    # Validation\n    val_loader = create_data_loader(\n        val_data, batch_size=training_config.batch_size, shuffle=False\n    )\n    val_loss, val_acc = evaluate(model, val_loader)\n\n    # Update metrics\n    metrics.update({\n        \"train_loss\": train_loss,\n        \"train_acc\": train_acc,\n        \"val_loss\": val_loss,\n        \"val_acc\": val_acc,\n    })\n\n    # Save checkpoint\n    if (epoch + 1) % training_config.save_frequency == 0:\n        save_checkpoint(model, optimizer, epoch + 1, training_config.checkpoint_dir)\n</code></pre>"},{"location":"examples/advanced/advanced-training/#6-metrics-tracking","title":"6. Metrics Tracking","text":"<p>The example includes a custom metrics tracker with visualization:</p> <pre><code>class TrainingMetrics:\n    def __init__(self):\n        self.history = {\n            \"train_loss\": [],\n            \"val_loss\": [],\n            \"train_acc\": [],\n            \"val_acc\": [],\n        }\n\n    def update(self, metrics: dict[str, float]):\n        for key, value in metrics.items():\n            if key in self.history:\n                self.history[key].append(float(value))\n\n    def plot(self, save_path=None):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n        # Plot loss and accuracy curves\n        # ...\n</code></pre> <p>This enables real-time monitoring and post-training analysis.</p>"},{"location":"examples/advanced/advanced-training/#expected-output","title":"Expected Output","text":"<p>When you run the example, you should see:</p> <pre><code>============================================================\nAdvanced Training Example\n============================================================\n\n1. Setting up configuration...\n  Model: classifier\n  Optimizer: adam\n  Scheduler: cosine\n  Epochs: 10\n  Batch size: 32\n\n2. Creating synthetic dataset...\n  Train samples: 1000\n  Validation samples: 200\n  Test samples: 200\n\n3. Creating model...\n  Model created with 2 hidden layers\n\n4. Setting up optimizer and scheduler...\n\n5. Starting training...\n----------------------------------------\nEpoch 1/10\n  Train - Loss: 2.3965, Acc: 0.1025\n  Val   - Loss: 2.3957, Acc: 0.1071\n...\nEpoch 10/10\n  Train - Loss: 0.0137, Acc: 1.0000\n  Val   - Loss: 4.3435, Acc: 0.0670\n----------------------------------------\n\n6. Evaluating on test set...\n  Test Loss: 4.1130\n  Test Accuracy: 0.1205\n\n7. Plotting training curves...\n  Training curves saved to examples_output/training_curves.png\n\n\u2705 Advanced training example completed successfully!\n</code></pre> <p>The example will also save a visualization of the training curves to <code>examples_output/training_curves.png</code>.</p>"},{"location":"examples/advanced/advanced-training/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Different Optimizers: Compare Adam, SGD with momentum, and AdamW</li> </ol> <pre><code>optimizer_config.optimizer_type = \"sgd\"\noptimizer_config.momentum = 0.9\n</code></pre> <ol> <li>Scheduler Variations: Test exponential decay vs cosine annealing</li> </ol> <pre><code>scheduler_config.scheduler_type = \"exponential\"\nscheduler_config.decay_steps = 100\nscheduler_config.decay_rate = 0.96\n</code></pre> <ol> <li>Architecture Changes: Experiment with different hidden layer configurations</li> </ol> <pre><code>model_config.hidden_dims = [512, 256, 128]  # Deeper network\n</code></pre> <ol> <li>Regularization: Adjust dropout and weight decay</li> </ol> <pre><code>model_config.dropout_rate = 0.2\noptimizer_config.weight_decay = 1e-3\n</code></pre> <ol> <li>Early Stopping: Implement early stopping based on validation loss</li> </ol> <pre><code># Track best validation loss\n# Stop training if no improvement for N epochs\n</code></pre>"},{"location":"examples/advanced/advanced-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/advanced/advanced-training/#high-validation-loss","title":"High Validation Loss","text":"<p>If validation loss is much higher than training loss:</p> <ul> <li>Reduce model complexity or add regularization</li> <li>Increase dropout rate</li> <li>Add weight decay to optimizer</li> <li>Use more training data</li> </ul>"},{"location":"examples/advanced/advanced-training/#slow-convergence","title":"Slow Convergence","text":"<p>If training is slow to converge:</p> <ul> <li>Increase learning rate (carefully)</li> <li>Use a learning rate warmup</li> <li>Try a different optimizer (e.g., Adam instead of SGD)</li> <li>Check gradient magnitudes</li> </ul>"},{"location":"examples/advanced/advanced-training/#numerical-instability","title":"Numerical Instability","text":"<p>If you encounter NaN or Inf values:</p> <ul> <li>Reduce learning rate</li> <li>Add gradient clipping</li> <li>Use mixed precision training</li> <li>Check for exploding/vanishing gradients</li> </ul>"},{"location":"examples/advanced/advanced-training/#next-steps","title":"Next Steps","text":"<ul> <li> <p> VAE Training</p> <p>Learn to train Variational Autoencoders with the ELBO loss</p> <p> VAE Examples</p> </li> <li> <p> GAN Training</p> <p>Master adversarial training with generator and discriminator</p> <p> GAN Examples</p> </li> <li> <p> Advanced Optimization</p> <p>Explore gradient clipping, mixed precision, and distributed training</p> <p> Advanced Techniques</p> </li> <li> <p> Model Deployment</p> <p>Learn to export and deploy trained models</p> <p> Deployment Guide</p> </li> </ul>"},{"location":"examples/advanced/advanced-training/#additional-resources","title":"Additional Resources","text":"<ul> <li>Flax NNX Training Guide</li> <li>Optax Documentation</li> <li>JAX Training Best Practices</li> <li>Artifex Training Configuration</li> </ul>"},{"location":"examples/advanced/advanced-vae/","title":"Advanced VAE Examples","text":"<p>Level: Advanced | Runtime: Varies by variant (30-60 min per model) | Format: Python + Jupyter</p> <p>Advanced Variational Autoencoder variants and techniques, including \u03b2-VAE, \u03b2-VAE with Capacity Control, Conditional VAE, and VQ-VAE.</p>"},{"location":"examples/advanced/advanced-vae/#prerequisites","title":"Prerequisites","text":"<p>Required Knowledge:</p> <ul> <li>Strong understanding of standard VAEs and ELBO</li> <li>Familiarity with the Basic VAE Tutorial</li> <li>Experience with JAX and Flax NNX</li> <li>Understanding of latent space representations</li> <li>Knowledge of training dynamics and loss functions</li> </ul> <p>Skill Level: Advanced - requires solid foundation in variational inference and generative modeling</p> <p>Estimated Time: 2-3 hours to work through all variants</p> <p>Multiple Implementations</p> <p>This guide contains four complete VAE variant implementations:</p> <ul> <li>\u03b2-VAE: Disentangled representations with \u03b2-weighting and annealing</li> <li>\u03b2-VAE with Capacity Control: Burgess et al. capacity-based training for stable disentanglement</li> <li>Conditional VAE: Label-conditioned generation for controlled sampling</li> <li>VQ-VAE: Discrete latent codes using vector quantization</li> </ul> <p>Each variant includes complete working code that you can run independently or integrate into your projects.</p> <ul> <li> <p> \u03b2-VAE</p> <p>Disentangled representations with \u03b2-weighting</p> <p> Learn more</p> </li> <li> <p> VQ-VAE</p> <p>Vector-Quantized VAE for discrete latent spaces</p> <p> Learn more</p> </li> <li> <p> Conditional VAE</p> <p>Condition generation on labels or attributes</p> <p> Learn more</p> </li> <li> <p> \u03b2-VAE with Capacity Control</p> <p>Burgess et al. capacity-based training for stable disentanglement</p> <p> Learn more</p> </li> </ul>"},{"location":"examples/advanced/advanced-vae/#beta-vae","title":"Beta-VAE","text":"<p>\u03b2-VAE adds a weight \u03b2 to the KL divergence term, encouraging disentangled representations.</p>"},{"location":"examples/advanced/advanced-vae/#basic-vae","title":"Basic \u03b2-VAE","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    BetaVAEConfig,\n    EncoderConfig,\n    DecoderConfig,\n)\nfrom artifex.generative_models.models.vae import BetaVAE\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\n# Create \u03b2-VAE configuration using frozen dataclass configs\nencoder_config = EncoderConfig(\n    name=\"beta_vae_encoder\",\n    input_shape=(64, 64, 3),  # RGB images\n    latent_dim=10,  # Smaller latent dim encourages disentanglement\n    hidden_dims=(512, 256, 128),  # Tuple for frozen dataclass\n    activation=\"relu\",\n)\n\ndecoder_config = DecoderConfig(\n    name=\"beta_vae_decoder\",\n    output_shape=(64, 64, 3),\n    latent_dim=10,\n    hidden_dims=(128, 256, 512),  # Tuple for frozen dataclass\n    activation=\"relu\",\n)\n\nconfig = BetaVAEConfig(\n    name=\"beta_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    beta_default=4.0,  # \u03b2 &gt; 1 for disentanglement\n)\n\n# Create model\nrngs = nnx.Rngs(params=0, dropout=1, sample=2)\nmodel = BetaVAE(config, rngs=rngs)\n\n# Custom \u03b2-VAE loss\ndef beta_vae_loss(model, batch, beta=4.0):\n    \"\"\"\u03b2-VAE loss with weighted KL divergence.\"\"\"\n    output = model(batch[\"data\"])\n\n    # Reconstruction loss\n    recon_loss = jnp.mean((batch[\"data\"] - output[\"reconstruction\"]) ** 2)\n\n    # KL divergence\n    kl_loss = -0.5 * jnp.mean(\n        1 + output[\"logvar\"] - output[\"mean\"] ** 2 - jnp.exp(output[\"logvar\"])\n    )\n\n    # \u03b2-weighted total loss\n    total_loss = recon_loss + beta * kl_loss\n\n    return total_loss, {\n        \"loss\": total_loss,\n        \"reconstruction_loss\": recon_loss,\n        \"kl_loss\": kl_loss,\n    }\n\n# Training step\n@jax.jit\ndef train_step(model_state, batch, optimizer_state, beta=4.0):\n    \"\"\"Training step with \u03b2-VAE loss.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n\n    (loss, metrics), grads = nnx.value_and_grad(\n        lambda m: beta_vae_loss(m, batch, beta=beta),\n        has_aux=True\n    )(model)\n\n    updates, optimizer_state = optimizer.update(grads, optimizer_state)\n    model_state = optax.apply_updates(model_state, updates)\n\n    return model_state, optimizer_state, metrics\n\n# Split model\nmodel_graphdef, model_state = nnx.split(model)\n\n# Training loop with different \u03b2 values\nfor epoch in range(num_epochs):\n    # Anneal \u03b2 from 1.0 to 4.0\n    beta = 1.0 + (4.0 - 1.0) * min(epoch / 10, 1.0)\n\n    for batch in dataloader:\n        model_state, optimizer_state, metrics = train_step(\n            model_state, batch, optimizer_state, beta=beta\n        )\n\n    print(f\"Epoch {epoch}, \u03b2={beta:.2f}, Loss={metrics['loss']:.4f}\")\n</code></pre> <p>Choosing \u03b2 Values</p> <p>\u03b2 value guidelines:</p> <ul> <li>\u03b2 = 1.0: Standard VAE (no disentanglement bias)</li> <li>\u03b2 = 2.0-4.0: Good balance for disentanglement on simple datasets</li> <li>\u03b2 = 6.0-10.0: Strong disentanglement, may sacrifice reconstruction quality</li> <li>\u03b2 annealing: Start at 1.0, gradually increase to target \u03b2 over 10-20 epochs</li> </ul> <p>Higher \u03b2 encourages independence between latent dimensions but can lead to posterior collapse if too large.</p>"},{"location":"examples/advanced/advanced-vae/#disentanglement-evaluation","title":"Disentanglement Evaluation","text":"<pre><code>def evaluate_disentanglement(model, dataset, num_samples=1000):\n    \"\"\"Evaluate disentanglement of learned representations.\"\"\"\n    import numpy as np\n\n    # Collect latent representations\n    latents = []\n    labels = []\n\n    for batch in dataset.take(num_samples // 32):\n        output = model(batch[\"data\"])\n        latents.append(np.array(output[\"mean\"]))\n        if \"labels\" in batch:\n            labels.append(np.array(batch[\"labels\"]))\n\n    latents = np.concatenate(latents, axis=0)\n    if labels:\n        labels = np.concatenate(labels, axis=0)\n\n    # Compute variance per latent dimension\n    latent_variances = np.var(latents, axis=0)\n\n    # Active dimensions (high variance)\n    active_dims = latent_variances &gt; 0.01\n\n    print(f\"Active dimensions: {np.sum(active_dims)} / {latents.shape[1]}\")\n    print(f\"Latent variances: {latent_variances}\")\n\n    # If labels available, compute mutual information\n    if labels:\n        from sklearn.metrics import mutual_info_score\n\n        mi_scores = []\n        for dim in range(latents.shape[1]):\n            # Discretize latent dimension\n            latent_discrete = np.digitize(latents[:, dim], bins=10)\n\n            # Compute MI with each label dimension\n            for label_dim in range(labels.shape[1]):\n                mi = mutual_info_score(label_dim, latent_discrete)\n                mi_scores.append(mi)\n\n        print(f\"Mean mutual information: {np.mean(mi_scores):.4f}\")\n\n    return {\n        \"active_dimensions\": int(np.sum(active_dims)),\n        \"latent_variances\": latent_variances.tolist(),\n    }\n\n# Evaluate\nresults = evaluate_disentanglement(model, val_dataset)\n</code></pre>"},{"location":"examples/advanced/advanced-vae/#latent-traversal-visualization","title":"Latent Traversal Visualization","text":"<pre><code>def visualize_latent_traversals(model, z_base, dim, values=None):\n    \"\"\"Visualize effect of traversing a single latent dimension.\"\"\"\n    import matplotlib.pyplot as plt\n\n    if values is None:\n        values = jnp.linspace(-3, 3, 11)\n\n    samples = []\n    for value in values:\n        z = z_base.copy()\n        z[dim] = value\n        sample = model.decode(z[None, :])[0]\n        samples.append(sample)\n\n    # Plot traversal\n    fig, axes = plt.subplots(1, len(values), figsize=(15, 2))\n    for i, (ax, sample) in enumerate(zip(axes, samples)):\n        ax.imshow(sample, cmap=\"gray\")\n        ax.set_title(f\"z[{dim}]={values[i]:.1f}\")\n        ax.axis(\"off\")\n\n    plt.suptitle(f\"Latent Dimension {dim} Traversal\")\n    plt.tight_layout()\n    return fig\n\n# Get base latent vector\nsample = next(iter(val_dataset))\noutput = model(sample[\"data\"][:1])\nz_base = jnp.array(output[\"mean\"][0])\n\n# Visualize each dimension\nfor dim in range(model.latent_dim):\n    fig = visualize_latent_traversals(model, z_base, dim)\n    # fig.savefig(f\"traversal_dim_{dim}.png\")\n</code></pre>"},{"location":"examples/advanced/advanced-vae/#vq-vae","title":"VQ-VAE","text":"<p>Vector-Quantized VAE uses discrete latent codes from a learnable codebook.</p>"},{"location":"examples/advanced/advanced-vae/#vq-vae-implementation","title":"VQ-VAE Implementation","text":"<pre><code>from flax import nnx\nimport jax\nimport jax.numpy as jnp\n\nclass VectorQuantizer(nnx.Module):\n    \"\"\"Vector quantization layer.\"\"\"\n\n    def __init__(\n        self,\n        embedding_dim: int,\n        num_embeddings: int,\n        commitment_cost: float = 0.25,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.commitment_cost = commitment_cost\n\n        # Codebook\n        self.embeddings = nnx.Param(\n            jax.random.uniform(\n                rngs.params(),\n                (num_embeddings, embedding_dim),\n                minval=-1.0,\n                maxval=1.0,\n            )\n        )\n\n    def __call__(self, z: jax.Array) -&gt; tuple[jax.Array, dict]:\n        \"\"\"Quantize continuous latents.\n\n        Args:\n            z: Continuous latents (batch, ..., embedding_dim)\n\n        Returns:\n            (quantized, info_dict)\n        \"\"\"\n        # Flatten spatial dimensions\n        flat_z = z.reshape(-1, self.embedding_dim)\n\n        # Compute distances to codebook vectors\n        distances = (\n            jnp.sum(flat_z ** 2, axis=1, keepdims=True)\n            + jnp.sum(self.embeddings.value ** 2, axis=1)\n            - 2 * flat_z @ self.embeddings.value.T\n        )\n\n        # Get nearest codebook indices\n        indices = jnp.argmin(distances, axis=1)\n\n        # Quantize\n        quantized_flat = self.embeddings.value[indices]\n\n        # Reshape to original shape\n        quantized = quantized_flat.reshape(z.shape)\n\n        # Compute losses\n        e_latent_loss = jnp.mean((jax.lax.stop_gradient(quantized) - z) ** 2)\n        q_latent_loss = jnp.mean((quantized - jax.lax.stop_gradient(z)) ** 2)\n\n        # VQ loss\n        vq_loss = q_latent_loss + self.commitment_cost * e_latent_loss\n\n        # Straight-through estimator\n        quantized = z + jax.lax.stop_gradient(quantized - z)\n\n        return quantized, {\n            \"vq_loss\": vq_loss,\n            \"perplexity\": self._compute_perplexity(indices),\n            \"indices\": indices,\n        }\n\n    def _compute_perplexity(self, indices: jax.Array) -&gt; jax.Array:\n        \"\"\"Compute codebook perplexity (measure of usage).\"\"\"\n        # Count frequency of each code\n        counts = jnp.bincount(indices, length=self.num_embeddings)\n        probs = counts / jnp.sum(counts)\n\n        # Perplexity\n        perplexity = jnp.exp(-jnp.sum(probs * jnp.log(probs + 1e-10)))\n\n        return perplexity\n\n\nclass VQVAE(nnx.Module):\n    \"\"\"VQ-VAE model.\"\"\"\n\n    def __init__(\n        self,\n        input_shape: tuple,\n        embedding_dim: int = 64,\n        num_embeddings: int = 512,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.input_shape = input_shape\n\n        # Encoder (CNN for images)\n        self.encoder = nnx.Sequential(\n            nnx.Conv(3, 64, kernel_size=(4, 4), strides=(2, 2), padding=\"SAME\", rngs=rngs),\n            nnx.relu,\n            nnx.Conv(64, 128, kernel_size=(4, 4), strides=(2, 2), padding=\"SAME\", rngs=rngs),\n            nnx.relu,\n            nnx.Conv(128, embedding_dim, kernel_size=(3, 3), padding=\"SAME\", rngs=rngs),\n        )\n\n        # Vector quantizer\n        self.vq = VectorQuantizer(\n            embedding_dim=embedding_dim,\n            num_embeddings=num_embeddings,\n            rngs=rngs,\n        )\n\n        # Decoder\n        self.decoder = nnx.Sequential(\n            nnx.Conv(embedding_dim, 128, kernel_size=(3, 3), padding=\"SAME\", rngs=rngs),\n            nnx.relu,\n            nnx.ConvTranspose(128, 64, kernel_size=(4, 4), strides=(2, 2), padding=\"SAME\", rngs=rngs),\n            nnx.relu,\n            nnx.ConvTranspose(64, 3, kernel_size=(4, 4), strides=(2, 2), padding=\"SAME\", rngs=rngs),\n        )\n\n    def __call__(self, x: jax.Array) -&gt; dict[str, jax.Array]:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input images (batch, height, width, channels)\n\n        Returns:\n            Dictionary with reconstruction and losses\n        \"\"\"\n        # Encode\n        z = self.encoder(x)\n\n        # Quantize\n        z_quantized, vq_info = self.vq(z)\n\n        # Decode\n        reconstruction = self.decoder(z_quantized)\n        reconstruction = nnx.sigmoid(reconstruction)\n\n        # Reconstruction loss\n        recon_loss = jnp.mean((x - reconstruction) ** 2)\n\n        # Total loss\n        total_loss = recon_loss + vq_info[\"vq_loss\"]\n\n        return {\n            \"reconstruction\": reconstruction,\n            \"loss\": total_loss,\n            \"reconstruction_loss\": recon_loss,\n            \"vq_loss\": vq_info[\"vq_loss\"],\n            \"perplexity\": vq_info[\"perplexity\"],\n        }\n\n\n# Create VQ-VAE\nvqvae = VQVAE(\n    input_shape=(64, 64, 3),\n    embedding_dim=64,\n    num_embeddings=512,\n    rngs=nnx.Rngs(0),\n)\n\n# Training\nx = jnp.ones((32, 64, 64, 3))\noutput = vqvae(x)\n\nprint(f\"Reconstruction loss: {output['reconstruction_loss']:.4f}\")\nprint(f\"VQ loss: {output['vq_loss']:.4f}\")\nprint(f\"Perplexity: {output['perplexity']:.2f}\")\n</code></pre> <p>Monitor Codebook Usage</p> <p>Perplexity measures how many codebook vectors are actively used:</p> <ul> <li>Perplexity = num_embeddings: Perfect usage, all codes used equally</li> <li>Perplexity &lt; 10%  of codebook: Codebook collapse - many codes unused</li> <li>Healthy range: 30-70% of codebook size</li> </ul> <p>If perplexity is low:</p> <ul> <li>Increase commitment cost (e.g., 0.25 \u2192 0.5)</li> <li>Use exponential moving average (EMA) updates for codebook</li> <li>Add codebook reset mechanism for unused codes</li> <li>Reduce learning rate for decoder</li> </ul>"},{"location":"examples/advanced/advanced-vae/#conditional-vae","title":"Conditional VAE","text":"<p>Conditional VAE generates samples conditioned on labels or attributes.</p>"},{"location":"examples/advanced/advanced-vae/#label-conditional-vae","title":"Label-Conditional VAE","text":"<pre><code>class ConditionalVAE(nnx.Module):\n    \"\"\"VAE conditioned on labels.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        latent_dim: int,\n        num_classes: int,\n        hidden_dims: list[int],\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.latent_dim = latent_dim\n        self.num_classes = num_classes\n\n        # Label embedding\n        self.label_embedding = nnx.Embed(\n            num_embeddings=num_classes,\n            features=hidden_dims[0],\n            rngs=rngs,\n        )\n\n        # Encoder (input + label embedding)\n        encoder_layers = []\n        encoder_layers.append(\n            nnx.Linear(input_dim + hidden_dims[0], hidden_dims[0], rngs=rngs)\n        )\n\n        for i in range(len(hidden_dims) - 1):\n            encoder_layers.append(\n                nnx.Linear(hidden_dims[i], hidden_dims[i + 1], rngs=rngs)\n            )\n\n        self.encoder = encoder_layers\n\n        # Latent layers\n        self.mean_layer = nnx.Linear(hidden_dims[-1], latent_dim, rngs=rngs)\n        self.logvar_layer = nnx.Linear(hidden_dims[-1], latent_dim, rngs=rngs)\n\n        # Decoder (latent + label embedding)\n        decoder_layers = []\n        decoder_layers.append(\n            nnx.Linear(latent_dim + hidden_dims[0], hidden_dims[-1], rngs=rngs)\n        )\n\n        for i in range(len(hidden_dims) - 1, 0, -1):\n            decoder_layers.append(\n                nnx.Linear(hidden_dims[i], hidden_dims[i - 1], rngs=rngs)\n            )\n\n        decoder_layers.append(\n            nnx.Linear(hidden_dims[0], input_dim, rngs=rngs)\n        )\n\n        self.decoder = decoder_layers\n\n    def encode(self, x: jax.Array, labels: jax.Array) -&gt; dict:\n        \"\"\"Encode with label conditioning.\"\"\"\n        # Embed labels\n        label_emb = self.label_embedding(labels)\n\n        # Concatenate input and label\n        h = jnp.concatenate([x, label_emb], axis=-1)\n\n        # Forward through encoder\n        for layer in self.encoder:\n            h = nnx.relu(layer(h))\n\n        # Latent parameters\n        mean = self.mean_layer(h)\n        logvar = self.logvar_layer(h)\n\n        return {\"mean\": mean, \"logvar\": logvar}\n\n    def decode(self, z: jax.Array, labels: jax.Array) -&gt; jax.Array:\n        \"\"\"Decode with label conditioning.\"\"\"\n        # Embed labels\n        label_emb = self.label_embedding(labels)\n\n        # Concatenate latent and label\n        h = jnp.concatenate([z, label_emb], axis=-1)\n\n        # Forward through decoder\n        for layer in self.decoder:\n            h = nnx.relu(layer(h))\n\n        # Sigmoid output\n        reconstruction = nnx.sigmoid(h)\n\n        return reconstruction\n\n    def __call__(\n        self,\n        x: jax.Array,\n        labels: jax.Array,\n        *,\n        rngs: nnx.Rngs | None = None,\n    ) -&gt; dict:\n        \"\"\"Forward pass with conditioning.\"\"\"\n        # Flatten input\n        batch_size = x.shape[0]\n        x_flat = x.reshape(batch_size, -1)\n\n        # Encode\n        latent_params = self.encode(x_flat, labels)\n\n        # Reparameterize\n        if rngs is not None and \"sample\" in rngs:\n            key = rngs.sample()\n        else:\n            key = jax.random.key(0)\n\n        std = jnp.exp(0.5 * latent_params[\"logvar\"])\n        eps = jax.random.normal(key, latent_params[\"mean\"].shape)\n        z = latent_params[\"mean\"] + eps * std\n\n        # Decode\n        reconstruction = self.decode(z, labels)\n\n        # Reshape\n        reconstruction = reconstruction.reshape(x.shape)\n\n        # Loss\n        recon_loss = jnp.mean((x_flat - reconstruction.reshape(batch_size, -1)) ** 2)\n        kl_loss = -0.5 * jnp.mean(\n            1 + latent_params[\"logvar\"]\n            - latent_params[\"mean\"] ** 2\n            - jnp.exp(latent_params[\"logvar\"])\n        )\n\n        return {\n            \"reconstruction\": reconstruction,\n            \"loss\": recon_loss + kl_loss,\n            \"reconstruction_loss\": recon_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n\n# Create conditional VAE\ncvae = ConditionalVAE(\n    input_dim=784,  # 28x28\n    latent_dim=20,\n    num_classes=10,  # MNIST digits\n    hidden_dims=[512, 256],\n    rngs=nnx.Rngs(0),\n)\n\n# Training with labels\nx = jnp.ones((32, 28, 28, 1))\nlabels = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 3 + [0, 1])\n\noutput = cvae(x, labels, rngs=nnx.Rngs(1))\nprint(f\"Loss: {output['loss']:.4f}\")\n\n# Generate specific digit\nz = jax.random.normal(jax.random.key(0), (10, 20))\ntarget_labels = jnp.arange(10)  # One of each digit\nsamples = cvae.decode(z, target_labels)\nsamples = samples.reshape(10, 28, 28, 1)\n</code></pre> <p>Conditional Generation Trade-offs</p> <p>Benefits: - Controlled generation: Produce specific classes or attributes on demand - Better sample quality: Conditioning provides additional guidance - Interpretability: Clear relationship between labels and outputs</p> <p>Considerations:</p> <ul> <li>Requires labeled data: Training needs paired (data, label) samples</li> <li>Reduced diversity: Model may ignore parts of latent space</li> <li>Label dependency: Cannot generate without knowing target labels</li> </ul> <p>Best for: Classification tasks, attribute manipulation, targeted generation</p>"},{"location":"examples/advanced/advanced-vae/#beta-vae-with-capacity-control","title":"Beta-VAE with Capacity Control","text":"<p>\u03b2-VAE with Capacity Control (Burgess et al.) addresses the training instability of standard \u03b2-VAE by gradually increasing the KL capacity instead of using a fixed \u03b2 weight.</p>"},{"location":"examples/advanced/advanced-vae/#key-concept","title":"Key Concept","text":"<p>Instead of minimizing <code>L = reconstruction_loss + \u03b2 * KL_loss</code>, capacity control minimizes:</p> <pre><code>L = reconstruction_loss + \u03b3 * |KL_loss - C|\n</code></pre> <p>Where:</p> <ul> <li><code>C</code> is the current capacity (gradually increased from 0 to C_max)</li> <li><code>\u03b3</code> is a large weight (e.g., 1000) to enforce the capacity constraint</li> <li>The model learns to match the KL divergence to the target capacity</li> </ul>"},{"location":"examples/advanced/advanced-vae/#implementation","title":"Implementation","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    BetaVAEConfig,\n    EncoderConfig,\n    DecoderConfig,\n    CapacityControlConfig,\n)\nfrom artifex.generative_models.models.vae import BetaVAEWithCapacity\nfrom flax import nnx\n\n# Create encoder config\nencoder_config = EncoderConfig(\n    name=\"capacity_encoder\",\n    input_shape=(28, 28, 1),  # MNIST\n    latent_dim=10,\n    hidden_dims=(512, 256),  # Tuple for frozen dataclass\n    activation=\"relu\",\n)\n\n# Create decoder config\ndecoder_config = DecoderConfig(\n    name=\"capacity_decoder\",\n    output_shape=(28, 28, 1),\n    latent_dim=10,\n    hidden_dims=(256, 512),  # Tuple for frozen dataclass\n    activation=\"relu\",\n)\n\n# Capacity control config\ncapacity_config = CapacityControlConfig(\n    capacity_max=25.0,  # Maximum KL capacity in nats\n    capacity_num_iter=5000,  # Steps to reach max capacity\n    gamma=1000.0,  # Weight for capacity constraint\n)\n\n# Create \u03b2-VAE with capacity control config\nconfig = BetaVAEConfig(\n    name=\"beta_vae_capacity\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    beta_default=1.0,  # \u03b2 fixed at 1.0 when using capacity control\n    reconstruction_loss_type=\"mse\",\n)\n\n# Create model\nrngs = nnx.Rngs(params=10, dropout=11, sample=12)\nmodel = BetaVAEWithCapacity(config, rngs=rngs)\n\n# Training produces stable disentanglement\n# Monitor: reconstruction_loss, kl_loss, capacity_loss, current_capacity\n</code></pre>"},{"location":"examples/advanced/advanced-vae/#training-dynamics","title":"Training Dynamics","text":"<pre><code># Forward pass\noutputs = model(x)\n\n# Compute losses with step parameter for capacity annealing\nlosses = model.loss_fn(x=x, outputs=outputs, step=current_step)\n\n# losses contains:\n# - \"loss\": Total loss to optimize\n# - \"reconstruction_loss\": Reconstruction term\n# - \"kl_loss\": KL divergence\n# - \"capacity_loss\": \u03b3 * |KL - C|\n# - \"current_capacity\": Current capacity value C\n</code></pre> <p>Capacity Control Benefits</p> <p>Why use capacity control over fixed \u03b2:</p> <ul> <li>More stable training: Gradual capacity increase prevents KL collapse</li> <li>Better reconstructions: Model isn't forced to compress early in training</li> <li>Easier to tune: Set C_max based on desired disentanglement level</li> <li>Automatic scheduling: No need to manually tune \u03b2 annealing</li> </ul> <p>Recommended settings for MNIST: - <code>capacity_max=25.0</code>: Good balance of quality and disentanglement - <code>capacity_num_iter=5000-10000</code>: ~2-4 epochs on MNIST - <code>gamma=1000.0</code>: Strong enough to enforce constraint</p>"},{"location":"examples/advanced/advanced-vae/#monitoring-training","title":"Monitoring Training","text":"<p>Track these metrics during training:</p> <pre><code>history = {\n    \"loss\": [],\n    \"reconstruction_loss\": [],\n    \"kl_loss\": [],\n    \"capacity_loss\": [],\n    \"current_capacity\": [],\n}\n\n# During training\nfor step in range(num_steps):\n    losses = train_step(model, optimizer, batch, step)\n\n    # Watch current_capacity increase from 0 to capacity_max\n    # KL should track current_capacity closely\n    print(f\"Step {step}: KL={losses['kl_loss']:.2f}, C={losses['current_capacity']:.2f}\")\n</code></pre>"},{"location":"examples/advanced/advanced-vae/#best-practices","title":"Best Practices","text":""},{"location":"examples/advanced/advanced-vae/#do","title":"DO","text":"<ul> <li>\u2705 Tune \u03b2 carefully - start with \u03b2=1, increase gradually</li> <li>\u2705 Monitor KL divergence - should not collapse to zero</li> <li>\u2705 Use \u03b2 annealing - gradually increase \u03b2 during training</li> <li>\u2705 Evaluate disentanglement - use traversals and metrics</li> <li>\u2705 Check codebook usage in VQ-VAE - perplexity should be high</li> <li>\u2705 Condition on relevant attributes - match task requirements</li> <li>\u2705 Monitor capacity in capacity-controlled \u03b2-VAE - KL should track current capacity</li> <li>\u2705 Visualize latent space - understand what's learned</li> <li>\u2705 Use adequate latent dimensions - not too small</li> <li>\u2705 Save best models - based on validation metrics</li> </ul>"},{"location":"examples/advanced/advanced-vae/#dont","title":"DON'T","text":"<ul> <li>\u274c Don't use \u03b2=1 if you want disentanglement</li> <li>\u274c Don't ignore posterior collapse - KL should not be zero</li> <li>\u274c Don't skip codebook monitoring in VQ-VAE</li> <li>\u274c Don't over-condition - limits generation diversity</li> <li>\u274c Don't use same architecture for all variants - customize per model type</li> <li>\u274c Don't skip capacity monitoring in capacity-controlled \u03b2-VAE</li> <li>\u274c Don't forget to normalize inputs - affects reconstruction</li> <li>\u274c Don't compare losses across variants - different objectives</li> <li>\u274c Don't skip visualization - hard to debug otherwise</li> <li>\u274c Don't use too small codebook in VQ-VAE</li> </ul>"},{"location":"examples/advanced/advanced-vae/#summary","title":"Summary","text":"<p>Advanced VAE variants covered:</p> <ol> <li>\u03b2-VAE: Disentangled representations with \u03b2-weighting (\u03b2 &gt; 1)</li> <li>\u03b2-VAE with Capacity Control: Stable disentanglement learning using gradual capacity increase</li> <li>Conditional VAE: Generation conditioned on labels or attributes</li> <li>VQ-VAE: Discrete latent space with vector quantization</li> </ol> <p>Each variant offers different trade-offs:</p> <ul> <li>\u03b2-VAE: Better disentanglement through KL weighting, trade-off with reconstruction quality</li> <li>\u03b2-VAE with Capacity Control: More stable training than standard \u03b2-VAE, automatic capacity scheduling</li> <li>Conditional VAE: Controlled generation for specific classes, requires labeled data</li> <li>VQ-VAE: Discrete latent codes, excellent for compression and hierarchical generation</li> </ul>"},{"location":"examples/advanced/advanced-vae/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Advanced GANs</p> <p>Explore StyleGAN and Progressive GAN techniques</p> <p> Advanced GANs</p> </li> <li> <p> Advanced Diffusion</p> <p>Learn classifier guidance and advanced sampling</p> <p> Advanced Diffusion</p> </li> <li> <p> Advanced Flows</p> <p>Implement continuous normalizing flows</p> <p> Advanced Flows</p> </li> <li> <p> VAE Guide</p> <p>Return to the comprehensive VAE documentation</p> <p> VAE guide</p> </li> </ul>"},{"location":"examples/advanced/clip-models/","title":"CLIP Models","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on CLIP model implementations.</p>"},{"location":"examples/advanced/clip-models/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Contrastive Language-Image Pre-training (CLIP)</li> <li>Zero-shot image classification</li> <li>Text-image similarity computation</li> <li>Fine-tuning CLIP for custom domains</li> </ul>"},{"location":"examples/advanced/clip-models/#planned-features","title":"Planned Features","text":"<ul> <li>Pre-trained CLIP model loading</li> <li>Custom CLIP training from scratch</li> <li>Image and text encoder architectures</li> <li>Contrastive loss implementation</li> </ul>"},{"location":"examples/advanced/clip-models/#related-documentation","title":"Related Documentation","text":"<ul> <li>Visual QA</li> <li>Image Captioning</li> <li>Cross-Modal Retrieval</li> </ul>"},{"location":"examples/advanced/clip-models/#references","title":"References","text":"<ul> <li>Radford et al., \"Learning Transferable Visual Models From Natural Language Supervision\" (2021)</li> </ul>"},{"location":"examples/advanced/cross-modal-retrieval/","title":"Cross-Modal Retrieval","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on cross-modal retrieval implementations.</p>"},{"location":"examples/advanced/cross-modal-retrieval/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Text-to-image retrieval</li> <li>Image-to-text retrieval</li> <li>Joint embedding spaces</li> <li>Similarity-based ranking</li> </ul>"},{"location":"examples/advanced/cross-modal-retrieval/#planned-features","title":"Planned Features","text":"<ul> <li>Dual encoder architectures</li> <li>Contrastive learning objectives</li> <li>Hard negative mining</li> <li>Efficient retrieval with approximate nearest neighbors</li> </ul>"},{"location":"examples/advanced/cross-modal-retrieval/#related-documentation","title":"Related Documentation","text":"<ul> <li>CLIP Models</li> <li>Visual QA</li> <li>Image Captioning</li> </ul>"},{"location":"examples/advanced/cross-modal-retrieval/#references","title":"References","text":"<ul> <li>Faghri et al., \"VSE++: Improving Visual-Semantic Embeddings with Hard Negatives\" (2018)</li> <li>Lee et al., \"Stacked Cross Attention for Image-Text Matching\" (2018)</li> </ul>"},{"location":"examples/advanced/image-captioning/","title":"Image Captioning","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on image captioning implementations.</p>"},{"location":"examples/advanced/image-captioning/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Image-to-text generation</li> <li>Attention-based captioning models</li> <li>Beam search decoding</li> <li>Caption quality evaluation (BLEU, CIDEr)</li> </ul>"},{"location":"examples/advanced/image-captioning/#planned-features","title":"Planned Features","text":"<ul> <li>CNN + RNN caption generation</li> <li>Transformer-based captioning</li> <li>Visual attention mechanisms</li> <li>Dense captioning for regions</li> </ul>"},{"location":"examples/advanced/image-captioning/#related-documentation","title":"Related Documentation","text":"<ul> <li>CLIP Models</li> <li>Visual QA</li> <li>Cross-Modal Retrieval</li> </ul>"},{"location":"examples/advanced/image-captioning/#references","title":"References","text":"<ul> <li>Xu et al., \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\" (2015)</li> <li>Vinyals et al., \"Show and Tell: A Neural Image Caption Generator\" (2015)</li> </ul>"},{"location":"examples/advanced/multimodal/","title":"Multimodal Generation","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on multimodal generation.</p>"},{"location":"examples/advanced/multimodal/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Joint image-text generation</li> <li>Cross-modal synthesis</li> <li>Multimodal embedding spaces</li> <li>Conditional generation across modalities</li> </ul>"},{"location":"examples/advanced/multimodal/#planned-features","title":"Planned Features","text":"<ul> <li>Text-to-image generation</li> <li>Image-to-text generation</li> <li>Audio-visual synthesis</li> <li>Unified multimodal models</li> </ul>"},{"location":"examples/advanced/multimodal/#related-documentation","title":"Related Documentation","text":"<ul> <li>CLIP Models</li> <li>Image Captioning</li> <li>Cross-Modal Retrieval</li> </ul>"},{"location":"examples/advanced/multimodal/#references","title":"References","text":"<ul> <li>Ramesh et al., \"Zero-Shot Text-to-Image Generation\" (2021)</li> <li>Alayrac et al., \"Flamingo: a Visual Language Model for Few-Shot Learning\" (2022)</li> </ul>"},{"location":"examples/advanced/seq2seq/","title":"Sequence-to-Sequence Models","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on seq2seq implementations.</p>"},{"location":"examples/advanced/seq2seq/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Encoder-decoder architectures</li> <li>Attention mechanisms</li> <li>Machine translation</li> <li>Text summarization</li> </ul>"},{"location":"examples/advanced/seq2seq/#planned-features","title":"Planned Features","text":"<ul> <li>Transformer encoder-decoder</li> <li>Cross-attention layers</li> <li>Beam search decoding</li> <li>Teacher forcing training</li> </ul>"},{"location":"examples/advanced/seq2seq/#related-documentation","title":"Related Documentation","text":"<ul> <li>Transformer Text</li> <li>Text Compression</li> <li>Autoregressive Models</li> </ul>"},{"location":"examples/advanced/seq2seq/#references","title":"References","text":"<ul> <li>Sutskever et al., \"Sequence to Sequence Learning with Neural Networks\" (2014)</li> <li>Bahdanau et al., \"Neural Machine Translation by Jointly Learning to Align and Translate\" (2015)</li> </ul>"},{"location":"examples/advanced/text-compression/","title":"Text Compression with Generative Models","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on text compression implementations.</p>"},{"location":"examples/advanced/text-compression/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Neural text compression</li> <li>Learned entropy coding</li> <li>VAE-based text compression</li> <li>Rate-distortion optimization</li> </ul>"},{"location":"examples/advanced/text-compression/#planned-features","title":"Planned Features","text":"<ul> <li>Latent space text representation</li> <li>Arithmetic coding integration</li> <li>Compression ratio vs. quality trade-offs</li> <li>Streaming compression</li> </ul>"},{"location":"examples/advanced/text-compression/#related-documentation","title":"Related Documentation","text":"<ul> <li>Transformer Text</li> <li>Seq2Seq Models</li> <li>VAE Guide</li> </ul>"},{"location":"examples/advanced/text-compression/#references","title":"References","text":"<ul> <li>Yang et al., \"Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets\" (2018)</li> <li>Townsend et al., \"Practical Lossless Compression with Latent Variables Using Bits Back Coding\" (2019)</li> </ul>"},{"location":"examples/advanced/visual-qa/","title":"Visual Question Answering","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on Visual QA implementations.</p>"},{"location":"examples/advanced/visual-qa/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Visual Question Answering (VQA) systems</li> <li>Multi-modal fusion techniques</li> <li>Attention mechanisms for vision-language</li> <li>Answer generation from image context</li> </ul>"},{"location":"examples/advanced/visual-qa/#planned-features","title":"Planned Features","text":"<ul> <li>VQA dataset loading and preprocessing</li> <li>Vision encoder integration</li> <li>Cross-attention mechanisms</li> <li>Answer classification and generation</li> </ul>"},{"location":"examples/advanced/visual-qa/#related-documentation","title":"Related Documentation","text":"<ul> <li>CLIP Models</li> <li>Image Captioning</li> <li>Cross-Modal Retrieval</li> </ul>"},{"location":"examples/advanced/visual-qa/#references","title":"References","text":"<ul> <li>Antol et al., \"VQA: Visual Question Answering\" (2015)</li> <li>Anderson et al., \"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\" (2018)</li> </ul>"},{"location":"examples/audio/simple-audio-generation/","title":"Simple Audio Generation","text":"<p>Level: Beginner | Runtime: ~10 seconds (CPU) | Format: Python + Jupyter</p> <p>Prerequisites: Basic neural networks and JAX | Target Audience: Users learning audio generation with neural networks</p>"},{"location":"examples/audio/simple-audio-generation/#overview","title":"Overview","text":"<p>This example demonstrates how to generate audio waveforms using neural networks with JAX and Flax NNX. Learn how to build a simple audio generator, create waveform variations, visualize audio in time and frequency domains, and save outputs for playback.</p>"},{"location":"examples/audio/simple-audio-generation/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Audio Generation</p> <p>Generate audio waveforms from random latent codes using neural networks</p> </li> <li> <p> Sound Variations</p> <p>Create variations of a base sound by perturbing latent space</p> </li> <li> <p> Waveform Visualization</p> <p>Plot audio signals in the time domain with proper scaling</p> </li> <li> <p> Spectrogram Analysis</p> <p>Convert waveforms to spectrograms using STFT for frequency analysis</p> </li> </ul>"},{"location":"examples/audio/simple-audio-generation/#files","title":"Files","text":"<p>This example is available in two formats:</p> <ul> <li>Python Script: <code>simple_audio_generation.py</code></li> <li>Jupyter Notebook: <code>simple_audio_generation.ipynb</code></li> </ul>"},{"location":"examples/audio/simple-audio-generation/#quick-start","title":"Quick Start","text":""},{"location":"examples/audio/simple-audio-generation/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/audio/simple_audio_generation.py\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/audio/simple_audio_generation.ipynb\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/audio/simple-audio-generation/#1-audio-waveform-representation","title":"1. Audio Waveform Representation","text":"<p>Audio signals are represented as 1D arrays of amplitude values over time:</p> <pre><code># Audio parameters\nsample_rate = 16000  # 16 kHz (16,000 samples per second)\nduration = 0.5       # 0.5 seconds\nnum_samples = int(sample_rate * duration)  # 8,000 samples\n\n# Waveform: array of shape (num_samples,) with values in [-1, 1]\nwaveform = jnp.array([...])  # Shape: (8000,)\n</code></pre> <p>Key Parameters:</p> <ul> <li>Sample Rate: Number of samples per second (Hz)</li> <li>CD quality: 44.1 kHz</li> <li>Speech: 16 kHz</li> <li>Phone: 8 kHz</li> <li>Duration: Length of audio in seconds</li> <li>Amplitude: Waveform values typically in range [-1, 1]</li> </ul>"},{"location":"examples/audio/simple-audio-generation/#2-neural-audio-generator","title":"2. Neural Audio Generator","text":"<p>The <code>SimpleAudioGenerator</code> uses a feedforward network to generate audio from latent codes:</p> \\[\\text{waveform} = \\text{Generator}(z), \\quad z \\sim \\mathcal{N}(0, I)\\] <pre><code>from flax import nnx\n\nclass SimpleAudioGenerator(nnx.Module):\n    def __init__(\n        self,\n        sample_rate: int = 16000,\n        duration: float = 1.0,\n        latent_dim: int = 32,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.duration = duration\n        self.latent_dim = latent_dim\n        self.num_samples = int(sample_rate * duration)\n\n        # Generator network: latent \u2192 waveform\n        self.generator = nnx.Sequential(\n            nnx.Linear(latent_dim, 128, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(128, 256, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(256, 512, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(512, self.num_samples, rngs=rngs),\n            nnx.tanh,  # Outputs in [-1, 1]\n        )\n</code></pre> <p>Architecture Details:</p> <ul> <li>Input: Latent vector of shape <code>(latent_dim,)</code></li> <li>Hidden layers: Progressive expansion (128 \u2192 256 \u2192 512)</li> <li>Output: Waveform of shape <code>(num_samples,)</code></li> <li>Activation: <code>tanh</code> ensures amplitude in [-1, 1]</li> </ul>"},{"location":"examples/audio/simple-audio-generation/#3-generating-audio","title":"3. Generating Audio","text":"<p>Generate audio waveforms from random latent codes:</p> <pre><code># Create generator\ngenerator = SimpleAudioGenerator(\n    sample_rate=16000,\n    duration=0.5,\n    latent_dim=32,\n    rngs=rngs\n)\n\n# Generate batch of waveforms\nwaveforms = generator.generate(batch_size=3, rngs=rngs)\n# Shape: (3, 8000) - 3 waveforms, each with 8000 samples\n\n# Save to WAV file (requires scipy or soundfile)\nimport scipy.io.wavfile as wav\nwav.write('generated_audio.wav', sample_rate, waveforms[0])\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#4-creating-variations","title":"4. Creating Variations","text":"<p>Generate variations of a sound by adding noise to the latent code:</p> \\[z_{\\text{varied}} = z_{\\text{base}} + \\epsilon \\cdot \\sigma, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\] <pre><code># Base latent vector\nbase_latent = jax.random.normal(rngs.sample(), (32,))\n\n# Generate variations\nvariations = generator.generate_with_variation(\n    base_latent=base_latent,\n    variation_scale=0.2,  # Amount of variation\n    num_variations=4,\n    rngs=rngs\n)\n# Shape: (4, 8000) - 4 variations of the base sound\n</code></pre> <p>Variation Scale:</p> <ul> <li>Small (0.05-0.1): Subtle variations</li> <li>Medium (0.1-0.3): Noticeable differences</li> <li>Large (0.3+): Very different sounds</li> </ul>"},{"location":"examples/audio/simple-audio-generation/#5-waveform-visualization","title":"5. Waveform Visualization","text":"<p>Plot audio signals in the time domain:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_waveforms(waveforms, sample_rate):\n    batch_size = waveforms.shape[0]\n    num_samples = waveforms.shape[1]\n    time = np.linspace(0, num_samples / sample_rate, num_samples)\n\n    fig, axes = plt.subplots(batch_size, 1, figsize=(12, 3 * batch_size))\n\n    for i, ax in enumerate(axes):\n        ax.plot(time, waveforms[i], linewidth=0.5)\n        ax.set_xlabel(\"Time (s)\")\n        ax.set_ylabel(\"Amplitude\")\n        ax.set_title(f\"Waveform {i + 1}\")\n        ax.set_ylim(-1.1, 1.1)\n        ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#6-spectrogram-generation","title":"6. Spectrogram Generation","text":"<p>Convert time-domain waveforms to frequency-domain spectrograms using STFT:</p> \\[S(t, f) = \\left| \\sum_{n} w[n] \\cdot x[n] \\cdot e^{-j2\\pi fn} \\right|\\] <pre><code>def generate_spectrogram(waveform, sample_rate):\n    window_size = 512\n    hop_size = 128\n\n    # Compute STFT frames\n    frames = []\n    for i in range(0, len(waveform) - window_size, hop_size):\n        frame = waveform[i : i + window_size]\n        window = jnp.hanning(window_size)\n        windowed_frame = frame * window\n\n        # Compute FFT\n        fft = jnp.fft.rfft(windowed_frame)\n        frames.append(jnp.abs(fft))\n\n    spectrogram = jnp.stack(frames).T\n\n    # Plot spectrogram in dB scale\n    fig, ax = plt.subplots(figsize=(12, 4))\n    time_axis = np.linspace(0, len(waveform) / sample_rate, spectrogram.shape[1])\n    freq_axis = np.linspace(0, sample_rate / 2, spectrogram.shape[0])\n\n    im = ax.imshow(\n        20 * jnp.log10(spectrogram + 1e-10),  # Convert to dB\n        aspect='auto',\n        origin='lower',\n        extent=[time_axis[0], time_axis[-1], freq_axis[0], freq_axis[-1]],\n        cmap='viridis',\n    )\n\n    ax.set_xlabel(\"Time (s)\")\n    ax.set_ylabel(\"Frequency (Hz)\")\n    ax.set_title(\"Spectrogram\")\n    plt.colorbar(im, ax=ax, label=\"Magnitude (dB)\")\n    plt.show()\n</code></pre> <p>STFT Parameters:</p> <ul> <li>Window Size: Number of samples in each FFT window (trade-off: time vs frequency resolution)</li> <li>Hop Size: Step size between windows (smaller = more temporal detail)</li> <li>Window Function: Hanning window reduces spectral leakage</li> </ul>"},{"location":"examples/audio/simple-audio-generation/#code-structure","title":"Code Structure","text":"<p>The example consists of four main components:</p> <ol> <li>SimpleAudioGenerator - Neural network that generates waveforms from latent codes</li> <li>visualize_waveforms - Plot waveforms in time domain</li> <li>generate_spectrogram - Convert waveforms to frequency-domain spectrograms</li> <li>main - Demo workflow: generate, visualize, and analyze audio</li> </ol>"},{"location":"examples/audio/simple-audio-generation/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>\u2705 Neural network-based audio generation from latent codes</li> <li>\u2705 Batch generation of multiple waveforms</li> <li>\u2705 Variation generation by perturbing latent space</li> <li>\u2705 Time-domain waveform visualization</li> <li>\u2705 STFT-based spectrogram computation</li> <li>\u2705 Frequency-domain analysis with dB scaling</li> <li>\u2705 Proper audio parameter handling (sample rate, duration)</li> <li>\u2705 Output saving for playback and analysis</li> </ul>"},{"location":"examples/audio/simple-audio-generation/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Adjust Audio Parameters</li> </ol> <pre><code># Generate longer audio\ngenerator = SimpleAudioGenerator(\n    sample_rate=16000,\n    duration=2.0,  # 2 seconds\n    latent_dim=64, # More expressive latent space\n    rngs=rngs\n)\n</code></pre> <ol> <li>Explore Latent Space</li> </ol> <pre><code># Interpolate between two sounds\nz1 = jax.random.normal(key1, (32,))\nz2 = jax.random.normal(key2, (32,))\n\nfor alpha in jnp.linspace(0, 1, 10):\n    z_interp = (1 - alpha) * z1 + alpha * z2\n    waveform = generator.generator(z_interp[None, :])[0]\n    # Play or save waveform\n</code></pre> <ol> <li>Modify Network Architecture</li> </ol> <pre><code># Deeper network for more complex audio\nself.generator = nnx.Sequential(\n    nnx.Linear(latent_dim, 256, rngs=rngs),\n    nnx.relu,\n    nnx.Linear(256, 512, rngs=rngs),\n    nnx.relu,\n    nnx.Linear(512, 1024, rngs=rngs),\n    nnx.relu,\n    nnx.Linear(1024, self.num_samples, rngs=rngs),\n    nnx.tanh,\n)\n</code></pre> <ol> <li>Adjust Spectrogram Parameters</li> </ol> <pre><code># Finer frequency resolution\nwindow_size = 1024  # Larger window\nhop_size = 256      # Smaller hop for more temporal detail\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Advanced Audio</p> <p>Learn WaveNet and autoregressive models</p> <p> WaveNet Tutorial</p> </li> <li> <p> Conditional Generation</p> <p>Generate audio conditioned on text or labels</p> <p> Conditional Audio</p> </li> <li> <p> Audio VAE</p> <p>Build variational autoencoders for audio</p> <p> Audio VAE Tutorial</p> </li> <li> <p> Framework Features</p> <p>Understand Artifex's modality system</p> <p> Framework Demo</p> </li> </ul>"},{"location":"examples/audio/simple-audio-generation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/audio/simple-audio-generation/#generated-audio-sounds-like-noise","title":"Generated Audio Sounds Like Noise","text":"<p>Symptom: Generated waveforms are random noise</p> <p>Cause: Untrained generator network</p> <p>Solution: This example shows the generator architecture. For quality audio, you need to train the generator on real audio data:</p> <pre><code># Training required for quality results\n# See audio VAE or GAN tutorials for training examples\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#clipping-in-audio-output","title":"Clipping in Audio Output","text":"<p>Symptom: Distorted audio with clipping artifacts</p> <p>Cause: Amplitude exceeds [-1, 1] range</p> <p>Solution: Normalize waveforms</p> <pre><code># Ensure amplitudes are in valid range\nwaveform = jnp.clip(waveform, -1.0, 1.0)\n\n# Or normalize to [-1, 1]\nwaveform = waveform / jnp.max(jnp.abs(waveform))\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#spectrogram-looks-wrong","title":"Spectrogram Looks Wrong","text":"<p>Symptom: Spectrogram is all one color or has artifacts</p> <p>Cause: Incorrect STFT parameters or dB scaling</p> <p>Solution: Adjust window size and add epsilon before log</p> <pre><code># Add small epsilon to avoid log(0)\nspectrogram_db = 20 * jnp.log10(spectrogram + 1e-10)\n\n# Clip extreme values\nspectrogram_db = jnp.clip(spectrogram_db, -80, 0)\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#out-of-memory-error","title":"Out of Memory Error","text":"<p>Symptom: OOM error when generating long audio</p> <p>Cause: Large network output dimension</p> <p>Solution: Generate audio in chunks</p> <pre><code># Generate shorter segments\ngenerator = SimpleAudioGenerator(\n    sample_rate=16000,\n    duration=0.5,  # Shorter duration\n    latent_dim=32,\n    rngs=rngs\n)\n</code></pre>"},{"location":"examples/audio/simple-audio-generation/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/audio/simple-audio-generation/#documentation","title":"Documentation","text":"<ul> <li>Audio Modality Guide - Audio-specific features in Artifex</li> </ul>"},{"location":"examples/audio/simple-audio-generation/#related-examples","title":"Related Examples","text":"<ul> <li>Framework Features Demo - Modality system overview</li> <li>Loss Examples - Loss functions for audio models</li> </ul>"},{"location":"examples/audio/simple-audio-generation/#papers-and-resources","title":"Papers and Resources","text":"<ul> <li>WaveNet: WaveNet: A Generative Model for Raw Audio (van den Oord et al., 2016)</li> <li>WaveGAN: Adversarial Audio Synthesis (Donahue et al., 2018)</li> <li>Jukebox: Jukebox: A Generative Model for Music (Dhariwal et al., 2020)</li> <li>Audio Signal Processing: Digital Signal Processing (Oppenheim &amp; Schafer)</li> </ul>"},{"location":"examples/basic/ar-text/","title":"Autoregressive Text Generation","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on autoregressive text generation.</p>"},{"location":"examples/basic/ar-text/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Character-level language modeling</li> <li>Word-level autoregressive generation</li> <li>RNN and LSTM text generation</li> <li>Temperature-based sampling</li> </ul>"},{"location":"examples/basic/ar-text/#planned-features","title":"Planned Features","text":"<ul> <li>Simple character-level model</li> <li>Vocabulary and tokenization</li> <li>Training on text corpora</li> <li>Interactive text generation</li> </ul>"},{"location":"examples/basic/ar-text/#related-documentation","title":"Related Documentation","text":"<ul> <li>Transformer Text</li> <li>Autoregressive Models API</li> <li>Autoregressive Concepts</li> </ul>"},{"location":"examples/basic/ar-text/#references","title":"References","text":"<ul> <li>Graves, \"Generating Sequences With Recurrent Neural Networks\" (2013)</li> <li>Bengio et al., \"A Neural Probabilistic Language Model\" (2003)</li> </ul>"},{"location":"examples/basic/diffusion-mnist-demo/","title":"Diffusion Model API Demo (MNIST)","text":"<p>A lightweight demonstration of Artifex's DDPM (Denoising Diffusion Probabilistic Model) API using MNIST. This example shows how to use the DDPMModel without training, focusing on API usage and different sampling techniques.</p> <p>\u23f1\ufe0f Duration: 5-10 minutes | \ud83d\udcbb Level: Beginner | \ud83c\udf93 Prerequisites: Basic Python</p>"},{"location":"examples/basic/diffusion-mnist-demo/#overview","title":"Overview","text":"<p>This demo covers:</p> <ol> <li>Creating a DDPM model with Artifex's API</li> <li>Understanding forward diffusion (noise addition)</li> <li>Sampling with DDPM (1000 steps)</li> <li>Fast sampling with DDIM (50 steps, 20x speedup)</li> <li>Visualizing progressive denoising</li> </ol> <p>What This Demo Is NOT:</p> <ul> <li>This is not a training tutorial (see diffusion-mnist.md for full training)</li> <li>Uses a freshly initialized model (not trained)</li> <li>Generates abstract patterns, not realistic digits</li> <li>Focused on API demonstration, not production use</li> </ul>"},{"location":"examples/basic/diffusion-mnist-demo/#quick-start","title":"Quick Start","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the demo\npython examples/generative_models/image/diffusion/diffusion_mnist.py\n</code></pre> <p>Expected Output:</p> <ul> <li>4 visualizations saved to <code>examples_output/</code></li> <li>Runtime: ~2-3 minutes on GPU, ~5-10 minutes on CPU</li> </ul>"},{"location":"examples/basic/diffusion-mnist-demo/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/basic/diffusion-mnist-demo/#1-model-creation","title":"1. Model Creation","text":"<p>The demo shows how to create a DDPM model using Artifex's unified configuration:</p> <pre><code>from artifex.generative_models.core.configuration import ModelConfig\nfrom artifex.generative_models.models.diffusion.ddpm import DDPMModel\n\n# Configure DDPM\nconfig = ModelConfig(\n    name=\"ddpm_mnist\",\n    model_class=\"DDPMModel\",\n    input_dim=(28, 28, 1),\n    parameters={\n        \"noise_steps\": 1000,\n        \"beta_start\": 1e-4,\n        \"beta_end\": 0.02,\n        \"beta_schedule\": \"linear\",\n    },\n)\n\n# Create model\nmodel = DDPMModel(config, rngs=rngs)\n</code></pre> <p>Key Points:</p> <ul> <li><code>ModelConfig</code> provides unified config across all Artifex models</li> <li><code>input_dim=(28, 28, 1)</code> specifies MNIST dimensions (grayscale 28x28)</li> <li><code>noise_steps=1000</code> sets the number of diffusion timesteps</li> <li>Beta schedule controls noise levels at each step</li> </ul>"},{"location":"examples/basic/diffusion-mnist-demo/#2-forward-diffusion","title":"2. Forward Diffusion","text":"<p>The demo visualizes how diffusion models add noise to images:</p> <pre><code># Add noise at different timesteps\nt_tensor = jnp.array([timestep])\nnoisy_x, added_noise = model.forward_diffusion(image, t_tensor, rngs=rngs)\n</code></pre> <p>Output: Visualization showing image \u2192 progressive noise levels \u2192 pure noise</p>"},{"location":"examples/basic/diffusion-mnist-demo/#3-model-forward-pass","title":"3. Model Forward Pass","text":"<p>Shows how the model predicts noise:</p> <pre><code># Predict noise for a batch\noutputs = model(noisy_images, timesteps, rngs=rngs)\npredicted_noise = outputs[\"predicted_noise\"]\n</code></pre> <p>Key API: <code>model(x, t, rngs)</code> returns dictionary with <code>\"predicted_noise\"</code> key</p>"},{"location":"examples/basic/diffusion-mnist-demo/#4-ddpm-sampling-slow-but-high-quality","title":"4. DDPM Sampling (Slow but High Quality)","text":"<p>Generate samples using the full 1000-step process:</p> <pre><code>samples_ddpm = model.sample(\n    n_samples_or_shape=8,\n    scheduler=\"ddpm\",  # Use DDPM scheduler\n    rngs=rngs,\n)\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 Original DDPM algorithm</li> <li>\u2705 1000 denoising steps</li> <li>\u23f1\ufe0f Slow (~40 seconds for 8 samples on GPU)</li> <li>\ud83c\udfa8 High quality (after training)</li> </ul>"},{"location":"examples/basic/diffusion-mnist-demo/#5-ddim-sampling-fast-with-comparable-quality","title":"5. DDIM Sampling (Fast with Comparable Quality)","text":"<p>Generate samples with only 50 steps:</p> <pre><code>samples_ddim = model.sample(\n    n_samples_or_shape=8,\n    scheduler=\"ddim\",  # Use DDIM scheduler\n    steps=50,          # Only 50 steps!\n    rngs=rngs,\n)\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 DDIM algorithm (deterministic)</li> <li>\u2705 Only 50 steps (configurable)</li> <li>\u26a1 20x faster than DDPM</li> <li>\ud83c\udfa8 Comparable quality to DDPM</li> </ul> <p>Speedup Comparison:</p> <pre><code>DDPM (1000 steps): ~40s\nDDIM (50 steps):   ~2s\nSpeedup:           20x\n</code></pre>"},{"location":"examples/basic/diffusion-mnist-demo/#6-progressive-denoising","title":"6. Progressive Denoising","text":"<p>Visualize how the model transforms noise into structure:</p> <pre><code># Capture snapshots during denoising\nsnapshots = []\nfor t in tqdm(range(model.noise_steps - 1, -1, -1)):\n    x_denoised = denoise_step(x, t)\n    if t % snapshot_interval == 0:\n        snapshots.append(x_denoised)\n</code></pre> <p>Output: Shows the gradual transformation from noise \u2192 structured patterns</p>"},{"location":"examples/basic/diffusion-mnist-demo/#generated-outputs","title":"Generated Outputs","text":"<p>The demo generates 4 visualization files:</p> <ol> <li><code>diffusion_mnist_forward.png</code></li> <li>Shows forward diffusion (clean \u2192 noisy)</li> <li> <p>5 timesteps: t=0, 250, 500, 750, 999</p> </li> <li> <p><code>diffusion_mnist_ddpm_samples.png</code></p> </li> <li>8 samples generated with DDPM</li> <li> <p>1000-step sampling process</p> </li> <li> <p><code>diffusion_mnist_ddim_samples.png</code></p> </li> <li>8 samples generated with DDIM</li> <li> <p>50-step sampling (20x faster)</p> </li> <li> <p><code>diffusion_mnist_trajectory.png</code></p> </li> <li>Progressive denoising over 6 snapshots</li> <li>Shows noise \u2192 pattern transformation</li> </ol>"},{"location":"examples/basic/diffusion-mnist-demo/#key-takeaways","title":"Key Takeaways","text":""},{"location":"examples/basic/diffusion-mnist-demo/#artifex-api-patterns","title":"Artifex API Patterns","text":"<ol> <li>Model Creation:</li> </ol> <pre><code>model = DDPMModel(config, rngs=rngs)\n</code></pre> <ol> <li>Forward Diffusion:</li> </ol> <pre><code>noisy_x, noise = model.forward_diffusion(x, t, rngs=rngs)\n</code></pre> <ol> <li>Noise Prediction:</li> </ol> <pre><code>outputs = model(x, t, rngs=rngs)\nnoise_pred = outputs[\"predicted_noise\"]\n</code></pre> <ol> <li>Sampling:</li> </ol> <pre><code># DDPM (slow)\nsamples = model.sample(n, scheduler=\"ddpm\", rngs=rngs)\n\n# DDIM (fast)\nsamples = model.sample(n, scheduler=\"ddim\", steps=50, rngs=rngs)\n</code></pre>"},{"location":"examples/basic/diffusion-mnist-demo/#ddpm-vs-ddim","title":"DDPM vs DDIM","text":"Aspect DDPM DDIM Steps 1000 (fixed) Configurable (20-100) Speed Slow 10-50x faster Quality High (baseline) Comparable Stochasticity Stochastic Deterministic Use Case Best quality Production/fast iteration"},{"location":"examples/basic/diffusion-mnist-demo/#when-to-use-each","title":"When to Use Each","text":"<p>Use DDPM when:</p> <ul> <li>You want the original algorithm</li> <li>Quality is critical</li> <li>Speed is not a concern</li> <li>Following research papers exactly</li> </ul> <p>Use DDIM when:</p> <ul> <li>You need fast sampling</li> <li>Deploying to production</li> <li>Iterating quickly during development</li> <li>GPU memory is limited</li> </ul>"},{"location":"examples/basic/diffusion-mnist-demo/#experiments-to-try","title":"Experiments to Try","text":""},{"location":"examples/basic/diffusion-mnist-demo/#1-different-step-counts-ddim","title":"1. Different Step Counts (DDIM)","text":"<pre><code># Very fast (lower quality)\nmodel.sample(8, scheduler=\"ddim\", steps=20, rngs=rngs)\n\n# Balanced (recommended)\nmodel.sample(8, scheduler=\"ddim\", steps=50, rngs=rngs)\n\n# Slower but better\nmodel.sample(8, scheduler=\"ddim\", steps=100, rngs=rngs)\n</code></pre>"},{"location":"examples/basic/diffusion-mnist-demo/#2-different-beta-schedules","title":"2. Different Beta Schedules","text":"<pre><code># Try cosine schedule\nconfig.parameters[\"beta_schedule\"] = \"cosine\"\nmodel = DDPMModel(config, rngs=rngs)\n</code></pre>"},{"location":"examples/basic/diffusion-mnist-demo/#3-different-image-sizes","title":"3. Different Image Sizes","text":"<pre><code># Larger images (CIFAR-10 size)\nconfig = ModelConfig(\n    name=\"ddpm_cifar\",\n    model_class=\"DDPMModel\",\n    input_dim=(32, 32, 3),  # RGB images\n    parameters={\"noise_steps\": 1000},\n)\n</code></pre>"},{"location":"examples/basic/diffusion-mnist-demo/#limitations-of-this-demo","title":"Limitations of This Demo","text":"<p>\u26a0\ufe0f Important Limitations:</p> <ol> <li>Untrained Model: The model is randomly initialized, not trained</li> <li>Generates abstract patterns, not realistic digits</li> <li> <p>For training, see diffusion-mnist.md</p> </li> <li> <p>Dummy Data: Uses synthetic data (random noise)</p> </li> <li>Not real MNIST images</li> <li> <p>Just for API demonstration</p> </li> <li> <p>No Evaluation: No metrics or quality assessment</p> </li> <li> <p>See training tutorial for FID scores and evaluation</p> </li> <li> <p>Simplified: Focuses on core API, not advanced techniques</p> </li> <li>No conditional generation</li> <li>No inpainting or interpolation</li> <li>No classifier guidance</li> </ol>"},{"location":"examples/basic/diffusion-mnist-demo/#next-steps","title":"Next Steps","text":""},{"location":"examples/basic/diffusion-mnist-demo/#for-learning","title":"For Learning","text":"<ol> <li>Training Tutorial</li> <li>Complete end-to-end training</li> <li>Real MNIST data</li> <li>Evaluation metrics</li> <li> <p>Model checkpointing</p> </li> <li> <p>Diffusion Concepts</p> </li> <li>Mathematical foundations</li> <li>Forward and reverse processes</li> <li> <p>Noise schedules</p> </li> <li> <p>Advanced Techniques</p> </li> <li>Conditional generation</li> <li>Inpainting and interpolation</li> <li>Classifier-free guidance</li> </ol>"},{"location":"examples/basic/diffusion-mnist-demo/#for-development","title":"For Development","text":"<ol> <li>Train Your Own Model:</li> </ol> <pre><code>python examples/generative_models/image/diffusion/diffusion_mnist_training.py\n</code></pre> <ol> <li>Try Other Models:</li> <li><code>vae_mnist.py</code> - Variational Autoencoders</li> <li><code>gan_mnist.py</code> - Generative Adversarial Networks</li> <li> <p><code>flow_mnist.py</code> - Normalizing Flows</p> </li> <li> <p>Explore Advanced Examples:</p> </li> <li><code>dit_demo.py</code> - Diffusion Transformers</li> <li><code>latent_diffusion.py</code> - High-resolution generation</li> </ol>"},{"location":"examples/basic/diffusion-mnist-demo/#complete-code","title":"Complete Code","text":"<p>The complete code is available at:</p> <pre><code>examples/generative_models/image/diffusion/diffusion_mnist.py\n</code></pre> <p>Or as a Jupyter notebook:</p> <pre><code>examples/generative_models/image/diffusion/diffusion_mnist.ipynb\n</code></pre>"},{"location":"examples/basic/diffusion-mnist-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/basic/diffusion-mnist-demo/#issue-import-error","title":"Issue: Import Error","text":"<p>Error: <code>ModuleNotFoundError: No module named 'artifex'</code></p> <p>Solution:</p> <pre><code># Make sure environment is activated\nsource activate.sh\n\n# Verify installation\npython -c \"import artifex; print(artifex.__version__)\"\n</code></pre>"},{"location":"examples/basic/diffusion-mnist-demo/#issue-slow-execution","title":"Issue: Slow Execution","text":"<p>Problem: Demo takes too long to run</p> <p>Solutions:</p> <ol> <li>Use GPU if available (20x faster)</li> <li>Reduce number of samples: <code>n_samples_or_shape=4</code></li> <li>Use DDIM with fewer steps: <code>steps=20</code></li> <li>Reduce noise steps in config: <code>noise_steps=100</code></li> </ol>"},{"location":"examples/basic/diffusion-mnist-demo/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Error: <code>RuntimeError: CUDA out of memory</code></p> <p>Solutions:</p> <pre><code># Reduce batch size\nn_samples_or_shape=4  # Instead of 8\n\n# Use CPU instead\n# JAX will automatically fallback to CPU\n\n# Use DDIM with fewer steps\nsteps=20  # Instead of 50\n</code></pre>"},{"location":"examples/basic/diffusion-mnist-demo/#additional-resources","title":"Additional Resources","text":"<ul> <li> <p> Full Training Tutorial</p> <p>Complete DDPM training pipeline with real data</p> </li> <li> <p> Diffusion Guide</p> <p>Comprehensive guide to diffusion models</p> </li> <li> <p> API Reference</p> <p>Complete API documentation</p> </li> <li> <p> Paper: DDPM</p> <p>Original paper by Ho et al., 2020</p> </li> <li> <p> Paper: DDIM</p> <p>Fast sampling by Song et al., 2020</p> </li> <li> <p> More Examples</p> <p>Additional code examples and notebooks</p> </li> </ul>"},{"location":"examples/basic/diffusion-mnist-demo/#summary","title":"Summary","text":"<p>This demo introduced you to:</p> <ul> <li>\u2705 Artifex's DDPMModel API</li> <li>\u2705 Forward and reverse diffusion</li> <li>\u2705 DDPM vs DDIM sampling</li> <li>\u2705 Visualization techniques</li> <li>\u2705 Speed vs quality tradeoffs</li> </ul> <p>Ready to train? Check out the complete training tutorial!</p>"},{"location":"examples/basic/diffusion-mnist/","title":"Training a Diffusion Model on MNIST","text":"<p>Level: Beginner | Runtime: ~30 minutes (GPU), ~2-3 hours (CPU) | Format: Python + Jupyter</p> <p>This tutorial provides a complete, production-ready example of training a DDPM (Denoising Diffusion Probabilistic Model) on the MNIST dataset. By the end, you'll have trained a diffusion model from scratch that generates realistic handwritten digits.</p>"},{"location":"examples/basic/diffusion-mnist/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/image/diffusion/diffusion_mnist_training.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/image/diffusion/diffusion_mnist_training.ipynb</code></li> </ul> <p>Dual-Format Implementation</p> <p>This example is available in two synchronized formats:</p> <ul> <li>Python Script (.py) - For version control, IDE development, and CI/CD integration</li> <li>Jupyter Notebook (.ipynb) - For interactive learning, experimentation, and exploration</li> </ul> <p>Both formats contain identical content and can be used interchangeably.</p>"},{"location":"examples/basic/diffusion-mnist/#quick-start","title":"Quick Start","text":"<pre><code># Activate Artifex environment\nsource activate.sh\n\n# Run the Python script\npython examples/generative_models/image/diffusion/diffusion_mnist_training.py\n\n# Or launch Jupyter notebook for interactive exploration\njupyter lab examples/generative_models/image/diffusion/diffusion_mnist_training.ipynb\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#overview","title":"Overview","text":"<p>Learning Objectives:</p> <ul> <li> Understand why certain training techniques matter for diffusion models</li> <li> Load and preprocess MNIST dataset with proper padding for UNet</li> <li> Configure a DDPM model with cosine noise schedule and Huber loss</li> <li> Implement learning rate warmup with cosine decay</li> <li> Train using Artifex's DiffusionTrainer framework</li> <li> Generate samples using DDIM (fast) sampling</li> <li> Troubleshoot common training issues</li> </ul> <p>Prerequisites:</p> <ul> <li>Basic understanding of neural networks and diffusion models</li> <li>Familiarity with JAX and Flax NNX basics</li> <li>Artifex installed with CUDA support (recommended)</li> </ul> <p>Estimated Time: 30-45 minutes</p>"},{"location":"examples/basic/diffusion-mnist/#whats-covered","title":"What's Covered","text":"<ul> <li> <p> Data Pipeline</p> <p>Loading MNIST, padding to 32x32, normalizing to [-1, 1] range</p> </li> <li> <p> Model Configuration</p> <p>DDPM with cosine noise schedule, Huber loss, optimized UNet</p> </li> <li> <p> Training Optimization</p> <p>Learning rate warmup + cosine decay to prevent early plateau</p> </li> <li> <p> Training Loop</p> <p>Using DiffusionTrainer with JIT compilation and EMA updates</p> </li> <li> <p> Sample Generation</p> <p>DDIM sampling (100 steps) for fast, high-quality generation</p> </li> <li> <p> Visualization</p> <p>Training curves with loss and learning rate monitoring</p> </li> </ul>"},{"location":"examples/basic/diffusion-mnist/#expected-results","title":"Expected Results","text":"<p>After 50 epochs of training:</p> <ul> <li>Training time: ~30 minutes on GPU (RTX 4090)</li> <li>Final loss: ~0.027 (benchmark: 0.021 for quality digits)</li> <li>Generated samples: Clear, readable handwritten digits</li> </ul> <p></p> <p></p>"},{"location":"examples/basic/diffusion-mnist/#why-these-techniques-matter","title":"Why These Techniques Matter","text":"<p>Before diving into the code, let's understand why we use specific techniques:</p> Technique Problem it Solves Our Solution Cosine noise schedule Linear schedule adds noise too quickly in early steps Cosine provides smoother noise progression Huber loss MSE loss can be unstable with outliers Huber is more robust, combines L1 and L2 LR warmup High learning rate causes early training instability Start at 0, gradually increase to peak Cosine LR decay Constant LR leads to oscillation near optimum Smoothly decrease LR for fine-tuning 32x32 padding 28x28 doesn't divide evenly for UNet 32\u219216\u21928\u21924 gives clean downsampling Uniform timestep sampling Biased sampling can hurt training stability Equal probability for all timesteps"},{"location":"examples/basic/diffusion-mnist/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/basic/diffusion-mnist/#installation","title":"Installation","text":"<pre><code># Install Artifex with CUDA support (recommended)\nuv sync --extra cuda-dev\n\n# Or CPU-only\nuv sync\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#gpu-memory-configuration","title":"GPU Memory Configuration","text":"<p>Important: Memory Configuration</p> <p>TensorFlow (used by tensorflow_datasets) can pre-allocate GPU memory, leaving none for JAX. Set these environment variables before any imports:</p> <pre><code>import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress TF warnings\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"  # Don't pre-allocate\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"  # JAX: don't pre-allocate\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\"  # JAX: use 80% of GPU\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#step-1-setup-and-imports","title":"Step 1: Setup and Imports","text":"<pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax\nfrom flax import nnx\nfrom tqdm import tqdm\n\n# Artifex imports\nfrom artifex.generative_models.core.configuration.backbone_config import UNetBackboneConfig\nfrom artifex.generative_models.core.configuration.diffusion_config import (\n    DDPMConfig,\n    NoiseScheduleConfig,\n)\nfrom artifex.generative_models.core.noise_schedule import create_noise_schedule\nfrom artifex.generative_models.models.diffusion.ddpm import DDPMModel\nfrom artifex.generative_models.training.trainers.diffusion_trainer import (\n    DiffusionTrainer,\n    DiffusionTrainingConfig,\n)\n\n# DataRax imports for data loading\nfrom datarax import from_source\nfrom datarax.sources import TFDSSource, TfdsDataSourceConfig\n\nprint(f\"JAX backend: {jax.default_backend()}\")\nprint(f\"Devices: {jax.devices()}\")\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#step-2-configuration","title":"Step 2: Configuration","text":"<p>We define our training hyperparameters based on research best practices:</p> <pre><code># Training configuration\nSEED = 42\nNUM_EPOCHS = 50      # 40-100 epochs needed for quality results\nBATCH_SIZE = 256     # Larger batch for better GPU utilization\nNUM_TIMESTEPS = 1000 # Standard for DDPM\nIMAGE_SIZE = 32      # Pad MNIST 28x28 \u2192 32x32 for clean UNet downsampling\n\n# Learning rate schedule\nBASE_LR = 1e-4       # Peak learning rate (after warmup)\nWARMUP_STEPS = 500   # ~2 epochs of warmup\n</code></pre> <p>Why these values?</p> <ul> <li>50 epochs: Diffusion models need sufficient training time. 10 epochs often results in blurry samples.</li> <li>Batch size 256: Larger batches provide better GPU utilization and faster training.</li> <li>1000 timesteps: Standard from the original DDPM paper, provides good quality-speed tradeoff.</li> <li>32x32 images: Enables clean UNet downsampling (32\u219216\u21928\u21924) without odd dimensions.</li> </ul>"},{"location":"examples/basic/diffusion-mnist/#step-3-data-loading-and-preprocessing","title":"Step 3: Data Loading and Preprocessing","text":"<p>We use DataRax for efficient data loading with built-in batching and shuffling.</p>"},{"location":"examples/basic/diffusion-mnist/#create-mnist-data-source","title":"Create MNIST Data Source","text":"<pre><code># Initialize RNG for data loading\ndata_rngs = nnx.Rngs(SEED)\n\n# Configure MNIST data source using DataRax\ntrain_source_config = TfdsDataSourceConfig(\n    name=\"mnist\",\n    split=\"train\",\n    shuffle=True,              # Shuffle training data\n    shuffle_buffer_size=10000, # Buffer size for shuffling\n)\ntrain_source = TFDSSource(train_source_config, rngs=data_rngs)\n\nprint(f\"\ud83d\udcca MNIST train dataset loaded: {len(train_source)} samples\")\n</code></pre> <p>DataRax Benefits:</p> <ul> <li>Automatic TF\u2192JAX conversion: No manual array conversion needed</li> <li>Built-in shuffling: Configurable shuffle buffer</li> <li>Memory efficient: Streams data instead of loading all at once</li> <li>JAX-native: Seamless integration with JAX workflows</li> </ul>"},{"location":"examples/basic/diffusion-mnist/#create-training-pipeline","title":"Create Training Pipeline","text":"<p>DataRax's <code>from_source</code> function creates a batched data pipeline:</p> <pre><code># Create training pipeline with batching and JIT compilation\ntrain_pipeline = from_source(train_source, batch_size=BATCH_SIZE, jit_compile=True)\n\n# Calculate number of batches per epoch\nn_batches = len(train_source) // BATCH_SIZE\nprint(f\"\u2705 Training pipeline created: {n_batches} batches per epoch\")\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#define-preprocessing-function","title":"Define Preprocessing Function","text":"<p>Diffusion models expect data in the range [-1, 1]. We also pad 28x28 \u2192 32x32 for clean UNet downsampling:</p> <pre><code>def preprocess_batch(batch):\n    \"\"\"Preprocess MNIST batch for diffusion training.\n\n    Args:\n        batch: Dictionary with 'image' key (uint8, shape: [B, 28, 28, 1])\n\n    Returns:\n        Dictionary with normalized and padded images (float32, shape: [B, 32, 32, 1])\n    \"\"\"\n    image = batch[\"image\"]\n\n    # Convert to float and normalize to [-1, 1]\n    image = image.astype(jnp.float32)\n    image = (image / 127.5) - 1.0\n\n    # Pad 28x28 to 32x32 (2 pixels on each side)\n    # This enables clean UNet downsampling: 32 -&gt; 16 -&gt; 8 -&gt; 4\n    image = jnp.pad(\n        image,\n        ((0, 0), (2, 2), (2, 2), (0, 0)),  # Batch, height, width, channels\n        mode=\"constant\",\n        constant_values=-1.0,               # Background value after normalization\n    )\n\n    return {\"image\": image}\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#iterate-over-data","title":"Iterate Over Data","text":"<p>The pipeline yields batches as dictionaries with JAX arrays:</p> <pre><code># Example: inspect first batch\nfor raw_batch in train_pipeline:\n    batch = preprocess_batch(raw_batch)\n    images = batch[\"image\"]  # Shape: (BATCH_SIZE, 32, 32, 1)\n    print(f\"  Batch shape: {images.shape}\")\n    print(f\"  Value range: [{float(images.min()):.2f}, {float(images.max()):.2f}]\")\n    break  # Just check first batch\n</code></pre> <p>Output:</p> <pre><code>  Batch shape: (64, 32, 32, 1)\n  Value range: [-1.00, 1.00]\n</code></pre> <p>DataRax Pipeline Features</p> <p>The DataRax pipeline automatically handles:</p> <ul> <li>Batching: Groups samples into batches of <code>BATCH_SIZE</code></li> <li>Shuffling: Randomizes order each epoch (configured in source)</li> <li>JAX Arrays: Returns JAX arrays ready for training</li> <li>Streaming: Memory-efficient iteration over large datasets</li> </ul>"},{"location":"examples/basic/diffusion-mnist/#step-4-model-configuration","title":"Step 4: Model Configuration","text":""},{"location":"examples/basic/diffusion-mnist/#initialize-random-number-generators","title":"Initialize Random Number Generators","text":"<pre><code>key = jax.random.key(SEED)\nparams_key, noise_key, sample_key, dropout_key, timestep_key = jax.random.split(key, 5)\nrngs = nnx.Rngs(\n    params=params_key,\n    noise=noise_key,\n    sample=sample_key,\n    dropout=dropout_key,\n    timestep=timestep_key,\n)\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#configure-unet-backbone","title":"Configure UNet Backbone","text":"<p>The UNet is the neural network that learns to predict noise:</p> <pre><code>backbone_config = UNetBackboneConfig(\n    name=\"unet_backbone\",\n    hidden_dims=(64, 128, 256),    # Channel progression at each resolution\n    activation=\"gelu\",              # GELU activation (smoother than ReLU)\n    in_channels=1,                  # MNIST is grayscale\n    out_channels=1,\n    time_embedding_dim=128,         # Dimension for timestep encoding\n    attention_resolutions=(8,),     # Apply attention at 8x8 resolution only\n    num_res_blocks=2,               # Residual blocks per resolution\n    channel_mult=(1, 2, 4),         # Channel multipliers\n    dropout_rate=0.0,               # No dropout for this example\n)\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#configure-noise-schedule","title":"Configure Noise Schedule","text":"<p>We use a cosine schedule instead of linear for smoother training:</p> <pre><code>noise_schedule_config = NoiseScheduleConfig(\n    name=\"cosine_schedule\",\n    schedule_type=\"cosine\",  # Key improvement: cosine &gt; linear\n    num_timesteps=NUM_TIMESTEPS,\n    beta_start=1e-4,\n    beta_end=0.02,\n)\n</code></pre> <p>Why cosine? The linear schedule adds noise too aggressively in early timesteps, making it harder for the model to learn. Cosine provides a gentler progression.</p>"},{"location":"examples/basic/diffusion-mnist/#configure-ddpm-model","title":"Configure DDPM Model","text":"<pre><code>ddpm_config = DDPMConfig(\n    name=\"ddpm_mnist\",\n    backbone=backbone_config,\n    noise_schedule=noise_schedule_config,\n    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1),  # 32x32 grayscale\n    loss_type=\"huber\",  # Key improvement: Huber &gt; MSE for stability\n    clip_denoised=True,\n)\n\n# Create the model\nmodel = DDPMModel(ddpm_config, rngs=rngs)\n\nprint(\"\u2705 DDPMModel created:\")\nprint(f\"   UNet: hidden_dims={backbone_config.hidden_dims}\")\nprint(f\"   Noise schedule: {noise_schedule_config.schedule_type}\")\nprint(f\"   Loss type: {ddpm_config.loss_type}\")\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#step-5-training-setup","title":"Step 5: Training Setup","text":""},{"location":"examples/basic/diffusion-mnist/#learning-rate-schedule","title":"Learning Rate Schedule","text":"<p>The warmup + cosine decay schedule is critical for stable training:</p> <pre><code># Create noise schedule for trainer\nnoise_schedule = create_noise_schedule(noise_schedule_config)\n\n# Calculate total training steps (n_batches defined in Step 3)\ntotal_steps = NUM_EPOCHS * n_batches\nprint(f\"Total training steps: {total_steps}\")\n\n# Learning rate schedule: warmup then cosine decay\nlr_schedule = optax.warmup_cosine_decay_schedule(\n    init_value=0.0,              # Start at 0\n    peak_value=BASE_LR,          # Ramp up to 1e-4\n    warmup_steps=WARMUP_STEPS,   # Over 1000 steps\n    decay_steps=total_steps,     # Then decay over remaining training\n    end_value=BASE_LR * 0.01,    # End at 1% of peak\n)\n</code></pre> <p>Why warmup? Without warmup, large gradients in early training can destabilize the model, causing the loss to plateau prematurely.</p>"},{"location":"examples/basic/diffusion-mnist/#create-optimizer","title":"Create Optimizer","text":"<pre><code>optimizer = nnx.Optimizer(\n    model,\n    optax.chain(\n        optax.clip_by_global_norm(1.0),  # Gradient clipping for stability\n        optax.adamw(lr_schedule, weight_decay=1e-5),  # AdamW with weight decay\n    ),\n    wrt=nnx.Param,\n)\nprint(f\"\u2705 Optimizer: AdamW with warmup ({WARMUP_STEPS} steps) + cosine decay\")\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#configure-diffusiontrainer","title":"Configure DiffusionTrainer","text":"<pre><code>diffusion_config = DiffusionTrainingConfig(\n    prediction_type=\"epsilon\",      # Predict the noise (classic DDPM)\n    timestep_sampling=\"uniform\",    # Sample timesteps uniformly\n    loss_weighting=\"uniform\",       # Equal weight for all timesteps\n    ema_decay=0.9999,              # Exponential moving average\n    ema_update_every=10,           # Update EMA every 10 steps\n)\n\ntrainer = DiffusionTrainer(noise_schedule, diffusion_config)\n\n# JIT-compile the training step for speed\njit_train_step = nnx.jit(trainer.train_step)\n\nprint(\"\u2705 DiffusionTrainer initialized\")\nprint(f\"   Prediction type: {diffusion_config.prediction_type}\")\nprint(f\"   Timestep sampling: {diffusion_config.timestep_sampling}\")\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#step-6-training-loop","title":"Step 6: Training Loop","text":"<p>Now we train the model, tracking both loss and learning rate:</p> <pre><code>history = {\"step\": [], \"loss\": [], \"epoch\": [], \"lr\": []}\ntrain_key = jax.random.key(999)\nglobal_step = 0\n\nprint(f\"\\nTraining for {NUM_EPOCHS} epochs ({total_steps} steps)...\")\nprint(\"-\" * 60)\n\nfor epoch in range(NUM_EPOCHS):\n    epoch_losses = []\n\n    # DataRax pipeline automatically handles batching and shuffling\n    pbar = tqdm(train_pipeline, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", total=n_batches)\n    for raw_batch in pbar:\n        train_key, step_key = jax.random.split(train_key)\n\n        # Preprocess batch (normalize and pad)\n        batch = preprocess_batch(raw_batch)\n\n        # Training step (JIT-compiled)\n        loss, metrics = jit_train_step(model, optimizer, batch, step_key)\n\n        # Update EMA weights\n        if global_step % diffusion_config.ema_update_every == 0:\n            trainer.update_ema(model)\n\n        # Track metrics\n        current_lr = float(lr_schedule(global_step))\n        epoch_losses.append(float(loss))\n        history[\"step\"].append(global_step)\n        history[\"loss\"].append(float(loss))\n        history[\"epoch\"].append(epoch)\n        history[\"lr\"].append(current_lr)\n\n        global_step += 1\n        pbar.set_postfix({\"loss\": f\"{loss:.4f}\", \"lr\": f\"{current_lr:.2e}\"})\n\n    avg_loss = np.mean(epoch_losses)\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: avg_loss = {avg_loss:.4f}\")\n\nprint(\"-\" * 60)\nprint(\"Training complete!\")\n</code></pre> <p>What to expect during training:</p> <ul> <li>Epoch 1-2: Loss drops rapidly (warmup phase)</li> <li>Epoch 3-20: Steady decrease as model learns</li> <li>Epoch 20-50: Gradual improvement with decaying LR</li> <li>Final loss: ~0.027 (close to benchmark 0.021)</li> </ul>"},{"location":"examples/basic/diffusion-mnist/#step-7-generate-samples","title":"Step 7: Generate Samples","text":"<p>After training, generate samples using DDIM for fast, high-quality results:</p> <pre><code>print(\"\\nGenerating samples...\")\nn_samples = 16\n\n# DDIM is 10-20x faster than DDPM with comparable quality\nsamples = model.sample(\n    n_samples_or_shape=n_samples,\n    scheduler=\"ddim\",\n    steps=100,  # 100 steps (vs 1000 for DDPM)\n)\n\nprint(f\"\u2705 Generated {n_samples} samples\")\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#visualize-results","title":"Visualize Results","text":"<pre><code>def visualize_samples(images, title=\"Samples\", n_cols=4):\n    \"\"\"Visualize a grid of generated images.\"\"\"\n    n_images = len(images)\n    n_rows = (n_images + n_cols - 1) // n_cols\n\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n    axes = axes.flatten()\n\n    for i, (ax, img) in enumerate(zip(axes, images)):\n        # Denormalize from [-1, 1] to [0, 1]\n        img = (np.array(img) + 1.0) / 2.0\n        img = np.clip(img, 0, 1)\n        ax.imshow(img.squeeze(), cmap=\"gray\")\n        ax.axis(\"off\")\n\n    for i in range(n_images, len(axes)):\n        axes[i].axis(\"off\")\n\n    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n    plt.tight_layout()\n    plt.show()\n\nvisualize_samples(samples, title=\"DDPM Generated MNIST Digits\")\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#step-8-plot-training-curve","title":"Step 8: Plot Training Curve","text":"<p>Visualize training progress with both loss and learning rate:</p> <pre><code>fig, ax1 = plt.subplots(figsize=(12, 5))\n\n# Loss curve (left axis)\nax1.plot(history[\"step\"], history[\"loss\"], alpha=0.3, linewidth=0.5, color=\"tab:blue\")\nif len(history[\"loss\"]) &gt; 100:\n    window = 100\n    smoothed = np.convolve(history[\"loss\"], np.ones(window) / window, mode=\"valid\")\n    ax1.plot(history[\"step\"][window-1:], smoothed, linewidth=2,\n             label=\"Loss (smoothed)\", color=\"tab:blue\")\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Loss\", color=\"tab:blue\")\nax1.grid(True, alpha=0.3)\n\n# Learning rate curve (right axis)\nax2 = ax1.twinx()\nax2.plot(history[\"step\"], history[\"lr\"], linewidth=1.5, color=\"tab:orange\",\n         label=\"Learning Rate\")\nax2.set_ylabel(\"Learning Rate\", color=\"tab:orange\")\n\n# Legend\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n\nax1.set_title(\"Diffusion Training (warmup + cosine decay)\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#experiments-to-try","title":"Experiments to Try","text":"<p>Once you have the basic training working, try these experiments:</p>"},{"location":"examples/basic/diffusion-mnist/#1-different-noise-schedules","title":"1. Different Noise Schedules","text":"<p>Compare cosine vs linear noise schedules:</p> <pre><code># Linear schedule (original DDPM)\nnoise_schedule_config = NoiseScheduleConfig(\n    name=\"linear_schedule\",\n    schedule_type=\"linear\",  # Change from cosine\n    num_timesteps=1000,\n    beta_start=1e-4,\n    beta_end=0.02,\n)\n</code></pre> <p>Expected Result: Linear schedule typically produces slightly worse samples with the same training time.</p>"},{"location":"examples/basic/diffusion-mnist/#2-adjust-model-capacity","title":"2. Adjust Model Capacity","text":"<p>Try a larger model for better quality (if GPU memory allows):</p> <pre><code># Larger UNet for higher quality\nbackbone_config = UNetBackboneConfig(\n    name=\"unet_large\",\n    hidden_dims=(128, 256, 512),  # Larger channels\n    attention_resolutions=(8, 16),  # Attention at multiple resolutions\n    num_res_blocks=3,  # More residual blocks\n    channel_mult=(1, 2, 4),\n)\n</code></pre> <p>Trade-off: Better quality but slower training and higher memory usage.</p>"},{"location":"examples/basic/diffusion-mnist/#3-different-sampling-methods","title":"3. Different Sampling Methods","text":"<p>Compare DDPM vs DDIM sampling:</p> <pre><code># DDPM sampling (slow but original algorithm)\nsamples_ddpm = model.sample(\n    n_samples_or_shape=16,\n    scheduler=\"ddpm\",  # 1000 steps\n)\n\n# DDIM sampling (fast with comparable quality)\nsamples_ddim = model.sample(\n    n_samples_or_shape=16,\n    scheduler=\"ddim\",\n    steps=50,  # Try 50, 100, 200\n)\n</code></pre> <p>Expected Result: DDIM with 50-100 steps is 10-20x faster with minimal quality loss.</p>"},{"location":"examples/basic/diffusion-mnist/#4-loss-function-comparison","title":"4. Loss Function Comparison","text":"<p>Try MSE loss instead of Huber:</p> <pre><code>ddpm_config = DDPMConfig(\n    name=\"ddpm_mse\",\n    backbone=backbone_config,\n    noise_schedule=noise_schedule_config,\n    input_shape=(32, 32, 1),\n    loss_type=\"mse\",  # Instead of \"huber\"\n    clip_denoised=True,\n)\n</code></pre> <p>Expected Result: MSE may be less stable but can work well with careful tuning.</p>"},{"location":"examples/basic/diffusion-mnist/#5-extend-to-other-datasets","title":"5. Extend to Other Datasets","text":"<p>Apply the same techniques to Fashion-MNIST:</p> <pre><code># Load Fashion-MNIST instead\nds_train = tfds.load(\"fashion_mnist\", split=\"train\", as_supervised=True)\n# Rest of the code remains the same!\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/basic/diffusion-mnist/#issue-1-gpu-out-of-memory","title":"Issue 1: GPU Out of Memory","text":"<p>Symptoms: <code>RuntimeError: CUDA out of memory</code> or TensorFlow allocation errors</p> <p>Solutions:</p> <pre><code># 1. Set environment variables BEFORE imports\nimport os\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\"\n\n# 2. Reduce batch size\nBATCH_SIZE = 32  # or 16\n\n# 3. Use smaller model\nhidden_dims = (32, 64, 128)  # instead of (64, 128, 256)\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#issue-2-loss-plateaus-early","title":"Issue 2: Loss Plateaus Early","text":"<p>Symptoms: Loss drops to ~0.03 in first 2 epochs and stops improving</p> <p>Causes &amp; Solutions:</p> <ol> <li>Missing warmup: Add learning rate warmup (critical!)</li> <li>Linear noise schedule: Switch to cosine schedule</li> <li>Too few epochs: Train for 50+ epochs, not 10</li> </ol>"},{"location":"examples/basic/diffusion-mnist/#issue-3-blurry-or-noisy-samples","title":"Issue 3: Blurry or Noisy Samples","text":"<p>Solutions:</p> <ol> <li>Train longer: 50-100 epochs for quality results</li> <li>Use cosine noise schedule: Smoother than linear</li> <li>Increase DDIM steps: Try <code>steps=100</code> or <code>steps=200</code></li> <li>Check final loss: Should be ~0.02-0.03 for good samples</li> </ol>"},{"location":"examples/basic/diffusion-mnist/#issue-4-training-is-slow","title":"Issue 4: Training is Slow","text":"<p>Solutions:</p> <pre><code># Ensure JIT compilation is working\njit_train_step = nnx.jit(trainer.train_step)\n\n# Check GPU is being used\nprint(jax.default_backend())  # Should print \"gpu\"\n\n# First epoch is slow (JIT compilation), subsequent epochs should be fast\n</code></pre>"},{"location":"examples/basic/diffusion-mnist/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <ol> <li>Why padding matters: 32x32 enables clean UNet downsampling</li> <li>Why cosine schedule: Smoother noise progression than linear</li> <li>Why Huber loss: More robust than MSE</li> <li>Why warmup: Prevents early training instability</li> <li>How to use DiffusionTrainer: Artifex's training framework with SOTA techniques</li> <li>How to generate samples: DDIM for 10-20x faster sampling</li> </ol>"},{"location":"examples/basic/diffusion-mnist/#key-takeaways","title":"Key Takeaways","text":"What Why How Pad to 32x32 Clean UNet downsampling <code>np.pad(..., constant_values=-1.0)</code> Cosine schedule Smoother training <code>schedule_type=\"cosine\"</code> Huber loss Stable gradients <code>loss_type=\"huber\"</code> LR warmup Prevent early plateau <code>warmup_cosine_decay_schedule</code> DDIM sampling Fast generation <code>scheduler=\"ddim\", steps=100</code>"},{"location":"examples/basic/diffusion-mnist/#next-steps","title":"Next Steps","text":""},{"location":"examples/basic/diffusion-mnist/#related-examples","title":"Related Examples","text":"<ul> <li> <p> VAE MNIST</p> <p>Compare diffusion with Variational Autoencoders</p> </li> <li> <p> GAN MNIST</p> <p>Learn adversarial training for image generation</p> </li> <li> <p> Flow MNIST</p> <p>Explore normalizing flows with exact likelihoods</p> </li> <li> <p> Diffusion Demo</p> <p>Quick API demo without training</p> </li> </ul>"},{"location":"examples/basic/diffusion-mnist/#further-exploration","title":"Further Exploration","text":"<ul> <li>Conditional Generation: Add class labels to control digit generation</li> <li>Fashion-MNIST: Apply the same techniques to clothing items</li> <li>CIFAR-10: Scale up to 32x32 color images (change <code>in_channels=3</code>)</li> <li>Advanced Techniques: Try v-prediction or min-SNR loss weighting</li> </ul>"},{"location":"examples/basic/diffusion-mnist/#additional-resources","title":"Additional Resources","text":"<ul> <li> <p> Diffusion Guide</p> <p>Comprehensive guide to diffusion models in Artifex</p> </li> <li> <p> API Reference</p> <p>Complete API documentation for DDPMModel</p> </li> <li> <p> DDPM Paper</p> <p>Original paper by Ho et al., 2020</p> </li> <li> <p> Annotated Diffusion</p> <p>HuggingFace's excellent tutorial</p> </li> <li> <p> DDIM Paper</p> <p>Fast sampling by Song et al., 2020</p> </li> </ul> <p>Congratulations! You've successfully trained a diffusion model that generates realistic handwritten digits!</p>"},{"location":"examples/basic/ebm-mnist/","title":"Energy-Based Model on MNIST","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on EBM implementations.</p>"},{"location":"examples/basic/ebm-mnist/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Energy-based modeling basics</li> <li>Training with contrastive divergence</li> <li>MCMC sampling for generation</li> <li>Visualizing energy landscapes</li> </ul>"},{"location":"examples/basic/ebm-mnist/#planned-features","title":"Planned Features","text":"<ul> <li>Simple MLP energy function</li> <li>Langevin dynamics sampling</li> <li>Training stability techniques</li> <li>Generation quality evaluation</li> </ul>"},{"location":"examples/basic/ebm-mnist/#related-documentation","title":"Related Documentation","text":"<ul> <li>Energy-Based Models API</li> <li>EBM Concepts</li> <li>EBM Guide</li> </ul>"},{"location":"examples/basic/ebm-mnist/#references","title":"References","text":"<ul> <li>LeCun et al., \"A Tutorial on Energy-Based Learning\" (2006)</li> <li>Du &amp; Mordatch, \"Implicit Generation and Modeling with Energy-Based Models\" (2019)</li> </ul>"},{"location":"examples/basic/flow-mnist/","title":"Training a Flow Model on MNIST with RealNVP","text":"<p>Level: Beginner | Runtime: ~50 minutes (GPU), ~4-5 hours (CPU) | Format: Python + Jupyter</p> <p>This tutorial demonstrates how to train a RealNVP normalizing flow model using Artifex's configuration-based API. Instead of implementing flow transformations from scratch, we use Artifex's <code>RealNVP</code> class with <code>RealNVPConfig</code> and <code>CouplingNetworkConfig</code> for clean, production-ready training. We also use <code>DataRax</code> for efficient GPU-accelerated data loading.</p>"},{"location":"examples/basic/flow-mnist/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/image/flow/flow_mnist.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/image/flow/flow_mnist.ipynb</code></li> </ul> <p>Dual-Format Implementation</p> <p>This example is available in two synchronized formats:</p> <ul> <li>Python Script (.py) - For version control, IDE development, and CI/CD integration</li> <li>Jupyter Notebook (.ipynb) - For interactive learning, experimentation, and exploration</li> </ul> <p>Both formats contain identical content and can be used interchangeably.</p>"},{"location":"examples/basic/flow-mnist/#quick-start","title":"Quick Start","text":"<pre><code># Activate Artifex environment\nsource activate.sh\n\n# Run the Python script\npython examples/generative_models/image/flow/flow_mnist.py\n\n# Or launch Jupyter notebook for interactive exploration\njupyter lab examples/generative_models/image/flow/flow_mnist.ipynb\n</code></pre>"},{"location":"examples/basic/flow-mnist/#overview","title":"Overview","text":"<p>Learning Objectives:</p> <ul> <li> Understand why dequantization is critical for flow models</li> <li> Use Artifex's <code>RealNVP</code> with <code>RealNVPConfig</code> and <code>CouplingNetworkConfig</code></li> <li> Use <code>DataRax</code> for efficient GPU-accelerated data loading</li> <li> Configure learning rate warmup with cosine decay for stable training</li> <li> Train using negative log-likelihood (maximum likelihood)</li> <li> Generate samples from the trained flow model</li> <li> Visualize training curves with smoothing</li> </ul> <p>Prerequisites:</p> <ul> <li>Basic understanding of neural networks and generative models</li> <li>Familiarity with JAX and Flax NNX basics</li> <li>Artifex installed with CUDA support (recommended)</li> </ul> <p>Estimated Time: 45-60 minutes</p>"},{"location":"examples/basic/flow-mnist/#whats-covered","title":"What's Covered","text":"<ul> <li> <p> Dequantization</p> <p>Converting discrete MNIST pixels to continuous data for flow models</p> </li> <li> <p> Model Configuration</p> <p>RealNVP with 8 coupling layers and MLP networks</p> </li> <li> <p> LR Schedule</p> <p>Warmup + cosine decay for stable training convergence</p> </li> <li> <p> Training Loop</p> <p>Negative log-likelihood optimization with gradient clipping</p> </li> <li> <p> Sample Generation</p> <p>Generating new digits from the base distribution</p> </li> <li> <p> Visualization</p> <p>Training curves with smoothing and learning rate monitoring</p> </li> </ul>"},{"location":"examples/basic/flow-mnist/#expected-results","title":"Expected Results","text":"<p>After 100 epochs of training:</p> <ul> <li>Training time: ~50 minutes on GPU (RTX 4090)</li> <li>Final NLL: ~-2500 to -2700 (more negative is better)</li> <li>Generated samples: Clear, recognizable digits</li> </ul> <p></p> <p></p>"},{"location":"examples/basic/flow-mnist/#why-these-techniques-matter","title":"Why These Techniques Matter","text":"<p>Before diving into the code, let's understand why we use specific techniques:</p> Technique Problem it Solves Our Solution Dequantization Flow models require continuous data, MNIST is discrete (0-255) Add uniform noise U(0, 1/256) to each pixel 12 coupling layers Insufficient layers limit expressiveness More layers for better expressiveness 4-layer MLPs (512 units) Coupling networks need enough capacity Deeper/wider networks capture complex transformations LR warmup (200 steps) Large gradients early in training cause instability Gradual warmup prevents early divergence Cosine decay Constant LR leads to oscillation late in training Decay allows fine-tuning as training progresses Gradient clipping Flow models can have exploding gradients Global norm clipping to 1.0 ensures stability Scale activation tanh Unbounded scale outputs cause numerical issues Tanh bounds scale to [-1, 1] for stability JIT compilation Python overhead slows training <code>nnx.jit</code> compiles training step for GPU acceleration DataRax Inefficient CPU-based data loading GPU-accelerated batching with JIT compilation"},{"location":"examples/basic/flow-mnist/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/basic/flow-mnist/#installation","title":"Installation","text":"<pre><code># Install Artifex with CUDA support (recommended)\nuv sync --extra cuda-dev\n\n# Or CPU-only\nuv sync\n</code></pre>"},{"location":"examples/basic/flow-mnist/#step-1-setup-and-imports","title":"Step 1: Setup and Imports","text":"<pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax\nfrom flax import nnx\nfrom tqdm import tqdm\n\n# Artifex imports\nfrom artifex.generative_models.core.configuration.flow_config import (\n    CouplingNetworkConfig,\n    RealNVPConfig,\n)\nfrom artifex.generative_models.models.flow.real_nvp import RealNVP\n\nprint(f\"JAX backend: {jax.default_backend()}\")\nprint(f\"Devices: {jax.devices()}\")\n</code></pre>"},{"location":"examples/basic/flow-mnist/#step-2-configuration","title":"Step 2: Configuration","text":"<p>Training configuration based on RealNVP best practices:</p> <pre><code># Configuration (based on RealNVP best practices)\nSEED = 42\nNUM_EPOCHS = 100  # 100 epochs for good quality\nBATCH_SIZE = 512  # Larger batch size for better GPU utilization\nBASE_LR = 1e-3  # Scale LR with batch size\nWARMUP_STEPS = 200  # Warmup steps\n</code></pre> <p>Why these values?</p> <ul> <li>100 epochs: Sufficient for clear, recognizable digits</li> <li>Batch size 512: Better GPU utilization and stable gradients</li> <li>LR 1e-3: Higher LR scaled with batch size for faster convergence</li> <li>Warmup 200 steps: Quick warmup to stabilize early training</li> </ul>"},{"location":"examples/basic/flow-mnist/#step-3-data-loading-and-preprocessing","title":"Step 3: Data Loading and Preprocessing","text":"<p>Flow models require continuous data. MNIST is discrete (0-255), so we apply dequantization:</p> <pre><code>import tensorflow_datasets as tfds\n\ndef load_mnist_data():\n    \"\"\"Load MNIST dataset.\"\"\"\n    ds_train = tfds.load(\"mnist\", split=\"train\", shuffle_files=False, as_supervised=True)\n\n    train_images = []\n    for image, _ in ds_train:\n        train_images.append(np.array(image))\n    return np.array(train_images)\n\ndef preprocess_for_flow(images, key):\n    \"\"\"Preprocess MNIST for flow models: normalize, flatten, dequantize, scale.\"\"\"\n    # Normalize to [0, 1]\n    images = images.astype(np.float32) / 255.0\n\n    # Flatten to (N, 784)\n    images = images.reshape(len(images), -1)\n\n    # Dequantization: add uniform noise\n    noise = jax.random.uniform(key, images.shape) / 256.0\n    images = images + noise\n\n    # Scale to [-1, 1]\n    images = (images - 0.5) / 0.5\n\n    return images\n\ntrain_images = load_mnist_data()\nprint(f\"Train: {train_images.shape}\")  # (60000, 28, 28, 1)\n</code></pre> <p>Why Dequantization?</p> <ul> <li>Flow models compute exact likelihoods using the change of variables formula</li> <li>Discrete data leads to infinite log-likelihood (delta functions)</li> <li>Adding uniform noise creates a valid continuous density</li> </ul>"},{"location":"examples/basic/flow-mnist/#step-4-create-realnvp-using-artifexs-api","title":"Step 4: Create RealNVP Using Artifex's API","text":"<p>Use Artifex's <code>RealNVP</code> class with configuration objects:</p> <pre><code># Initialize RNGs\nkey = jax.random.key(SEED)\nparams_key, noise_key, sample_key, dropout_key = jax.random.split(key, 4)\nrngs = nnx.Rngs(\n    params=params_key,\n    noise=noise_key,\n    sample=sample_key,\n    dropout=dropout_key,\n)\n\n# Coupling network config (4 hidden layers with 512 units each for better capacity)\ncoupling_config = CouplingNetworkConfig(\n    name=\"coupling_mlp\",\n    hidden_dims=(512, 512, 512, 512),  # 4 hidden layers with more capacity\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\n# RealNVP config (12 coupling layers for better expressiveness)\nflow_config = RealNVPConfig(\n    name=\"realnvp_mnist\",\n    coupling_network=coupling_config,\n    input_dim=784,  # 28*28\n    base_distribution=\"normal\",\n    num_coupling_layers=12,  # More layers for better expressiveness\n    mask_type=\"checkerboard\",\n)\n\n# Create model\nmodel = RealNVP(flow_config, rngs=rngs)\nprint(f\"RealNVP created: {flow_config.num_coupling_layers} coupling layers\")\n</code></pre> <p>Artifex Model Features:</p> <ul> <li>Frozen dataclass configurations for type safety</li> <li>Configurable coupling networks with MLP architecture</li> <li>Checkerboard masking for alternating dimensions</li> <li>Normal base distribution for standard flow training</li> </ul>"},{"location":"examples/basic/flow-mnist/#step-5-create-optimizer-with-lr-schedule","title":"Step 5: Create Optimizer with LR Schedule","text":"<p>Use learning rate warmup with cosine decay for stable training:</p> <pre><code># Calculate total training steps for learning rate schedule\ntotal_steps = NUM_EPOCHS * (60000 // BATCH_SIZE)\nprint(f\"Total training steps: {total_steps}\")\n\n# Learning rate schedule: warmup + cosine decay\nlr_schedule = optax.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=BASE_LR,\n    warmup_steps=WARMUP_STEPS,\n    decay_steps=total_steps,\n    end_value=BASE_LR * 0.01,\n)\n\n# Optimizer with gradient clipping and LR schedule\noptimizer = nnx.Optimizer(\n    model,\n    optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.adam(lr_schedule)\n    ),\n    wrt=nnx.Param\n)\n</code></pre> <p>Why This LR Schedule?</p> <ul> <li>Warmup: Prevents large gradients from destabilizing early training</li> <li>Cosine decay: Smooth reduction allows fine-tuning without sudden jumps</li> <li>End value 1% of peak: Small LR at end for final refinement</li> </ul>"},{"location":"examples/basic/flow-mnist/#step-6-training-step","title":"Step 6: Training Step","text":"<p>Define the training step for maximum likelihood training:</p> <pre><code>def train_step(model, optimizer, batch):\n    \"\"\"Training step for RealNVP (maximum likelihood).\"\"\"\n    def loss_fn(model):\n        outputs = model(batch, training=True)\n        log_prob = outputs[\"log_prob\"]\n        loss = -jnp.mean(log_prob)  # Negative log-likelihood\n        return loss, {\"nll\": loss, \"log_prob\": jnp.mean(log_prob)}\n\n    (loss, metrics), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n    optimizer.update(model, grads)\n    return metrics\n\n\n# JIT-compile training step for performance\njit_train_step = nnx.jit(train_step)\n</code></pre> <p>Training Objective:</p> <ul> <li>Maximize log-likelihood = minimize negative log-likelihood</li> <li><code>log_prob</code> includes both base distribution probability and log-determinant of transformation</li> <li>Loss should decrease as model learns the data distribution</li> </ul>"},{"location":"examples/basic/flow-mnist/#step-7-training-loop","title":"Step 7: Training Loop","text":"<p>Train the model for multiple epochs:</p> <pre><code>history = {\"step\": [], \"loss\": [], \"log_prob\": [], \"lr\": []}\ntrain_key = jax.random.key(999)\nglobal_step = 0\n\nprint(f\"Training for {NUM_EPOCHS} epochs...\")\n\nfor epoch in range(NUM_EPOCHS):\n    epoch_losses = []\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n        train_key, dequant_key = jax.random.split(train_key)\n\n        # Preprocess: dequantize and scale\n        batch_processed = preprocess_for_flow(batch, dequant_key)\n        batch_jax = jnp.array(batch_processed)\n\n        # JIT-compiled training step\n        metrics = jit_train_step(model, optimizer, batch_jax)\n\n        # Record history\n        current_lr = float(lr_schedule(global_step))\n        history[\"step\"].append(global_step)\n        history[\"loss\"].append(float(metrics[\"nll\"]))\n        history[\"log_prob\"].append(float(metrics[\"log_prob\"]))\n        history[\"lr\"].append(current_lr)\n\n        epoch_losses.append(float(metrics[\"nll\"]))\n        global_step += 1\n\n    avg_loss = np.mean(epoch_losses)\n    print(f\"Epoch {epoch+1}: NLL={avg_loss:.2f}\")\n</code></pre> <p>Training Procedure per Epoch:</p> <ol> <li>Iterate over batches from data loader</li> <li>Apply fresh dequantization noise each batch</li> <li>Compute NLL loss and update model parameters</li> <li>Track loss and learning rate for visualization</li> </ol>"},{"location":"examples/basic/flow-mnist/#step-8-generate-samples","title":"Step 8: Generate Samples","text":"<p>Generate new digits from the trained model:</p> <pre><code>n_samples = 16\ngenerated_samples = model.generate(n_samples=n_samples)\n\n# Denormalize from [-1, 1] to [0, 1]\ngenerated_samples = (generated_samples * 0.5) + 0.5\ngenerated_samples = jnp.clip(generated_samples, 0, 1)\n\n# Reshape to images\ngenerated_images = generated_samples.reshape(n_samples, 28, 28)\n\nprint(f\"Generated {n_samples} samples\")\n</code></pre> <p>Generation Process:</p> <ol> <li>Sample from base distribution (standard normal)</li> <li>Apply inverse transformation through all coupling layers</li> <li>Denormalize to image range</li> </ol>"},{"location":"examples/basic/flow-mnist/#step-9-visualize-results","title":"Step 9: Visualize Results","text":"<pre><code># Generated samples grid\nfig, axes = plt.subplots(4, 4, figsize=(8, 8))\nfor i, ax in enumerate(axes.flatten()):\n    if i &lt; len(generated_images):\n        ax.imshow(np.array(generated_images[i]), cmap='gray')\n    ax.axis('off')\n\nplt.suptitle(\"RealNVP Generated MNIST Digits\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nplt.savefig(\"flow_samples.png\", dpi=150, bbox_inches=\"tight\")\n</code></pre>"},{"location":"examples/basic/flow-mnist/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Quick Training: Fewer epochs for faster iteration</li> </ol> <pre><code>NUM_EPOCHS = 50  # Faster training, slightly lower quality\n</code></pre> <ol> <li>Smaller Model: Trade quality for speed</li> </ol> <pre><code>coupling_config = CouplingNetworkConfig(\n    hidden_dims=(256, 256, 256),  # Smaller networks\n)\nflow_config = RealNVPConfig(\n    num_coupling_layers=8,  # Fewer layers\n)\n</code></pre> <ol> <li>Lower Learning Rate: More stable, slower convergence</li> </ol> <pre><code>BASE_LR = 5e-4    # Lower LR for more stability\nWARMUP_STEPS = 500  # More warmup with lower LR\n</code></pre> <ol> <li>Different Base Distribution: Explore other priors</li> </ol> <pre><code>flow_config = RealNVPConfig(\n    base_distribution=\"uniform\",  # Uniform instead of normal\n)\n</code></pre> <ol> <li>Smaller Batch Size: Less memory usage</li> </ol> <pre><code>BATCH_SIZE = 128  # Smaller batches, may need lower LR\n</code></pre>"},{"location":"examples/basic/flow-mnist/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/basic/flow-mnist/#nan-loss","title":"NaN Loss","text":"<ul> <li>Solution: Reduce learning rate to <code>1e-4</code> or <code>5e-5</code></li> <li>Check: Ensure dequantization is applied (data should be continuous)</li> <li>Fix: Increase gradient clipping threshold or warmup steps</li> </ul>"},{"location":"examples/basic/flow-mnist/#poor-sample-quality","title":"Poor Sample Quality","text":"<ul> <li>Solution: Train for more epochs (100+)</li> <li>Check: Loss should decrease consistently over training</li> <li>Fix: Increase model capacity (more layers, wider networks)</li> </ul>"},{"location":"examples/basic/flow-mnist/#training-plateau","title":"Training Plateau","text":"<ul> <li>Solution: Try different learning rate schedule</li> <li>Check: Learning rate should still be non-zero</li> <li>Fix: Increase batch size for smoother gradients</li> </ul>"},{"location":"examples/basic/flow-mnist/#out-of-memory","title":"Out of Memory","text":"<ul> <li>Solution: Reduce batch size or model size</li> <li>Check: MNIST 784-dim flattened images + 8 coupling layers fits in 8GB+</li> <li>Fix: Use gradient checkpointing for large models</li> </ul>"},{"location":"examples/basic/flow-mnist/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <ul> <li>Artifex Flow API: Using <code>RealNVP</code> with configuration classes</li> <li>Dequantization: Why and how to preprocess discrete data for flows</li> <li>LR Scheduling: Warmup + cosine decay for stable training</li> <li>Training: Maximum likelihood via negative log-likelihood</li> <li>Generation: Sampling from trained flow model</li> </ul>"},{"location":"examples/basic/flow-mnist/#next-steps","title":"Next Steps","text":""},{"location":"examples/basic/flow-mnist/#related-examples","title":"Related Examples","text":"<ul> <li> <p> Simple GAN</p> <p>Compare flow with GAN approach on 2D data</p> </li> <li> <p> Diffusion on MNIST</p> <p>Learn about diffusion models for image generation</p> </li> <li> <p> VAE on MNIST</p> <p>Compare with variational autoencoders</p> </li> <li> <p> Advanced Flow</p> <p>Glow, Neural Spline Flows, and more</p> </li> </ul>"},{"location":"examples/basic/flow-mnist/#documentation-resources","title":"Documentation Resources","text":"<ul> <li>Flow Concepts: Deep dive into flow theory</li> <li>Flow User Guide: Advanced usage patterns</li> <li>Flow API Reference: Complete API documentation</li> </ul>"},{"location":"examples/basic/flow-mnist/#papers","title":"Papers","text":"<ol> <li>Density Estimation using Real-NVP (Dinh et al., 2016)</li> <li> <p>RealNVP paper: https://arxiv.org/abs/1605.08803</p> </li> <li> <p>Glow: Generative Flow with Invertible 1x1 Convolutions (Kingma &amp; Dhariwal, 2018)</p> </li> <li> <p>Glow paper: https://arxiv.org/abs/1807.03039</p> </li> <li> <p>Neural Spline Flows (Durkan et al., 2019)</p> </li> <li>NSF paper: https://arxiv.org/abs/1906.04032</li> </ol> <p>Congratulations! You've successfully trained a Flow model using Artifex's RealNVP API!</p>"},{"location":"examples/basic/simple-gan/","title":"Training a GAN on 2D Data with GANTrainer","text":"<p>Level: Beginner | Runtime: ~2-3 minutes (GPU/CPU) | Format: Python + Jupyter</p> <p>This tutorial demonstrates how to train a GAN using Artifex's high-level <code>GANTrainer</code> API. Instead of implementing training from scratch, we use Artifex's WGAN-GP (Wasserstein GAN with Gradient Penalty) for stable, mode-collapse-free training on a simple 2D circular distribution.</p>"},{"location":"examples/basic/simple-gan/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/image/gan/simple_gan.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/image/gan/simple_gan.ipynb</code></li> </ul> <p>Dual-Format Implementation</p> <p>This example is available in two synchronized formats:</p> <ul> <li>Python Script (.py) - For version control, IDE development, and CI/CD integration</li> <li>Jupyter Notebook (.ipynb) - For interactive learning, experimentation, and exploration</li> </ul> <p>Both formats contain identical content and can be used interchangeably.</p>"},{"location":"examples/basic/simple-gan/#quick-start","title":"Quick Start","text":"<pre><code># Activate Artifex environment\nsource activate.sh\n\n# Run the Python script\npython examples/generative_models/image/gan/simple_gan.py\n\n# Or launch Jupyter notebook for interactive exploration\njupyter lab examples/generative_models/image/gan/simple_gan.ipynb\n</code></pre>"},{"location":"examples/basic/simple-gan/#overview","title":"Overview","text":"<p>Learning Objectives:</p> <ul> <li> Understand WGAN-GP and why it provides stable training</li> <li> Use Artifex's <code>Generator</code> and <code>Discriminator</code> classes with configs</li> <li> Configure <code>GANTrainer</code> with WGAN-GP loss and gradient penalty</li> <li> Train using JIT-compiled training steps for performance</li> <li> Monitor Wasserstein distance during training</li> <li> Visualize real vs. generated data distributions</li> </ul> <p>Prerequisites:</p> <ul> <li>Basic understanding of neural networks and GANs</li> <li>Familiarity with JAX and Flax NNX basics</li> <li>Artifex installed</li> </ul> <p>Estimated Time: 10-15 minutes</p>"},{"location":"examples/basic/simple-gan/#whats-covered","title":"What's Covered","text":"<ul> <li> <p> 2D Data</p> <p>Simple circular distribution for easy visualization of GAN learning</p> </li> <li> <p> Model Configuration</p> <p>Generator and Discriminator with <code>GeneratorConfig</code>, <code>DiscriminatorConfig</code></p> </li> <li> <p> GANTrainer API</p> <p>WGAN-GP training with gradient penalty for stability</p> </li> <li> <p> JIT Compilation</p> <p><code>nnx.jit</code> for optimized training performance</p> </li> <li> <p> Training Monitoring</p> <p>Wasserstein distance and loss curves visualization</p> </li> <li> <p> Evaluation</p> <p>Comparing real and generated data distributions</p> </li> </ul>"},{"location":"examples/basic/simple-gan/#expected-results","title":"Expected Results","text":"<p>After 5,000 training steps:</p> <ul> <li>Training time: ~2-3 minutes on GPU/CPU</li> <li>Final Wasserstein distance: Near 0 (distributions match)</li> <li>Generated samples: Points forming a circle matching real data</li> </ul> <p></p> <p></p>"},{"location":"examples/basic/simple-gan/#why-these-techniques-matter","title":"Why These Techniques Matter","text":"<p>Before diving into the code, let's understand why we use specific techniques:</p> Technique Problem it Solves Our Solution WGAN-GP loss Vanilla GAN loss causes training instability and mode collapse Wasserstein loss provides meaningful gradients throughout training Gradient penalty (\u03bb=0.1) Weight clipping in WGAN limits model capacity GP enforces Lipschitz constraint smoothly without clipping 5 critic iterations Generator may overpower discriminator Train critic more to provide better gradients to generator Adam \u03b21=0.5, \u03b22=0.9 Standard Adam \u03b21=0.9 causes GAN instability Lower \u03b21 reduces momentum, \u03b22=0.9 is WGAN-GP standard No batch normalization Batch norm can cause issues with gradient penalty WGAN-GP works best without batch norm Matched latent/output dim Complex latent\u2192output mapping 2D latent for 2D output simplifies learning"},{"location":"examples/basic/simple-gan/#prerequisites","title":"Prerequisites","text":""},{"location":"examples/basic/simple-gan/#installation","title":"Installation","text":"<pre><code># Install Artifex\nuv sync\n\n# Or with CUDA support\nuv sync --extra cuda-dev\n</code></pre>"},{"location":"examples/basic/simple-gan/#step-1-setup-and-imports","title":"Step 1: Setup and Imports","text":"<pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\nfrom flax import nnx\nfrom tqdm import tqdm\n\n# Artifex imports\nfrom artifex.generative_models.core.configuration.network_configs import (\n    DiscriminatorConfig,\n    GeneratorConfig,\n)\nfrom artifex.generative_models.models.gan import Discriminator, Generator\nfrom artifex.generative_models.training.trainers.gan_trainer import (\n    GANTrainer,\n    GANTrainingConfig,\n)\n\nprint(f\"JAX backend: {jax.default_backend()}\")\nprint(f\"Devices: {jax.devices()}\")\n</code></pre>"},{"location":"examples/basic/simple-gan/#step-2-configuration","title":"Step 2: Configuration","text":"<p>Training configuration based on the official WGAN-GP implementation:</p> <pre><code># Configuration (based on official WGAN-GP toy implementation)\nSEED = 42\nNUM_STEPS = 5000  # Training iterations\nBATCH_SIZE = 256\nLATENT_DIM = 2  # Match output dim for simpler mapping\nN_CRITIC = 5  # Critic iterations per generator step\nLR = 1e-4  # Adam learning rate\nGP_WEIGHT = 0.1  # Gradient penalty weight (0.1 for toy data)\nHIDDEN_DIM = 128  # Hidden layer dimension\n</code></pre> <p>Why these values?</p> <ul> <li>5,000 steps: Sufficient for 2D toy data to converge</li> <li>Batch size 256: Standard for WGAN-GP toy experiments</li> <li>Latent dim 2: Matches output dimension for simpler mapping</li> <li>GP weight 0.1: Lower than standard 10.0, faster convergence for toy data</li> <li>Hidden dim 128: Balance between capacity and training speed</li> </ul>"},{"location":"examples/basic/simple-gan/#step-3-data-generation","title":"Step 3: Data Generation","text":"<p>We use a simple circular distribution for visualization:</p> <pre><code>def generate_circle_data(key, batch_size):\n    \"\"\"Generate 2D points on a unit circle with noise.\"\"\"\n    theta_key, noise_key = jax.random.split(key)\n    theta = jax.random.uniform(theta_key, (batch_size,)) * 2 * jnp.pi\n    r = 1.0 + jax.random.normal(noise_key, (batch_size,)) * 0.05\n    x = r * jnp.cos(theta)\n    y = r * jnp.sin(theta)\n    return jnp.stack([x, y], axis=-1)\n</code></pre> <p>Why Circular Data?</p> <ul> <li>Simple 2D distribution is easy to visualize</li> <li>Clear success metric: generated points should form a circle</li> <li>Fast training for demonstration purposes</li> </ul>"},{"location":"examples/basic/simple-gan/#step-4-create-models-using-artifexs-api","title":"Step 4: Create Models Using Artifex's API","text":"<p>Use Artifex's <code>Generator</code> and <code>Discriminator</code> classes with configuration objects:</p> <pre><code># Generator configuration\ngen_config = GeneratorConfig(\n    name=\"circle_generator\",\n    hidden_dims=(HIDDEN_DIM, HIDDEN_DIM, HIDDEN_DIM),  # 3-layer MLP\n    output_shape=(1, 2),  # 2D output\n    latent_dim=LATENT_DIM,\n    activation=\"relu\",\n    batch_norm=False,  # No batch norm for WGAN-GP\n    dropout_rate=0.0,\n)\n\n# Discriminator (critic) configuration\ndisc_config = DiscriminatorConfig(\n    name=\"circle_discriminator\",\n    input_shape=(1, 2),  # 2D input\n    hidden_dims=(HIDDEN_DIM, HIDDEN_DIM, HIDDEN_DIM),\n    activation=\"relu\",\n    batch_norm=False,\n    dropout_rate=0.0,\n)\n\n# Create models\ngen_rngs = nnx.Rngs(params=gen_key)\ndisc_rngs = nnx.Rngs(params=disc_key)\n\ngenerator = Generator(config=gen_config, rngs=gen_rngs)\ndiscriminator = Discriminator(config=disc_config, rngs=disc_rngs)\n</code></pre> <p>Artifex Model Features:</p> <ul> <li>Frozen dataclass configurations for type safety</li> <li>Configurable MLP architectures</li> <li>Optional batch normalization and dropout</li> <li>Automatic initialization with RNGs</li> </ul>"},{"location":"examples/basic/simple-gan/#step-5-create-gantrainer-with-wgan-gp","title":"Step 5: Create GANTrainer with WGAN-GP","text":"<p>Use Artifex's <code>GANTrainer</code> with WGAN-GP configuration:</p> <pre><code># Optimizers (Adam with beta1=0.5, beta2=0.9 as per official WGAN-GP)\ngen_optimizer = nnx.Optimizer(\n    generator,\n    optax.adam(LR, b1=0.5, b2=0.9),\n    wrt=nnx.Param,\n)\ndisc_optimizer = nnx.Optimizer(\n    discriminator,\n    optax.adam(LR, b1=0.5, b2=0.9),\n    wrt=nnx.Param,\n)\n\n# GANTrainer configuration with WGAN-GP\ngan_config = GANTrainingConfig(\n    loss_type=\"wasserstein\",  # WGAN loss\n    n_critic=N_CRITIC,\n    gp_weight=GP_WEIGHT,  # Gradient penalty\n    gp_target=1.0,\n    r1_weight=0.0,\n    label_smoothing=0.0,\n)\n\ntrainer = GANTrainer(config=gan_config)\n\n# JIT-compile training steps for performance\njit_d_step = nnx.jit(trainer.discriminator_step)\njit_g_step = nnx.jit(trainer.generator_step)\n</code></pre> <p>GANTrainer Features:</p> <ul> <li>WGAN-GP loss with configurable gradient penalty</li> <li>JIT-compatible training steps</li> <li>Returns metrics for monitoring (d_real, d_fake scores)</li> </ul>"},{"location":"examples/basic/simple-gan/#step-6-training-loop","title":"Step 6: Training Loop","text":"<p>The training loop uses Artifex's <code>GANTrainer</code> methods:</p> <pre><code>pbar = tqdm(range(NUM_STEPS), desc=\"Training\")\nfor step in pbar:\n    train_key, *step_keys = jax.random.split(train_key, 2 + N_CRITIC * 2)\n\n    # Train Discriminator (N_CRITIC steps)\n    for i in range(N_CRITIC):\n        d_data_key, d_z_key, d_gp_key = jax.random.split(step_keys[i], 3)\n        real_data = generate_circle_data(d_data_key, BATCH_SIZE)\n        z = jax.random.normal(d_z_key, (BATCH_SIZE, LATENT_DIM))\n\n        # Use Artifex's JIT-compiled discriminator step\n        d_loss, d_metrics = jit_d_step(\n            generator, discriminator, disc_optimizer, real_data, z, d_gp_key\n        )\n\n    # Train Generator (1 step)\n    z = jax.random.normal(step_keys[-1], (BATCH_SIZE, LATENT_DIM))\n    g_loss, g_metrics = jit_g_step(generator, discriminator, gen_optimizer, z)\n\n    # Monitor Wasserstein distance\n    w_dist = d_metrics.get(\"d_real\", 0.0) - d_metrics.get(\"d_fake\", 0.0)\n    pbar.set_postfix({\"D\": f\"{d_loss:.3f}\", \"G\": f\"{g_loss:.3f}\", \"W\": f\"{w_dist:.3f}\"})\n</code></pre> <p>Training Procedure per Step:</p> <ol> <li>Train discriminator (critic) for <code>N_CRITIC</code> iterations</li> <li>Each critic step: sample real data, sample latent noise, compute loss with gradient penalty</li> <li>Train generator for 1 iteration</li> <li>Monitor Wasserstein distance (D(real) - D(fake))</li> </ol>"},{"location":"examples/basic/simple-gan/#step-7-generate-samples-and-visualize","title":"Step 7: Generate Samples and Visualize","text":"<pre><code># Generate samples\nn_samples = 1000\nfinal_real = generate_circle_data(jax.random.key(5000), n_samples)\nz_final = jax.random.normal(jax.random.key(6000), (n_samples, LATENT_DIM))\nfinal_fake = generator(z_final)\n\n# Evaluate radius statistics\nreal_radius = jnp.sqrt(jnp.sum(final_real**2, axis=1))\nfake_radius = jnp.sqrt(jnp.sum(final_fake**2, axis=1))\n\nprint(f\"Real: mean_radius = {jnp.mean(real_radius):.4f} \u00b1 {jnp.std(real_radius):.4f}\")\nprint(f\"Fake: mean_radius = {jnp.mean(fake_radius):.4f} \u00b1 {jnp.std(fake_radius):.4f}\")\n</code></pre> <p>Interpretation:</p> <ul> <li>Mean radius should be close to 1.0 (the true circle radius)</li> <li>Standard deviation should be small (~0.05, matching the noise)</li> <li>Generated and real statistics should be similar</li> </ul>"},{"location":"examples/basic/simple-gan/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Increase Training Steps: Train for 10,000+ steps for even better convergence</li> </ol> <pre><code>NUM_STEPS = 10000\n</code></pre> <ol> <li>Adjust Gradient Penalty: Try different GP weights</li> </ol> <pre><code>GP_WEIGHT = 1.0   # Higher penalty (more regularization)\nGP_WEIGHT = 0.01  # Lower penalty (less regularization)\n</code></pre> <ol> <li>Change Architecture: Add more layers or neurons</li> </ol> <pre><code>HIDDEN_DIM = 256  # Larger network\n</code></pre> <ol> <li>Different Data Distribution: Try other 2D distributions</li> </ol> <pre><code># Two concentric circles, spiral, etc.\n</code></pre> <ol> <li>Adjust Critic Iterations: Try more or fewer critic steps</li> </ol> <pre><code>N_CRITIC = 10  # More critic training per generator step\n</code></pre>"},{"location":"examples/basic/simple-gan/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/basic/simple-gan/#generator-output-doesnt-match-the-circle","title":"Generator output doesn't match the circle","text":"<ul> <li>Solution: Train for more steps (5,000-10,000)</li> <li>Check: Wasserstein distance should decrease during training</li> <li>Fix: Increase <code>HIDDEN_DIM</code> for more model capacity</li> </ul>"},{"location":"examples/basic/simple-gan/#training-is-unstable","title":"Training is unstable","text":"<ul> <li>Solution: Lower learning rate to <code>5e-5</code></li> <li>Check: Ensure batch normalization is disabled for WGAN-GP</li> <li>Fix: Increase gradient penalty weight to <code>1.0</code></li> </ul>"},{"location":"examples/basic/simple-gan/#mode-collapse-all-points-in-one-location","title":"Mode collapse (all points in one location)","text":"<ul> <li>Solution: This is rare with WGAN-GP, but try increasing <code>N_CRITIC</code></li> <li>Check: Wasserstein distance should not be near 0 immediately</li> <li>Fix: Ensure you're using <code>loss_type=\"wasserstein\"</code>, not vanilla</li> </ul>"},{"location":"examples/basic/simple-gan/#summary","title":"Summary","text":"<p>In this tutorial, you learned:</p> <ul> <li>Artifex GANTrainer: Using the high-level training API instead of manual implementation</li> <li>WGAN-GP: Why Wasserstein loss with gradient penalty provides stable training</li> <li>Configuration: Setting up Generator and Discriminator with config objects</li> <li>JIT Compilation: Using <code>nnx.jit</code> for optimized training performance</li> <li>Monitoring: Tracking Wasserstein distance as a training metric</li> </ul>"},{"location":"examples/basic/simple-gan/#next-steps","title":"Next Steps","text":""},{"location":"examples/basic/simple-gan/#related-examples","title":"Related Examples","text":"<ul> <li> <p> VAE on MNIST</p> <p>Compare GAN with VAE generative approach</p> </li> <li> <p> Diffusion on MNIST</p> <p>Learn about diffusion models for image generation</p> </li> <li> <p> Flow on MNIST</p> <p>Explore normalizing flows for exact likelihood</p> </li> <li> <p> Advanced GAN</p> <p>DCGAN, conditional GAN, and more advanced techniques</p> </li> </ul>"},{"location":"examples/basic/simple-gan/#documentation-resources","title":"Documentation Resources","text":"<ul> <li>GAN Concepts: Deep dive into GAN theory</li> <li>GAN User Guide: Advanced usage patterns</li> <li>GAN API Reference: Complete API documentation</li> </ul>"},{"location":"examples/basic/simple-gan/#papers","title":"Papers","text":"<ol> <li>Generative Adversarial Networks (Goodfellow et al., 2014)</li> <li> <p>Original GAN paper: https://arxiv.org/abs/1406.2661</p> </li> <li> <p>Wasserstein GAN (Arjovsky et al., 2017)</p> </li> <li> <p>WGAN paper: https://arxiv.org/abs/1701.07875</p> </li> <li> <p>Improved Training of Wasserstein GANs (Gulrajani et al., 2017)</p> </li> <li>WGAN-GP paper: https://arxiv.org/abs/1704.00028</li> </ol> <p>Congratulations! You've successfully trained a GAN using Artifex's GANTrainer API!</p>"},{"location":"examples/basic/transformer-text/","title":"Transformer Text Generation","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on transformer-based text generation.</p>"},{"location":"examples/basic/transformer-text/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Transformer architecture for text</li> <li>Autoregressive text generation</li> <li>Tokenization and vocabulary handling</li> <li>Nucleus and top-k sampling</li> </ul>"},{"location":"examples/basic/transformer-text/#planned-features","title":"Planned Features","text":"<ul> <li>GPT-style decoder-only transformer</li> <li>Positional encodings</li> <li>Multi-head self-attention</li> <li>Layer normalization and residual connections</li> </ul>"},{"location":"examples/basic/transformer-text/#related-documentation","title":"Related Documentation","text":"<ul> <li>Text Compression</li> <li>Seq2Seq Models</li> <li>Autoregressive Models</li> </ul>"},{"location":"examples/basic/transformer-text/#references","title":"References","text":"<ul> <li>Vaswani et al., \"Attention Is All You Need\" (2017)</li> <li>Radford et al., \"Language Models are Unsupervised Multitask Learners\" (2019)</li> </ul>"},{"location":"examples/basic/vae-mnist/","title":"VAE MNIST Example - Variational Autoencoder Demonstration","text":"<p>Level: Beginner | Runtime: ~2-3 minutes (CPU/GPU) | Format: Python + Jupyter</p> <p>This example demonstrates how to build a Variational Autoencoder (VAE) on MNIST using Artifex's modular encoder/decoder components. It showcases explicit component creation, proper RNG handling, and VAE inference (no training - this is an architecture demonstration).</p>"},{"location":"examples/basic/vae-mnist/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/image/vae/vae_mnist.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/image/vae/vae_mnist.ipynb</code></li> </ul> <p>Dual-Format Implementation</p> <p>This example is available in two synchronized formats:</p> <ul> <li>Python Script (.py) - For version control, IDE development, and CI/CD integration</li> <li>Jupyter Notebook (.ipynb) - For interactive learning, experimentation, and exploration</li> </ul> <p>Both formats contain identical content and can be used interchangeably. Choose the format that best suits your workflow.</p>"},{"location":"examples/basic/vae-mnist/#quick-start","title":"Quick Start","text":"<pre><code># Activate Artifex environment\nsource activate.sh\n\n# Run the Python script\npython examples/generative_models/image/vae/vae_mnist.py\n\n# Or launch Jupyter notebook\njupyter lab examples/generative_models/image/vae/vae_mnist.ipynb\n</code></pre>"},{"location":"examples/basic/vae-mnist/#overview","title":"Overview","text":"<p>Learning Objectives:</p> <ul> <li> Understand VAE architecture: encoder \u2192 latent space \u2192 decoder</li> <li> Use Artifex's MLPEncoder and MLPDecoder components</li> <li> Handle RNGs properly in Flax NNX with sample streams</li> <li> Understand the reparameterization trick</li> <li> Generate samples from learned latent space</li> <li> Visualize reconstructions and generations</li> </ul> <p>Prerequisites:</p> <ul> <li>Basic understanding of autoencoders and latent representations</li> <li>Familiarity with JAX and Flax NNX basics</li> <li>Understanding of variational inference concepts (ELBO, KL divergence)</li> <li>Artifex installed</li> </ul> <p>Estimated Time: 5 minutes</p>"},{"location":"examples/basic/vae-mnist/#whats-covered","title":"What's Covered","text":"<ul> <li> <p> Modular Components</p> <p>MLPEncoder and MLPDecoder for building VAEs from reusable parts</p> </li> <li> <p> RNG Handling</p> <p>Proper random number generation with separate streams for sampling</p> </li> <li> <p> VAE Architecture</p> <p>Encoder (x \u2192 \u03bc, \u03c3), reparameterization (z), decoder (z \u2192 x\u0302)</p> </li> <li> <p> Visualization</p> <p>Original, reconstructed, and generated samples side-by-side</p> </li> </ul> <p>Expected Results:</p> <ul> <li>Quick demonstration (~2-3 minutes on CPU, ~30 seconds on GPU)</li> <li>Synthetic MNIST-like data (for fast execution without downloads)</li> <li>Visualization showing three rows: original, reconstructed, generated images</li> <li>Understanding of how to assemble VAE from Artifex components</li> </ul>"},{"location":"examples/basic/vae-mnist/#theory-background","title":"Theory Background","text":""},{"location":"examples/basic/vae-mnist/#variational-autoencoder-vae","title":"Variational Autoencoder (VAE)","text":"<p>A VAE is a generative model that learns a probabilistic latent representation:</p> <p>Mathematical Framework:</p> <ul> <li>Encoder: \\(q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi^2(x))\\) - Approximate posterior</li> <li>Decoder: \\(p_\\theta(x|z)\\) - Likelihood of data given latent code</li> <li>Prior: \\(p(z) = \\mathcal{N}(0, I)\\) - Standard normal prior</li> </ul> <p>VAE Loss (ELBO - Evidence Lower Bound):</p> \\[\\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\text{KL}(q_\\phi(z|x) \\| p(z))\\] <p>Where:</p> <ul> <li>Reconstruction term: \\(\\mathbb{E}_{q(z|x)}[\\log p(x|z)] \\approx -\\|x - \\hat{x}\\|^2\\) (MSE)</li> <li>KL term: \\(\\text{KL}(q(z|x) \\| p(z))\\) has closed form for Gaussians</li> </ul>"},{"location":"examples/basic/vae-mnist/#reparameterization-trick","title":"Reparameterization Trick","text":"<p>To enable backpropagation through stochastic sampling:</p> \\[z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\] <p>This separates the stochastic component (\u03b5) from learnable parameters (\u03bc, \u03c3), allowing gradients to flow through the sampling operation.</p> <p>Why Reparameterization?</p> <p>Without this trick, we couldn't backpropagate through random sampling because sampling is not differentiable. By expressing z as a deterministic function of \u03b5, \u03bc, and \u03c3, we can compute gradients with respect to \u03bc and \u03c3.</p>"},{"location":"examples/basic/vae-mnist/#imports","title":"Imports","text":"<p>Import Artifex's modular VAE components:</p> <ul> <li>MLPEncoder: Maps inputs to latent distribution parameters (\u03bc, log \u03c3\u00b2)</li> <li>MLPDecoder: Maps latent codes to reconstructions</li> <li>VAE: Base VAE class that combines encoder + decoder with ELBO loss</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration.network_configs import (\n    DecoderConfig,\n    EncoderConfig,\n)\nfrom artifex.generative_models.core.configuration.vae_config import VAEConfig\nfrom artifex.generative_models.models.vae import VAE\n</code></pre> <p>Config-Based API</p> <p>Artifex uses frozen dataclass configurations for all model components. This provides type-safe, validated configuration with clear parameter structure. See the \"Configuration Pattern\" section below for details.</p>"},{"location":"examples/basic/vae-mnist/#data-loading","title":"Data Loading","text":"<p>For this example, we create synthetic MNIST-like data. In production, you would use real MNIST from <code>tensorflow_datasets</code> or <code>torchvision</code>.</p> <p>Data Format:</p> <ul> <li>Images: 28\u00d728\u00d71 (grayscale)</li> <li>Values: [0, 1] range (normalized)</li> <li>Shape: (batch_size, height, width, channels)</li> </ul> <pre><code>def load_mnist_data():\n    \"\"\"Load MNIST dataset.\n\n    In this example, we use synthetic data for quick demonstration.\n    Replace this with real MNIST loading for production use.\n\n    Returns:\n        Tuple of (train_images, test_images)\n\n    Note:\n        Real MNIST loading would look like:\n        ```python\n        import tensorflow_datasets as tfds\n        ds = tfds.load('mnist', split='train', as_supervised=True)\n        images = ds.map(lambda x, y: x / 255.0)  # Normalize to [0, 1]\n        ```\n    \"\"\"\n    # Create synthetic MNIST-like data with proper dimensions\n    key = jax.random.key(42)\n    train_key, test_key = jax.random.split(key)\n\n    # Create synthetic data: 28\u00d728\u00d71 images in [0, 1] range\n    train_images = jax.random.uniform(train_key, (1000, 28, 28, 1))\n    test_images = jax.random.uniform(test_key, (100, 28, 28, 1))\n\n    return train_images, test_images\n</code></pre>"},{"location":"examples/basic/vae-mnist/#visualization-function","title":"Visualization Function","text":"<p>The visualization function shows three rows to assess VAE quality:</p> <ol> <li>Original: Input images from the dataset</li> <li>Reconstructed: VAE reconstructions (tests encoder + decoder quality)</li> <li>Generated: Samples from random latent codes (tests learned prior)</li> </ol> <pre><code>def visualize_vae_results(original, reconstructed, generated, num_samples=5):\n    \"\"\"Visualize VAE results side-by-side.\n\n    Args:\n        original: Original images [batch, height, width, channels]\n        reconstructed: Reconstructed images (same shape as original)\n        generated: Generated images from random latent codes\n        num_samples: Number of samples to display (default: 5)\n\n    Returns:\n        matplotlib.figure.Figure: The created figure\n\n    Note:\n        All images should be in [0, 1] range for proper visualization.\n        Images are clipped to [0, 1] before display to handle any overshooting.\n    \"\"\"\n    fig, axes = plt.subplots(3, num_samples, figsize=(12, 7))\n\n    for i in range(num_samples):\n        # Row 1: Original images\n        axes[0, i].imshow(jnp.clip(original[i, :, :, 0], 0, 1), cmap=\"gray\", vmin=0, vmax=1)\n        axes[0, i].axis(\"off\")\n        if i == 0:\n            axes[0, i].set_ylabel(\"Original\", fontsize=12, fontweight=\"bold\")\n\n        # Row 2: Reconstructed images\n        axes[1, i].imshow(jnp.clip(reconstructed[i, :, :, 0], 0, 1), cmap=\"gray\", vmin=0, vmax=1)\n        axes[1, i].axis(\"off\")\n        if i == 0:\n            axes[1, i].set_ylabel(\"Reconstructed\", fontsize=12, fontweight=\"bold\")\n\n        # Row 3: Generated images (from random latent codes)\n        axes[2, i].imshow(jnp.clip(generated[i, :, :, 0], 0, 1), cmap=\"gray\", vmin=0, vmax=1)\n        axes[2, i].axis(\"off\")\n        if i == 0:\n            axes[2, i].set_ylabel(\"Generated\", fontsize=12, fontweight=\"bold\")\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"examples/basic/vae-mnist/#main-pipeline","title":"Main Pipeline","text":"<p>The main function demonstrates the VAE workflow using Artifex's modular components:</p> <ol> <li>Setup: Initialize RNG for reproducibility</li> <li>Data: Load MNIST (synthetic in this demo)</li> <li>Encoder: Create MLPEncoder to map x \u2192 (\u03bc, log \u03c3\u00b2)</li> <li>Decoder: Create MLPDecoder to map z \u2192 x\u0302</li> <li>VAE: Combine encoder + decoder into full VAE</li> <li>Forward Pass: Run reconstruction</li> <li>Generation: Sample from prior</li> <li>Visualization: Display results</li> </ol> <p>Why Explicit Component Creation?</p> <p>We explicitly create encoder and decoder to demonstrate:</p> <ul> <li>How to configure Artifex's components</li> <li>The modular design pattern (reusable, swappable parts)</li> <li>How components connect in the VAE</li> <li>Easy customization (swap MLP \u2192 CNN, adjust layers, etc.)</li> </ul> <p>This approach gives you full control and understanding. For production workflows where you want consistency across model types, the factory pattern (see below) is also available.</p> <pre><code>def main():\n    \"\"\"Run the VAE MNIST example.\n\n    This function demonstrates the complete VAE pipeline using Artifex's\n    modular encoder/decoder components.\n    \"\"\"\n    print(\"=\" * 80)\n    print(\"VAE MNIST Example - Using Artifex's MLPEncoder &amp; MLPDecoder\")\n    print(\"=\" * 80)\n</code></pre>"},{"location":"examples/basic/vae-mnist/#step-1-setup-rng","title":"Step 1: Setup RNG","text":"<p>In Flax NNX, we use <code>nnx.Rngs</code> to manage random number generation. We need separate streams for:</p> <ul> <li>params: Parameter initialization</li> <li>dropout: Dropout layers (if used)</li> <li>sample: Stochastic sampling in VAE (reparameterization trick)</li> </ul> <pre><code>    # Step 1: Set random seed for reproducibility\n    seed = 42\n    key = jax.random.key(seed)\n    params_key, dropout_key, sample_key = jax.random.split(key, 3)\n\n    # Create RNG streams for different purposes\n    rngs = nnx.Rngs(params=params_key, dropout=dropout_key, sample=sample_key)\n</code></pre> <p>RNG Best Practices</p> <p>Always split your RNG into separate streams for different purposes. This ensures:</p> <ul> <li>Reproducibility across runs</li> <li>Proper handling of stochastic operations</li> <li>Thread-safe random state management</li> <li>No interference between parameter init and sampling</li> </ul>"},{"location":"examples/basic/vae-mnist/#step-2-load-data","title":"Step 2: Load Data","text":"<p>MNIST consists of 28\u00d728 grayscale images of handwritten digits (0-9).</p> <ul> <li>Training set: 60,000 images (we use 1,000 synthetic for demo)</li> <li>Test set: 10,000 images (we use 100 synthetic for demo)</li> </ul> <p>Images are normalized to [0, 1] range for stable training.</p> <pre><code>    # Step 2: Load data\n    print(\"\\n\ud83d\udcca Loading MNIST data...\")\n    train_images, test_images = load_mnist_data()\n    print(f\"  Train data shape: {train_images.shape}\")  # (1000, 28, 28, 1)\n    print(f\"  Test data shape: {test_images.shape}\")  # (100, 28, 28, 1)\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udcca Loading MNIST data...\n  Train data shape: (1000, 28, 28, 1)\n  Test data shape: (100, 28, 28, 1)\n</code></pre>"},{"location":"examples/basic/vae-mnist/#step-3-create-encoder","title":"Step 3: Create Encoder","text":"<p>Artifex's <code>MLPEncoder</code> maps inputs to latent distribution parameters:</p> <ul> <li>Input: x (28\u00d728\u00d71 = 784 features after flattening)</li> <li>Output: (mean, log_var) for latent distribution q(z|x)</li> </ul> <p>Parameters:</p> <ul> <li><code>hidden_dims=[256, 128]</code>: Two hidden layers with decreasing dimensions</li> <li><code>latent_dim=32</code>: Dimension of latent space z</li> <li><code>activation=\"relu\"</code>: ReLU activation between layers</li> <li><code>input_dim=(28, 28, 1)</code>: Shape of input images (auto-flattened to 784)</li> </ul> <pre><code>    # Step 3: Create encoder config using Artifex's EncoderConfig\n    print(\"\\n\ud83d\udd27 Creating VAE components using Artifex APIs...\")\n\n    latent_dim = 32\n\n    # Create encoder configuration (frozen dataclass)\n    encoder_config = EncoderConfig(\n        name=\"vae_encoder\",\n        hidden_dims=(256, 128),  # Encoder architecture (tuple for frozen dataclass)\n        latent_dim=latent_dim,  # Latent space dimension\n        activation=\"relu\",  # Activation function\n        input_shape=(28, 28, 1),  # Input image shape\n    )\n    print(f\"  \u2705 Encoder config: hidden_dims={encoder_config.hidden_dims}, latent_dim={latent_dim}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd27 Creating VAE components using Artifex APIs...\n  \u2705 Encoder config: hidden_dims=(256, 128), latent_dim=32\n</code></pre>"},{"location":"examples/basic/vae-mnist/#step-4-create-decoder","title":"Step 4: Create Decoder","text":"<p>Artifex's <code>MLPDecoder</code> maps latent codes to reconstructions:</p> <ul> <li>Input: z (32-dimensional latent vector)</li> <li>Output: x\u0302 (28\u00d728\u00d71 reconstructed image)</li> </ul> <p>Parameters:</p> <ul> <li><code>hidden_dims=[128, 256]</code>: Reversed encoder dims (symmetric architecture)</li> <li><code>output_dim=(28, 28, 1)</code>: Shape of reconstructed images</li> <li><code>latent_dim=32</code>: Dimension of latent space (must match encoder)</li> <li><code>activation=\"relu\"</code>: ReLU activation (except final layer uses sigmoid)</li> </ul> <p>Automatic Output Activation</p> <p>The decoder automatically applies sigmoid activation to the output to ensure pixel values are in [0, 1] range. This matches the input image range.</p> <pre><code>    # Step 4: Create decoder config using Artifex's DecoderConfig\n    decoder_config = DecoderConfig(\n        name=\"vae_decoder\",\n        hidden_dims=(128, 256),  # Decoder architecture (reversed, tuple)\n        output_shape=(28, 28, 1),  # Output image shape\n        latent_dim=latent_dim,  # Latent space dimension\n        activation=\"relu\",  # Activation function\n    )\n    print(f\"  \u2705 Decoder config: hidden_dims={decoder_config.hidden_dims}, output_shape={decoder_config.output_shape}\")\n</code></pre> <p>Output:</p> <pre><code>  \u2705 Decoder config: hidden_dims=(128, 256), output_shape=(28, 28, 1)\n</code></pre>"},{"location":"examples/basic/vae-mnist/#step-5-create-vae-model","title":"Step 5: Create VAE Model","text":"<p>Artifex's <code>VAE</code> class combines encoder + decoder with:</p> <ul> <li>Forward pass: x \u2192 encoder \u2192 (\u03bc, log \u03c3\u00b2) \u2192 sample z \u2192 decoder \u2192 x\u0302</li> <li>ELBO loss: Reconstruction loss + KL divergence</li> <li>Sampling methods: Generate from prior p(z) = N(0, I)</li> </ul> <p>Parameters:</p> <ul> <li><code>encoder</code>: The MLPEncoder we created above</li> <li><code>decoder</code>: The MLPDecoder we created above</li> <li><code>latent_dim=32</code>: Must match encoder/decoder latent dimensions</li> <li><code>kl_weight=1.0</code>: Weight for KL term (\u03b2-VAE uses \u03b2\u22601 for disentanglement)</li> </ul> <pre><code>    # Step 5: Create VAE model with config\n    vae_config = VAEConfig(\n        name=\"vae_mnist\",\n        encoder=encoder_config,\n        decoder=decoder_config,\n        encoder_type=\"dense\",  # MLP encoder\n        kl_weight=1.0,  # Standard VAE (\u03b2=1), increase for \u03b2-VAE\n        reconstruction_loss_type=\"mse\",\n    )\n\n    model = VAE(config=vae_config, rngs=rngs)\n    print(f\"  \u2705 VAE model created: latent_dim={vae_config.encoder.latent_dim}, kl_weight={vae_config.kl_weight}\")\n</code></pre> <p>Output:</p> <pre><code>  \u2705 VAE model created: latent_dim=32, kl_weight=1.0\n</code></pre>"},{"location":"examples/basic/vae-mnist/#step-6-forward-pass-reconstruction","title":"Step 6: Forward Pass (Reconstruction)","text":"<p>The forward pass demonstrates the full VAE pipeline:</p> <ol> <li>Encoding: x \u2192 encoder \u2192 (\u03bc, log \u03c3\u00b2)</li> <li>Reparameterization: z = \u03bc + \u03c3 \u2299 \u03b5, where \u03b5 ~ N(0, I)</li> <li>Decoding: z \u2192 decoder \u2192 x\u0302 (reconstruction)</li> </ol> <p>Output Dictionary:</p> <ul> <li><code>reconstructed</code> or <code>reconstruction</code>: Reconstructed images x\u0302</li> <li><code>mean</code>: Latent distribution mean \u03bc</li> <li><code>log_var</code> or <code>logvar</code>: Latent distribution log variance log \u03c3\u00b2</li> <li><code>z</code>: Sampled latent codes (used for reconstruction)</li> </ul> <p>RNG for Sampling</p> <p>We pass <code>rngs</code> with a <code>sample</code> stream for the reparameterization trick's random sampling. Without this, the VAE would use deterministic (mean) latent codes.</p> <pre><code>    # Step 6: Test the model with a batch\n    print(\"\\n\ud83e\uddea Testing model forward pass...\")\n    test_batch = train_images[:8]  # Use 8 images for testing\n\n    # Forward pass - the VAE uses its internal rngs for reparameterization\n    # The 'sample' RNG stream is used internally for the reparameterization trick\n    outputs = model(test_batch)\n\n    # Extract reconstructions (check both possible keys)\n    reconstructed = outputs.get(\"reconstructed\")\n    if reconstructed is None:\n        reconstructed = outputs[\"reconstruction\"]\n    print(f\"  \u2705 Reconstruction shape: {reconstructed.shape}\")\n\n    # Extract latent codes\n    latent = outputs.get(\"z\")\n    if latent is None:\n        latent = outputs[\"latent\"]\n    print(f\"  \u2705 Latent shape: {latent.shape}\")\n\n    # Show latent statistics to verify reasonable values\n    print(\"  \ud83d\udcca Latent statistics:\")\n    print(f\"     Mean: {jnp.mean(latent):.4f} (should be near 0)\")\n    print(f\"     Std: {jnp.std(latent):.4f} (should be near 1 for standard normal)\")\n</code></pre> <p>Output:</p> <pre><code>\ud83e\uddea Testing model forward pass...\n  \u2705 Reconstruction shape: (8, 28, 28, 1)\n  \u2705 Latent shape: (8, 32)\n  \ud83d\udcca Latent statistics:\n     Mean: 0.0315 (should be near 0)\n     Std: 1.0909 (should be near 1 for standard normal)\n</code></pre> <p>Interpreting Latent Statistics</p> <p>The latent codes should have:</p> <ul> <li>Mean near 0: The KL term pushes the posterior toward the prior N(0, I)</li> <li>Std near 1: Standard deviation close to 1 indicates good regularization</li> </ul> <p>If mean or std deviate significantly, consider adjusting <code>kl_weight</code> or using KL annealing during training.</p>"},{"location":"examples/basic/vae-mnist/#step-7-generation-from-prior","title":"Step 7: Generation from Prior","text":"<p>To generate new samples:</p> <ol> <li>Sample z ~ N(0, I) from the standard normal prior</li> <li>Decode: x_new = decoder(z)</li> </ol> <p>This tests whether the VAE has learned a meaningful latent space.</p> <p>Quality Indicators:</p> <ul> <li>Diversity: Generated samples should vary (not all identical)</li> <li>Realism: Samples should resemble training data distribution</li> <li>Smoothness: Similar z should produce similar x (interpolation works)</li> </ul> <p>Synthetic Data Limitation</p> <p>With synthetic random data, generations won't be realistic digits, but the shapes should match the training distribution. With real MNIST, you'd see clear digit reconstructions and realistic generated digits.</p> <pre><code>    # Step 7: Generate new samples from the prior\n    print(\"\\n\ud83c\udfa8 Generating new samples from prior...\")\n    n_samples = 5\n    generated = model.generate(n_samples=n_samples)  # VAE uses internal rngs\n    print(f\"  \u2705 Generated shape: {generated.shape}\")\n    print(f\"  \ud83d\udcca Generated pixels range: [{jnp.min(generated):.3f}, {jnp.max(generated):.3f}]\")\n</code></pre> <p>Output:</p> <pre><code>\ud83c\udfa8 Generating new samples from prior...\n  \u2705 Generated shape: (5, 28, 28, 1)\n  \ud83d\udcca Generated pixels range: [0.059, 0.943]\n</code></pre>"},{"location":"examples/basic/vae-mnist/#step-8-visualization","title":"Step 8: Visualization","text":"<p>The visualization shows:</p> <ul> <li>Top row: Original input images</li> <li>Middle row: Reconstructions (tests encoder + decoder quality)</li> <li>Bottom row: Generated samples (tests learned prior)</li> </ul> <p>What to look for:</p> <ul> <li>Reconstructions should closely match originals (good reconstruction loss)</li> <li>Generated samples should look plausible (good latent space)</li> <li>Diversity in generated samples indicates good latent space coverage</li> </ul> <pre><code>    # Step 8: Visualize results\n    print(\"\\n\ud83d\udcca Visualizing results...\")\n    fig = visualize_vae_results(\n        original=test_batch[:n_samples],\n        reconstructed=reconstructed[:n_samples],\n        generated=generated[:n_samples],\n    )\n\n    # Step 9: Save figure\n    import os\n\n    output_dir = \"examples_output\"\n    os.makedirs(output_dir, exist_ok=True)\n    output_path = os.path.join(output_dir, \"vae_mnist_results.png\")\n    fig.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n    print(f\"  \u2705 Results saved to {output_path}\")\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udcca Visualizing results...\n  \u2705 Results saved to examples_output/vae_mnist_results.png\n</code></pre>"},{"location":"examples/basic/vae-mnist/#summary","title":"Summary","text":"<p>In this example, you learned:</p> <ul> <li>\u2705 VAE architecture: encoder \u2192 latent space \u2192 decoder</li> <li>\u2705 Reparameterization trick: enables backpropagation through sampling</li> <li>\u2705 Artifex's MLPEncoder and MLPDecoder: modular, reusable components</li> <li>\u2705 Proper RNG handling: use rngs with 'sample' stream for stochastic operations</li> <li>\u2705 VAE base class: handles ELBO loss computation automatically</li> </ul> <p>Key Insights:</p> <ul> <li>VAEs trade reconstruction quality for smooth, structured latent spaces</li> <li>The latent dimension (32) controls representation capacity</li> <li>KL weight controls reconstruction vs. regularization tradeoff</li> <li>Modular design allows easy swapping (MLP \u2192 CNN, different layers, etc.)</li> </ul> <p>Artifex APIs Used:</p> <ul> <li><code>MLPEncoder</code>: Maps inputs \u2192 (\u03bc, log \u03c3\u00b2)</li> <li><code>MLPDecoder</code>: Maps latent codes \u2192 reconstructions</li> <li><code>VAE</code>: Combines encoder/decoder with ELBO loss</li> </ul>"},{"location":"examples/basic/vae-mnist/#configuration-pattern","title":"Configuration Pattern","text":"<p>Artifex uses frozen dataclass configurations for type-safe, validated model setup. The nested config pattern provides clear structure:</p> <pre><code>from artifex.generative_models.core.configuration.network_configs import (\n    DecoderConfig,\n    EncoderConfig,\n)\nfrom artifex.generative_models.core.configuration.vae_config import VAEConfig\nfrom artifex.generative_models.models.vae import VAE\n\n# Create encoder configuration\nencoder_config = EncoderConfig(\n    name=\"vae_encoder\",\n    hidden_dims=(256, 128),  # Must be tuple (frozen dataclass)\n    latent_dim=32,\n    activation=\"relu\",\n    input_shape=(28, 28, 1),\n)\n\n# Create decoder configuration\ndecoder_config = DecoderConfig(\n    name=\"vae_decoder\",\n    hidden_dims=(128, 256),\n    output_shape=(28, 28, 1),\n    latent_dim=32,\n    activation=\"relu\",\n)\n\n# Create VAE configuration with nested configs\nvae_config = VAEConfig(\n    name=\"vae_mnist\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    kl_weight=1.0,\n    reconstruction_loss_type=\"mse\",\n)\n\n# Create model with config\nmodel = VAE(config=vae_config, rngs=rngs)\n</code></pre> <p>Benefits of config-based API:</p> Feature Benefit \u2705 Type validation Catches errors at config creation time \u2705 Frozen dataclasses Immutable, hashable configurations \u2705 Nested structure Clear component relationships \u2705 Serializable Easy to save/load configurations <p>This is the recommended approach for all Artifex models.</p>"},{"location":"examples/basic/vae-mnist/#experiments-to-try","title":"Experiments to Try","text":""},{"location":"examples/basic/vae-mnist/#1-cnn-architecture","title":"1. CNN Architecture","text":"<p>CNNs often work better for image data than MLPs:</p> <pre><code># Use CNN encoder type in config\nencoder_config = EncoderConfig(\n    name=\"cnn_encoder\",\n    hidden_dims=(32, 64, 128),  # Channel progression\n    latent_dim=32,\n    activation=\"relu\",\n    input_shape=(28, 28, 1),\n)\n\ndecoder_config = DecoderConfig(\n    name=\"cnn_decoder\",\n    hidden_dims=(128, 64, 32),\n    output_shape=(28, 28, 1),\n    latent_dim=32,\n    activation=\"relu\",\n)\n\nvae_config = VAEConfig(\n    name=\"cnn_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"cnn\",  # Use CNN encoder\n    kl_weight=1.0,\n)\n\nmodel = VAE(config=vae_config, rngs=rngs)\n</code></pre>"},{"location":"examples/basic/vae-mnist/#2-latent-dimension-experiments","title":"2. Latent Dimension Experiments","text":"<ul> <li>Try <code>latent_dim=16</code>: Smaller capacity, faster training, may lose details</li> <li>Try <code>latent_dim=64</code>: Larger capacity, better reconstructions</li> <li>Try <code>latent_dim=128</code>: Very high capacity, risk of overfitting</li> </ul> <p>Trade-off: Larger latent dims \u2192 better reconstruction but less structured space</p>"},{"location":"examples/basic/vae-mnist/#3-vae-for-disentanglement","title":"3. \u03b2-VAE for Disentanglement","text":"<pre><code>from artifex.generative_models.core.configuration.vae_config import BetaVAEConfig\nfrom artifex.generative_models.models.vae import BetaVAE\n\nbeta_config = BetaVAEConfig(\n    name=\"beta_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    beta_default=4.0,  # \u03b2=4 encourages disentangled representations\n    beta_warmup_steps=1000,  # Gradual warmup\n)\n\nmodel = BetaVAE(config=beta_config, rngs=rngs)\n</code></pre> <ul> <li>Higher \u03b2 (4.0-10.0): More regularization, worse reconstruction, better disentanglement</li> <li>Lower \u03b2 (0.1-0.5): Better reconstruction, less structured latent space</li> <li>\u03b2=1: Standard VAE</li> </ul>"},{"location":"examples/basic/vae-mnist/#4-architecture-variations","title":"4. Architecture Variations","text":"<pre><code># Deeper network with different activation\nencoder_config = EncoderConfig(\n    name=\"deep_encoder\",\n    hidden_dims=(512, 256, 128),  # More layers\n    latent_dim=32,\n    activation=\"gelu\",  # GELU often works better than ReLU\n    input_shape=(28, 28, 1),\n)\n</code></pre> <ul> <li>More layers: Higher capacity but slower training</li> <li>Different activations: GELU often works better than ReLU</li> <li>Batch normalization: Can help with deeper networks</li> </ul>"},{"location":"examples/basic/vae-mnist/#5-real-mnist-data","title":"5. Real MNIST Data","text":"<pre><code>import tensorflow_datasets as tfds\n\n# Load real MNIST\nds = tfds.load('mnist', split='train', as_supervised=True)\n\ndef preprocess(image, label):\n    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n    return image\n\nds = ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)\n</code></pre> <p>Real MNIST will give much better results and realistic digit generation.</p>"},{"location":"examples/basic/vae-mnist/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/basic/vae-mnist/#encoderdecoder-creation-fails","title":"Encoder/decoder creation fails","text":"<p>Solution: Check that <code>input_dim</code> matches your data shape</p> <p>Common mistake: Forgetting to pass <code>rngs</code> parameter</p> <p>Fix: All Artifex modules require <code>rngs</code> for initialization</p>"},{"location":"examples/basic/vae-mnist/#reconstructions-are-very-blurry","title":"Reconstructions are very blurry","text":"<p>Solution: This is expected with MSE loss on images</p> <p>Explanation: MSE averages over pixel space, causing blur</p> <p>Alternatives:</p> <ul> <li>Use CNNEncoder/CNNDecoder instead of MLP</li> <li>Try perceptual loss or adversarial training</li> <li>Use VQVAE for sharper reconstructions</li> </ul>"},{"location":"examples/basic/vae-mnist/#generated-samples-look-like-noise","title":"Generated samples look like noise","text":"<p>Solution: VAE needs training; this example only demonstrates architecture</p> <p>Note: With synthetic random data, generations won't be meaningful</p> <p>Fix: Train on real MNIST with a proper training loop (see training examples)</p>"},{"location":"examples/basic/vae-mnist/#kl-collapse-all-latent-codes-become-identical","title":"KL collapse (all latent codes become identical)","text":"<p>Solution: Reduce <code>kl_weight</code> to allow more latent variance</p> <p>Monitoring: Check <code>jnp.std(latent)</code> - should be &gt; 0.5</p> <p>Fix: Use KL annealing schedule (start with kl_weight=0.1, increase gradually)</p>"},{"location":"examples/basic/vae-mnist/#model-output-shape-mismatch","title":"Model output shape mismatch","text":"<p>Solution: Ensure encoder <code>latent_dim</code> matches decoder <code>latent_dim</code></p> <p>Check: Verify <code>output_dim</code> matches input shape for reconstruction</p>"},{"location":"examples/basic/vae-mnist/#next-steps","title":"Next Steps","text":"<p>After understanding this basic VAE demonstration, explore:</p>"},{"location":"examples/basic/vae-mnist/#related-examples","title":"Related Examples","text":"<ul> <li> <p> Training VAE</p> <p>See training examples for full training loops with optimizers and loss monitoring</p> </li> <li> <p> Advanced VAEs</p> <p>\u03b2-VAE, Conditional VAE, VQ-VAE, Hierarchical VAE with disentanglement</p> </li> <li> <p> Disentanglement</p> <p>Multi-\u03b2-VAE benchmark with MIG score evaluation and metrics</p> </li> <li> <p> VAE vs GAN</p> <p>Compare <code>simple_gan.py</code> to understand trade-offs between approaches</p> </li> </ul>"},{"location":"examples/basic/vae-mnist/#documentation-resources","title":"Documentation Resources","text":"<ul> <li>VAE Concepts: Deep dive into VAE theory</li> <li>VAE User Guide: Advanced usage patterns</li> <li>VAE API Reference: Complete API documentation</li> <li>Training Guide: How to train VAEs from scratch</li> </ul>"},{"location":"examples/basic/vae-mnist/#research-papers","title":"Research Papers","text":"<ol> <li> <p>Auto-Encoding Variational Bayes (Kingma &amp; Welling, 2014)    Original VAE paper: https://arxiv.org/abs/1312.6114</p> </li> <li> <p>\u03b2-VAE: Learning Basic Visual Concepts (Higgins et al., 2017)    Disentanglement via \u03b2 parameter: https://openreview.net/forum?id=Sy2fzU9gl</p> </li> <li> <p>Understanding disentangling in \u03b2-VAE (Burgess et al., 2018)    Analysis of \u03b2-VAE disentanglement: https://arxiv.org/abs/1804.03599</p> </li> </ol> <p>Congratulations! You've learned how to build VAEs with Artifex's modular components! \ud83c\udf89</p>"},{"location":"examples/diffusion/advanced-diffusion/","title":"Advanced Diffusion Techniques","text":"<p>Coming Soon</p> <p>This example is planned for a future release. Check back for updates on advanced diffusion techniques.</p>"},{"location":"examples/diffusion/advanced-diffusion/#overview","title":"Overview","text":"<p>This example will demonstrate:</p> <ul> <li>Classifier-free guidance</li> <li>Latent diffusion models</li> <li>Accelerated sampling (DDIM, DPM-Solver)</li> <li>Conditional diffusion with various conditions</li> </ul>"},{"location":"examples/diffusion/advanced-diffusion/#planned-features","title":"Planned Features","text":"<ul> <li>Text-conditioned diffusion</li> <li>Image inpainting</li> <li>Super-resolution</li> <li>ControlNet-style conditioning</li> </ul>"},{"location":"examples/diffusion/advanced-diffusion/#related-documentation","title":"Related Documentation","text":"<ul> <li>Simple Diffusion</li> <li>Diffusion API</li> <li>Diffusion Concepts</li> </ul>"},{"location":"examples/diffusion/advanced-diffusion/#references","title":"References","text":"<ul> <li>Ho &amp; Salimans, \"Classifier-Free Diffusion Guidance\" (2022)</li> <li>Rombach et al., \"High-Resolution Image Synthesis with Latent Diffusion Models\" (2022)</li> <li>Song et al., \"Denoising Diffusion Implicit Models\" (2021)</li> </ul>"},{"location":"examples/diffusion/dit-demo/","title":"Diffusion Transformer (DiT) Demo","text":"<p>Level: Advanced | Runtime: ~5 minutes (CPU) / ~1-2 minutes (GPU) | Format: Python + Jupyter</p>"},{"location":"examples/diffusion/dit-demo/#overview","title":"Overview","text":"<p>This advanced example demonstrates Diffusion Transformers (DiT), which combines the power of Vision Transformers with diffusion models. DiT represents a significant advancement in diffusion model architectures, using transformer blocks instead of traditional U-Net architectures for the denoising process.</p>"},{"location":"examples/diffusion/dit-demo/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>DiffusionTransformer backbone architecture</li> <li>DiT model sizes and scaling (S, B, L, XL configurations)</li> <li>Conditional generation with classifier-free guidance (CFG)</li> <li>Patch-based image processing</li> <li>Performance benchmarking across model sizes</li> <li>Advanced sampling techniques</li> </ul>"},{"location":"examples/diffusion/dit-demo/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/diffusion/dit_demo.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/diffusion/dit_demo.ipynb</code></li> </ul>"},{"location":"examples/diffusion/dit-demo/#quick-start","title":"Quick Start","text":""},{"location":"examples/diffusion/dit-demo/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/diffusion/dit_demo.py\n</code></pre>"},{"location":"examples/diffusion/dit-demo/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/diffusion/dit_demo.ipynb\n</code></pre>"},{"location":"examples/diffusion/dit-demo/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/diffusion/dit-demo/#diffusion-transformer-architecture","title":"Diffusion Transformer Architecture","text":"<p>DiT replaces the traditional U-Net backbone with a Vision Transformer:</p> <ul> <li>Patch Embedding: Images are divided into patches and linearly embedded</li> <li>Positional Encoding: Added to maintain spatial information</li> <li>Transformer Blocks: Self-attention and feed-forward layers</li> <li>Adaptive Layer Normalization: Conditioned on timestep and class labels</li> </ul>"},{"location":"examples/diffusion/dit-demo/#classifier-free-guidance-cfg","title":"Classifier-Free Guidance (CFG)","text":"<p>CFG enables stronger conditional generation by learning both conditional and unconditional models simultaneously:</p> \\[\\tilde{\\epsilon}_\\theta(x_t, c) = \\epsilon_\\theta(x_t, \\emptyset) + s \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\emptyset))\\] <p>Where:</p> <ul> <li>\\(c\\) is the conditioning (e.g., class label)</li> <li>\\(\\emptyset\\) is the unconditional case</li> <li>\\(s\\) is the guidance scale (higher = stronger conditioning)</li> </ul>"},{"location":"examples/diffusion/dit-demo/#model-scaling","title":"Model Scaling","text":"<p>DiT comes in different sizes, trading off quality and speed:</p> Model Hidden Dim Depth Heads Parameters DiT-S 384 12 6 ~33M DiT-B 768 12 12 ~130M DiT-L 1024 24 16 ~458M DiT-XL 1152 28 16 ~675M"},{"location":"examples/diffusion/dit-demo/#patch-based-processing","title":"Patch-Based Processing","text":"<p>Images are processed as sequences of patches:</p> <ol> <li>Divide image into non-overlapping patches (e.g., 16\u00d716)</li> <li>Flatten each patch into a vector</li> <li>Apply linear projection</li> <li>Add positional embeddings</li> <li>Process through transformer blocks</li> </ol>"},{"location":"examples/diffusion/dit-demo/#code-structure","title":"Code Structure","text":"<p>The example demonstrates 7 major sections:</p> <ol> <li>Import Dependencies: Setting up the environment</li> <li>Test DiT Components: Verifying backbone and full model</li> <li>Test Different Model Sizes: Comparing S, B, L configurations</li> <li>Conditional Generation: Using classifier-free guidance</li> <li>Visualization: Displaying generated samples</li> <li>Performance Benchmark: Measuring throughput across sizes</li> <li>Summary: Key takeaways and next steps</li> </ol>"},{"location":"examples/diffusion/dit-demo/#example-code","title":"Example Code","text":""},{"location":"examples/diffusion/dit-demo/#creating-dit-model","title":"Creating DiT Model","text":"<pre><code>import jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.core.configuration import DiTConfig, NoiseScheduleConfig\nfrom artifex.generative_models.models.diffusion.dit import DiTModel\n\n# Initialize RNG\nrngs = nnx.Rngs(42)\n\n# Create noise schedule config\nnoise_schedule_config = NoiseScheduleConfig(\n    name=\"dit_schedule\",\n    schedule_type=\"linear\",\n    num_timesteps=1000,\n    beta_start=1e-4,\n    beta_end=0.02,\n)\n\n# Create DiT-S model config\nconfig = DiTConfig(\n    name=\"dit_s\",\n    noise_schedule=noise_schedule_config,\n    input_shape=(3, 32, 32),  # C, H, W format\n    patch_size=4,\n    hidden_size=384,  # DiT-S\n    depth=12,\n    num_heads=6,\n    num_classes=10,\n)\n\n# Create model\ndit_model = DiTModel(config, rngs=rngs)\n\n# Test forward pass\nbatch_size = 4\nimages = jnp.ones((batch_size, 32, 32, 3))\ntimesteps = jnp.array([100, 200, 300, 400])\nlabels = jnp.array([0, 1, 2, 3])\n\n# Predict noise\nnoise_pred = dit_model(images, timesteps, labels, deterministic=True)\nprint(f\"Noise prediction shape: {noise_pred.shape}\")  # (4, 32, 32, 3)\n</code></pre>"},{"location":"examples/diffusion/dit-demo/#testing-different-model-sizes","title":"Testing Different Model Sizes","text":"<pre><code># Create shared noise schedule config\nnoise_schedule = NoiseScheduleConfig(\n    name=\"schedule\",\n    schedule_type=\"linear\",\n    num_timesteps=1000,\n    beta_start=1e-4,\n    beta_end=0.02,\n)\n\n# DiT-S (Small) - Fast, good for prototyping\nconfig_s = DiTConfig(\n    name=\"dit_s\",\n    noise_schedule=noise_schedule,\n    input_shape=(3, 32, 32),\n    patch_size=4,\n    hidden_size=384,\n    depth=12,\n    num_heads=6,\n)\ndit_s = DiTModel(config_s, rngs=rngs)\n\n# DiT-B (Base) - Balanced quality/speed\nconfig_b = DiTConfig(\n    name=\"dit_b\",\n    noise_schedule=noise_schedule,\n    input_shape=(3, 32, 32),\n    patch_size=4,\n    hidden_size=768,\n    depth=12,\n    num_heads=12,\n)\ndit_b = DiTModel(config_b, rngs=rngs)\n\n# DiT-L (Large) - High quality, slower\nconfig_l = DiTConfig(\n    name=\"dit_l\",\n    noise_schedule=noise_schedule,\n    input_shape=(3, 32, 32),\n    patch_size=4,\n    hidden_size=1024,\n    depth=24,\n    num_heads=16,\n)\ndit_l = DiTModel(config_l, rngs=rngs)\n</code></pre>"},{"location":"examples/diffusion/dit-demo/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<pre><code># Create conditional model with CFG support\nconfig = DiTConfig(\n    name=\"conditional_dit\",\n    noise_schedule=noise_schedule_config,\n    input_shape=(3, 16, 16),  # Smaller for faster demo\n    patch_size=2,\n    hidden_size=384,  # DiT-S for speed\n    depth=12,\n    num_heads=6,\n    num_classes=10,  # 10 class labels (like CIFAR-10)\n    cfg_scale=3.0,   # Guidance scale\n)\n\nmodel = DiTModel(config, rngs=rngs)\n\n# Test conditional forward pass\nx = jnp.ones((2, 16, 16, 3))\nt = jnp.array([5, 8])\ny = jnp.array([2, 7])  # Class labels\n\noutput = model(x, t, y, deterministic=True, cfg_scale=3.0)\n\n# Generate samples using the built-in generate method\nsamples = model.generate(\n    n_samples=4,\n    rngs=rngs,\n    num_steps=10,\n    y=jnp.array([0, 1, 2, 3]),  # One sample per class\n    cfg_scale=3.0,\n    img_size=16,\n)\nprint(f\"Generated samples shape: {samples.shape}\")  # (4, 16, 16, 3)\n</code></pre>"},{"location":"examples/diffusion/dit-demo/#performance-benchmarking","title":"Performance Benchmarking","text":"<pre><code>import time\n\n# Create benchmark model (DiT-B)\nbenchmark_schedule = NoiseScheduleConfig(\n    name=\"benchmark_schedule\",\n    schedule_type=\"linear\",\n    num_timesteps=1000,\n    beta_start=1e-4,\n    beta_end=0.02,\n)\n\nconfig = DiTConfig(\n    name=\"benchmark_dit\",\n    noise_schedule=benchmark_schedule,\n    input_shape=(3, 64, 64),\n    patch_size=4,\n    hidden_size=768,  # DiT-B\n    depth=12,\n    num_heads=12,\n)\n\nmodel = DiTModel(config, rngs=rngs)\n\n# Prepare inputs\nbatch_size = 8\nx = jnp.ones((batch_size, 64, 64, 3))\nt = jnp.array([100] * batch_size)\n\n# Warmup (JIT compilation)\nfor _ in range(3):\n    _ = model(x, t, deterministic=True)\n\n# Benchmark\nnum_iterations = 10\nstart = time.time()\nfor _ in range(num_iterations):\n    output = model(x, t, deterministic=True)\n    output.block_until_ready()\nelapsed = time.time() - start\n\nthroughput = (batch_size * num_iterations) / elapsed\nprint(f\"DiT-B Throughput: {throughput:.1f} samples/sec\")\n</code></pre>"},{"location":"examples/diffusion/dit-demo/#features-demonstrated","title":"Features Demonstrated","text":""},{"location":"examples/diffusion/dit-demo/#diffusiontransformer-backbone","title":"DiffusionTransformer Backbone","text":"<ul> <li>Vision Transformer architecture</li> <li>Adaptive layer normalization (adaLN)</li> <li>Position-wise feed-forward networks</li> <li>Multi-head self-attention</li> </ul>"},{"location":"examples/diffusion/dit-demo/#dit-model-sizes","title":"DiT Model Sizes","text":"<ul> <li>Small (S): Fast prototyping and testing</li> <li>Base (B): Production-ready performance</li> <li>Large (L): High-quality generation</li> <li>Configurable depth, hidden size, and heads</li> </ul>"},{"location":"examples/diffusion/dit-demo/#conditional-generation","title":"Conditional Generation","text":"<ul> <li>Class-conditional generation</li> <li>Classifier-free guidance</li> <li>Guidance scale tuning</li> <li>Null conditioning for unconditional mode</li> </ul>"},{"location":"examples/diffusion/dit-demo/#patch-based-processing_1","title":"Patch-Based Processing","text":"<ul> <li>Efficient patch embeddings</li> <li>Positional encoding strategies</li> <li>Sequence-to-image reconstruction</li> <li>Variable patch sizes</li> </ul>"},{"location":"examples/diffusion/dit-demo/#performance-analysis","title":"Performance Analysis","text":"<ul> <li>Throughput benchmarking</li> <li>Memory profiling</li> <li>Quality vs. speed trade-offs</li> <li>Scaling behavior analysis</li> </ul>"},{"location":"examples/diffusion/dit-demo/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Vary patch size: Try 2\u00d72, 4\u00d74, 8\u00d78 patches and observe quality/speed trade-offs</li> <li>Modify model size: Create custom configurations between S/B/L</li> <li>Tune guidance scale: Experiment with CFG scales from 1.0 to 5.0</li> <li>Custom conditioning: Add additional conditioning (text, attributes, etc.)</li> <li>Training from scratch: Implement full training loop on your dataset</li> <li>Distillation: Train a smaller model to match larger model's quality</li> </ol>"},{"location":"examples/diffusion/dit-demo/#next-steps","title":"Next Steps","text":"<p>After understanding this example:</p> <ol> <li>Full Training: Implement training loop with ImageNet or custom data</li> <li>Custom Conditioning: Add text or multi-modal conditioning</li> <li>Faster Sampling: Explore DDIM, DPM-Solver, or other fast samplers</li> <li>Latent DiT: Apply DiT in latent space (like Stable Diffusion)</li> <li>Model Compression: Distillation, pruning, quantization</li> <li>Evaluation: FID, Inception Score, and other metrics</li> </ol>"},{"location":"examples/diffusion/dit-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/diffusion/dit-demo/#out-of-memory","title":"Out of Memory","text":"<ul> <li>Reduce model size (use DiT-S instead of DiT-L)</li> <li>Decrease batch size</li> <li>Use smaller images or larger patch size</li> <li>Enable gradient checkpointing</li> </ul>"},{"location":"examples/diffusion/dit-demo/#slow-generation","title":"Slow Generation","text":"<ul> <li>Use GPU acceleration</li> <li>Reduce number of denoising steps (try 50-100 instead of 1000)</li> <li>Use smaller model (DiT-S)</li> <li>Implement faster samplers (DDIM)</li> </ul>"},{"location":"examples/diffusion/dit-demo/#poor-sample-quality","title":"Poor Sample Quality","text":"<ul> <li>Increase model size (DiT-B or DiT-L)</li> <li>Tune classifier-free guidance scale</li> <li>Increase number of denoising steps</li> <li>Check training convergence</li> </ul>"},{"location":"examples/diffusion/dit-demo/#patch-size-issues","title":"Patch Size Issues","text":"<p>Ensure image size is divisible by patch size:</p> <pre><code>assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n</code></pre>"},{"location":"examples/diffusion/dit-demo/#additional-resources","title":"Additional Resources","text":"<ul> <li>Paper: Scalable Diffusion Models with Transformers</li> <li>Paper: Classifier-Free Diffusion Guidance</li> <li>Artifex Diffusion Guide: Diffusion Models Guide</li> <li>API Reference: DiffusionTransformer API</li> <li>Vision Transformer: An Image is Worth 16x16 Words</li> </ul>"},{"location":"examples/diffusion/dit-demo/#related-examples","title":"Related Examples","text":"<ul> <li>Simple Diffusion - Diffusion basics</li> <li>Simple EBM - Energy-based models</li> <li>Advanced Diffusion - More diffusion techniques</li> </ul>"},{"location":"examples/diffusion/dit-demo/#performance-comparison","title":"Performance Comparison","text":"<p>Expected performance on a modern GPU (A100):</p> Model Samples/sec Memory (GB) FID (ImageNet) DiT-S ~120 ~4 ~9.5 DiT-B ~50 ~12 ~5.3 DiT-L ~25 ~24 ~3.4 DiT-XL ~15 ~32 ~2.3 <p>Note: Actual performance depends on hardware, image size, and implementation details.</p>"},{"location":"examples/diffusion/simple-diffusion/","title":"Simple Diffusion Example","text":"<p>Level: Beginner | Runtime: ~30 seconds (CPU) / ~10 seconds (GPU) | Format: Python + Jupyter</p>"},{"location":"examples/diffusion/simple-diffusion/#overview","title":"Overview","text":"<p>This example demonstrates the fundamentals of diffusion models using a simple implementation. It covers the core concepts of the forward diffusion process, reverse denoising, and sample generation.</p>"},{"location":"examples/diffusion/simple-diffusion/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to create and configure a basic diffusion model</li> <li>Understanding noise schedules (beta schedules)</li> <li>Forward diffusion process (adding noise)</li> <li>Reverse process (denoising)</li> <li>Generating samples from random noise</li> <li>Visualizing diffusion model outputs</li> </ul>"},{"location":"examples/diffusion/simple-diffusion/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/diffusion/simple_diffusion_example.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/diffusion/simple_diffusion_example.ipynb</code></li> </ul>"},{"location":"examples/diffusion/simple-diffusion/#quick-start","title":"Quick Start","text":""},{"location":"examples/diffusion/simple-diffusion/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/diffusion/simple_diffusion_example.py\n</code></pre>"},{"location":"examples/diffusion/simple-diffusion/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/diffusion/simple_diffusion_example.ipynb\n</code></pre>"},{"location":"examples/diffusion/simple-diffusion/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/diffusion/simple-diffusion/#forward-diffusion-process","title":"Forward Diffusion Process","text":"<p>The forward process gradually adds Gaussian noise to data according to a variance schedule:</p> \\[q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)\\] <p>Where \\(\\beta_t\\) is the noise schedule at timestep \\(t\\).</p>"},{"location":"examples/diffusion/simple-diffusion/#reverse-process","title":"Reverse Process","text":"<p>The model learns to reverse this process, removing noise step by step:</p> \\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\\]"},{"location":"examples/diffusion/simple-diffusion/#noise-schedules","title":"Noise Schedules","text":"<p>The example demonstrates different beta schedules:</p> <ul> <li>Linear schedule: Simple linear increase from \\(\\beta_{min}\\) to \\(\\beta_{max}\\)</li> <li>Cosine schedule: Smoother noise addition following a cosine curve</li> </ul>"},{"location":"examples/diffusion/simple-diffusion/#code-structure","title":"Code Structure","text":"<p>The example is organized into clear sections:</p> <ol> <li>Model Definition: Creating a <code>SimpleDiffusionModel</code> class</li> <li>Configuration: Setting up model parameters and noise schedule</li> <li>Model Instantiation: Creating the model with proper RNG handling</li> <li>Sample Generation: Generating samples from random noise</li> <li>Visualization: Displaying generated samples</li> </ol>"},{"location":"examples/diffusion/simple-diffusion/#example-code","title":"Example Code","text":"<pre><code>from flax import nnx\nimport jax.numpy as jnp\nfrom artifex.generative_models.core.base import GenerativeModel\n\n# Create RNG\nrngs = nnx.Rngs(params=42, dropout=1, sample=2)\n\n# Model configuration\nconfig = {\n    \"input_dim\": (32, 32, 3),  # Image shape (H, W, C)\n    \"noise_steps\": 50,          # Number of denoising steps\n    \"beta_start\": 1e-4,         # Starting noise level\n    \"beta_end\": 0.02,           # Ending noise level\n}\n\n# The SimpleDiffusionModel in the example demonstrates:\n# - Inheriting from GenerativeModel\n# - Linear beta schedule for noise control\n# - Simplified denoising process with three phases:\n#   1. Early steps: Reduce noise magnitude\n#   2. Middle steps: Introduce spatial structure\n#   3. Late steps: Refine and sharpen output\n\n# Run the example to see the full implementation:\n# python examples/generative_models/diffusion/simple_diffusion_example.py\n\n# Output will be saved to: examples_output/diffusion_samples.png\n</code></pre>"},{"location":"examples/diffusion/simple-diffusion/#features-demonstrated","title":"Features Demonstrated","text":""},{"location":"examples/diffusion/simple-diffusion/#simplediffusionmodel-creation","title":"SimpleDiffusionModel Creation","text":"<ul> <li>Custom model class extending <code>GenerativeModel</code></li> <li>Proper initialization with RNG handling</li> <li>Beta schedule setup for noise control</li> </ul>"},{"location":"examples/diffusion/simple-diffusion/#noise-schedule","title":"Noise Schedule","text":"<ul> <li>Linear schedule implementation</li> <li>Alpha and alpha_bar calculations</li> <li>Understanding variance schedules</li> </ul>"},{"location":"examples/diffusion/simple-diffusion/#sample-generation","title":"Sample Generation","text":"<ul> <li>Starting from random noise</li> <li>Iterative denoising process</li> <li>Controlling generation quality</li> </ul>"},{"location":"examples/diffusion/simple-diffusion/#visualization","title":"Visualization","text":"<ul> <li>Displaying generated samples</li> <li>Comparing different timesteps</li> <li>Analyzing generation quality</li> </ul>"},{"location":"examples/diffusion/simple-diffusion/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Modify the noise schedule: Try different beta ranges or cosine schedules</li> <li>Change timesteps: Experiment with different numbers of diffusion steps</li> <li>Vary sample size: Generate different numbers of samples</li> <li>Add conditioning: Extend to conditional generation</li> <li>Custom architecture: Implement different denoising networks</li> </ol>"},{"location":"examples/diffusion/simple-diffusion/#next-steps","title":"Next Steps","text":"<p>After understanding this basic example:</p> <ol> <li>DiT Demo: Learn about Diffusion Transformers for more advanced architectures</li> <li>Training: Implement a full training loop for your own dataset</li> <li>Advanced Schedules: Explore more sophisticated noise schedules</li> <li>Conditional Generation: Add class or text conditioning</li> </ol>"},{"location":"examples/diffusion/simple-diffusion/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/diffusion/simple-diffusion/#import-errors","title":"Import Errors","text":"<p>Make sure you've activated the Artifex environment:</p> <pre><code>source activate.sh\n</code></pre>"},{"location":"examples/diffusion/simple-diffusion/#cuda-issues","title":"CUDA Issues","text":"<p>If you encounter GPU errors, try running on CPU:</p> <pre><code>export JAX_PLATFORMS=cpu\npython examples/generative_models/diffusion/simple_diffusion_example.py\n</code></pre>"},{"location":"examples/diffusion/simple-diffusion/#memory-issues","title":"Memory Issues","text":"<p>Reduce the batch size or number of timesteps if you run out of memory.</p>"},{"location":"examples/diffusion/simple-diffusion/#additional-resources","title":"Additional Resources","text":"<ul> <li>Paper: Denoising Diffusion Probabilistic Models (DDPM)</li> <li>Artifex Diffusion Guide: Diffusion Models Guide</li> <li>API Reference: Diffusion API</li> </ul>"},{"location":"examples/diffusion/simple-diffusion/#related-examples","title":"Related Examples","text":"<ul> <li>DiT Demo - Advanced diffusion with transformers</li> <li>Simple EBM - Energy-based models with MCMC sampling</li> </ul>"},{"location":"examples/energy/simple-ebm/","title":"Simple Energy-Based Model (EBM) Example","text":"<p>Level: Intermediate | Runtime: ~2 minutes (CPU) / ~30 seconds (GPU) | Format: Python + Jupyter</p>"},{"location":"examples/energy/simple-ebm/#overview","title":"Overview","text":"<p>This comprehensive example demonstrates Energy-Based Models (EBMs) with MCMC sampling. It covers the fundamentals of energy functions, Langevin dynamics sampling, and contrastive divergence training, including advanced techniques like persistent contrastive divergence and deep EBM architectures.</p>"},{"location":"examples/energy/simple-ebm/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Energy function computation and interpretation</li> <li>Langevin dynamics for MCMC sampling</li> <li>Contrastive divergence (CD) training</li> <li>Persistent contrastive divergence with sample buffers</li> <li>Deep EBM architectures with residual connections</li> <li>Score function estimation</li> </ul>"},{"location":"examples/energy/simple-ebm/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/energy/simple_ebm_example.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/energy/simple_ebm_example.ipynb</code></li> </ul>"},{"location":"examples/energy/simple-ebm/#quick-start","title":"Quick Start","text":""},{"location":"examples/energy/simple-ebm/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/energy/simple_ebm_example.py\n</code></pre>"},{"location":"examples/energy/simple-ebm/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/energy/simple_ebm_example.ipynb\n</code></pre>"},{"location":"examples/energy/simple-ebm/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/energy/simple-ebm/#energy-based-models","title":"Energy-Based Models","text":"<p>EBMs define a probability distribution through an energy function:</p> \\[p(x) = \\frac{\\exp(-E(x))}{Z}\\] <p>Where:</p> <ul> <li>\\(E(x)\\) is the energy function (lower energy = higher probability)</li> <li>\\(Z\\) is the partition function (normalization constant)</li> </ul>"},{"location":"examples/energy/simple-ebm/#langevin-dynamics","title":"Langevin Dynamics","text":"<p>Sampling from the model using Langevin dynamics:</p> \\[x_{t+1} = x_t - \\frac{\\epsilon}{2} \\nabla_x E(x_t) + \\sqrt{\\epsilon} \\cdot \\text{noise}\\] <p>Where \\(\\epsilon\\) is the step size and noise is Gaussian.</p>"},{"location":"examples/energy/simple-ebm/#contrastive-divergence-loss","title":"Contrastive Divergence Loss","text":"<p>Training objective that contrasts real data with model samples:</p> \\[\\mathcal{L}_{CD} = E_{x \\sim p_{data}}[E(x)] - E_{x \\sim p_{model}}[E(x)]\\] <p>The goal is to:</p> <ul> <li>Lower energy for real data samples</li> <li>Raise energy for generated samples</li> </ul>"},{"location":"examples/energy/simple-ebm/#persistent-contrastive-divergence","title":"Persistent Contrastive Divergence","text":"<p>An improved version of CD that maintains a buffer of persistent samples across training iterations, leading to better gradient estimates.</p>"},{"location":"examples/energy/simple-ebm/#code-structure","title":"Code Structure","text":"<p>The example demonstrates 9 major sections:</p> <ol> <li>Simple EBM Creation: Basic energy-based model for MNIST</li> <li>Energy Computation: Computing and interpreting energy values</li> <li>MCMC Sampling: Generating samples using Langevin dynamics</li> <li>Configuration System: Declarative model creation</li> <li>Contrastive Divergence: Training with CD loss</li> <li>Persistent CD: Advanced training with sample buffers</li> <li>Deep EBM: Complex architectures with residual connections</li> <li>Visualization: Analyzing samples and energy landscapes</li> <li>Summary: Key takeaways and experiments</li> </ol>"},{"location":"examples/energy/simple-ebm/#example-code","title":"Example Code","text":""},{"location":"examples/energy/simple-ebm/#creating-a-simple-ebm","title":"Creating a Simple EBM","text":"<pre><code>from flax import nnx\nimport jax.numpy as jnp\nfrom artifex.generative_models.models.energy import create_mnist_ebm\n\n# Initialize RNG\nrngs = nnx.Rngs(0)\n\n# Create EBM using factory function (recommended for MNIST)\nebm = create_mnist_ebm(rngs=rngs)\n\n# Compute energy for data\ndata = jnp.ones((32, 28, 28, 1))\nenergy = ebm.energy(data)\nprint(f\"Energy shape: {energy.shape}\")  # (32,)\n</code></pre>"},{"location":"examples/energy/simple-ebm/#langevin-dynamics-sampling","title":"Langevin Dynamics Sampling","text":"<pre><code>import jax\nfrom artifex.generative_models.models.energy import langevin_dynamics\n\n# Initialize samples\nkey = jax.random.key(0)\ninit_samples = jax.random.normal(key, (16, 28, 28, 1))\n\n# Generate samples using MCMC (Langevin dynamics)\nsamples = langevin_dynamics(\n    energy_fn=ebm.energy,\n    initial_samples=init_samples,\n    n_steps=100,\n    step_size=0.01,\n    noise_scale=0.005,\n)\n</code></pre>"},{"location":"examples/energy/simple-ebm/#contrastive-divergence-training","title":"Contrastive Divergence Training","text":"<pre><code># EBM uses built-in loss_fn for contrastive divergence training\n# The loss is computed during the forward pass with MCMC samples\n\n# Forward pass on real data\nreal_data = data_batch  # Your training data\noutputs = ebm(real_data)\n\n# Compute CD loss using model's loss_fn (handles MCMC internally)\nloss_dict = ebm.loss_fn(x=real_data, outputs=outputs)\n\nprint(f\"CD Loss: {loss_dict['loss']:.4f}\")\nprint(f\"Real Energy: {loss_dict['real_energy_mean']:.4f}\")\nprint(f\"Fake Energy: {loss_dict['fake_energy_mean']:.4f}\")\nprint(f\"Real Energy: {loss_dict['real_energy_mean']:.4f}\")\nprint(f\"Fake Energy: {loss_dict['fake_energy_mean']:.4f}\")\n</code></pre>"},{"location":"examples/energy/simple-ebm/#persistent-contrastive-divergence_1","title":"Persistent Contrastive Divergence","text":"<pre><code># Persistent CD is handled through EBM configuration\n# Configure with sample buffer when creating the model\n\nfrom artifex.generative_models.core.configuration.energy_config import (\n    EBMConfig,\n    EnergyNetworkConfig,\n    MCMCConfig,\n    SampleBufferConfig,\n)\nfrom artifex.generative_models.models.energy import EBM\n\n# Configure sample buffer for persistent CD\nbuffer_config = SampleBufferConfig(\n    name=\"buffer\",\n    capacity=10000,\n    reinit_prob=0.05,  # Probability to reinitialize from noise\n)\n\n# Full EBM config with sample buffer\nconfig = EBMConfig(\n    name=\"ebm_pcd\",\n    input_dim=784,\n    energy_network=EnergyNetworkConfig(\n        name=\"energy_net\",\n        hidden_dims=(256, 128),\n        activation=\"swish\",\n    ),\n    mcmc=MCMCConfig(name=\"mcmc\", n_steps=60, step_size=0.01),\n    sample_buffer=buffer_config,  # Enable persistent CD\n)\n\n# Create model - buffer is managed internally\nebm_pcd = EBM(config, rngs=nnx.Rngs(0))\n\n# Training uses persistent samples automatically\n# The model's loss_fn handles buffer management internally\n        init_samples=init_samples,\n        step_size=0.01,\n        n_steps=20  # Fewer steps with persistent buffer\n    )\n\n    # Update buffer\n    buffer.add(samples)\n\n    # Compute loss and update model\n    loss = contrastive_divergence_loss(ebm, real_data, samples)\n</code></pre>"},{"location":"examples/energy/simple-ebm/#deep-ebm-architecture","title":"Deep EBM Architecture","text":"<pre><code>from artifex.generative_models.core.configuration.energy_config import (\n    DeepEBMConfig,\n    EnergyNetworkConfig,\n    MCMCConfig,\n    SampleBufferConfig,\n)\nfrom artifex.generative_models.models.energy import DeepEBM\n\n# Create energy network configuration\nenergy_network_config = EnergyNetworkConfig(\n    name=\"deep_energy_network\",\n    hidden_dims=(32, 64, 128),  # Channel progression (tuple)\n    activation=\"silu\",\n    network_type=\"cnn\",\n    use_spectral_norm=True,\n    use_residual=True,\n)\n\n# Create MCMC sampling configuration\nmcmc_config = MCMCConfig(\n    name=\"langevin_mcmc\",\n    n_steps=100,\n    step_size=0.005,\n    noise_scale=0.001,\n)\n\n# Create sample buffer configuration\nsample_buffer_config = SampleBufferConfig(\n    name=\"replay_buffer\",\n    capacity=8192,\n    reinit_prob=0.05,\n)\n\n# Create deep EBM configuration with nested configs\ndeep_config = DeepEBMConfig(\n    name=\"deep_ebm\",\n    input_shape=(32, 32, 3),\n    energy_network=energy_network_config,\n    mcmc=mcmc_config,\n    sample_buffer=sample_buffer_config,\n    alpha=0.001,\n)\n\n# Create model\ndeep_ebm = DeepEBM(config=deep_config, rngs=rngs)\n</code></pre>"},{"location":"examples/energy/simple-ebm/#features-demonstrated","title":"Features Demonstrated","text":""},{"location":"examples/energy/simple-ebm/#energy-function-design","title":"Energy Function Design","text":"<ul> <li>MLP-based energy functions</li> <li>CNN-based energy functions</li> <li>Deep architectures with residual connections</li> <li>Spectral normalization for stability</li> </ul>"},{"location":"examples/energy/simple-ebm/#mcmc-sampling","title":"MCMC Sampling","text":"<ul> <li>Langevin dynamics implementation</li> <li>Step size tuning</li> <li>Temperature control</li> <li>Burn-in periods</li> </ul>"},{"location":"examples/energy/simple-ebm/#training-techniques","title":"Training Techniques","text":"<ul> <li>Standard contrastive divergence</li> <li>Persistent contrastive divergence</li> <li>Sample buffer management</li> <li>Gradient estimation</li> </ul>"},{"location":"examples/energy/simple-ebm/#advanced-architectures","title":"Advanced Architectures","text":"<ul> <li>Deep convolutional EBMs</li> <li>Residual connections</li> <li>Batch normalization</li> <li>Spectral normalization</li> </ul>"},{"location":"examples/energy/simple-ebm/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Modify energy architecture: Try different hidden dimensions or activation functions</li> <li>Tune MCMC parameters: Experiment with step sizes, number of steps, temperature</li> <li>Compare CD vs Persistent CD: Observe training stability and sample quality</li> <li>Add noise annealing: Gradually reduce noise during sampling</li> <li>Conditional EBMs: Extend to conditional generation with labels</li> <li>Hybrid models: Combine EBMs with other generative models</li> </ol>"},{"location":"examples/energy/simple-ebm/#next-steps","title":"Next Steps","text":"<p>After understanding this example:</p> <ol> <li>Training Loop: Implement full training on real datasets (MNIST, CIFAR-10)</li> <li>Score Matching: Explore score matching as an alternative to CD</li> <li>Conditional Generation: Add class or attribute conditioning</li> <li>Energy Landscape Analysis: Visualize and analyze learned energy functions</li> <li>Compositional Generation: Combine multiple EBMs for complex generation</li> </ol>"},{"location":"examples/energy/simple-ebm/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/energy/simple-ebm/#mcmc-not-converging","title":"MCMC Not Converging","text":"<ul> <li>Increase number of sampling steps</li> <li>Reduce step size</li> <li>Add noise annealing schedule</li> <li>Check energy function gradients</li> </ul>"},{"location":"examples/energy/simple-ebm/#training-instability","title":"Training Instability","text":"<ul> <li>Use spectral normalization</li> <li>Reduce learning rate</li> <li>Increase batch size</li> <li>Use persistent CD with larger buffer</li> </ul>"},{"location":"examples/energy/simple-ebm/#slow-sampling","title":"Slow Sampling","text":"<ul> <li>Use GPU acceleration</li> <li>Reduce number of MCMC steps</li> <li>Use persistent buffers to start from better initializations</li> <li>Consider faster sampling methods (e.g., HMC)</li> </ul>"},{"location":"examples/energy/simple-ebm/#memory-issues","title":"Memory Issues","text":"<ul> <li>Reduce buffer size for persistent CD</li> <li>Use smaller batch sizes</li> <li>Reduce model size (fewer hidden dims)</li> </ul>"},{"location":"examples/energy/simple-ebm/#additional-resources","title":"Additional Resources","text":"<ul> <li>Paper: A Tutorial on Energy-Based Learning</li> <li>Paper: Training Products of Experts by Minimizing Contrastive Divergence</li> <li>Artifex EBM Guide: (Coming soon)</li> <li>API Reference: (Coming soon)</li> </ul>"},{"location":"examples/energy/simple-ebm/#related-examples","title":"Related Examples","text":"<ul> <li>Simple Diffusion - Diffusion models basics</li> <li>BlackJAX Sampling - Advanced MCMC methods</li> <li>DiT Demo - Transformer-based generation</li> </ul>"},{"location":"examples/framework/framework-features-demo/","title":"Artifex Framework Features Demonstration","text":"<p>Level: Intermediate | Runtime: ~1-2 minutes (CPU) | Format: Python + Jupyter</p> <p>Prerequisites: Basic understanding of generative models and JAX | Target Audience: Users learning the framework's architecture</p>"},{"location":"examples/framework/framework-features-demo/#overview","title":"Overview","text":"<p>This example provides a comprehensive tour of the Artifex framework's core features and design patterns. Learn how to leverage the unified configuration system, factory pattern, composable losses, sampling methods, and modality adapters for building production-ready generative models.</p>"},{"location":"examples/framework/framework-features-demo/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Unified Configuration</p> <p>Type-safe model, training, and data configurations with Pydantic validation</p> </li> <li> <p> Factory Pattern</p> <p>Consistent model creation interface across all model types (VAE, GAN, diffusion)</p> </li> <li> <p> Composable Losses</p> <p>Flexible loss composition with weighted components and tracking</p> </li> <li> <p> Sampling Methods</p> <p>MCMC and SDE sampling for energy-based and diffusion models</p> </li> <li> <p> Modality System</p> <p>Domain-specific adapters for images, text, audio, proteins, and 3D data</p> </li> </ul>"},{"location":"examples/framework/framework-features-demo/#files","title":"Files","text":"<p>This example is available in two formats:</p> <ul> <li>Python Script: <code>framework_features_demo.py</code></li> <li>Jupyter Notebook: <code>framework_features_demo.ipynb</code></li> </ul>"},{"location":"examples/framework/framework-features-demo/#quick-start","title":"Quick Start","text":""},{"location":"examples/framework/framework-features-demo/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/framework_features_demo.py\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/framework_features_demo.ipynb\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/framework/framework-features-demo/#1-unified-configuration-system","title":"1. Unified Configuration System","text":"<p>Artifex uses frozen dataclass configuration classes for type-safe, validated model definitions:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    VAEConfig,\n    EncoderConfig,\n    DecoderConfig,\n)\n\n# Create encoder config (nested configuration)\nencoder_config = EncoderConfig(\n    name=\"encoder\",\n    input_shape=(28, 28, 1),\n    latent_dim=32,\n    hidden_dims=(256, 128),  # Tuple for frozen dataclass\n    activation=\"relu\",\n)\n\n# Create decoder config\ndecoder_config = DecoderConfig(\n    name=\"decoder\",\n    output_shape=(28, 28, 1),\n    latent_dim=32,\n    hidden_dims=(128, 256),  # Tuple for frozen dataclass\n    activation=\"relu\",\n)\n\n# Create a model configuration with nested configs\nconfig = VAEConfig(\n    name=\"my_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    kl_weight=1.0,\n)\n</code></pre> <p>Benefits:</p> <ul> <li>Type-safe with automatic validation</li> <li>Immutable (frozen) for consistency</li> <li>Serialization to JSON/YAML for reproducibility</li> <li>Nested composition for complex models</li> <li>Easy parameter sweeps for hyperparameter tuning</li> </ul>"},{"location":"examples/framework/framework-features-demo/#2-factory-pattern","title":"2. Factory Pattern","text":"<p>The factory pattern provides unified model creation:</p> <pre><code>from artifex.generative_models.factory import create_model\nfrom flax import nnx\n\n# Setup RNGs\nrngs = nnx.Rngs(params=42, dropout=42)\n\n# Create any model from configuration\nmodel = create_model(config, rngs=rngs)\n\n# Test forward pass\noutputs = model(test_data, rngs=rngs)\n</code></pre> <p>Why Use Factories?</p> <ul> <li>Consistency across all model types</li> <li>Validation before instantiation</li> <li>Easy to swap models for experimentation</li> <li>Proper RNG management</li> </ul>"},{"location":"examples/framework/framework-features-demo/#3-composable-loss-system","title":"3. Composable Loss System","text":"<p>Combine multiple loss functions with different weights:</p> \\[L_{\\text{total}} = \\sum_{i=1}^{n} w_i \\cdot L_i(\\text{pred}, \\text{target})\\] <pre><code>from artifex.generative_models.core.losses import (\n    CompositeLoss,\n    WeightedLoss,\n    mse_loss,\n    mae_loss,\n)\n\n# Create composite loss\ncomposite = CompositeLoss([\n    WeightedLoss(mse_loss, weight=1.0, name=\"reconstruction\"),\n    WeightedLoss(mae_loss, weight=0.5, name=\"l1_penalty\"),\n], return_components=True)\n\n# Compute loss with component tracking\ntotal_loss, components = composite(predictions, targets)\n# components = {\"reconstruction\": 0.15, \"l1_penalty\": 0.08}\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#4-sampling-methods","title":"4. Sampling Methods","text":""},{"location":"examples/framework/framework-features-demo/#mcmc-sampling-energy-based-models","title":"MCMC Sampling (Energy-Based Models)","text":"<pre><code>from artifex.generative_models.core.sampling import mcmc_sampling\n\ndef log_prob_fn(x):\n    return -0.5 * jnp.sum(x**2)  # Log probability\n\nsamples = mcmc_sampling(\n    log_prob_fn=log_prob_fn,\n    init_state=jnp.zeros(10),\n    key=jax.random.key(42),\n    n_samples=1000,\n    n_burnin=200,\n    step_size=0.1,\n)\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#sde-sampling-diffusion-models","title":"SDE Sampling (Diffusion Models)","text":"<pre><code>from artifex.generative_models.core.sampling import sde_sampling\n\ndef drift_fn(x, t):\n    return -x  # Drift function\n\ndef diffusion_fn(x, t):\n    return jnp.ones_like(x) * 0.1  # Diffusion coefficient\n\nsample = sde_sampling(\n    drift_fn=drift_fn,\n    diffusion_fn=diffusion_fn,\n    init_state=x0,\n    t_span=(0.0, 1.0),\n    key=key,\n    n_steps=100,\n)\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#5-modality-system","title":"5. Modality System","text":"<p>Domain-specific features for different data types:</p> <pre><code>from artifex.generative_models.modalities import get_modality\n\n# Get image modality\nimage_modality = get_modality('image', rngs=rngs)\n\n# Create dataset\ndataset = image_modality.create_dataset(data_config)\n\n# Compute metrics\nmetrics = image_modality.evaluate(model, test_data)\n# metrics = {\"fid\": 12.5, \"is_score\": 8.3, ...}\n\n# Get modality adapter\nadapter = image_modality.get_adapter('vae')\nadapted_model = adapter.adapt(base_model, config)\n</code></pre> <p>Available Modalities:</p> <ul> <li><code>image</code>: Convolutional layers, FID, IS metrics</li> <li><code>text</code>: Tokenization, perplexity, BLEU</li> <li><code>audio</code>: Spectrograms, MFCCs</li> <li><code>protein</code>: Structure prediction, sequence modeling</li> <li><code>geometric</code>: Point clouds, mesh processing</li> </ul>"},{"location":"examples/framework/framework-features-demo/#code-structure","title":"Code Structure","text":"<p>The example demonstrates framework features in five sections:</p> <ol> <li>Configuration System - Create type-safe configs for models, training, data</li> <li>Factory Pattern - Instantiate models from configurations</li> <li>Composable Losses - Combine weighted loss functions</li> <li>Sampling Methods - MCMC and SDE sampling for generation</li> <li>Modality System - Domain-specific adapters and evaluation</li> </ol> <p>Each section is self-contained and can be run independently.</p>"},{"location":"examples/framework/framework-features-demo/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>\u2705 Type-safe configuration with automatic validation</li> <li>\u2705 Unified model creation across all types (VAE, GAN, diffusion, flow, EBM)</li> <li>\u2705 Flexible loss composition with component tracking</li> <li>\u2705 MCMC sampling for energy-based models</li> <li>\u2705 SDE sampling for diffusion models</li> <li>\u2705 Modality-specific dataset loading and evaluation</li> <li>\u2705 Proper RNG management with <code>nnx.Rngs</code></li> <li>\u2705 JIT compilation for performance</li> </ul>"},{"location":"examples/framework/framework-features-demo/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Create Different Model Types</li> </ol> <pre><code>from artifex.generative_models.core.configuration import (\n    GANConfig,\n    GeneratorConfig,\n    DiscriminatorConfig,\n)\n\n# Create GAN config with nested generator/discriminator\ngen_config = GeneratorConfig(\n    name=\"generator\",\n    latent_dim=100,\n    output_shape=(28, 28, 1),\n    hidden_dims=(256, 512),\n)\ndisc_config = DiscriminatorConfig(\n    name=\"discriminator\",\n    input_shape=(28, 28, 1),\n    hidden_dims=(512, 256),\n)\ngan_config = GANConfig(\n    name=\"my_gan\",\n    generator=gen_config,\n    discriminator=disc_config,\n)\ngan = create_model(gan_config, rngs=rngs)\n</code></pre> <ol> <li>Custom Loss Combinations</li> </ol> <pre><code># Add perceptual loss to composite\nfrom artifex.generative_models.core.losses import PerceptualLoss\n\ncomposite = CompositeLoss([\n    WeightedLoss(mse_loss, weight=1.0, name=\"recon\"),\n    WeightedLoss(PerceptualLoss(), weight=0.1, name=\"perceptual\"),\n])\n</code></pre> <ol> <li>Adjust Sampling Parameters</li> </ol> <pre><code># Try different MCMC settings\nsamples = mcmc_sampling(\n    log_prob_fn=log_prob_fn,\n    init_state=x0,\n    key=key,\n    n_samples=5000,  # More samples\n    step_size=0.01,  # Smaller steps\n)\n</code></pre> <ol> <li>Experiment with Modalities</li> </ol> <pre><code># Try different modalities\naudio_modality = get_modality('audio', rngs=rngs)\naudio_dataset = audio_modality.create_dataset(audio_config)\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#next-steps","title":"Next Steps","text":"<ul> <li> <p> VAE Examples</p> <p>Learn VAE implementation patterns</p> <p> Basic VAE Tutorial</p> </li> <li> <p> GAN Examples</p> <p>Explore GAN training</p> <p> Basic GAN Tutorial</p> </li> <li> <p> Diffusion Examples</p> <p>Understand diffusion models</p> <p> Simple Diffusion</p> </li> <li> <p> Loss Examples</p> <p>Deep dive into loss functions</p> <p> Loss Function Guide</p> </li> </ul>"},{"location":"examples/framework/framework-features-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/framework/framework-features-demo/#missing-configuration-fields","title":"Missing Configuration Fields","text":"<p>Symptom: <code>ValidationError</code> or <code>TypeError</code> when creating configuration</p> <p>Solution: Check required fields in the specific config class. Each model type has its own config class with nested configs.</p> <pre><code># View VAE config structure\nfrom artifex.generative_models.core.configuration import VAEConfig, EncoderConfig\nprint(VAEConfig.__dataclass_fields__)  # See required fields\nprint(EncoderConfig.__dataclass_fields__)  # See nested config fields\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#factory-creation-fails","title":"Factory Creation Fails","text":"<p>Symptom: <code>TypeError</code> or <code>AttributeError</code> during model creation</p> <p>Solution: Use the specific config class for your model type with properly nested configs</p> <pre><code>from artifex.generative_models.core.configuration import (\n    VAEConfig,\n    EncoderConfig,\n    DecoderConfig,\n)\n\n# Create with proper nested config structure\nencoder = EncoderConfig(\n    name=\"encoder\",\n    input_shape=(28, 28, 1),\n    latent_dim=32,\n    hidden_dims=(256, 128),\n)\ndecoder = DecoderConfig(\n    name=\"decoder\",\n    output_shape=(28, 28, 1),\n    latent_dim=32,\n    hidden_dims=(128, 256),\n)\nconfig = VAEConfig(\n    name=\"my_vae\",\n    encoder=encoder,\n    decoder=decoder,\n    kl_weight=1.0,\n)\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#rng-key-errors","title":"RNG Key Errors","text":"<p>Symptom: <code>KeyError</code> for missing RNG streams</p> <p>Solution: Initialize all required RNG streams</p> <pre><code># VAE needs params, dropout, and sample streams\nrngs = nnx.Rngs(\n    params=42,\n    dropout=43,\n    sample=44,\n)\n</code></pre>"},{"location":"examples/framework/framework-features-demo/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/framework/framework-features-demo/#documentation","title":"Documentation","text":"<ul> <li>Configuration System Guide - Deep dive into configurations</li> <li>Factory Pattern Guide - Advanced factory usage</li> <li>Loss Functions API - Complete loss function reference</li> <li>Sampling Methods - Sampling algorithm details</li> </ul>"},{"location":"examples/framework/framework-features-demo/#related-examples","title":"Related Examples","text":"<ul> <li>Loss Examples - Complete loss function showcase</li> <li>VAE MNIST Tutorial - Step-by-step VAE implementation</li> <li>Simple Diffusion - Diffusion model basics</li> </ul>"},{"location":"examples/framework/framework-features-demo/#papers","title":"Papers","text":"<ul> <li>Pydantic: Pydantic Documentation - Configuration validation</li> <li>JAX: JAX Documentation - Array programming and JIT compilation</li> <li>Flax NNX: Flax NNX Guide - Neural network library</li> </ul>"},{"location":"examples/framework/model-deployment/","title":"Model Deployment","text":"<p>Guide to deploying trained Artifex models for inference in production environments.</p>"},{"location":"examples/framework/model-deployment/#overview","title":"Overview","text":"<p>This guide covers strategies for deploying generative models for production inference.</p> <ul> <li> <p> Model Export</p> <p>Export trained models for deployment</p> <p> Model Export</p> </li> <li> <p> Inference Optimization</p> <p>Optimize models for fast inference</p> <p> Inference Optimization</p> </li> <li> <p> Serving Patterns</p> <p>Common deployment architectures</p> <p> Serving Patterns</p> </li> </ul>"},{"location":"examples/framework/model-deployment/#model-export","title":"Model Export","text":"<p>Save and load trained models for deployment.</p> <pre><code>from artifex.generative_models.core import save_model, load_model\n\n# Save trained model\nsave_model(model, \"checkpoints/vae_model\")\n\n# Load for inference\nmodel = load_model(\"checkpoints/vae_model\")\n</code></pre>"},{"location":"examples/framework/model-deployment/#inference-optimization","title":"Inference Optimization","text":"<p>Optimize models for production inference speed.</p> <pre><code>import jax\n\n# JIT compile for faster inference\n@jax.jit\ndef generate(model, rng_key, num_samples):\n    return model.sample(num_samples=num_samples, rng=rng_key)\n\n# Warm up JIT compilation\n_ = generate(model, jax.random.key(0), 1)\n\n# Fast inference\nsamples = generate(model, jax.random.key(42), 64)\n</code></pre>"},{"location":"examples/framework/model-deployment/#serving-patterns","title":"Serving Patterns","text":"<p>Common patterns for serving generative models.</p>"},{"location":"examples/framework/model-deployment/#batch-processing","title":"Batch Processing","text":"<pre><code>def batch_generate(model, batch_size=64, total_samples=1000):\n    \"\"\"Generate samples in batches for efficiency.\"\"\"\n    samples = []\n    for i in range(0, total_samples, batch_size):\n        batch = model.sample(\n            num_samples=min(batch_size, total_samples - i),\n            rng=jax.random.key(i),\n        )\n        samples.append(batch)\n    return jnp.concatenate(samples, axis=0)\n</code></pre>"},{"location":"examples/framework/model-deployment/#api-endpoint","title":"API Endpoint","text":"<pre><code>from fastapi import FastAPI\nimport jax.numpy as jnp\n\napp = FastAPI()\n\n@app.post(\"/generate\")\nasync def generate_samples(num_samples: int = 16):\n    samples = model.sample(num_samples=num_samples, rng=jax.random.key(0))\n    return {\"samples\": samples.tolist()}\n</code></pre>"},{"location":"examples/framework/model-deployment/#related-documentation","title":"Related Documentation","text":"<ul> <li>Inference Overview - Inference fundamentals</li> <li>Optimization Guide - Performance optimization</li> <li>Sampling Methods - Sampling techniques</li> </ul>"},{"location":"examples/framework/training-strategies/","title":"Training Strategies","text":"<p>Advanced training strategies and patterns for Artifex generative models.</p>"},{"location":"examples/framework/training-strategies/#overview","title":"Overview","text":"<p>This guide covers advanced training strategies for optimizing your generative model training workflows.</p> <ul> <li> <p> Gradient Accumulation</p> <p>Train with larger effective batch sizes on limited memory</p> <p> Gradient Accumulation</p> </li> <li> <p> Mixed Precision Training</p> <p>Speed up training with bfloat16 computation</p> <p> Mixed Precision</p> </li> <li> <p> Distributed Training</p> <p>Scale training across multiple devices</p> <p> Distributed Training</p> </li> <li> <p> Curriculum Learning</p> <p>Progressive training for improved convergence</p> <p> Curriculum Learning</p> </li> </ul>"},{"location":"examples/framework/training-strategies/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Accumulate gradients over multiple steps to simulate larger batch sizes.</p> <pre><code>from artifex.generative_models.training import Trainer, TrainingConfig\n\nconfig = TrainingConfig(\n    num_epochs=100,\n    batch_size=16,\n    gradient_accumulation_steps=4,  # Effective batch size: 64\n)\n\ntrainer = Trainer(model=model, training_config=config)\ntrainer.train(train_data)\n</code></pre>"},{"location":"examples/framework/training-strategies/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Use bfloat16 for faster training while maintaining accuracy.</p> <pre><code>import jax\n\n# Enable mixed precision\nconfig = TrainingConfig(\n    dtype=\"bfloat16\",\n    num_epochs=100,\n)\n\ntrainer = Trainer(model=model, training_config=config)\n</code></pre>"},{"location":"examples/framework/training-strategies/#distributed-training","title":"Distributed Training","text":"<p>Scale training across multiple GPUs or TPUs.</p> <pre><code>import jax\n\n# Detect available devices\ndevices = jax.local_devices()\nprint(f\"Available devices: {len(devices)}\")\n\n# Data parallel training\nconfig = TrainingConfig(\n    num_epochs=100,\n    distributed=True,\n)\n</code></pre>"},{"location":"examples/framework/training-strategies/#curriculum-learning","title":"Curriculum Learning","text":"<p>Progressive training from simple to complex samples.</p> <pre><code>def create_curriculum_loader(dataset, epoch, max_epochs):\n    \"\"\"Create data loader with curriculum difficulty.\"\"\"\n    difficulty = min(1.0, epoch / (max_epochs * 0.5))\n    return filter_by_difficulty(dataset, max_difficulty=difficulty)\n</code></pre>"},{"location":"examples/framework/training-strategies/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Guide - Core training concepts</li> <li>Configuration Guide - Training configuration options</li> <li>Framework Features Demo - Comprehensive framework example</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/","title":"Comprehensive Geometric Benchmark Demo","text":"<p>Level: Advanced | Runtime: ~10-15 minutes (CPU) / ~3-5 minutes (GPU) | Format: Python + Jupyter</p> <p>Prerequisites: Understanding of 3D geometry, point clouds, and transformer architectures | Target Audience: Users training 3D generative models</p>"},{"location":"examples/geometric/geometric-benchmark-demo/#overview","title":"Overview","text":"<p>This example demonstrates a complete end-to-end pipeline for training and evaluating point cloud generation models with Artifex. Learn how to load ShapeNet datasets, train transformer-based geometric models, use Chamfer distance loss, and evaluate with comprehensive 3D metrics.</p>"},{"location":"examples/geometric/geometric-benchmark-demo/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> ShapeNet Dataset</p> <p>PyTorch3D-style data loading with automatic fallbacks to synthetic data</p> </li> <li> <p> Point Cloud Models</p> <p>Transformer-based architecture for generating 3D point clouds</p> </li> <li> <p> Chamfer Distance</p> <p>Primary loss function for measuring point cloud similarity</p> </li> <li> <p> Training Pipeline</p> <p>Complete training with Adam optimizer, cosine scheduler, and checkpointing</p> </li> <li> <p> Evaluation Metrics</p> <p>Diversity, coverage, quality, and geometric fidelity scores</p> </li> <li> <p> Benchmark Suite</p> <p>Compare results against standard geometric benchmarks</p> </li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#files","title":"Files","text":"<p>This example is available in two formats:</p> <ul> <li>Python Script: <code>geometric_benchmark_demo.py</code></li> <li>Jupyter Notebook: <code>geometric_benchmark_demo.ipynb</code></li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#quick-start","title":"Quick Start","text":""},{"location":"examples/geometric/geometric-benchmark-demo/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the complete demo (trains for 50 epochs)\npython examples/generative_models/geometric/geometric_benchmark_demo.py\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/geometric/geometric_benchmark_demo.ipynb\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/geometric/geometric-benchmark-demo/#1-point-cloud-representation","title":"1. Point Cloud Representation","text":"<p>Point clouds are sets of 3D coordinates representing object surfaces:</p> <pre><code># Point cloud shape: (batch_size, num_points, 3)\npoint_cloud = jnp.array([\n    [[x1, y1, z1],\n     [x2, y2, z2],\n     ...\n     [xN, yN, zN]]\n])  # Shape: (1, 1024, 3)\n</code></pre> <p>Key Properties:</p> <ul> <li>Unordered: No canonical ordering of points</li> <li>Variable size: Different objects may have different numbers of points</li> <li>Surface representation: Points typically lie on object surface</li> <li>Normalized: Usually normalized to unit sphere or box</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#2-shapenet-dataset","title":"2. ShapeNet Dataset","text":"<p>Large-scale 3D object dataset with 51,300 models across 55 categories:</p> <pre><code>from artifex.benchmarks.datasets.geometric import ShapeNetDataset\n\ndataset = ShapeNetDataset(\n    data_path=\"./data/shapenet\",\n    config=data_config,\n    rngs=rngs\n)\n\n# Get batch\nbatch = dataset.get_batch(batch_size=8, split=\"train\")\n# batch = {\n#     \"point_clouds\": (8, 1024, 3),  # 8 samples, 1024 points each\n#     \"labels\": (8,),                 # Category labels\n#     \"synsets\": [\"02691156\", ...],   # Category IDs\n# }\n</code></pre> <p>Synset Categories (examples):</p> <ul> <li><code>02691156</code>: Airplane</li> <li><code>02958343</code>: Car</li> <li><code>03001627</code>: Chair</li> <li><code>04379243</code>: Table</li> <li>More: See ShapeNet documentation</li> </ul> <p>Automatic Fallbacks:</p> <ol> <li>Try downloading ShapeNet data</li> <li>Fall back to ModelNet if available</li> <li>Generate synthetic data if needed</li> </ol>"},{"location":"examples/geometric/geometric-benchmark-demo/#3-chamfer-distance-loss","title":"3. Chamfer Distance Loss","text":"<p>Primary loss function for point clouds, measuring bidirectional nearest-neighbor distances:</p> \\[L_{\\text{Chamfer}}(X, Y) = \\frac{1}{|X|}\\sum_{x \\in X} \\min_{y \\in Y} \\|x - y\\|^2 + \\frac{1}{|Y|}\\sum_{y \\in Y} \\min_{x \\in X} \\|x - y\\|^2\\] <pre><code>from artifex.generative_models.core.losses.geometric import chamfer_distance\n\n# Compute Chamfer distance\nloss = chamfer_distance(pred_points, target_points)\n\n# pred_points: (batch, num_points, 3)\n# target_points: (batch, num_points, 3)\n# loss: scalar value (lower is better)\n</code></pre> <p>Interpretation:</p> <ul> <li>First term: Average distance from predicted to closest real point</li> <li>Second term: Average distance from real to closest predicted point</li> <li>Symmetric: Penalizes both missing points and spurious points</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#4-point-cloud-model-architecture","title":"4. Point Cloud Model Architecture","text":"<p>Transformer-based model for generating point clouds:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    PointCloudConfig,\n    PointCloudNetworkConfig,\n)\nfrom artifex.generative_models.models.geometric.point_cloud import PointCloudModel\n\n# Network config with transformer architecture\nnetwork_config = PointCloudNetworkConfig(\n    name=\"benchmark_network\",\n    hidden_dims=(128,),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=128,\n    num_heads=8,\n    num_layers=4,\n    dropout_rate=0.1,\n)\n\n# Point cloud config\nmodel_config = PointCloudConfig(\n    name=\"point_cloud_model\",\n    network=network_config,\n    num_points=1024,\n    dropout_rate=0.1,\n)\n\nmodel = PointCloudModel(config=model_config, rngs=rngs)\n</code></pre> <p>Architecture:</p> <ul> <li>Encoder: Point cloud \u2192 latent embedding (via self-attention)</li> <li>Transformer layers: Multi-head self-attention with residual connections</li> <li>Decoder: Latent embedding \u2192 reconstructed point cloud</li> <li>Permutation invariance: Order-independent processing via attention</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#5-training-configuration","title":"5. Training Configuration","text":"<p>Complete training setup with optimizer and scheduler:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    OptimizerConfig,\n    SchedulerConfig,\n    TrainingConfig,\n)\n\n# Optimizer\noptimizer_config = OptimizerConfig(\n    name=\"optimizer\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-4,\n    weight_decay=1e-5,\n    beta1=0.9,\n    beta2=0.999,\n)\n\n# Learning rate schedule\nscheduler_config = SchedulerConfig(\n    name=\"scheduler\",\n    scheduler_type=\"cosine\",\n    warmup_steps=100,\n    min_lr_ratio=0.01,\n)\n\n# Training\ntraining_config = TrainingConfig(\n    name=\"training\",\n    batch_size=8,\n    num_epochs=50,\n    optimizer=optimizer_config,\n    scheduler=scheduler_config,\n)\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#6-evaluation-metrics","title":"6. Evaluation Metrics","text":"<p>Comprehensive metrics for point cloud generation:</p> <pre><code>from artifex.benchmarks.metrics.geometric import PointCloudMetrics\n\nmetrics = PointCloudMetrics(rngs=rngs, config=eval_config)\n\nresults = metrics.compute(\n    real_data=real_point_clouds,\n    generated_data=generated_point_clouds\n)\n\n# results = {\n#     \"1nn_accuracy\": 0.85,          # 1-NN classification accuracy\n#     \"coverage\": 0.72,              # Coverage of real distribution\n#     \"geometric_fidelity\": 0.68,    # Geometric quality score\n#     \"chamfer_distance\": 0.012,     # Average Chamfer distance\n# }\n</code></pre> <p>Metric Definitions:</p> <ul> <li>1-NN Accuracy: Classification accuracy using 1-nearest neighbor</li> <li>Tests if generated samples are realistic</li> <li> <p>Higher is better (target: &gt;0.8)</p> </li> <li> <p>Coverage: Fraction of real samples covered by generated samples</p> </li> <li>Tests distribution diversity</li> <li> <p>Higher is better (target: &gt;0.6)</p> </li> <li> <p>Geometric Fidelity: Quality of geometric structure</p> </li> <li>Measures surface smoothness and completeness</li> <li> <p>Higher is better (target: &gt;0.7)</p> </li> <li> <p>Chamfer Distance: Average point-to-point distance</p> </li> <li>Direct reconstruction quality</li> <li>Lower is better (target: &lt;0.02)</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#7-training-pipeline","title":"7. Training Pipeline","text":"<p>Complete training loop with logging and checkpointing:</p> <pre><code>class GeometricDemoTrainer:\n    def train(self):\n        for epoch in range(num_epochs):\n            # Training phase\n            train_metrics = self._train_epoch(trainer, epoch)\n\n            # Validation phase\n            val_metrics = self._validate_epoch(trainer, epoch)\n\n            # Update learning rate\n            current_lr = self._update_learning_rate(trainer, epoch)\n\n            # Log metrics\n            self._log_epoch_metrics(epoch, train_metrics, val_metrics, current_lr)\n\n            # Save checkpoint\n            if (epoch + 1) % save_freq == 0:\n                self._save_checkpoint(trainer, epoch)\n\n            # Visualize progress\n            if (epoch + 1) % 25 == 0:\n                self._visualize_progress(trainer, epoch)\n\n        # Final evaluation\n        final_metrics = self._final_evaluation(trainer)\n\n        return trainer, final_metrics\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#code-structure","title":"Code Structure","text":"<p>The example consists of three main components:</p> <ol> <li>GeometricDemoTrainer - Complete trainer orchestrating:</li> <li>Dataset setup (ShapeNet with fallbacks)</li> <li>Model initialization (transformer architecture)</li> <li>Training loop (optimizer, scheduler, logging)</li> <li>Evaluation (comprehensive metrics)</li> <li> <p>Visualization (training curves, samples)</p> </li> <li> <p>Training Pipeline - Real optimization:</p> </li> <li>Forward pass through model</li> <li>Chamfer distance loss computation</li> <li>Gradient computation and parameter updates</li> <li> <p>Learning rate scheduling</p> </li> <li> <p>Evaluation Suite - Comprehensive metrics:</p> </li> <li>Diversity score (sample variation)</li> <li>Coverage score (distribution coverage)</li> <li>Quality score (geometric properties)</li> <li>Comparison with benchmarks</li> </ol>"},{"location":"examples/geometric/geometric-benchmark-demo/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>\u2705 PyTorch3D-style ShapeNet dataset loading</li> <li>\u2705 Automatic fallback to synthetic data</li> <li>\u2705 Transformer-based point cloud model</li> <li>\u2705 Chamfer distance loss function</li> <li>\u2705 Adam optimizer with cosine decay schedule</li> <li>\u2705 Complete training loop with real optimization</li> <li>\u2705 Training/validation split with proper evaluation</li> <li>\u2705 Checkpointing and model saving</li> <li>\u2705 Training visualization (loss curves, samples)</li> <li>\u2705 Comprehensive evaluation metrics</li> <li>\u2705 Benchmark comparison</li> <li>\u2705 Production-ready logging and reporting</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Use Real ShapeNet Data</li> </ol> <pre><code>demo_config = {\n    \"dataset\": {\n        \"data_path\": \"./data/shapenet\",\n        \"data_source\": \"auto\",  # Try real data download\n        # ...\n    }\n}\n</code></pre> <ol> <li>Add More Categories</li> </ol> <pre><code>demo_config = {\n    \"dataset\": {\n        \"synsets\": [\n            \"02691156\",  # Airplane\n            \"02958343\",  # Car\n            \"03001627\",  # Chair\n        ],\n        # ...\n    }\n}\n</code></pre> <ol> <li>Increase Model Capacity</li> </ol> <pre><code>demo_config = {\n    \"model\": {\n        \"embed_dim\": 256,     # More expressive\n        \"num_layers\": 8,      # Deeper network\n        \"num_heads\": 16,      # More attention\n    }\n}\n</code></pre> <ol> <li>Longer Training</li> </ol> <pre><code>demo_config = {\n    \"training\": {\n        \"num_epochs\": 200,    # More training\n        \"batch_size\": 16,     # Larger batches (if GPU allows)\n    }\n}\n</code></pre> <ol> <li>Different Optimizers</li> </ol> <pre><code>demo_config = {\n    \"training\": {\n        \"optimizer\": {\n            \"optimizer_type\": \"adamw\",\n            \"weight_decay\": 1e-4,  # More regularization\n        }\n    }\n}\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Advanced Architectures</p> <p>Try PointNet++, DGCNN, or diffusion models</p> <p> Advanced 3D Models</p> </li> <li> <p> Conditional Generation</p> <p>Generate point clouds conditioned on category</p> <p> Conditional 3D</p> </li> <li> <p> Mesh Generation</p> <p>Extend to surface reconstruction and meshing</p> <p> Mesh Models</p> </li> <li> <p> Loss Functions</p> <p>Explore geometric loss functions</p> <p> Loss Examples</p> </li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/geometric/geometric-benchmark-demo/#dataset-download-fails","title":"Dataset Download Fails","text":"<p>Symptom: Error downloading ShapeNet data</p> <p>Solution: The example automatically falls back to synthetic data</p> <pre><code># Synthetic data is generated automatically\n# To try real data:\ndemo_config[\"dataset\"][\"data_source\"] = \"auto\"\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#training-too-slow","title":"Training Too Slow","text":"<p>Symptom: Training takes &gt;20 minutes</p> <p>Solution: Reduce epochs or batch size</p> <pre><code>demo_config[\"training\"][\"num_epochs\"] = 25  # Faster\ndemo_config[\"training\"][\"batch_size\"] = 4   # Less memory\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Symptom: <code>CUDA out of memory</code> error during training</p> <p>Solution: Reduce batch size or model size</p> <pre><code>demo_config[\"training\"][\"batch_size\"] = 4\ndemo_config[\"model\"][\"embed_dim\"] = 64\ndemo_config[\"dataset\"][\"num_points\"] = 512  # Fewer points\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#poor-generation-quality","title":"Poor Generation Quality","text":"<p>Symptom: Generated point clouds look random</p> <p>Cause: Insufficient training or model capacity</p> <p>Solution: Train longer or increase model size</p> <pre><code>demo_config[\"training\"][\"num_epochs\"] = 100\ndemo_config[\"model\"][\"embed_dim\"] = 256\ndemo_config[\"model\"][\"num_layers\"] = 8\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#loss-not-decreasing","title":"Loss Not Decreasing","text":"<p>Symptom: Training loss plateaus or increases</p> <p>Cause: Learning rate too high or optimizer issue</p> <p>Solution: Reduce learning rate or adjust optimizer</p> <pre><code>demo_config[\"training\"][\"optimizer\"][\"learning_rate\"] = 5e-5  # Lower LR\ndemo_config[\"training\"][\"optimizer\"][\"weight_decay\"] = 1e-6   # Less regularization\n</code></pre>"},{"location":"examples/geometric/geometric-benchmark-demo/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/geometric/geometric-benchmark-demo/#documentation","title":"Documentation","text":"<ul> <li>Geometric Benchmark Suite - Complete benchmarking guide</li> <li>Point Cloud Models - Model architecture details</li> <li>Chamfer Distance - Loss function documentation</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#related-examples","title":"Related Examples","text":"<ul> <li>Loss Examples - Geometric loss functions</li> <li>Framework Features Demo - Configuration system</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#papers-and-resources","title":"Papers and Resources","text":"<ul> <li>PointNet: PointNet: Deep Learning on Point Sets (Qi et al., 2017)</li> <li>PointNet++: PointNet++: Deep Hierarchical Feature Learning (Qi et al., 2017)</li> <li>ShapeNet: ShapeNet: An Information-Rich 3D Model Repository (Chang et al., 2015)</li> <li>Point Cloud Transformers: PCT: Point Cloud Transformer (Guo et al., 2021)</li> <li>Chamfer Distance: Learning Representations and Generative Models for 3D Point Clouds</li> </ul>"},{"location":"examples/geometric/geometric-benchmark-demo/#external-tools","title":"External Tools","text":"<ul> <li>PyTorch3D: PyTorch library for 3D deep learning</li> <li>Open3D: Modern library for 3D data processing</li> <li>Kaolin: PyTorch library for 3D deep learning research</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/","title":"Geometric Model Loss Functions Demo","text":"<p>Comprehensive demonstration of loss functions for geometric models including point clouds, meshes, and voxel grids.</p>"},{"location":"examples/geometric/geometric-losses-demo/#files","title":"Files","text":"<ul> <li>Python Script: <code>geometric_losses_demo.py</code></li> <li>Jupyter Notebook: <code>geometric_losses_demo.ipynb</code></li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#quick-start","title":"Quick Start","text":"<pre><code># Run the Python script\npython examples/generative_models/geometric/geometric_losses_demo.py\n\n# Or use Jupyter notebook\njupyter notebook examples/generative_models/geometric/geometric_losses_demo.ipynb\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#overview","title":"Overview","text":"<p>This example provides a comprehensive tour of loss functions used for different 3D geometric representations. Understanding these losses is crucial for training effective generative models for 3D data.</p>"},{"location":"examples/geometric/geometric-losses-demo/#learning-objectives","title":"Learning Objectives","text":"<ul> <li> Understand permutation-invariant losses for point clouds</li> <li> Learn composite loss functions for meshes</li> <li> Explore specialized losses for voxel grids</li> <li> See how to configure and balance different loss components</li> <li> Compare different loss types for the same representation</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of 3D representations (point clouds, meshes, voxels)</li> <li>Familiarity with loss functions in machine learning</li> <li>Knowledge of JAX and Artifex's configuration system</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#background-3d-representations-and-their-losses","title":"Background: 3D Representations and Their Losses","text":""},{"location":"examples/geometric/geometric-losses-demo/#why-geometric-losses-matter","title":"Why Geometric Losses Matter","text":"<p>Standard image losses (MSE, L1) don't work well for 3D data because:</p> <ol> <li>Point clouds are unordered: Permuting points shouldn't change the loss</li> <li>Meshes have topology: Need to preserve connectivity and smoothness</li> <li>Voxels are sparse: Most voxels are empty, causing class imbalance</li> </ol>"},{"location":"examples/geometric/geometric-losses-demo/#the-three-representations","title":"The Three Representations","text":""},{"location":"examples/geometric/geometric-losses-demo/#point-clouds","title":"Point Clouds","text":"<p>Unordered sets of 3D points: \\({(x_i, y_i, z_i)}_{i=1}^N\\)</p> <ul> <li>Compact representation</li> <li>Permutation invariance required</li> <li>Use Chamfer Distance or Earth Mover's Distance</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#meshes","title":"Meshes","text":"<p>Vertices connected by edges and faces</p> <ul> <li>Explicit topology</li> <li>Need vertex, normal, and edge losses</li> <li>Balancing multiple objectives</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#voxels","title":"Voxels","text":"<p>Regular 3D grids with occupancy values</p> <ul> <li>Like 3D images</li> <li>Sparse (mostly empty)</li> <li>Use BCE, Focal, or Dice loss</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/geometric/geometric-losses-demo/#1-point-cloud-losses","title":"1. Point Cloud Losses","text":""},{"location":"examples/geometric/geometric-losses-demo/#chamfer-distance","title":"Chamfer Distance","text":"<p>The Chamfer Distance is the workhorse of point cloud generation. It measures how well two point sets match by finding nearest neighbors:</p> \\[ \\mathcal{L}_{\\text{CD}}(X, Y) = \\frac{1}{|X|}\\sum_{x \\in X} \\min_{y \\in Y} \\|x - y\\|^2 + \\frac{1}{|Y|}\\sum_{y \\in Y} \\min_{x \\in X} \\|x - y\\|^2 \\] <p>Key properties:</p> <ul> <li>Permutation invariant</li> <li>Fast to compute (O(N\u00b2 ) with optimizations)</li> <li>Good for most applications</li> </ul> <pre><code>from artifex.generative_models.core.configuration import (\n    PointCloudConfig,\n    PointCloudNetworkConfig,\n)\n\nnetwork_config = PointCloudNetworkConfig(\n    name=\"chamfer_network\",\n    hidden_dims=(64,),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=64,\n    num_heads=4,\n    num_layers=2,\n    dropout_rate=0.1,\n)\n\nchamfer_config = PointCloudConfig(\n    name=\"chamfer_point_cloud\",\n    network=network_config,\n    num_points=125,\n    loss_type=\"chamfer\",  # Chamfer distance loss\n    dropout_rate=0.1,\n)\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#earth-movers-distance-emd","title":"Earth Mover's Distance (EMD)","text":"<p>EMD finds the optimal transport plan between point sets. More accurate but slower:</p> \\[ \\mathcal{L}_{\\text{EMD}}(X, Y) = \\min_{\\phi: X \\to Y} \\sum_{x \\in X} \\|x - \\phi(x)\\| \\] <p>Where \\(\\phi\\) is a bijection between X and Y.</p> <p>When to use:</p> <ul> <li>Quality is more important than speed</li> <li>Small point clouds (&lt;1000 points)</li> <li>Fine geometric details matter</li> </ul> <pre><code># Same network config, different loss type\nearth_mover_config = PointCloudConfig(\n    name=\"earth_mover_point_cloud\",\n    network=network_config,\n    num_points=125,\n    loss_type=\"earth_mover\",  # EMD loss\n    dropout_rate=0.1,\n)\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#2-mesh-losses","title":"2. Mesh Losses","text":"<p>Meshes require balancing multiple geometric properties:</p> \\[ \\mathcal{L}_{\\text{mesh}} = w_v \\mathcal{L}_{\\text{vertex}} + w_n \\mathcal{L}_{\\text{normal}} + w_e \\mathcal{L}_{\\text{edge}} \\] <p>Vertex Loss: L2 distance between vertex positions</p> \\[ \\mathcal{L}_{\\text{vertex}} = \\|\\mathbf{V}_{\\text{pred}} - \\mathbf{V}_{\\text{true}}\\|^2 \\] <p>Normal Loss: Ensures surface smoothness</p> \\[ \\mathcal{L}_{\\text{normal}} = 1 - \\frac{1}{|F|}\\sum_{f \\in F} \\mathbf{n}_{\\text{pred}}^f \\cdot \\mathbf{n}_{\\text{true}}^f \\] <p>Edge Loss: Preserves edge lengths</p> \\[ \\mathcal{L}_{\\text{edge}} = \\sum_{(i,j) \\in E} (|\\mathbf{v}_i - \\mathbf{v}_j|_{\\text{pred}} - |\\mathbf{v}_i - \\mathbf{v}_j|_{\\text{true}})^2 \\]"},{"location":"examples/geometric/geometric-losses-demo/#configuring-weights","title":"Configuring Weights","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    MeshConfig,\n    MeshNetworkConfig,\n)\n\nmesh_network = MeshNetworkConfig(\n    name=\"mesh_network\",\n    hidden_dims=(128, 64),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n)\n\n# Smooth surfaces (e.g., CAD models)\nnormal_config = MeshConfig(\n    name=\"smooth_mesh\",\n    network=mesh_network,\n    num_vertices=512,\n    vertex_loss_weight=0.5,   # Reduce vertex constraint\n    normal_loss_weight=1.0,   # Emphasize smoothness\n    edge_loss_weight=0.1,     # Light edge preservation\n)\n\n# Sharp edges (e.g., furniture)\nedge_config = MeshConfig(\n    name=\"sharp_mesh\",\n    network=mesh_network,\n    num_vertices=512,\n    vertex_loss_weight=0.5,\n    normal_loss_weight=0.1,   # Less smoothing\n    edge_loss_weight=1.0,     # Strong edge preservation\n)\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#3-voxel-losses","title":"3. Voxel Losses","text":"<p>Voxel grids can use image-like losses, but some are better for 3D:</p>"},{"location":"examples/geometric/geometric-losses-demo/#binary-cross-entropy-bce","title":"Binary Cross-Entropy (BCE)","text":"<p>Standard loss for binary voxels:</p> \\[ \\mathcal{L}_{\\text{BCE}} = -\\frac{1}{N}\\sum_{i=1}^N [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)] \\] <p>Best for:</p> <ul> <li>Balanced datasets (50% occupied voxels)</li> <li>Dense 3D shapes</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#focal-loss","title":"Focal Loss","text":"<p>Down-weights easy examples, focuses on hard ones:</p> \\[ \\mathcal{L}_{\\text{focal}} = -\\frac{1}{N}\\sum_{i=1}^N (1-p_t)^\\gamma \\log(p_t) \\] <p>Where \\(p_t = \\hat{y}_i\\) if \\(y_i=1\\), else \\(1-\\hat{y}_i\\).</p> <p>Best for:</p> <ul> <li>Imbalanced data (sparse objects)</li> <li>\\(\\gamma=2.0\\) is typical</li> <li>Higher \\(\\gamma\\) \u2192 more focus on hard examples</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#dice-loss","title":"Dice Loss","text":"<p>Directly optimizes overlap (similar to IoU):</p> \\[ \\mathcal{L}_{\\text{dice}} = 1 - \\frac{2\\sum_i y_i \\hat{y}_i + \\epsilon}{\\sum_i y_i + \\sum_i \\hat{y}_i + \\epsilon} \\] <p>Best for:</p> <ul> <li>Segmentation-like tasks</li> <li>Maximizing shape overlap</li> <li>Handles class imbalance well</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#comparison","title":"Comparison","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    VoxelConfig,\n    VoxelNetworkConfig,\n)\n\nvoxel_network = VoxelNetworkConfig(\n    name=\"voxel_network\",\n    hidden_dims=(64, 32),  # Required base config\n    activation=\"relu\",\n    base_channels=32,  # Base number of 3D CNN channels\n    num_layers=4,       # Number of 3D convolutional layers\n)\n\n# Dense shapes \u2192 BCE\nbce_config = VoxelConfig(\n    name=\"dense_voxel\",\n    network=voxel_network,\n    resolution=16,\n    loss_type=\"bce\",\n)\n\n# Sparse shapes \u2192 Focal\nfocal_config = VoxelConfig(\n    name=\"sparse_voxel\",\n    network=voxel_network,\n    resolution=16,\n    loss_type=\"focal\",\n    focal_gamma=2.0,  # Adjust based on sparsity\n)\n\n# Overlap optimization \u2192 Dice\ndice_config = VoxelConfig(\n    name=\"overlap_voxel\",\n    network=voxel_network,\n    resolution=16,\n    loss_type=\"dice\",\n)\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#expected-output","title":"Expected Output","text":"<pre><code>===== Point Cloud Loss Functions Demo =====\nChamfer distance loss: {'total_loss': 2.92, 'mse_loss': 2.92}\nEarth Mover distance loss: {'total_loss': 3.61, 'mse_loss': 3.61}\n\n===== Mesh Loss Functions Demo =====\nDefault model vertex weight: 1.0\nDefault model normal weight: 1.0\nDefault model edge weight: 1.0\nNormal-focused model vertex weight: 0.5\nNormal-focused model normal weight: 1.0\nNormal-focused model edge weight: 0.1\n\n===== Voxel Loss Functions Demo =====\nBinary cross-entropy loss: {'total_loss': 0.68, ...}\nFocal loss (gamma=2.0): {'total_loss': 0.15, ...}\nDice loss: {'total_loss': 0.42, ...}\n\nLoss function demos completed!\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/geometric/geometric-losses-demo/#permutation-invariance","title":"Permutation Invariance","text":"<p>Point cloud losses must be invariant to point ordering:</p> <pre><code># These should have the same loss\npoints_A = [[0, 0, 0], [1, 1, 1], [2, 2, 2]]\npoints_B = [[2, 2, 2], [0, 0, 0], [1, 1, 1]]  # Same points, different order\n\nloss(points_A, target) == loss(points_B, target)  # Must be true\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#loss-component-balancing","title":"Loss Component Balancing","text":"<p>For composite losses (meshes), balance is key:</p> <ul> <li>Start with equal weights (1.0, 1.0, 1.0)</li> <li>Identify the most important property (smoothness vs sharp edges)</li> <li>Increase weight for that component</li> <li>Reduce others proportionally</li> <li>Validate on test shapes</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#class-imbalance-in-voxels","title":"Class Imbalance in Voxels","text":"<p>Voxel grids are typically 90-99% empty:</p> <pre><code># Sparse object (5% occupied)\noccupancy_ratio = 0.05\n\n# BCE: Treats all voxels equally \u2192 biased toward empty\n# Focal (\u03b3=2): Down-weights easy empties \u2192 balanced\n# Dice: Focuses on overlap \u2192 invariant to sparsity\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Compare Chamfer vs EMD</li> </ol> <p>Generate the same point cloud with both losses and compare quality/speed</p> <ol> <li>Mesh Weight Tuning</li> </ol> <p>Try different weight combinations for different mesh types (organic vs geometric)</p> <ol> <li>Voxel Sparsity Study</li> </ol> <p>Compare BCE, Focal, Dice on grids with 1%, 10%, 50% occupancy</p> <ol> <li>Focal Gamma Sweep</li> </ol> <p>Test \\(\\gamma \\in [0.5, 1.0, 2.0, 5.0]\\) on sparse voxels</p> <ol> <li>Visualization</li> </ol> <p>Plot generated shapes with different losses to see visual differences</p>"},{"location":"examples/geometric/geometric-losses-demo/#next-steps","title":"Next Steps","text":"<p>Explore related examples to deepen your understanding:</p> <ul> <li> <p> Geometric Models Overview</p> <p>Learn about the three geometric representations and when to use each.</p> <p> geometric_models_demo.py</p> </li> <li> <p> Point Cloud Generation</p> <p>Generate and visualize 3D point clouds with transformers.</p> <p> simple_point_cloud_example.py</p> </li> <li> <p> Geometric Benchmarks</p> <p>Evaluate geometric models with specialized metrics.</p> <p> geometric_benchmark_demo.py</p> </li> <li> <p> Protein Modeling</p> <p>Apply geometric models to protein structure prediction.</p> <p> ../protein/protein-point-cloud-example.md</p> </li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/geometric/geometric-losses-demo/#high-chamfer-distance","title":"High Chamfer Distance","text":"<p>Problem: Chamfer loss is unexpectedly high.</p> <p>Solutions:</p> <ol> <li>Check point cloud normalization (scale to [-1, 1])</li> <li>Verify number of points matches between pred and target</li> <li>Ensure points are in same coordinate system</li> </ol>"},{"location":"examples/geometric/geometric-losses-demo/#mesh-loss-imbalance","title":"Mesh Loss Imbalance","text":"<p>Problem: One loss component dominates others.</p> <p>Solutions:</p> <ol> <li>Normalize each loss to [0, 1] range before weighting</li> <li>Use relative weights (sum to 1.0)</li> <li>Monitor individual losses during training</li> </ol>"},{"location":"examples/geometric/geometric-losses-demo/#voxel-loss-not-decreasing","title":"Voxel Loss Not Decreasing","text":"<p>Problem: Loss plateaus early in training.</p> <p>Solutions:</p> <ol> <li>Switch from BCE to Focal for sparse grids</li> <li>Adjust focal gamma (try 2.0 \u2192 3.0)</li> <li>Check for label imbalance (&gt;95% empty \u2192 use Dice)</li> </ol>"},{"location":"examples/geometric/geometric-losses-demo/#out-of-memory","title":"Out of Memory","text":"<p>Problem: Voxel models run out of GPU memory.</p> <p>Solutions:</p> <ol> <li>Reduce voxel resolution (32\u00b3 \u2192 16\u00b3)</li> <li>Reduce batch size</li> <li>Use gradient checkpointing</li> <li>Consider point cloud representation instead</li> </ol>"},{"location":"examples/geometric/geometric-losses-demo/#additional-resources","title":"Additional Resources","text":"<ul> <li>Loss Functions API</li> <li>Point Cloud Models</li> </ul>"},{"location":"examples/geometric/geometric-losses-demo/#citation","title":"Citation","text":"<p>If you use these loss functions in your research, please cite:</p> <pre><code>@software{artifex2025,\n  title={Artifex: Modular Generative Modeling Library},\n  author={Artifex Contributors},\n  year={2025},\n  url={https://github.com/avitai/artifex}\n}\n</code></pre>"},{"location":"examples/geometric/geometric-losses-demo/#references","title":"References","text":"<ol> <li>Chamfer Distance: Fan et al., \"A Point Set Generation Network for 3D Object Reconstruction from a Single Image\", CVPR 2017</li> <li>Earth Mover's Distance: Rubner et al., \"The Earth Mover's Distance as a Metric for Image Retrieval\", IJCV 2000</li> <li>Focal Loss: Lin et al., \"Focal Loss for Dense Object Detection\", ICCV 2017</li> <li>Dice Loss: Milletari et al., \"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation\", 3DV 2016</li> </ol>"},{"location":"examples/geometric/geometric-models-demo/","title":"Geometric Models Demo","text":"Beginner \u26a1 5-10 seconds \ud83d\udcd3 Dual Format <p>A quick reference guide demonstrating how to configure and instantiate three types of geometric models in Artifex: point clouds, meshes, and voxels.</p>"},{"location":"examples/geometric/geometric-models-demo/#files","title":"Files","text":"<ul> <li>Python Script: <code>geometric_models_demo.py</code></li> <li>Jupyter Notebook: <code>geometric_models_demo.ipynb</code></li> </ul>"},{"location":"examples/geometric/geometric-models-demo/#quick-start","title":"Quick Start","text":"<pre><code># Clone and setup\ncd artifex\nsource activate.sh\n\n# Run Python script\npython examples/generative_models/geometric/geometric_models_demo.py\n\n# Or use Jupyter notebook\njupyter notebook examples/generative_models/geometric/geometric_models_demo.ipynb\n</code></pre>"},{"location":"examples/geometric/geometric-models-demo/#overview","title":"Overview","text":"<p>This example provides a concise demonstration of Artifex's three main geometric representations:</p>"},{"location":"examples/geometric/geometric-models-demo/#learning-objectives","title":"Learning Objectives","text":"<ul> <li> Understand point cloud, mesh, and voxel representations</li> <li> Configure models using frozen dataclass configs (<code>PointCloudConfig</code>, <code>MeshConfig</code>, <code>VoxelConfig</code>)</li> <li> Use the unified factory pattern with <code>create_model()</code></li> <li> Understand model-specific parameters for each geometry type</li> </ul>"},{"location":"examples/geometric/geometric-models-demo/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of 3D geometric representations</li> <li>Familiarity with JAX and Flax NNX</li> <li>Artifex installed and activated</li> </ul>"},{"location":"examples/geometric/geometric-models-demo/#geometric-representations-explained","title":"Geometric Representations Explained","text":""},{"location":"examples/geometric/geometric-models-demo/#1-point-clouds","title":"1. Point Clouds","text":"<p>Unordered sets of 3D points - flexible, permutation-invariant</p> <pre><code>from artifex.generative_models.core.configuration import (\n    PointCloudConfig,\n    PointCloudNetworkConfig,\n)\n\n# Network config for point cloud transformer\nnetwork_config = PointCloudNetworkConfig(\n    name=\"point_cloud_network\",\n    hidden_dims=(128, 128),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=128,\n    num_heads=4,\n    num_layers=4,\n    dropout_rate=0.1,\n)\n\n# Point cloud config with nested network\npoint_cloud_config = PointCloudConfig(\n    name=\"demo_point_cloud\",\n    network=network_config,\n    num_points=512,\n    loss_type=\"chamfer\",\n    dropout_rate=0.1,\n)\npoint_cloud_model = create_model(point_cloud_config, rngs=rngs)\n</code></pre> <p>Use cases:</p> <ul> <li>LiDAR data processing</li> <li>3D object detection</li> <li>Molecular structure modeling (proteins, molecules)</li> </ul> <p>Key parameters:</p> <ul> <li><code>num_points</code>: Number of points in the cloud</li> <li><code>embed_dim</code>: Embedding dimension for features</li> <li><code>num_layers</code>: Network depth</li> <li><code>loss_type</code>: Distance metric (chamfer, earth mover's distance)</li> </ul>"},{"location":"examples/geometric/geometric-models-demo/#2-mesh-models","title":"2. Mesh Models","text":"<p>Connected vertex structures with topology - surface-oriented</p> <pre><code>from artifex.generative_models.core.configuration import (\n    MeshConfig,\n    MeshNetworkConfig,\n)\n\n# Network config for mesh processing\nmesh_network_config = MeshNetworkConfig(\n    name=\"mesh_network\",\n    hidden_dims=(256, 128, 64),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n)\n\n# Mesh config with nested network and loss weights\nmesh_config = MeshConfig(\n    name=\"demo_mesh\",\n    network=mesh_network_config,\n    num_vertices=512,\n    template_type=\"sphere\",\n    vertex_loss_weight=1.0,\n    normal_loss_weight=0.2,\n    edge_loss_weight=0.1,\n)\nmesh_model = create_model(mesh_config, rngs=rngs)\n</code></pre> <p>Use cases:</p> <ul> <li>3D graphics and rendering</li> <li>Shape analysis and generation</li> <li>Surface reconstruction</li> </ul> <p>Key parameters:</p> <ul> <li><code>num_vertices</code>: Number of mesh vertices</li> <li><code>template_type</code>: Initial mesh template (sphere, cube, etc.)</li> <li>Loss weights balance geometric properties:</li> <li><code>vertex_loss_weight</code>: Vertex position accuracy</li> <li><code>normal_loss_weight</code>: Surface normal consistency</li> <li><code>edge_loss_weight</code>: Edge length regularization</li> </ul>"},{"location":"examples/geometric/geometric-models-demo/#3-voxel-models","title":"3. Voxel Models","text":"<p>Regular 3D grids - easy to process with CNNs</p> <pre><code>from artifex.generative_models.core.configuration import (\n    VoxelConfig,\n    VoxelNetworkConfig,\n)\n\n# Network config for voxel 3D CNN\nvoxel_network_config = VoxelNetworkConfig(\n    name=\"voxel_network\",\n    hidden_dims=(128, 64),  # Required base config\n    activation=\"relu\",\n    base_channels=64,  # Base number of 3D CNN channels\n    num_layers=4,       # Number of 3D convolutional layers\n)\n\n# Voxel config with nested network\nvoxel_config = VoxelConfig(\n    name=\"demo_voxel\",\n    network=voxel_network_config,\n    resolution=16,\n    use_conditioning=True,\n    conditioning_dim=10,\n    loss_type=\"focal\",\n    focal_gamma=2.0,\n)\nvoxel_model = create_model(voxel_config, rngs=rngs)\n</code></pre> <p>Use cases:</p> <ul> <li>Medical imaging (CT, MRI scans)</li> <li>3D scene understanding</li> <li>Volumetric shape generation</li> </ul> <p>Key parameters:</p> <ul> <li><code>resolution</code>: Grid resolution (16\u00b3 = 4,096 voxels)</li> <li><code>channels</code>: Multi-scale architecture layers</li> <li><code>use_conditioning</code>: Enable class-conditional generation</li> <li><code>loss_type</code>: \"focal\" handles sparse voxel data</li> <li><code>focal_gamma</code>: Focus on hard-to-classify voxels (2.0 is standard)</li> </ul>"},{"location":"examples/geometric/geometric-models-demo/#expected-output","title":"Expected Output","text":"<pre><code>Creating point cloud model...\nCreated model: PointCloudModel\nSample shape: (1, 512, 3)\n\nCreating mesh model...\nCreated model: MeshModel\n\nCreating voxel model with conditioning...\nCreated model: VoxelModel\n\nDemo completed successfully!\n</code></pre>"},{"location":"examples/geometric/geometric-models-demo/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/geometric/geometric-models-demo/#step-1-setup-random-number-generation","title":"Step 1: Setup Random Number Generation","text":"<pre><code>rng = jax.random.PRNGKey(42)\nrngs = nnx.Rngs(params=rng)\n</code></pre> <p>Initialize RNG using Flax NNX patterns for reproducible model creation.</p>"},{"location":"examples/geometric/geometric-models-demo/#step-2-create-models-via-factory","title":"Step 2: Create Models via Factory","text":"<p>All three models use the unified <code>create_model()</code> factory pattern:</p> <pre><code>model = create_model(config, rngs=rngs)\n</code></pre> <p>This abstracts away model-specific initialization details and provides a consistent API.</p>"},{"location":"examples/geometric/geometric-models-demo/#step-3-generate-samples-optional","title":"Step 3: Generate Samples (Optional)","text":"<pre><code>sample = point_cloud_model.sample(1, rngs=rngs)\n# shape: (1, 512, 3) - batch_size=1, num_points=512, xyz=3\n</code></pre>"},{"location":"examples/geometric/geometric-models-demo/#choosing-the-right-representation","title":"Choosing the Right Representation","text":"Representation When to Use Strengths Limitations Point Cloud Raw sensor data, irregular shapes, molecular structures Flexible, no topology required, permutation-invariant No explicit surface, harder to render Mesh Graphics, animation, smooth surfaces Explicit topology, efficient rendering, smooth surfaces Requires consistent topology, harder to optimize Voxel Medical imaging, volumetric data, regular grids Easy CNN processing, regular structure Memory-intensive, discretization artifacts"},{"location":"examples/geometric/geometric-models-demo/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Change loss types - Try different distance metrics for point clouds:</li> </ol> <pre><code>\"loss_type\": \"emd\"  # Earth Mover's Distance (slower but more accurate)\n</code></pre> <ol> <li>Adjust mesh templates - Experiment with different starting shapes:</li> </ol> <pre><code>\"template_type\": \"cube\"  # or \"icosahedron\", \"octahedron\"\n</code></pre> <ol> <li>Scale voxel resolution - Balance memory vs. detail:</li> </ol> <pre><code>\"resolution\": 32  # 32\u00b3 = 32,768 voxels (8x more memory)\n</code></pre> <ol> <li>Conditional generation - Create class-specific voxel shapes:</li> </ol> <pre><code>labels = jnp.array([0, 1, 5, 9])  # Different classes\nvoxel_model.generate(labels, rngs=rngs)\n</code></pre>"},{"location":"examples/geometric/geometric-models-demo/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Point Cloud Deep Dive</p> <p>Learn advanced point cloud techniques with PointNet and Set Abstraction</p> <p> simple_point_cloud_example.py</p> </li> <li> <p> Protein Modeling</p> <p>Apply point clouds to protein structure prediction</p> <p> protein_point_cloud_example.py</p> </li> <li> <p> Geometric Losses</p> <p>Explore specialized losses for geometric data</p> <p> geometric_losses_demo.py</p> </li> <li> <p> Geometric Benchmarks</p> <p>Comprehensive evaluation on geometric tasks</p> <p> geometric_benchmark_demo.py</p> </li> </ul>"},{"location":"examples/geometric/geometric-models-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/geometric/geometric-models-demo/#backend-cuda-is-not-in-the-list-of-known-backends","title":"\"Backend 'cuda' is not in the list of known backends\"","text":"<p>Solution: JAX is looking for CUDA but can't find it. Run with CPU:</p> <pre><code>JAX_PLATFORMS=cpu python examples/generative_models/geometric/geometric_models_demo.py\n</code></pre>"},{"location":"examples/geometric/geometric-models-demo/#modulenotfounderror-no-module-named-artifex","title":"\"ModuleNotFoundError: No module named 'artifex'\"","text":"<p>Solution: Activate the environment first:</p> <pre><code>source activate.sh\npython examples/generative_models/geometric/geometric_models_demo.py\n</code></pre>"},{"location":"examples/geometric/geometric-models-demo/#memory-errors-with-high-resolution-voxels","title":"Memory errors with high resolution voxels","text":"<p>Solution: Reduce voxel resolution or use gradient checkpointing:</p> <pre><code>\"resolution\": 8,  # Lower resolution (8\u00b3 = 512 voxels)\n</code></pre>"},{"location":"examples/geometric/geometric-models-demo/#additional-resources","title":"Additional Resources","text":"<ul> <li>Configuration Reference</li> <li>Factory Pattern Guide</li> <li>JAX Geometric Processing</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/","title":"Simple Point Cloud Example","text":"Beginner \u26a1 10-15 seconds \ud83d\udcd3 Dual Format <p>Learn how to generate and visualize 3D point clouds using Artifex's PointCloudModel with transformer-based architecture.</p>"},{"location":"examples/geometric/simple-point-cloud-example/#files","title":"Files","text":"<ul> <li>Python Script: <code>simple_point_cloud_example.py</code></li> <li>Jupyter Notebook: <code>simple_point_cloud_example.ipynb</code></li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#quick-start","title":"Quick Start","text":"<pre><code># Clone and setup\ncd artifex\nsource activate.sh\n\n# Run Python script\npython examples/generative_models/geometric/simple_point_cloud_example.py\n\n# Or use Jupyter notebook\njupyter notebook examples/generative_models/geometric/simple_point_cloud_example.ipynb\n</code></pre>"},{"location":"examples/geometric/simple-point-cloud-example/#overview","title":"Overview","text":"<p>This tutorial teaches you how to work with point clouds\u2014the fundamental representation for 3D data in machine learning. Point clouds are unordered sets of 3D coordinates that represent objects, scenes, or molecular structures.</p>"},{"location":"examples/geometric/simple-point-cloud-example/#learning-objectives","title":"Learning Objectives","text":"<ul> <li> Understand point cloud representation and properties</li> <li> Configure PointCloudModel with transformer architecture</li> <li> Generate 3D point clouds from learned distributions</li> <li> Visualize point clouds in 3D space</li> <li> Control generation diversity with temperature</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of 3D coordinates (x, y, z)</li> <li>Familiarity with JAX and Flax NNX</li> <li>Basic knowledge of attention mechanisms (helpful but not required)</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#what-are-point-clouds","title":"What Are Point Clouds?","text":"<p>Point clouds are collections of 3D points that represent the shape or structure of objects:</p> <pre><code>Point Cloud = {(x\u2081, y\u2081, z\u2081), (x\u2082, y\u2082, z\u2082), ..., (x\u2099, y\u2099, z\u2099)}\n</code></pre>"},{"location":"examples/geometric/simple-point-cloud-example/#key-properties","title":"Key Properties","text":"<ol> <li>Unordered (Permutation-Invariant): The order of points doesn't matter</li> <li><code>{A, B, C}</code> is the same as <code>{C, A, B}</code></li> <li> <p>This is why transformers work well (they're permutation-invariant)</p> </li> <li> <p>Sparse: Represents surfaces without filling volumes</p> </li> <li>A sphere needs only surface points, not interior</li> <li> <p>More efficient than voxels for many tasks</p> </li> <li> <p>Flexible: Can represent arbitrary shapes</p> </li> <li>No fixed topology required</li> <li>Handles complex, irregular geometries</li> </ol>"},{"location":"examples/geometric/simple-point-cloud-example/#common-sources","title":"Common Sources","text":"Source Example Resolution LiDAR Autonomous vehicles 10K-1M points 3D Scanners Industrial inspection 100K-10M points Depth Cameras Robotics, AR/VR 10K-100K points Molecular Protein structures 100-10K atoms Photogrammetry 3D reconstruction 100K-10M points"},{"location":"examples/geometric/simple-point-cloud-example/#transformer-architecture-for-point-clouds","title":"Transformer Architecture for Point Clouds","text":""},{"location":"examples/geometric/simple-point-cloud-example/#why-transformers","title":"Why Transformers?","text":"<p>Traditional CNNs require regular grids. Point clouds are irregular, so we need architectures that:</p> <ol> <li>Handle variable-size inputs: Different objects have different numbers of points</li> <li>Are permutation-invariant: Point order shouldn't matter</li> <li>Model long-range relationships: Distant points may be related</li> </ol> <p>Transformers satisfy all three requirements!</p>"},{"location":"examples/geometric/simple-point-cloud-example/#architecture-components","title":"Architecture Components","text":"<pre><code>Input Points (N, 3)\n      \u2193\nPoint Embedding \u2192 (N, 128)\n      \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Transformer     \u2502 \u2190 Self-Attention Layers (\u00d73)\n\u2502  Layer 1        \u2502    Each point attends to all others\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Transformer     \u2502 \u2190 Multi-Head (\u00d74)\n\u2502  Layer 2        \u2502    Different attention patterns\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Transformer     \u2502 \u2190 Layer Normalization\n\u2502  Layer 3        \u2502    Stable training\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2193\nOutput Points (N, 3)\n</code></pre>"},{"location":"examples/geometric/simple-point-cloud-example/#key-parameters","title":"Key Parameters","text":"<ul> <li><code>num_points</code>: 512 - Number of 3D points</li> <li><code>embed_dim</code>: 128 - Feature dimension for attention</li> <li><code>num_layers</code>: 3 - Transformer depth</li> <li><code>num_heads</code>: 4 - Multi-head attention heads</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/geometric/simple-point-cloud-example/#step-1-setup-and-imports","title":"Step 1: Setup and Imports","text":"<pre><code>import jax\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration import (\n    PointCloudConfig,\n    PointCloudNetworkConfig,\n)\nfrom artifex.generative_models.models.geometric import PointCloudModel\n</code></pre> <p>We use:</p> <ul> <li>JAX: Fast numerical computing with GPU support</li> <li>Flax NNX: Modern neural network framework</li> <li>Matplotlib: 3D visualization</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#step-2-configure-the-model","title":"Step 2: Configure the Model","text":"<pre><code># Create network config for the point cloud transformer\nnetwork_config = PointCloudNetworkConfig(\n    name=\"point_cloud_network\",\n    hidden_dims=(128, 128, 128),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=128,      # Feature dimension\n    num_heads=4,        # Attention heads\n    num_layers=3,       # Transformer depth\n    dropout_rate=0.1,   # Regularization\n)\n\n# Create point cloud config with nested network config\nconfig = PointCloudConfig(\n    name=\"point_cloud_generator\",\n    network=network_config,\n    num_points=512,     # Number of points\n    dropout_rate=0.1,\n)\n</code></pre> <p>Configuration breakdown:</p> <ul> <li><code>input_dim</code> &amp; <code>output_dim</code>: Point cloud shape (points \u00d7 dimensions)</li> <li><code>hidden_dims</code>: Internal processing dimensions</li> <li><code>parameters</code>: Model-specific settings</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#step-3-create-the-model","title":"Step 3: Create the Model","text":"<pre><code>rngs = nnx.Rngs(params=jax.random.key(42))\nmodel = PointCloudModel(config=config, rngs=rngs)\n</code></pre> <p>The model initializes its transformer layers with random weights.</p>"},{"location":"examples/geometric/simple-point-cloud-example/#step-4-generate-point-clouds","title":"Step 4: Generate Point Clouds","text":"<pre><code>point_clouds = model.generate(\n    rngs=rngs,\n    n_samples=2,        # Generate 2 point clouds\n    temperature=0.8,    # Control diversity\n)\n</code></pre> <p>Temperature effects:</p> <ul> <li>0.5-0.7: Focused, consistent samples</li> <li>0.8-1.0: Balanced diversity \u2190 Recommended</li> <li>1.0+: High diversity, may be noisy</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#step-5-visualize-in-3d","title":"Step 5: Visualize in 3D","text":"<pre><code>def plot_point_cloud(points, filename=None):\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection=\"3d\")\n\n    # Color by distance from origin\n    norm = np.sqrt(np.sum(points**2, axis=1))\n    norm = (norm - norm.min()) / (norm.max() - norm.min() + 1e-8)\n\n    ax.scatter(points[:, 0], points[:, 1], points[:, 2],\n               c=norm, cmap=\"viridis\", s=20, alpha=0.7)\n    plt.colorbar(scatter)\n    return fig\n</code></pre> <p>The visualization:</p> <ul> <li>Projects 3D points onto 2D screen</li> <li>Colors points by distance from origin</li> <li>Saves plots to <code>examples_output/</code> directory</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#expected-output","title":"Expected Output","text":"<pre><code>Creating point cloud model...\nGenerating point clouds...\nVisualizing point clouds...\nExample completed! Point clouds saved as PNG files.\n</code></pre> <p>Generated files:</p> <ul> <li><code>examples_output/point_cloud_1.png</code> - First generated point cloud</li> <li><code>examples_output/point_cloud_2.png</code> - Second generated point cloud</li> </ul> <p>Each visualization shows a 3D scatter plot with:</p> <ul> <li>X, Y, Z axes labeled</li> <li>Color gradient indicating spatial distribution</li> <li>512 points representing the generated shape</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#experiments-to-try","title":"Experiments to Try","text":""},{"location":"examples/geometric/simple-point-cloud-example/#1-vary-number-of-points","title":"1. Vary Number of Points","text":"<pre><code># Sparse point cloud (faster, less detail)\n\"num_points\": 256\n\n# Dense point cloud (slower, more detail)\n\"num_points\": 1024\n</code></pre> <p>Tradeoff: More points = better shape representation but slower generation</p>"},{"location":"examples/geometric/simple-point-cloud-example/#2-adjust-temperature","title":"2. Adjust Temperature","text":"<pre><code># Low temperature (focused samples)\ntemperature=0.6\n\n# High temperature (diverse samples)\ntemperature=1.2\n</code></pre> <p>Try it: Generate 5 samples at different temperatures and compare diversity</p>"},{"location":"examples/geometric/simple-point-cloud-example/#3-modify-architecture-depth","title":"3. Modify Architecture Depth","text":"<pre><code># Shallow network (faster, simpler patterns)\n\"num_layers\": 2\n\n# Deep network (slower, complex patterns)\n\"num_layers\": 6\n</code></pre> <p>Note: Deeper networks may need more training data</p>"},{"location":"examples/geometric/simple-point-cloud-example/#4-multi-head-attention","title":"4. Multi-Head Attention","text":"<pre><code># Fewer heads (simpler attention)\n\"num_heads\": 2\n\n# More heads (richer attention patterns)\n\"num_heads\": 8\n</code></pre> <p>Heads must divide embed_dim evenly: e.g., 128 \u00f7 4 = 32 \u2713</p>"},{"location":"examples/geometric/simple-point-cloud-example/#understanding-point-cloud-applications","title":"Understanding Point Cloud Applications","text":""},{"location":"examples/geometric/simple-point-cloud-example/#1-autonomous-vehicles","title":"1. Autonomous Vehicles","text":"<p>LiDAR sensors generate point clouds for:</p> <ul> <li>Obstacle detection</li> <li>Lane tracking</li> <li>3D scene understanding</li> </ul> <p>Typical setup: 64-128 laser beams \u2192 100K+ points per second</p>"},{"location":"examples/geometric/simple-point-cloud-example/#2-robotics","title":"2. Robotics","text":"<p>Point clouds enable:</p> <ul> <li>Object grasping (find grip points)</li> <li>Navigation (3D mapping)</li> <li>Human-robot interaction (gesture recognition)</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#3-molecular-modeling","title":"3. Molecular Modeling","text":"<p>Proteins as point clouds:</p> <ul> <li>Each atom is a 3D point</li> <li>Backbone atoms: N, C-alpha, C, O</li> <li>Sidechains: variable number of atoms</li> </ul> <p>See <code>protein_point_cloud_example.py</code> for details</p>"},{"location":"examples/geometric/simple-point-cloud-example/#4-3d-content-creation","title":"4. 3D Content Creation","text":"<p>Generate 3D models for:</p> <ul> <li>Video games (procedural generation)</li> <li>Movies (digital assets)</li> <li>Virtual reality (environments)</li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Geometric Models Overview</p> <p>Quick reference for point clouds, meshes, and voxels</p> <p> geometric_models_demo.py</p> </li> <li> <p> Geometric Losses</p> <p>Learn specialized loss functions for point clouds</p> <p> geometric_losses_demo.py</p> </li> <li> <p> Protein Point Clouds</p> <p>Apply point clouds to protein structure modeling</p> <p> protein_point_cloud_example.py</p> </li> <li> <p> Geometric Benchmarks</p> <p>Evaluate on standard geometric datasets</p> <p> geometric_benchmark_demo.py</p> </li> </ul>"},{"location":"examples/geometric/simple-point-cloud-example/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/geometric/simple-point-cloud-example/#points-look-randomunstructured","title":"Points look random/unstructured","text":"<p>Cause: Model is untrained or poorly trained</p> <p>Solutions:</p> <ol> <li>Train on a dataset first (this example uses pretrained weights)</li> <li>Increase <code>num_layers</code> for more capacity</li> <li>Adjust <code>temperature</code> for different sampling behavior</li> </ol>"},{"location":"examples/geometric/simple-point-cloud-example/#figurecanvasagg-is-non-interactive","title":"\"FigureCanvasAgg is non-interactive\"","text":"<p>Cause: matplotlib trying to show plots in non-GUI environment</p> <p>Solution: This is just a warning, plots are still saved. To suppress:</p> <pre><code>import matplotlib\nmatplotlib.use('Agg')  # Add before importing pyplot\n</code></pre>"},{"location":"examples/geometric/simple-point-cloud-example/#out-of-memory-errors","title":"Out of memory errors","text":"<p>Solutions:</p> <pre><code># Reduce number of points\n\"num_points\": 256  # Instead of 512\n\n# Reduce batch size in generation\nn_samples=1  # Instead of 2\n\n# Use CPU instead of GPU\nJAX_PLATFORMS=cpu python simple_point_cloud_example.py\n</code></pre>"},{"location":"examples/geometric/simple-point-cloud-example/#generated-point-clouds-are-identical","title":"Generated point clouds are identical","text":"<p>Cause: Not providing fresh random keys</p> <p>Solution: Split RNG keys for each generation:</p> <pre><code>for i in range(n_samples):\n    key, subkey = jax.random.split(key)\n    rngs_new = nnx.Rngs(params=subkey)\n    point_cloud = model.generate(rngs=rngs_new, n_samples=1)\n</code></pre>"},{"location":"examples/geometric/simple-point-cloud-example/#additional-resources","title":"Additional Resources","text":"<ul> <li>PointNet: Deep Learning on Point Sets - Original point cloud deep learning paper</li> <li>Attention is All You Need - Transformer architecture</li> <li>Point Cloud Models - Model documentation</li> <li>JAX Point Cloud Processing - Community discussions</li> </ul>"},{"location":"examples/losses/loss-examples/","title":"Loss Functions for Generative Models","text":"<p>Level: Intermediate | Runtime: ~30 seconds (CPU) | Format: Python + Jupyter</p> <p>Prerequisites: Basic understanding of loss functions and JAX | Target Audience: Users learning Artifex's loss API</p>"},{"location":"examples/losses/loss-examples/#overview","title":"Overview","text":"<p>This example provides a comprehensive guide to loss functions in Artifex, covering everything from simple functional losses to advanced composable loss systems. Learn how to use built-in losses, create custom compositions, and apply specialized losses for VAEs, GANs, and geometric models.</p>"},{"location":"examples/losses/loss-examples/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Functional Losses</p> <p>Simple loss functions (MSE, MAE) with flexible reduction modes</p> </li> <li> <p> Composable System</p> <p>Combine weighted losses with component tracking</p> </li> <li> <p> VAE Losses</p> <p>Reconstruction + KL divergence for variational autoencoders</p> </li> <li> <p> GAN Losses</p> <p>Generator and discriminator losses (Standard, LS-GAN, Wasserstein)</p> </li> <li> <p> Scheduled Losses</p> <p>Time-varying loss weights for curriculum learning</p> </li> <li> <p> Geometric Losses</p> <p>Chamfer distance and mesh losses for 3D data</p> </li> </ul>"},{"location":"examples/losses/loss-examples/#files","title":"Files","text":"<p>This example is available in two formats:</p> <ul> <li>Python Script: <code>loss_examples.py</code></li> <li>Jupyter Notebook: <code>loss_examples.ipynb</code></li> </ul>"},{"location":"examples/losses/loss-examples/#quick-start","title":"Quick Start","text":""},{"location":"examples/losses/loss-examples/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/loss_examples.py\n</code></pre>"},{"location":"examples/losses/loss-examples/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/loss_examples.ipynb\n</code></pre>"},{"location":"examples/losses/loss-examples/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/losses/loss-examples/#1-functional-losses","title":"1. Functional Losses","text":"<p>Simple, stateless loss functions for common use cases:</p> <pre><code>from artifex.generative_models.core.losses import mse_loss, mae_loss\n\n# Mean Squared Error\nloss = mse_loss(predictions, targets, reduction=\"mean\")\n\n# Mean Absolute Error\nloss = mae_loss(predictions, targets, reduction=\"sum\")\n</code></pre> <p>Available Reductions:</p> <ul> <li><code>\"mean\"</code>: Average over all elements (default)</li> <li><code>\"sum\"</code>: Sum all elements</li> <li><code>\"none\"</code>: Return per-element losses</li> </ul>"},{"location":"examples/losses/loss-examples/#2-weighted-losses","title":"2. Weighted Losses","text":"<p>Apply fixed weights to loss components:</p> <pre><code>from artifex.generative_models.core.losses import WeightedLoss\n\n# Create weighted loss\nweighted_mse = WeightedLoss(\n    loss_fn=mse_loss,\n    weight=2.0,\n    name=\"weighted_reconstruction\"\n)\n\n# Compute weighted loss\nloss_value = weighted_mse(predictions, targets)\n</code></pre>"},{"location":"examples/losses/loss-examples/#3-composite-losses","title":"3. Composite Losses","text":"<p>Combine multiple loss functions:</p> \\[L_{\\text{total}} = \\sum_{i=1}^{n} w_i \\cdot L_i(\\theta)\\] <pre><code>from artifex.generative_models.core.losses import CompositeLoss\n\ncomposite = CompositeLoss([\n    WeightedLoss(mse_loss, weight=1.0, name=\"reconstruction\"),\n    WeightedLoss(mae_loss, weight=0.5, name=\"l1_penalty\"),\n], return_components=True)\n\n# Get total loss and components\ntotal_loss, components = composite(predictions, targets)\n# components = {\"reconstruction\": 0.15, \"l1_penalty\": 0.08}\n</code></pre>"},{"location":"examples/losses/loss-examples/#4-vae-losses","title":"4. VAE Losses","text":"<p>VAE loss combines reconstruction and KL divergence:</p> \\[\\mathcal{L}_{\\text{VAE}} = \\underbrace{\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{Reconstruction}} - \\underbrace{\\beta \\cdot \\text{KL}(q(z|x) \\| p(z))}_{\\text{Regularization}}\\] <pre><code>def vae_loss(reconstruction, targets, mean, logvar, beta=1.0):\n    # Reconstruction loss\n    recon_loss = mse_loss(reconstruction, targets)\n\n    # KL divergence (assuming standard normal prior)\n    kl_loss = -0.5 * jnp.sum(1 + logvar - mean**2 - jnp.exp(logvar))\n    kl_loss = kl_loss / targets.shape[0]  # Normalize by batch size\n\n    # Total VAE loss\n    return recon_loss + beta * kl_loss\n</code></pre> <p>\u03b2 Parameter:</p> <ul> <li><code>\u03b2 = 1.0</code>: Standard VAE</li> <li><code>\u03b2 &gt; 1.0</code>: \u03b2-VAE (encourages disentanglement)</li> <li><code>\u03b2 &lt; 1.0</code>: Less regularization, better reconstruction</li> </ul>"},{"location":"examples/losses/loss-examples/#5-gan-losses","title":"5. GAN Losses","text":"<p>Artifex provides pre-configured GAN loss suites:</p> <pre><code>from artifex.generative_models.core.losses import create_gan_loss_suite\n\n# Create GAN losses\ngen_loss, disc_loss = create_gan_loss_suite(\n    generator_loss_type=\"lsgan\",\n    discriminator_loss_type=\"lsgan\"\n)\n\n# Generator loss (want discriminator to output 1 for fake)\ng_loss = gen_loss(fake_scores)\n\n# Discriminator loss (real\u21921, fake\u21920)\nd_loss = disc_loss(real_scores, fake_scores)\n</code></pre> <p>Available GAN Loss Types:</p> <ul> <li><code>\"standard\"</code>: Binary cross-entropy (original GAN)</li> <li><code>\"lsgan\"</code>: Least-squares GAN (more stable)</li> <li><code>\"wgan\"</code>: Wasserstein GAN (requires gradient penalty)</li> </ul>"},{"location":"examples/losses/loss-examples/#6-scheduled-losses","title":"6. Scheduled Losses","text":"<p>Time-varying loss weights for curriculum learning:</p> <pre><code>from artifex.generative_models.core.losses import ScheduledLoss\n\n# Define schedule function\ndef warmup_schedule(step):\n    \"\"\"Linear warmup from 0 to 1 over 1000 steps.\"\"\"\n    return jnp.minimum(1.0, step / 1000.0)\n\n# Create scheduled loss\nscheduled_loss = ScheduledLoss(\n    loss_fn=perceptual_loss,\n    schedule_fn=warmup_schedule,\n    name=\"scheduled_perceptual\"\n)\n\n# Loss weight increases with training steps\nloss_value = scheduled_loss(..., step=500)  # weight = 0.5\n</code></pre>"},{"location":"examples/losses/loss-examples/#7-geometric-losses","title":"7. Geometric Losses","text":"<p>Specialized losses for 3D data:</p>"},{"location":"examples/losses/loss-examples/#chamfer-distance","title":"Chamfer Distance","text":"<p>Measures point cloud similarity:</p> \\[L_{\\text{Chamfer}}(X, Y) = \\frac{1}{|X|}\\sum_{x \\in X} \\min_{y \\in Y} \\|x - y\\|^2 + \\frac{1}{|Y|}\\sum_{y \\in Y} \\min_{x \\in X} \\|x - y\\|^2\\] <pre><code>from artifex.generative_models.core.losses import chamfer_distance\n\n# Point clouds: (batch, num_points, 3)\npred_points = jax.random.normal(key, (4, 1000, 3))\ntarget_points = jax.random.normal(key, (4, 1000, 3))\n\nloss = chamfer_distance(pred_points, target_points)\n</code></pre>"},{"location":"examples/losses/loss-examples/#mesh-loss","title":"Mesh Loss","text":"<p>Multi-component loss for mesh quality:</p> <pre><code>from artifex.generative_models.core.losses import MeshLoss\n\nmesh_loss = MeshLoss(\n    vertex_weight=1.0,      # Vertex position accuracy\n    normal_weight=0.1,      # Surface normal consistency\n    edge_weight=0.1,        # Edge length preservation\n    laplacian_weight=0.01   # Smoothness regularization\n)\n\n# Mesh format: (vertices, faces, normals)\npred_mesh = (vertices_pred, faces, normals_pred)\ntarget_mesh = (vertices_target, faces, normals_target)\n\nloss = mesh_loss(pred_mesh, target_mesh)\n</code></pre>"},{"location":"examples/losses/loss-examples/#8-perceptual-loss","title":"8. Perceptual Loss","text":"<p>Feature-based loss using pre-trained networks:</p> <pre><code>from artifex.generative_models.core.losses import PerceptualLoss\n\nperceptual = PerceptualLoss(\n    content_weight=1.0,\n    style_weight=0.01\n)\n\n# Requires feature extraction from images\nloss = perceptual(\n    pred_images=generated_images,\n    target_images=real_images,\n    features_pred=extracted_features_pred,\n    features_target=extracted_features_target\n)\n</code></pre>"},{"location":"examples/losses/loss-examples/#9-total-variation-loss","title":"9. Total Variation Loss","text":"<p>Smoothness regularization for images:</p> <pre><code>from artifex.generative_models.core.losses import total_variation_loss\n\n# Encourages spatial smoothness\ntv_loss = total_variation_loss(generated_images)\n\n# Often combined with other losses\ntotal_loss = reconstruction_loss + 0.001 * tv_loss\n</code></pre>"},{"location":"examples/losses/loss-examples/#code-structure","title":"Code Structure","text":"<p>The example demonstrates seven loss usage patterns:</p> <ol> <li>Functional Usage - Simple MSE and MAE losses</li> <li>Composable Loss - Weighted loss combination</li> <li>VAE Training - Reconstruction + KL divergence</li> <li>GAN Training - Generator and discriminator losses</li> <li>Scheduled Loss - Progressive loss weight ramping</li> <li>Geometric Losses - Chamfer distance and mesh losses</li> <li>Complete Training - Full training loop with losses</li> </ol>"},{"location":"examples/losses/loss-examples/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>\u2705 Functional losses with flexible reduction modes</li> <li>\u2705 Weighted loss composition with component tracking</li> <li>\u2705 VAE loss (reconstruction + KL divergence)</li> <li>\u2705 GAN loss suites (standard, LS-GAN, Wasserstein)</li> <li>\u2705 Scheduled losses for curriculum learning</li> <li>\u2705 Geometric losses for 3D data (Chamfer, mesh)</li> <li>\u2705 Perceptual loss with feature extraction</li> <li>\u2705 Total variation loss for smoothness</li> <li>\u2705 Integration with Flax NNX training loops</li> </ul>"},{"location":"examples/losses/loss-examples/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Adjust Loss Weights</li> </ol> <pre><code># Try different \u03b2 values for VAE\ncomposite = CompositeLoss([\n    WeightedLoss(recon_loss, weight=1.0, name=\"recon\"),\n    WeightedLoss(kl_loss, weight=4.0, name=\"kl\"),  # \u03b2 = 4.0\n])\n</code></pre> <ol> <li>Compare GAN Loss Types</li> </ol> <pre><code># Standard GAN\ngen_loss, disc_loss = create_gan_loss_suite(\"standard\", \"standard\")\n\n# LS-GAN (often more stable)\ngen_loss, disc_loss = create_gan_loss_suite(\"lsgan\", \"lsgan\")\n</code></pre> <ol> <li>Custom Schedule Functions</li> </ol> <pre><code># Exponential warmup\ndef exp_schedule(step):\n    return 1.0 - jnp.exp(-step / 1000.0)\n\n# Cosine annealing\ndef cosine_schedule(step):\n    return 0.5 * (1 + jnp.cos(jnp.pi * step / total_steps))\n</code></pre> <ol> <li>Geometric Loss Weights</li> </ol> <pre><code># Adjust mesh loss components\nmesh_loss = MeshLoss(\n    vertex_weight=2.0,      # Emphasize position accuracy\n    normal_weight=0.5,      # More weight on normals\n    edge_weight=0.1,\n    laplacian_weight=0.01\n)\n</code></pre>"},{"location":"examples/losses/loss-examples/#next-steps","title":"Next Steps","text":"<ul> <li> <p> VAE Examples</p> <p>Apply losses in VAE training</p> <p> VAE MNIST Tutorial</p> </li> <li> <p> GAN Examples</p> <p>Use GAN losses in training</p> <p> GAN MNIST Tutorial</p> </li> <li> <p> Geometric Models</p> <p>Apply geometric losses</p> <p> Geometric Benchmark</p> </li> <li> <p> Framework Features</p> <p>Understand composable design</p> <p> Framework Demo</p> </li> </ul>"},{"location":"examples/losses/loss-examples/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/losses/loss-examples/#shape-mismatch-errors","title":"Shape Mismatch Errors","text":"<p>Symptom: <code>ValueError</code> about incompatible shapes</p> <p>Solution: Ensure predictions and targets have the same shape</p> <pre><code>print(f\"Predictions: {predictions.shape}\")\nprint(f\"Targets: {targets.shape}\")\n\n# Reshape if needed\npredictions = predictions.reshape(targets.shape)\n</code></pre>"},{"location":"examples/losses/loss-examples/#nan-in-kl-divergence","title":"NaN in KL Divergence","text":"<p>Symptom: KL loss becomes NaN during VAE training</p> <p>Cause: Numerical instability in <code>exp(logvar)</code> for large <code>logvar</code></p> <p>Solution: Clip logvar values</p> <pre><code>logvar = jnp.clip(logvar, -10.0, 10.0)\nkl_loss = -0.5 * jnp.sum(1 + logvar - mean**2 - jnp.exp(logvar))\n</code></pre>"},{"location":"examples/losses/loss-examples/#gan-loss-not-converging","title":"GAN Loss Not Converging","text":"<p>Symptom: Generator or discriminator loss diverges</p> <p>Solution: Try LS-GAN loss instead of standard GAN</p> <pre><code># LS-GAN is often more stable\ngen_loss, disc_loss = create_gan_loss_suite(\"lsgan\", \"lsgan\")\n</code></pre>"},{"location":"examples/losses/loss-examples/#composite-loss-component-mismatch","title":"Composite Loss Component Mismatch","text":"<p>Symptom: <code>KeyError</code> when accessing loss components</p> <p>Solution: Set <code>return_components=True</code> in CompositeLoss</p> <pre><code>composite = CompositeLoss([...], return_components=True)\ntotal, components = composite(pred, target)  # Returns tuple\n</code></pre>"},{"location":"examples/losses/loss-examples/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/losses/loss-examples/#documentation","title":"Documentation","text":"<ul> <li>Loss Functions API Reference - Complete loss function documentation</li> </ul>"},{"location":"examples/losses/loss-examples/#related-examples","title":"Related Examples","text":"<ul> <li>Framework Features Demo - Composable loss system</li> <li>VAE MNIST Tutorial - VAE loss in practice</li> <li>GAN MNIST Tutorial - GAN loss in practice</li> <li>Geometric Benchmark - Geometric losses</li> </ul>"},{"location":"examples/losses/loss-examples/#papers","title":"Papers","text":"<ul> <li>VAE: Auto-Encoding Variational Bayes (Kingma &amp; Welling, 2013)</li> <li>\u03b2-VAE: \u03b2-VAE: Learning Basic Visual Concepts (Higgins et al., 2017)</li> <li>LS-GAN: Least Squares GAN (Mao et al., 2017)</li> <li>Perceptual Loss: Perceptual Losses (Johnson et al., 2016)</li> <li>Chamfer Distance: Learning Representations and Generative Models for 3D Point Clouds</li> </ul>"},{"location":"examples/multimodal/simple-image-text/","title":"Simple Image-Text Multimodal Learning","text":"Intermediate Runtime: ~10min \ud83d\udcd3 Dual Format"},{"location":"examples/multimodal/simple-image-text/#files","title":"Files","text":"<ul> <li>Python Script: <code>simple_image_text.py</code></li> <li>Jupyter Notebook: <code>simple_image_text.ipynb</code></li> </ul>"},{"location":"examples/multimodal/simple-image-text/#quick-start","title":"Quick Start","text":"<pre><code># Run the Python script\nuv run python examples/generative_models/multimodal/simple_image_text.py\n\n# Or open the Jupyter notebook\njupyter lab examples/generative_models/multimodal/simple_image_text.ipynb\n</code></pre>"},{"location":"examples/multimodal/simple-image-text/#overview","title":"Overview","text":"<p>This example demonstrates multimodal learning by combining image and text modalities in a unified model. Learn how to build separate encoders for different modalities, create shared embedding spaces, and perform cross-modal retrieval tasks.</p>"},{"location":"examples/multimodal/simple-image-text/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this example, you will understand:</p> <ul> <li> Multimodal model architectures with separate encoders</li> <li> Creating shared embedding spaces for multiple modalities</li> <li> Computing cross-modal similarities</li> <li> Performing cross-modal retrieval (image-to-text, text-to-image)</li> <li> Visualizing multimodal embedding spaces</li> </ul>"},{"location":"examples/multimodal/simple-image-text/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of CNNs for image processing</li> <li>Familiarity with text embeddings and sequence models</li> <li>Knowledge of similarity metrics and representation learning</li> <li>Basic understanding of JAX/Flax NNX patterns</li> </ul>"},{"location":"examples/multimodal/simple-image-text/#theory","title":"Theory","text":""},{"location":"examples/multimodal/simple-image-text/#multimodal-learning","title":"Multimodal Learning","text":"<p>Multimodal models learn joint representations from multiple input modalities. The goal is to create a shared embedding space where semantically similar inputs from different modalities are close together.</p>"},{"location":"examples/multimodal/simple-image-text/#contrastive-learning","title":"Contrastive Learning","text":"<p>The model learns by maximizing similarity between matching pairs while minimizing similarity between non-matching pairs:</p> \\[\\mathcal{L} = -\\log \\frac{\\exp(\\text{sim}(f_I(I), f_T(T)) / \\tau)}{\\sum_{i=1}^N \\exp(\\text{sim}(f_I(I), f_T(T_i)) / \\tau)}\\] <p>where:</p> <ul> <li>\\(f_I\\) is the image encoder</li> <li>\\(f_T\\) is the text encoder</li> <li>\\(\\tau\\) is the temperature parameter</li> <li>\\(\\text{sim}\\) is the similarity function (typically cosine similarity)</li> </ul>"},{"location":"examples/multimodal/simple-image-text/#architecture-components","title":"Architecture Components","text":"<ol> <li>Image Encoder: CNN-based encoder mapping images to embeddings</li> <li>Text Encoder: Embedding + MLP mapping text sequences to embeddings</li> <li>Fusion Layer: Combines modalities for joint predictions</li> </ol>"},{"location":"examples/multimodal/simple-image-text/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/multimodal/simple-image-text/#1-image-encoder","title":"1. Image Encoder","text":"<pre><code>class SimpleImageEncoder(nnx.Module):\n    def __init__(self, image_size=32, embed_dim=128, *, rngs: nnx.Rngs):\n        super().__init__()\n        # CNN encoder with global average pooling\n        self.encoder = nnx.Sequential(\n            nnx.Conv(3, 32, kernel_size=(3, 3), rngs=rngs),\n            nnx.relu,\n            nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs),\n            nnx.relu,\n            lambda x: jnp.mean(x, axis=(1, 2)),  # Global pooling\n            nnx.Linear(64, embed_dim, rngs=rngs),\n        )\n</code></pre>"},{"location":"examples/multimodal/simple-image-text/#2-text-encoder","title":"2. Text Encoder","text":"<pre><code>class SimpleTextEncoder(nnx.Module):\n    def __init__(self, vocab_size=128, embed_dim=128, *, rngs: nnx.Rngs):\n        super().__init__()\n        self.embedding = nnx.Embed(vocab_size, embed_dim, rngs=rngs)\n        self.encoder = nnx.Sequential(\n            nnx.Linear(embed_dim, 64, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(64, embed_dim, rngs=rngs)\n        )\n\n    def __call__(self, text_ids):\n        embedded = self.embedding(text_ids)\n        pooled = jnp.mean(embedded, axis=1)  # Average pooling\n        return self.encoder(pooled)\n</code></pre>"},{"location":"examples/multimodal/simple-image-text/#3-multimodal-model","title":"3. Multimodal Model","text":"<pre><code>class SimpleMultimodalModel(nnx.Module):\n    def __init__(self, image_size=32, vocab_size=128,\n                 embed_dim=128, output_dim=10, *, rngs: nnx.Rngs):\n        super().__init__()\n        # Separate encoders\n        self.image_encoder = SimpleImageEncoder(image_size, embed_dim, rngs=rngs)\n        self.text_encoder = SimpleTextEncoder(vocab_size, embed_dim, rngs=rngs)\n\n        # Fusion layer\n        self.fusion = nnx.Sequential(\n            nnx.Linear(embed_dim * 2, embed_dim, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(embed_dim, output_dim, rngs=rngs),\n        )\n</code></pre>"},{"location":"examples/multimodal/simple-image-text/#4-cross-modal-similarity","title":"4. Cross-Modal Similarity","text":"<pre><code>def compute_similarity(self, images, text_ids):\n    image_features = self.encode_image(images)\n    text_features = self.encode_text(text_ids)\n\n    # Normalize features\n    image_features = image_features / (\n        jnp.linalg.norm(image_features, axis=-1, keepdims=True) + 1e-8\n    )\n    text_features = text_features / (\n        jnp.linalg.norm(text_features, axis=-1, keepdims=True) + 1e-8\n    )\n\n    # Compute cosine similarity\n    similarity = jnp.sum(image_features * text_features, axis=-1)\n    return similarity\n</code></pre>"},{"location":"examples/multimodal/simple-image-text/#experiments-to-try","title":"Experiments to Try","text":""},{"location":"examples/multimodal/simple-image-text/#1-architecture-improvements","title":"1. Architecture Improvements","text":"<ul> <li>Add attention mechanisms for better feature aggregation</li> <li>Use pre-trained encoders (ResNet for images, BERT for text)</li> <li>Implement transformer-based fusion layers</li> <li>Add residual connections</li> </ul>"},{"location":"examples/multimodal/simple-image-text/#2-training-enhancements","title":"2. Training Enhancements","text":"<ul> <li>Implement contrastive loss (InfoNCE, SimCLR)</li> <li>Add hard negative mining</li> <li>Use temperature-scaled training</li> <li>Implement data augmentation</li> </ul>"},{"location":"examples/multimodal/simple-image-text/#3-advanced-features","title":"3. Advanced Features","text":"<ul> <li>Multi-head attention for fusion</li> <li>Cross-modal attention mechanisms</li> <li>Hierarchical embeddings</li> <li>Multi-task learning objectives</li> </ul>"},{"location":"examples/multimodal/simple-image-text/#next-steps","title":"Next Steps","text":"<ul> <li> <p> CLIP Models</p> <p>Explore large-scale contrastive image-text models</p> <p> CLIP Examples</p> </li> <li> <p> Visual QA</p> <p>Build models for visual question answering</p> <p> VQA Examples</p> </li> <li> <p> Image Captioning</p> <p>Generate text descriptions from images</p> <p> Captioning Examples</p> </li> <li> <p> Cross-Modal Retrieval</p> <p>Advanced retrieval across modalities</p> <p> Retrieval Examples</p> </li> </ul>"},{"location":"examples/multimodal/simple-image-text/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/multimodal/simple-image-text/#common-issues","title":"Common Issues","text":"<p>Embedding Dimension Mismatch:</p> <ul> <li>Ensure both encoders output same embedding dimension</li> <li>Check fusion layer input dimensions</li> <li>Verify concatenation axis</li> </ul> <p>Poor Similarity Scores:</p> <ul> <li>Normalize features before computing similarity</li> <li>Check for numerical instability (add epsilon)</li> <li>Tune temperature parameter</li> </ul> <p>Memory Issues:</p> <ul> <li>Reduce batch size or embedding dimensions</li> <li>Use gradient checkpointing</li> <li>Enable mixed precision training</li> </ul>"},{"location":"examples/multimodal/simple-image-text/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/multimodal/simple-image-text/#documentation","title":"Documentation","text":"<ul> <li>Flax NNX Guide</li> <li>JAX Transformations</li> <li>Multimodal Learning Survey</li> </ul>"},{"location":"examples/multimodal/simple-image-text/#research-papers","title":"Research Papers","text":"<ul> <li>Learning Transferable Visual Models From Natural Language Supervision (CLIP)</li> <li>ALIGN: Scaling Up Visual and Vision-Language Representation Learning</li> <li>Multimodal Learning with Transformers: A Survey</li> </ul> <p>Author: Artifex Team Last Updated: 2025-10-22 Difficulty: Intermediate Time to Complete: ~45 minutes</p>"},{"location":"examples/protein/protein-diffusion-example/","title":"Protein Diffusion Example","text":"<p>Level: Advanced Runtime: ~5 minutes Format: Dual (.py script | .ipynb notebook)</p> <p>Comprehensive protein diffusion modeling with two approaches: high-level API with extensions and direct model creation, including quality assessment and visualization.</p>"},{"location":"examples/protein/protein-diffusion-example/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/protein/protein_diffusion_example.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/protein/protein_diffusion_example.ipynb</code></li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#quick-start","title":"Quick Start","text":"<pre><code># Run the Python script\npython examples/generative_models/protein/protein_diffusion_example.py\n\n# Or open the Jupyter notebook\njupyter notebook examples/generative_models/protein/protein_diffusion_example.ipynb\n</code></pre>"},{"location":"examples/protein/protein-diffusion-example/#overview","title":"Overview","text":"<p>This comprehensive example demonstrates how to build and use protein diffusion models for generating 3D protein structures. You'll learn two distinct approaches to protein modeling, understand protein-specific geometric constraints, and explore quality assessment techniques.</p>"},{"location":"examples/protein/protein-diffusion-example/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this example, you will understand:</p> <ul> <li> How to create protein diffusion models with Artifex's high-level API</li> <li> Direct model creation and manipulation for protein structures</li> <li> Protein-specific loss functions and geometric constraints</li> <li> Quality assessment metrics for generated proteins</li> <li> Visualization techniques for 3D protein structures</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of diffusion models and denoising processes</li> <li>Familiarity with protein structure representations</li> <li>Knowledge of geometric constraints in biomolecules</li> <li>Experience with JAX and Flax NNX</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#theory-and-key-concepts","title":"Theory and Key Concepts","text":""},{"location":"examples/protein/protein-diffusion-example/#protein-structure-representation","title":"Protein Structure Representation","text":"<p>Proteins are complex biomolecules composed of amino acid residues. Each residue contains multiple atoms with specific 3D coordinates:</p> <p>Backbone Atoms: The main chain of every protein contains four atoms per residue:</p> <ul> <li>N (Nitrogen): Backbone nitrogen</li> <li>CA (Alpha Carbon): Central carbon atom</li> <li>C (Carbonyl Carbon): Carbonyl carbon</li> <li>O (Oxygen): Carbonyl oxygen</li> </ul> <p>Representation Approaches:</p> <ol> <li>Point Cloud: Unordered set of 3D points representing atom positions</li> <li>Advantages: Simple, flexible, good for local geometry</li> <li> <p>Use case: Backbone modeling, local structure refinement</p> </li> <li> <p>Graph: Nodes (residues/atoms) connected by edges (bonds)</p> </li> <li>Advantages: Captures connectivity, enforces topology</li> <li>Use case: Full protein modeling, contact prediction</li> </ol>"},{"location":"examples/protein/protein-diffusion-example/#geometric-constraints","title":"Geometric Constraints","text":"<p>Valid protein structures must satisfy strict geometric constraints:</p> <p>Bond Lengths: Distance between bonded atoms must fall within specific ranges:</p> <ul> <li>C-C bonds: ~1.5 \u00c5</li> <li>C-N bonds: ~1.3 \u00c5</li> <li>C=O bonds: ~1.2 \u00c5</li> </ul> <p>Bond Angles: Angles between consecutive bonds follow specific distributions:</p> <ul> <li>Tetrahedral angles: ~109.5\u00b0</li> <li>Planar peptide bonds: ~120\u00b0</li> </ul> <p>Dihedral Angles: Rotation around bonds defines protein conformation:</p> <ul> <li>Phi (\u03c6): Rotation around N-CA bond</li> <li>Psi (\u03c8): Rotation around CA-C bond</li> <li>Ramachandran plot: Shows allowed (\u03c6, \u03c8) combinations</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#protein-specific-loss-functions","title":"Protein-Specific Loss Functions","text":"<p>RMSD (Root Mean Square Deviation): Measures structural similarity between predicted and target structures:</p> \\[ \\text{RMSD} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N ||x_i - y_i||^2} \\] <p>where \\(N\\) is the number of atoms, \\(x_i\\) is the predicted position, and \\(y_i\\) is the target position.</p> <p>Backbone Loss: Enforces correct backbone geometry:</p> \\[ \\mathcal{L}_{\\text{backbone}} = \\sum_{i=1}^{N-1} ||d(x_i, x_{i+1}) - d_{\\text{ideal}}||^2 \\] <p>where \\(d(x_i, x_{i+1})\\) is the distance between consecutive residues and \\(d_{\\text{ideal}}\\) is the ideal distance.</p> <p>Composite Loss: Combines multiple geometric constraints:</p> \\[ \\mathcal{L}_{\\text{total}} = \\lambda_{\\text{rmsd}} \\mathcal{L}_{\\text{rmsd}} + \\lambda_{\\text{backbone}} \\mathcal{L}_{\\text{backbone}} + \\lambda_{\\text{angle}} \\mathcal{L}_{\\text{angle}} \\]"},{"location":"examples/protein/protein-diffusion-example/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/protein/protein-diffusion-example/#part-1-high-level-api-with-extensions","title":"Part 1: High-Level API with Extensions","text":"<p>The example demonstrates using Artifex's extension system for protein modeling:</p> <pre><code># Create model with extensions\nextension_config = {\n    \"name\": \"protein_diffusion_extensions\",\n    \"description\": \"Extensions for protein diffusion model\",\n    \"enabled\": True,\n    \"use_backbone_constraints\": True,\n    \"use_protein_mixin\": True,\n}\n\nextensions = create_protein_extensions(extension_config, rngs=rngs)\nmodel = nnx.Module()\nmodel.extensions = extensions\n</code></pre> <p>The extension system provides:</p> <ul> <li>Backbone Constraints: Automatic enforcement of backbone geometry</li> <li>Protein Mixin: Domain-specific operations for proteins</li> <li>Quality Assessment: Built-in metrics for structure validation</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#part-2-direct-model-creation","title":"Part 2: Direct Model Creation","text":"<p>For full control, create models directly:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    PointCloudNetworkConfig,\n    ProteinConstraintConfig,\n    ProteinPointCloudConfig,\n)\n\n# Create network config for point cloud processing\nnetwork_config = PointCloudNetworkConfig(\n    name=\"protein_network\",\n    hidden_dims=(128, 128, 128, 128),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=128,\n    num_heads=4,\n    num_layers=4,\n    dropout_rate=0.1,\n)\n\n# Create constraint config for structural constraints\nconstraint_config = ProteinConstraintConfig(\n    bond_weight=1.0,\n    angle_weight=0.5,\n)\n\n# Create protein point cloud config with nested configs\nconfig = ProteinPointCloudConfig(\n    name=\"protein_point_cloud_model\",\n    network=network_config,\n    num_points=64 * 4,  # num_residues \u00d7 num_atoms (flattened)\n    dropout_rate=0.1,\n    num_residues=64,\n    num_atoms_per_residue=4,\n    backbone_indices=(0, 1, 2, 3),  # N, CA, C, O\n    use_constraints=True,\n    constraint_config=constraint_config,\n)\n\nmodel = ProteinPointCloudModel(config, rngs=rngs)\n</code></pre>"},{"location":"examples/protein/protein-diffusion-example/#dataset-preparation","title":"Dataset Preparation","text":"<p>Load synthetic or real protein datasets:</p> <pre><code># Create synthetic dataset for demonstration\ndataset = create_synthetic_protein_dataset(\n    num_proteins=50,\n    min_seq_length=32,\n    max_seq_length=64,\n    random_seed=42,\n)\n\n# Prepare batch\nbatch = prepare_batch(dataset, batch_size=8, random_seed=42)\n\n# Add noise for diffusion training\nnoisy_batch = add_noise_to_batch(batch, noise_level=0.1, random_seed=42)\n</code></pre>"},{"location":"examples/protein/protein-diffusion-example/#loss-function-configuration","title":"Loss Function Configuration","text":"<p>Combine multiple protein-specific losses:</p> <pre><code>from artifex.generative_models.modalities.protein.losses import (\n    CompositeLoss,\n    create_backbone_loss,\n    create_rmsd_loss,\n)\n\nloss_fn = CompositeLoss({\n    \"rmsd\": (create_rmsd_loss(), 1.0),      # Weight: 1.0\n    \"backbone\": (create_backbone_loss(), 0.5),  # Weight: 0.5\n})\n\n# Calculate losses\noutputs = model(noisy_batch)\nlosses = loss_fn(batch, outputs)\n</code></pre>"},{"location":"examples/protein/protein-diffusion-example/#visualization-and-quality-assessment","title":"Visualization and Quality Assessment","text":"<p>Visualize generated structures and assess quality:</p> <pre><code>from artifex.visualization.protein_viz import ProteinVisualizer\n\n# Extract positions\ntarget_pos = batch[\"atom_positions\"][0]\npred_pos = outputs[\"positions\"][0]\nmask = batch[\"atom_mask\"][0]\n\n# Calculate dihedral angles\ntarget_phi, target_psi = ProteinVisualizer.calculate_dihedral_angles(target_pos, mask)\npred_phi, pred_psi = ProteinVisualizer.calculate_dihedral_angles(pred_pos, mask)\n\n# Plot Ramachandran plots\nProteinVisualizer.plot_ramachandran(target_phi, target_psi, title=\"Target\")\nProteinVisualizer.plot_ramachandran(pred_phi, pred_psi, title=\"Predicted\")\n\n# 3D visualization (requires py3Dmol)\nviewer = ProteinVisualizer.visualize_structure(\n    pred_pos,\n    mask,\n    show_sidechains=False,\n    color_by=\"chain\"\n)\nviewer.show()\n</code></pre>"},{"location":"examples/protein/protein-diffusion-example/#expected-output","title":"Expected Output","text":"<p>The example runs both approaches and displays results:</p> <pre><code>=== Protein Diffusion Examples ===\nThis example demonstrates two approaches to protein diffusion:\n1. High-level API with extension components\n2. Direct model creation and manipulation\n\n=== Running Extensions Example ===\n\nModel structure:\n- Type: Module\n- Extensions: ['bond_length', 'bond_angle', 'protein_mixin']\nGenerated 2 protein samples\n- Sample shape: (2, 64, 4, 3)\n- Atom mask shape: (2, 64, 4)\n\nQuality metrics:\n- rmsd: 1.2345\n- bond_violations: 0.0234\n- angle_violations: 0.0156\n\n=== Running Direct Model Example ===\n\nCreating model...\nLoading dataset...\nPreparing batch...\nAdding noise to batch...\nCreating loss function...\nRunning model...\nCalculating losses...\nLosses:\n  rmsd: 0.1234\n  backbone: 0.0567\n  total: 0.1801\n\nDisplaying results...\n</code></pre> <p>The example also generates:</p> <ul> <li>2D plots of protein structures</li> <li>Ramachandran plots showing dihedral angle distributions</li> <li>3D interactive visualizations (if py3Dmol is installed)</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Compare Model Types: Test point cloud vs graph representations</li> </ol> <pre><code>point_cloud_model = create_protein_diffusion_model(model_type=\"point_cloud\")\ngraph_model = create_protein_diffusion_model(model_type=\"graph\")\n</code></pre> <ol> <li>Adjust Constraint Weights: Balance different geometric constraints</li> </ol> <pre><code>constraint_config = {\n    \"backbone_weight\": 2.0,  # Emphasize backbone connectivity\n    \"bond_weight\": 1.5,      # Strong bond length enforcement\n    \"angle_weight\": 1.0,     # Moderate angle constraints\n    \"dihedral_weight\": 0.5,  # Soft dihedral constraints\n}\n</code></pre> <ol> <li>Larger Proteins: Scale to longer sequences</li> </ol> <pre><code>model = create_protein_diffusion_model(\n    num_residues=128,  # Double the default size\n    hidden_dim=256,    # Increase capacity\n)\n</code></pre> <ol> <li>Custom Loss Functions: Create domain-specific losses</li> </ol> <pre><code>def create_contact_loss():\n    \"\"\"Enforce protein contact map constraints.\"\"\"\n    def loss_fn(batch, outputs):\n        # Calculate contact map loss\n        return contact_loss\n    return loss_fn\n\nloss_fn = CompositeLoss({\n    \"rmsd\": (create_rmsd_loss(), 1.0),\n    \"backbone\": (create_backbone_loss(), 0.5),\n    \"contact\": (create_contact_loss(), 0.3),\n})\n</code></pre> <ol> <li>Real Datasets: Load actual protein structures</li> </ol> <pre><code>dataset = ProteinDataset(\n    data_dir=\"path/to/pdb/files\",\n    max_seq_length=128,\n    random_seed=42,\n)\n</code></pre>"},{"location":"examples/protein/protein-diffusion-example/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/protein/protein-diffusion-example/#size-mismatch-warnings","title":"Size Mismatch Warnings","text":"<p>If you see \"Target size doesn't match prediction size\":</p> <ul> <li>Check that <code>num_residues</code> matches between model and data</li> <li>Ensure batch collation handles variable-length sequences</li> <li>Use masking to handle different protein lengths</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#geometric-constraint-violations","title":"Geometric Constraint Violations","text":"<p>If structures have high constraint violations:</p> <ul> <li>Increase constraint weights in <code>constraint_config</code></li> <li>Add more training epochs for constraint satisfaction</li> <li>Use smaller noise levels during training</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#visualization-issues","title":"Visualization Issues","text":"<p>If 3D visualization fails:</p> <ul> <li>Install py3Dmol: <code>pip install py3Dmol</code></li> <li>For Jupyter notebooks, ensure proper widget support</li> <li>Fall back to 2D plots if 3D is unavailable</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#memory-issues","title":"Memory Issues","text":"<p>For large proteins:</p> <ul> <li>Reduce batch size</li> <li>Use gradient checkpointing</li> <li>Process proteins in chunks</li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#next-steps","title":"Next Steps","text":"<ul> <li> <p>:material-protein: Advanced Protein Models</p> <p>Explore AlphaFold-style architectures and multi-scale modeling</p> <p> Protein Extensions</p> </li> <li> <p> Point Cloud Models</p> <p>Learn specialized techniques for point cloud protein representations</p> <p> Point Cloud Example</p> </li> <li> <p> Diffusion Training</p> <p>Master advanced diffusion techniques for proteins</p> <p> Diffusion Guide</p> </li> <li> <p> Protein Benchmarks</p> <p>Evaluate protein models with standard benchmarks</p> <p> Benchmarking</p> </li> </ul>"},{"location":"examples/protein/protein-diffusion-example/#additional-resources","title":"Additional Resources","text":"<ul> <li>Protein Data Bank (PDB) - Repository of 3D protein structures</li> <li>AlphaFold Documentation - State-of-the-art protein structure prediction</li> <li>Diffusion Models for Proteins - Research paper on protein diffusion</li> <li>Artifex Protein Modeling Guide - Comprehensive guide</li> <li>Ramachandran Plot - Understanding dihedral angles</li> </ul>"},{"location":"examples/protein/protein-diffusion-tech-validation/","title":"Protein Diffusion Technical Validation","text":"Beginner \u26a1 5 seconds \ud83d\udcd3 Dual Format <p>A minimal validation script to verify your environment is correctly set up for protein diffusion modeling with JAX and Flax NNX.</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#files","title":"Files","text":"<ul> <li>Python Script: <code>protein_diffusion_tech_validation.py</code></li> <li>Jupyter Notebook: <code>protein_diffusion_tech_validation.ipynb</code></li> </ul>"},{"location":"examples/protein/protein-diffusion-tech-validation/#quick-start","title":"Quick Start","text":"<pre><code># Clone and setup\ncd artifex\nsource activate.sh\n\n# Run validation\npython examples/generative_models/protein/protein_diffusion_tech_validation.py\n\n# Expected output:\n# JAX version: 0.7.2\n# Biopython available: False\n# Protein coordinates shape: (100, 3)\n# Model output shape: (100, 3)\n# Loss: 1.467...\n# Technology validation successful!\n</code></pre>"},{"location":"examples/protein/protein-diffusion-tech-validation/#overview","title":"Overview","text":"<p>This script performs a quick technology stack validation for protein modeling. It's designed to be the first thing you run to ensure your environment is correctly configured.</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#learning-objectives","title":"Learning Objectives","text":"<ul> <li> Validate JAX and Flax NNX installation</li> <li> Understand protein point cloud representation</li> <li> Implement a minimal protein structure model</li> <li> Test forward pass and loss computation</li> <li> Handle optional dependencies gracefully</li> </ul>"},{"location":"examples/protein/protein-diffusion-tech-validation/#prerequisites","title":"Prerequisites","text":"<ul> <li>JAX installed</li> <li>Flax NNX installed</li> <li>Basic understanding of protein structure (helpful)</li> <li>Familiarity with point clouds (helpful)</li> </ul>"},{"location":"examples/protein/protein-diffusion-tech-validation/#what-gets-validated","title":"What Gets Validated","text":"<p>This script checks 5 critical components:</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#1-jax-functionality","title":"1. JAX Functionality","text":"<p>Tests:</p> <ul> <li>Random number generation (<code>jax.random.key</code>)</li> <li>Array operations (<code>jax.numpy</code>)</li> <li>Device placement (CPU/GPU)</li> </ul> <p>Expected: JAX 0.7.2+ working correctly</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#2-flax-nnx","title":"2. Flax NNX","text":"<p>Tests:</p> <ul> <li>Module creation (<code>nnx.Module</code>)</li> <li>Linear layers (<code>nnx.Linear</code>)</li> <li>Activation functions (<code>nnx.relu</code>)</li> <li>Parameter initialization</li> </ul> <p>Expected: Flax NNX modules instantiate and run</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#3-protein-representation","title":"3. Protein Representation","text":"<p>Tests:</p> <ul> <li>Point cloud format (N \u00d7 3 arrays)</li> <li>C-alpha atom extraction (if BioPython available)</li> <li>Synthetic data generation (fallback)</li> </ul> <p>Expected: Proteins represented as 3D point clouds</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#4-forward-pass","title":"4. Forward Pass","text":"<p>Tests:</p> <ul> <li>Model inference</li> <li>Shape preservation</li> <li>Numerical stability</li> </ul> <p>Expected: Output shape matches input shape</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#5-loss-computation","title":"5. Loss Computation","text":"<p>Tests:</p> <ul> <li>MSE calculation</li> <li>Gradient flow (implicit)</li> <li>JAX autodiff compatibility</li> </ul> <p>Expected: Loss value computed successfully</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/protein/protein-diffusion-tech-validation/#step-1-import-and-check-dependencies","title":"Step 1: Import and Check Dependencies","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\ntry:\n    from Bio.PDB import PDBParser\n    HAS_BIOPYTHON = True\nexcept ImportError:\n    HAS_BIOPYTHON = False\n    print(\"Biopython not installed. Will use synthetic data.\")\n</code></pre> <p>The script gracefully handles missing BioPython by falling back to synthetic data.</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#step-2-define-simple-protein-model","title":"Step 2: Define Simple Protein Model","text":"<pre><code>class SimpleProteinPointCloud(nnx.Module):\n    features: int = 32\n    hidden_dim: int = 64\n    output_dim: int = 3  # 3D coordinates\n\n    def __init__(self, rngs: nnx.Rngs):\n        super().__init__()\n        self.encoder = nnx.Linear(in_features=3, out_features=self.features, rngs=rngs)\n        self.hidden = nnx.Linear(in_features=self.features, out_features=self.hidden_dim, rngs=rngs)\n        self.decoder = nnx.Linear(in_features=self.hidden_dim, out_features=self.output_dim, rngs=rngs)\n\n    def __call__(self, points):\n        x = self.encoder(points)\n        x = nnx.relu(x)\n        x = self.hidden(x)\n        x = nnx.relu(x)\n        x = self.decoder(x)\n        return x\n</code></pre> <p>Architecture:</p> <ul> <li>Input: (N, 3) point cloud</li> <li>Encoder: 3 \u2192 32 dimensions</li> <li>Hidden: 32 \u2192 64 dimensions</li> <li>Decoder: 64 \u2192 3 dimensions</li> <li>Output: (N, 3) transformed point cloud</li> </ul>"},{"location":"examples/protein/protein-diffusion-tech-validation/#step-3-generate-or-load-protein-data","title":"Step 3: Generate or Load Protein Data","text":"<pre><code>def create_synthetic_protein_data(n_points=100):\n    \"\"\"Create synthetic protein point cloud data.\"\"\"\n    key = jax.random.key(42)\n    points = jax.random.normal(key, (n_points, 3))\n    return points\n</code></pre> <p>Synthetic data is a simple Gaussian distribution in 3D space, simulating protein atom positions.</p> <p>Real data (with BioPython):</p> <pre><code>def load_protein_from_pdb(pdb_file):\n    \"\"\"Load protein coordinates from a PDB file.\"\"\"\n    parser = PDBParser()\n    structure = parser.get_structure(\"protein\", pdb_file)\n\n    # Extract C-alpha atoms\n    coords = []\n    for model in structure:\n        for chain in model:\n            for residue in chain:\n                if \"CA\" in residue:\n                    ca_atom = residue[\"CA\"]\n                    coords.append(ca_atom.get_coord())\n\n    return jnp.array(coords)\n</code></pre> <p>C-alpha (CA) atoms form the protein backbone and provide a coarse-grained representation.</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#step-4-run-validation","title":"Step 4: Run Validation","text":"<pre><code># Create model\nrngs = nnx.Rngs(params=jax.random.key(0))\nmodel = SimpleProteinPointCloud(rngs=rngs)\n\n# Generate data\nprotein_coords = create_synthetic_protein_data(n_points=100)\n\n# Forward pass\noutput = model(protein_coords)\n\n# Compute loss\nloss = jnp.mean((output - protein_coords) ** 2)\n</code></pre> <p>Validation checklist:</p> <p>\u2705 Model instantiates without errors</p> <p>\u2705 Forward pass completes</p> <p>\u2705 Output shape is (100, 3)</p> <p>\u2705 Loss is a valid number</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#expected-output","title":"Expected Output","text":"<pre><code>Biopython not installed. Will use synthetic data.\nJAX version: 0.7.2\nBiopython available: False\nProtein coordinates shape: (100, 3)\nModel output shape: (100, 3)\nLoss: 1.4674725532531738\nTechnology validation successful!\n</code></pre> <p>What each line means:</p> Output Meaning <code>JAX version: 0.7.2</code> JAX is installed and version is \u2265 0.7 <code>Biopython available: False</code> Optional dependency status <code>Protein coordinates shape: (100, 3)</code> 100 atoms in 3D space <code>Model output shape: (100, 3)</code> Forward pass preserves shape <code>Loss: 1.467...</code> MSE between input and output <code>Technology validation successful!</code> All checks passed \u2713"},{"location":"examples/protein/protein-diffusion-tech-validation/#understanding-protein-point-clouds","title":"Understanding Protein Point Clouds","text":""},{"location":"examples/protein/protein-diffusion-tech-validation/#what-are-c-alpha-ca-atoms","title":"What are C-alpha (CA) Atoms?","text":"<p>Proteins are chains of amino acids. Each amino acid has:</p> <ul> <li>N: Nitrogen (backbone)</li> <li>C-alpha (CA): Central carbon (backbone)</li> <li>C: Carbonyl carbon (backbone)</li> <li>O: Oxygen (backbone)</li> <li>Sidechain: Variable atoms (different for each amino acid)</li> </ul> <p>C-alpha atoms trace the protein backbone and are commonly used for:</p> <ul> <li>Structural alignment</li> <li>Coarse-grained modeling</li> <li>Fast structure prediction</li> <li>Low-resolution analysis</li> </ul>"},{"location":"examples/protein/protein-diffusion-tech-validation/#point-cloud-representation","title":"Point Cloud Representation","text":"<pre><code>Protein sequence: M-E-T-H-I-O-N-I-N-E\n                  \u2193  \u2193  \u2193  \u2193  \u2193  \u2193  \u2193  \u2193  \u2193  \u2193\nCA atoms:        (x\u2081,y\u2081,z\u2081), (x\u2082,y\u2082,z\u2082), ...\n</code></pre> <p>Point cloud format:</p> <pre><code>coordinates.shape  # (N, 3) where N = number of residues\ncoordinates[0]     # [x, y, z] for first CA atom\n</code></pre>"},{"location":"examples/protein/protein-diffusion-tech-validation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/protein/protein-diffusion-tech-validation/#no-module-named-jax","title":"\"No module named 'jax'\"","text":"<p>Cause: JAX not installed</p> <p>Solution:</p> <pre><code>uv sync --extra cuda-dev  # For GPU\n# or\nuv sync  # For CPU only\n</code></pre>"},{"location":"examples/protein/protein-diffusion-tech-validation/#no-module-named-flax","title":"\"No module named 'flax'\"","text":"<p>Cause: Flax not installed</p> <p>Solution:</p> <pre><code>source activate.sh  # Activates environment with Flax\n</code></pre>"},{"location":"examples/protein/protein-diffusion-tech-validation/#biopython-not-installed-warning","title":"\"Biopython not installed\" warning","text":"<p>Cause: BioPython is optional and not installed</p> <p>Impact: None - synthetic data works fine for validation</p> <p>To install (optional):</p> <pre><code>uv add biopython\n</code></pre>"},{"location":"examples/protein/protein-diffusion-tech-validation/#loss-value-is-nan-or-inf","title":"Loss value is NaN or Inf","text":"<p>Possible causes:</p> <ol> <li>Model initialization issue: Check RNG keys are valid</li> <li>Numerical instability: Add layer normalization</li> <li>Data issue: Verify protein_coords contains valid floats</li> </ol> <p>Debug:</p> <pre><code>print(\"Coords min/max:\", protein_coords.min(), protein_coords.max())\nprint(\"Output min/max:\", output.min(), output.max())\n</code></pre>"},{"location":"examples/protein/protein-diffusion-tech-validation/#different-loss-value-than-documentation","title":"Different loss value than documentation","text":"<p>This is normal! The exact loss depends on:</p> <ul> <li>Random initialization (from RNG seed)</li> <li>JAX version</li> <li>Hardware (CPU vs GPU)</li> <li>Floating point precision</li> </ul> <p>As long as the loss is a reasonable number (not NaN/Inf), validation passed.</p>"},{"location":"examples/protein/protein-diffusion-tech-validation/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Protein Point Cloud</p> <p>Full protein structure modeling with constraints</p> <p> protein_point_cloud_example.py</p> </li> <li> <p> Protein Extensions</p> <p>Domain-specific extensions for proteins</p> <p> protein_extensions_example.py</p> </li> <li> <p> Protein with Modality</p> <p>Using the modality architecture for proteins</p> <p> protein_model_with_modality.py</p> </li> <li> <p> Protein-Ligand Benchmark</p> <p>Advanced: SE(3) equivariant protein modeling</p> <p> protein_ligand_benchmark_demo.py</p> </li> </ul>"},{"location":"examples/protein/protein-diffusion-tech-validation/#additional-resources","title":"Additional Resources","text":"<ul> <li>JAX Documentation - JAX fundamentals</li> <li>Flax NNX Guide - NNX module system</li> <li>BioPython Tutorial - PDB file parsing</li> <li>Protein Data Bank (PDB) - Download protein structures</li> <li>AlphaFold - Protein structure prediction</li> </ul>"},{"location":"examples/protein/protein-extensions-example/","title":"Protein Extensions Example","text":"Intermediate \u26a1 10 seconds \ud83d\udcd3 Dual Format <p>Learn how to use protein-specific extensions to add domain knowledge and physical constraints to geometric models.</p>"},{"location":"examples/protein/protein-extensions-example/#files","title":"Files","text":"<ul> <li>Python Script: <code>protein_extensions_example.py</code></li> <li>Jupyter Notebook: <code>protein_extensions_example.ipynb</code></li> </ul>"},{"location":"examples/protein/protein-extensions-example/#quick-start","title":"Quick Start","text":"<pre><code># Clone and setup\ncd artifex\nsource activate.sh\n\n# Run Python script\npython examples/generative_models/protein/protein_extensions_example.py\n\n# Or use Jupyter notebook\njupyter notebook examples/generative_models/protein/protein_extensions_example.ipynb\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#overview","title":"Overview","text":"<p>This tutorial demonstrates Artifex's extension system for incorporating protein-specific knowledge into geometric models. Extensions are modular components that add domain expertise without modifying the base model architecture.</p>"},{"location":"examples/protein/protein-extensions-example/#learning-objectives","title":"Learning Objectives","text":"<ul> <li> Understand the extension architecture in Artifex</li> <li> Use bond length constraints for realistic protein geometry</li> <li> Apply bond angle constraints for proper backbone structure</li> <li> Incorporate amino acid sequence information with mixins</li> <li> Combine multiple extensions for comprehensive modeling</li> <li> Calculate extension-aware losses automatically</li> </ul>"},{"location":"examples/protein/protein-extensions-example/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of protein structure (backbone atoms: N, CA, C, O)</li> <li>Familiarity with PointCloudModel from Artifex</li> <li>Basic knowledge of chemical bonds and angles</li> <li>Understanding of loss functions</li> </ul>"},{"location":"examples/protein/protein-extensions-example/#why-use-extensions","title":"Why Use Extensions?","text":""},{"location":"examples/protein/protein-extensions-example/#the-problem","title":"The Problem","text":"<p>Generic geometric models don't know about protein physics:</p> <p>\u274c Without Extensions:</p> <ul> <li>No knowledge of realistic bond lengths (C-C ~1.5\u00c5)</li> <li>No enforcement of proper bond angles (~109.5\u00b0 tetrahedral)</li> <li>No awareness of amino acid types (A, G, L, etc.)</li> <li>Models can generate physically impossible structures</li> </ul> <p>\u2705 With Extensions:</p> <ul> <li>Enforces chemical bond constraints</li> <li>Maintains proper molecular geometry</li> <li>Incorporates sequence information</li> <li>Produces chemically valid structures</li> </ul>"},{"location":"examples/protein/protein-extensions-example/#the-solution-modular-extensions","title":"The Solution: Modular Extensions","text":"<p>Extensions are plug-and-play components:</p> <pre><code>Base Model (PointCloud)\n       \u2193\n  + Extensions\n       \u2193\nProtein-Aware Model\n</code></pre> <p>Key advantage: Same base model can be used for proteins, molecules, materials, etc., by swapping extensions.</p>"},{"location":"examples/protein/protein-extensions-example/#extension-types","title":"Extension Types","text":""},{"location":"examples/protein/protein-extensions-example/#1-bond-length-extension","title":"1. Bond Length Extension","text":"<p>Purpose: Enforce realistic distances between bonded atoms</p> <p>How it works:</p> <ol> <li>Identifies bonded atom pairs (e.g., CA-C, C-N, N-CA)</li> <li>Measures current distances</li> <li>Compares to ideal bond lengths</li> <li>Adds penalty for deviations</li> </ol> <p>Typical bond lengths:</p> Bond Type Ideal Length (\u00c5) Tolerance C-C (single) 1.54 \u00b10.02 C=C (double) 1.34 \u00b10.02 C-N 1.47 \u00b10.02 C=O 1.23 \u00b10.02 N-H 1.01 \u00b10.02 <p>Loss formula:</p> <pre><code>L_bond_length = (1/N) \u03a3 w_i * (d_i - d_ideal)\u00b2\n</code></pre> <p>Where:</p> <ul> <li><code>d_i</code> = measured distance</li> <li><code>d_ideal</code> = target distance</li> <li><code>w_i</code> = bond weight (stronger bonds = higher weight)</li> <li><code>N</code> = number of bonds</li> </ul>"},{"location":"examples/protein/protein-extensions-example/#2-bond-angle-extension","title":"2. Bond Angle Extension","text":"<p>Purpose: Maintain proper angles between consecutive bonds</p> <p>How it works:</p> <ol> <li>Identifies triplets of bonded atoms (e.g., CA-C-N)</li> <li>Calculates current angle</li> <li>Compares to ideal geometry</li> <li>Penalizes deviations</li> </ol> <p>Common bond angles:</p> Geometry Ideal Angle Example Tetrahedral 109.5\u00b0 sp\u00b3 carbon (CA) Trigonal planar 120\u00b0 sp\u00b2 carbon (C=O) Linear 180\u00b0 sp carbon (rare) Peptide bond ~120\u00b0 C-N-CA <p>Loss formula:</p> <pre><code>L_bond_angle = (1/M) \u03a3 w_j * (\u03b8_j - \u03b8_ideal)\u00b2\n</code></pre> <p>Where:</p> <ul> <li><code>\u03b8_j</code> = measured angle</li> <li><code>\u03b8_ideal</code> = target angle</li> <li><code>w_j</code> = angle weight</li> <li><code>M</code> = number of angles</li> </ul>"},{"location":"examples/protein/protein-extensions-example/#3-protein-mixin-extension","title":"3. Protein Mixin Extension","text":"<p>Purpose: Add amino acid sequence information</p> <p>How it works:</p> <ol> <li>Takes amino acid types as input (20 standard amino acids)</li> <li>Embeds each type into a learned vector</li> <li>Adds sequence-aware features to model</li> <li>Helps model understand residue-specific properties</li> </ol> <p>Amino acid properties encoded:</p> <ul> <li>Hydrophobicity (water-loving vs water-fearing)</li> <li>Size (small glycine vs large tryptophan)</li> <li>Charge (positive, negative, neutral)</li> <li>Aromaticity (ring structures)</li> <li>Secondary structure preference (helix, sheet, loop)</li> </ul> <p>Architecture:</p> <pre><code>Amino Acid Type (0-19)\n       \u2193\nEmbedding Layer (learned)\n       \u2193\nFeature Vector (e.g., 32-dim)\n       \u2193\nConcatenate with position features\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/protein/protein-extensions-example/#step-1-setup-and-create-test-data","title":"Step 1: Setup and Create Test Data","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# Create synthetic protein data\nbatch_size = 2\nnum_residues = 10\nnum_atoms = 4  # N, CA, C, O backbone atoms\n\n# Random 3D coordinates\npositions = jax.random.normal(key, (batch_size, num_residues * num_atoms, 3))\n\n# Random amino acid types (0-19 for 20 standard amino acids)\naatype = jax.random.randint(key, (batch_size, num_residues), 0, 20)\n\n# Atom mask (1 = present, 0 = missing)\natom_mask = jnp.ones((batch_size, num_residues * num_atoms))\n\n# Package into batch\nbatch = {\n    \"positions\": positions,\n    \"aatype\": aatype,\n    \"atom_mask\": atom_mask,\n}\n</code></pre> <p>Batch structure:</p> <ul> <li><code>positions</code>: (2, 40, 3) - 2 proteins, 40 atoms each, xyz coordinates</li> <li><code>aatype</code>: (2, 10) - 2 proteins, 10 residues each</li> <li><code>atom_mask</code>: (2, 40) - which atoms are present</li> </ul>"},{"location":"examples/protein/protein-extensions-example/#step-2-create-extensions-via-utility-function","title":"Step 2: Create Extensions via Utility Function","text":"<pre><code>from artifex.generative_models.extensions.protein import create_protein_extensions\n\nextension_config = {\n    \"use_backbone_constraints\": True,      # Enable bond length/angle\n    \"bond_length_weight\": 1.0,             # Weight for bond length loss\n    \"bond_angle_weight\": 0.5,              # Weight for bond angle loss (lower = softer constraint)\n    \"use_protein_mixin\": True,             # Enable amino acid encoding\n    \"aa_embedding_dim\": 16,                # Embedding dimension\n}\n\nextensions = create_protein_extensions(extension_config, rngs=rngs)\n</code></pre> <p>This creates an <code>nnx.Dict</code> containing:</p> <ul> <li><code>bond_length</code>: BondLengthExtension</li> <li><code>bond_angle</code>: BondAngleExtension</li> <li><code>protein_mixin</code>: ProteinMixinExtension</li> </ul> <p>Why use the utility function?</p> <p>\u2705 Pros:</p> <ul> <li>Handles compatibility between extensions</li> <li>Sets up proper dependencies</li> <li>Uses sensible defaults</li> <li>Less boilerplate code</li> </ul> <p>\u274c When not to use:</p> <ul> <li>Need very custom extension combinations</li> <li>Debugging specific extension behavior</li> <li>Research/experimentation with new extensions</li> </ul>"},{"location":"examples/protein/protein-extensions-example/#step-3-attach-extensions-to-model","title":"Step 3: Attach Extensions to Model","text":"<pre><code>from artifex.generative_models.models.geometric import PointCloudModel\nfrom artifex.generative_models.core.configuration import (\n    PointCloudConfig,\n    PointCloudNetworkConfig,\n)\n\n# Create network config for point cloud processing\nnetwork_config = PointCloudNetworkConfig(\n    name=\"protein_network\",\n    hidden_dims=(64, 64, 64),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=64,\n    num_heads=4,\n    num_layers=3,\n    dropout_rate=0.1,\n)\n\n# Create point cloud config with nested network config\nmodel_config = PointCloudConfig(\n    name=\"protein_point_cloud_with_extensions\",\n    network=network_config,\n    num_points=num_residues * num_atoms,  # 40 points\n    dropout_rate=0.1,\n)\n\nmodel = PointCloudModel(\n    model_config,\n    extensions=extensions,  # \u2190 Extensions attached here\n    rngs=rngs,\n)\n</code></pre> <p>What happens internally:</p> <ol> <li>Model stores extensions as attributes</li> <li>During forward pass, model calls extensions automatically</li> <li>During loss calculation, extension losses are aggregated</li> <li>Total loss = base_loss + sum(ext_weight * ext_loss)</li> </ol>"},{"location":"examples/protein/protein-extensions-example/#step-4-run-model-and-calculate-losses","title":"Step 4: Run Model and Calculate Losses","text":"<pre><code># Forward pass\noutputs = model(batch)\nprint(f\"Model output shape: {outputs['positions'].shape}\")\n# Output: (2, 40, 3)\n\n# Calculate total loss (includes extension losses)\nloss_fn = model.get_loss_fn()\nloss = loss_fn(batch, outputs)\nprint(f\"Loss with extensions: {loss}\")\n# Output: {'total_loss': 3.56, 'mse_loss': 2.28, 'bond_length': 0.60, 'bond_angle': 0.69, 'protein_mixin': 0.0}\n</code></pre> <p>Loss breakdown:</p> Component Value Weight Contribution <code>mse_loss</code> 2.28 1.0 Base reconstruction <code>bond_length</code> 0.60 1.0 Bond length constraint <code>bond_angle</code> 0.69 0.5 Bond angle constraint (weighted) <code>protein_mixin</code> 0.0 1.0 No loss (encoding only) Total 3.56 - Sum of all components <p>Formula:</p> <pre><code>total_loss = mse_loss + (1.0 * bond_length) + (0.5 * bond_angle) + (1.0 * protein_mixin)\n          = 2.28 + 0.60 + 0.345 + 0.0\n          = 3.225  (approximately, due to rounding)\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#step-5-access-extension-outputs","title":"Step 5: Access Extension Outputs","text":"<pre><code># Get detailed metrics from each extension\nfor name, extension in extensions.items():\n    ext_outputs = extension(batch, outputs)\n    print(f\"Extension {name} outputs: {list(ext_outputs.keys())}\")\n</code></pre> <p>Output:</p> <pre><code>Extension bond_length outputs: ['bond_distances', 'bond_violations', 'extension_type']\nExtension bond_angle outputs: ['bond_angles', 'angle_violations', 'extension_type']\nExtension protein_mixin outputs: ['extension_type', 'aa_encoding']\n</code></pre> <p>What each output contains:</p> <p>BondLengthExtension:</p> <ul> <li><code>bond_distances</code>: Measured distances for all bonds (\u00c5)</li> <li><code>bond_violations</code>: Count of bonds outside tolerance</li> <li><code>extension_type</code>: \"bond_length\"</li> </ul> <p>BondAngleExtension:</p> <ul> <li><code>bond_angles</code>: Measured angles for all triplets (degrees)</li> <li><code>angle_violations</code>: Count of angles outside tolerance</li> <li><code>extension_type</code>: \"bond_angle\"</li> </ul> <p>ProteinMixinExtension:</p> <ul> <li><code>aa_encoding</code>: Embedded amino acid features (batch, num_residues, embedding_dim)</li> <li><code>extension_type</code>: \"protein_mixin\"</li> </ul>"},{"location":"examples/protein/protein-extensions-example/#step-6-using-individual-extensions","title":"Step 6: Using Individual Extensions","text":"<p>For fine-grained control, create extensions manually:</p> <pre><code>from artifex.generative_models.extensions.base.extensions import ExtensionConfig\nfrom artifex.generative_models.extensions.protein import BondLengthExtension\n\n# Create extension config\nbond_length_config = ExtensionConfig(\n    name=\"bond_length\",\n    weight=1.0,\n    enabled=True,\n    extensions={}  # Extension-specific params (if needed)\n)\n\n# Instantiate extension\nbond_length_ext = BondLengthExtension(bond_length_config, rngs=rngs)\n\n# Use extension\nmetrics = bond_length_ext(batch, outputs)\nloss = bond_length_ext.loss_fn(batch, outputs)\n\nprint(f\"Bond length loss: {loss}\")  # 0.598\n</code></pre> <p>When to use individual extensions:</p> <ol> <li>Debugging: Isolate specific extension behavior</li> <li>Custom loss weighting: Dynamic weight schedules</li> <li>Selective application: Apply only to certain batches</li> <li>Research: Experiment with new extension combinations</li> </ol>"},{"location":"examples/protein/protein-extensions-example/#expected-output","title":"Expected Output","text":"<pre><code>Model output shape: (2, 40, 3)\nLoss with extensions: {'total_loss': Array(3.56, dtype=float32), 'mse_loss': Array(2.28, dtype=float32), 'bond_length': Array(0.60, dtype=float32), 'bond_angle': Array(0.69, dtype=float32), 'protein_mixin': Array(0., dtype=float32)}\nExtension bond_length outputs: ['bond_distances', 'bond_violations', 'extension_type']\nExtension bond_angle outputs: ['bond_angles', 'angle_violations', 'extension_type']\nExtension protein_mixin outputs: ['extension_type', 'aa_encoding']\n\nUsing individual extensions:\nBond length metrics: ['bond_distances', 'bond_violations', 'extension_type']\nBond length loss: 0.5976787209510803\nBond angle metrics: ['bond_angles', 'angle_violations', 'extension_type']\nBond angle loss: 0.6547483801841736\nAmino acid encoding shape: (2, 10, 21)\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#understanding-extension-architecture","title":"Understanding Extension Architecture","text":""},{"location":"examples/protein/protein-extensions-example/#design-principles","title":"Design Principles","text":""},{"location":"examples/protein/protein-extensions-example/#1-modularity","title":"1. Modularity","text":"<p>Extensions are independent and composable:</p> <pre><code># Can mix and match\nextensions_A = {\"bond_length\": ext1}\nextensions_B = {\"bond_length\": ext1, \"bond_angle\": ext2}\nextensions_C = {\"protein_mixin\": ext3}\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#2-compatibility","title":"2. Compatibility","text":"<p>All extensions follow the same protocol:</p> <pre><code>class Extension(Protocol):\n    def __call__(self, batch, outputs) -&gt; Dict:\n        \"\"\"Compute extension outputs\"\"\"\n        ...\n\n    def loss_fn(self, batch, outputs) -&gt; float:\n        \"\"\"Compute extension loss\"\"\"\n        ...\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#3-automatic-integration","title":"3. Automatic Integration","text":"<p>Models handle extensions transparently:</p> <pre><code># Model automatically:\n# 1. Calls each extension during forward pass\n# 2. Aggregates losses with weights\n# 3. Returns combined loss\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#extension-lifecycle","title":"Extension Lifecycle","text":"<pre><code>1. Initialization\n   \u251c\u2500 Create extension config\n   \u251c\u2500 Instantiate extension with RNGs\n   \u2514\u2500 Attach to model\n\n2. Forward Pass\n   \u251c\u2500 Model processes input\n   \u251c\u2500 Extension processes (batch, outputs)\n   \u2514\u2500 Extension returns metrics dict\n\n3. Loss Calculation\n   \u251c\u2500 Extension computes its loss\n   \u251c\u2500 Model weights extension loss\n   \u2514\u2500 Adds to total loss\n\n4. Backward Pass\n   \u2514\u2500 Gradients flow through extension\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#experiments-to-try","title":"Experiments to Try","text":""},{"location":"examples/protein/protein-extensions-example/#1-adjust-extension-weights","title":"1. Adjust Extension Weights","text":"<pre><code># Experiment with different weight combinations\nconfigs = [\n    {\"bond_length_weight\": 1.0, \"bond_angle_weight\": 0.0},  # Only length\n    {\"bond_length_weight\": 0.0, \"bond_angle_weight\": 1.0},  # Only angle\n    {\"bond_length_weight\": 2.0, \"bond_angle_weight\": 1.0},  # Stronger length\n    {\"bond_length_weight\": 0.5, \"bond_angle_weight\": 2.0},  # Stronger angle\n]\n\nfor config in configs:\n    extensions = create_protein_extensions(config, rngs=rngs)\n    # Train and compare results\n</code></pre> <p>Observation: Higher weights enforce stricter constraints but may limit flexibility.</p>"},{"location":"examples/protein/protein-extensions-example/#2-compare-with-and-without-extensions","title":"2. Compare With and Without Extensions","text":"<pre><code># Model without extensions\nmodel_vanilla = PointCloudModel(model_config, rngs=rngs)\noutputs_vanilla = model_vanilla(batch)\n\n# Model with extensions\nmodel_extended = PointCloudModel(model_config, extensions=extensions, rngs=rngs)\noutputs_extended = model_extended(batch)\n\n# Compare outputs\n# Which produces more realistic bond lengths?\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#3-visualize-extension-effects","title":"3. Visualize Extension Effects","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Extract bond lengths\nmetrics = bond_length_ext(batch, outputs)\nbond_distances = metrics['bond_distances']\n\n# Plot distribution\nplt.hist(bond_distances, bins=50)\nplt.axvline(x=1.54, color='r', linestyle='--', label='Ideal C-C')\nplt.axvline(x=1.47, color='g', linestyle='--', label='Ideal C-N')\nplt.legend()\nplt.xlabel('Bond Length (\u00c5)')\nplt.ylabel('Count')\nplt.title('Bond Length Distribution')\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#4-custom-extension-combinations","title":"4. Custom Extension Combinations","text":"<pre><code># Create custom extension set\nfrom artifex.generative_models.extensions.protein import ProteinBackboneConstraint\n\ncustom_extensions = nnx.Dict({\n    \"bond_length\": BondLengthExtension(config1, rngs=rngs),\n    \"backbone\": ProteinBackboneConstraint(config2, rngs=rngs),\n    # No angle constraint - looser model\n})\n\nmodel = PointCloudModel(model_config, extensions=custom_extensions, rngs=rngs)\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/protein/protein-extensions-example/#dynamic-extension-weighting","title":"Dynamic Extension Weighting","text":"<p>Adjust weights during training:</p> <pre><code>def get_extension_weights(epoch):\n    \"\"\"Gradually increase constraint strength\"\"\"\n    return {\n        \"bond_length_weight\": min(1.0, epoch / 100),  # Ramp up over 100 epochs\n        \"bond_angle_weight\": min(0.5, epoch / 200),   # Ramp up slower\n        \"use_protein_mixin\": True,\n        \"aa_embedding_dim\": 16,\n    }\n\nfor epoch in range(num_epochs):\n    config = get_extension_weights(epoch)\n    extensions = create_protein_extensions(config, rngs=rngs)\n    model = PointCloudModel(model_config, extensions=extensions, rngs=rngs)\n    # Train...\n</code></pre> <p>Rationale: Start with weak constraints (let model learn), then tighten (refine to physics).</p>"},{"location":"examples/protein/protein-extensions-example/#extension-specific-loss-weighting","title":"Extension-Specific Loss Weighting","text":"<pre><code># Access individual losses for custom weighting\nloss_dict = loss_fn(batch, outputs)\n\ncustom_total_loss = (\n    1.0 * loss_dict['mse_loss'] +\n    2.0 * loss_dict['bond_length'] +     # Prioritize bond lengths\n    0.1 * loss_dict['bond_angle'] +      # Soft angle constraint\n    0.5 * loss_dict['protein_mixin']     # Moderate mixin contribution\n)\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#conditional-extensions","title":"Conditional Extensions","text":"<p>Apply extensions selectively:</p> <pre><code>def conditional_loss(batch, outputs, is_training):\n    if is_training:\n        # Use all extensions during training\n        return model.get_loss_fn()(batch, outputs)\n    else:\n        # Use only base loss during evaluation\n        return {'total_loss': loss_dict['mse_loss']}\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Protein Model Extension</p> <p>More extension examples with backbone constraints</p> <p> protein_model_extension.py</p> </li> <li> <p> Protein Extensions with Config</p> <p>Using the configuration system for extensions</p> <p> protein_extensions_with_config.py</p> </li> <li> <p> Protein Point Cloud</p> <p>Full protein modeling with constraints</p> <p> protein_point_cloud_example.py</p> </li> <li> <p> Custom Extensions</p> <p>Create your own domain-specific extensions</p> <p> ../../guides/custom-extensions.md</p> </li> </ul>"},{"location":"examples/protein/protein-extensions-example/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/protein/protein-extensions-example/#typeerror-config-must-be-extensionconfig","title":"TypeError: config must be ExtensionConfig","text":"<p>Cause: Passing a plain dict instead of ExtensionConfig object</p> <p>Wrong:</p> <pre><code>ext = BondLengthExtension({\"name\": \"bond\", \"weight\": 1.0}, rngs=rngs)\n</code></pre> <p>Correct:</p> <pre><code>config = ExtensionConfig(name=\"bond\", weight=1.0, enabled=True, extensions={})\next = BondLengthExtension(config, rngs=rngs)\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#extension-loss-is-nan","title":"Extension loss is NaN","text":"<p>Possible causes:</p> <ol> <li>Missing required batch keys: Extensions need <code>positions</code>, <code>aatype</code>, <code>atom_mask</code></li> <li>Invalid atom positions: Check for Inf/NaN in input</li> <li>Division by zero: Empty atom mask (all zeros)</li> </ol> <p>Debug:</p> <pre><code>print(\"Batch keys:\", batch.keys())\nprint(\"Positions range:\", batch['positions'].min(), batch['positions'].max())\nprint(\"Atom mask sum:\", batch['atom_mask'].sum())\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#extension-not-affecting-loss","title":"Extension not affecting loss","text":"<p>Cause: Extension weight is 0 or extension is disabled</p> <p>Check:</p> <pre><code>print(\"Extension config:\", extension.config)\nprint(\"Weight:\", extension.config.weight)\nprint(\"Enabled:\", extension.config.enabled)\n</code></pre> <p>Fix:</p> <pre><code>extension.config.weight = 1.0\nextension.config.enabled = True\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#bond-violations-are-high","title":"Bond violations are high","text":"<p>Expected: Initial violations are normal with random initialization</p> <p>Solutions:</p> <ol> <li>Train longer: Extensions need time to learn constraints</li> <li>Increase weight: Stronger penalty for violations</li> <li>Check bond topology: Ensure atom connectivity is correct</li> <li>Verify atom mask: Missing atoms can cause false violations</li> </ol> <p>Monitor:</p> <pre><code>metrics = bond_length_ext(batch, outputs)\nviolations = metrics['bond_violations']\nprint(f\"Violations: {violations} / {total_bonds}\")\n</code></pre>"},{"location":"examples/protein/protein-extensions-example/#additional-resources","title":"Additional Resources","text":"<ul> <li>Extension System Design - Architecture overview</li> <li>Creating Custom Extensions - Build your own</li> <li>Protein Modeling Guide - Comprehensive protein tutorial</li> <li>Chemical Constraints - Theory behind constraints</li> <li>Artifex Extension API - Full API reference</li> </ul>"},{"location":"examples/protein/protein-extensions-with-config/","title":"Protein Extensions with Configuration System","text":"<p>Level: Intermediate | Runtime: ~10 seconds (CPU/GPU) | Format: Python + Jupyter</p>"},{"location":"examples/protein/protein-extensions-with-config/#overview","title":"Overview","text":"<p>This example demonstrates how to use protein extensions with Artifex's Pydantic-based configuration system. Protein extensions add domain-specific capabilities (backbone constraints, amino acid embeddings) to geometric models through a modular, composable architecture. You'll learn how to load configurations from YAML files, create extensions programmatically, and integrate them with geometric models.</p>"},{"location":"examples/protein/protein-extensions-with-config/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Understand protein extensions and their modular architecture</li> <li>Load extension configurations from YAML files</li> <li>Create protein extensions programmatically with Pydantic models</li> <li>Integrate extensions with geometric models (PointCloudModel)</li> <li>Use configuration validation and serialization features</li> <li>Calculate extension-specific losses (bond length, bond angle)</li> </ul>"},{"location":"examples/protein/protein-extensions-with-config/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/protein/protein_extensions_with_config.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/protein/protein_extensions_with_config.ipynb</code></li> </ul>"},{"location":"examples/protein/protein-extensions-with-config/#quick-start","title":"Quick Start","text":""},{"location":"examples/protein/protein-extensions-with-config/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/protein/protein_extensions_with_config.py\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/protein/protein_extensions_with_config.ipynb\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/protein/protein-extensions-with-config/#protein-extensions","title":"Protein Extensions","text":"<p>Modular components that add protein-specific functionality to generic geometric models:</p> <p>Backbone Constraints Extension:</p> <ul> <li>Enforces realistic bond lengths between atoms</li> <li>Enforces realistic bond angles</li> <li>Penalizes violations during training</li> </ul> <p>Protein Mixin Extension:</p> <ul> <li>Embeds amino acid types (20 standard amino acids)</li> <li>Processes sequence information</li> <li>Integrates with geometric features</li> </ul> <p>Extensibility:</p> <ul> <li>Easy to add new protein-specific features</li> <li>Composable: mix and match extensions</li> <li>Minimal coupling with base models</li> </ul>"},{"location":"examples/protein/protein-extensions-with-config/#configuration-system","title":"Configuration System","text":"<p>Artifex uses Pydantic models for type-safe, validated configurations:</p> <p>Type Safety:</p> <pre><code>class ProteinExtensionConfig(BaseModel):\n    name: str                              # Required field\n    use_backbone_constraints: bool = True  # With default\n    bond_length_weight: float = 1.0        # Validated type\n</code></pre> <p>YAML Integration:</p> <pre><code># protein.yaml\nname: \"protein_extensions\"\nuse_backbone_constraints: true\nbond_length_weight: 1.0\n</code></pre> <p>Benefits:</p> <ul> <li>Automatic validation at load time</li> <li>Self-documenting through schemas</li> <li>Easy to serialize/deserialize</li> <li>Version control friendly</li> </ul>"},{"location":"examples/protein/protein-extensions-with-config/#code-structure","title":"Code Structure","text":"<p>The example demonstrates nine major sections:</p> <ol> <li>Setup: Import libraries and initialize RNGs</li> <li>Load Configuration: From YAML or create programmatically</li> <li>Convert Config: Map to extension parameters</li> <li>Create Extensions: Using factory function</li> <li>Configure Model: Set up PointCloudModel</li> <li>Create Model: With extensions attached</li> <li>Prepare Data: Synthetic protein structures</li> <li>Forward Pass: Test model and extensions</li> <li>Calculate Losses: Reconstruction + extension losses</li> </ol>"},{"location":"examples/protein/protein-extensions-with-config/#example-code","title":"Example Code","text":""},{"location":"examples/protein/protein-extensions-with-config/#loading-configuration-from-yaml","title":"Loading Configuration from YAML","text":"<pre><code>from artifex.configs.schema.extensions import ProteinExtensionConfig\nfrom artifex.configs.utils import create_config_from_yaml\n\n# Load from YAML file\nconfig_path = \"configs/protein.yaml\"\nextension_config = create_config_from_yaml(config_path, ProteinExtensionConfig)\n\n# Automatic validation\nprint(f\"Loaded config: {extension_config.name}\")\nprint(f\"Backbone constraints: {extension_config.use_backbone_constraints}\")\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#creating-configuration-programmatically","title":"Creating Configuration Programmatically","text":"<pre><code># Fallback: create config in code\nextension_config = ProteinExtensionConfig(\n    name=\"my_protein_extensions\",\n    description=\"Custom protein extension config\",\n    use_backbone_constraints=True,\n    use_protein_mixin=True,\n)\n\n# Pydantic validates all fields automatically\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#creating-protein-extensions","title":"Creating Protein Extensions","text":"<pre><code>from artifex.generative_models.extensions.protein import create_protein_extensions\n\n# Convert config to extension parameters\nprotein_config = {\n    \"use_backbone_constraints\": True,\n    \"bond_length_weight\": 1.0,\n    \"bond_angle_weight\": 0.5,\n    \"use_protein_mixin\": True,\n    \"aa_embedding_dim\": 16,\n    \"num_aa_types\": 20,\n}\n\n# Create extensions\nextensions = create_protein_extensions(protein_config, rngs=rngs)\nprint(f\"Created extensions: {', '.join(extensions.keys())}\")\n# Output: Created extensions: backbone_constraints, protein_mixin\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#integrating-with-models","title":"Integrating with Models","text":"<pre><code>from artifex.generative_models.models.geometric import PointCloudModel\nfrom artifex.generative_models.core.configuration import (\n    PointCloudConfig,\n    PointCloudNetworkConfig,\n)\n\n# Create network config for point cloud processing\nnetwork_config = PointCloudNetworkConfig(\n    name=\"protein_network\",\n    hidden_dims=(64, 64),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=64,\n    num_heads=4,\n    num_layers=2,\n    dropout_rate=0.1,\n)\n\n# Create model config with nested network config\nmodel_config = PointCloudConfig(\n    name=\"protein_point_cloud\",\n    network=network_config,\n    num_points=40,\n    dropout_rate=0.1,\n)\n\n# Create model with extensions\nmodel = PointCloudModel(model_config, extensions=extensions, rngs=rngs)\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#extension-loss-calculation","title":"Extension Loss Calculation","text":"<pre><code># Forward pass\noutputs = model(coords, deterministic=True)\n\n# Calculate extension losses\ntotal_loss = 0.0\nfor ext_name, extension in model.extensions.items():\n    if hasattr(extension, \"loss_fn\"):\n        ext_loss = extension.loss_fn(batch, outputs)\n        total_loss += ext_loss\n        print(f\"{ext_name}: {ext_loss:.6f}\")\n\n# Output:\n#   backbone_constraints: 0.452000\n#   protein_mixin: 0.123000\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>Modular Extensions: Composable protein-specific capabilities</li> <li>Configuration System: Type-safe Pydantic models with validation</li> <li>YAML Support: Load/save configurations from files</li> <li>Integration: Extensions seamlessly integrate with geometric models</li> <li>Loss Composition: Extension losses combine with reconstruction loss</li> <li>Type Safety: Automatic validation prevents configuration errors</li> </ul>"},{"location":"examples/protein/protein-extensions-with-config/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Adjust Constraint Weights: Control strength of geometric constraints</li> </ol> <pre><code>protein_config[\"bond_length_weight\"] = 2.0  # Stricter\nprotein_config[\"bond_angle_weight\"] = 1.0\n</code></pre> <ol> <li>Disable Extensions: Compare with/without specific extensions</li> </ol> <pre><code>protein_config[\"use_backbone_constraints\"] = False\n# Observe effect on loss and generated structures\n</code></pre> <ol> <li>Modify Embedding Dimension: Change amino acid representation capacity</li> </ol> <pre><code>protein_config[\"aa_embedding_dim\"] = 32  # Larger embeddings\n</code></pre> <ol> <li>Save Configuration to YAML: Version control your settings</li> </ol> <pre><code>import yaml\n\nwith open(\"my_config.yaml\", \"w\") as f:\n    yaml.dump(extension_config.model_dump(), f)\n</code></pre> <ol> <li>Create Custom Extension: Add your own protein-specific functionality</li> </ol> <pre><code># Implement custom extension class\n# Add to create_protein_extensions factory\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/protein/protein-extensions-with-config/#common-issues","title":"Common Issues","text":""},{"location":"examples/protein/protein-extensions-with-config/#configuration-validation-error","title":"Configuration validation error","text":"<p>Symptom:</p> <pre><code>pydantic.ValidationError: 1 validation error for ProteinExtensionConfig\n</code></pre> <p>Cause: Invalid value for a configuration field (wrong type, out of range, etc.)</p> <p>Solution:</p> <pre><code># Check field requirements\nprint(ProteinExtensionConfig.model_json_schema())\n\n# Fix the config\nextension_config = ProteinExtensionConfig(\n    name=\"valid_name\",  # Must be string\n    bond_length_weight=1.0,  # Must be float\n)\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#yaml-file-not-found","title":"YAML file not found","text":"<p>Symptom:</p> <pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'protein.yaml'\n</code></pre> <p>Cause: Config file doesn't exist at specified path.</p> <p>Solution:</p> <pre><code># Use absolute path\nimport os\nconfig_path = os.path.join(os.getcwd(), \"configs/protein.yaml\")\n\n# Or create programmatically as fallback\ntry:\n    config = create_config_from_yaml(config_path, ProteinExtensionConfig)\nexcept FileNotFoundError:\n    config = ProteinExtensionConfig(...)  # Fallback\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#extension-has-no-loss_fn","title":"Extension has no loss_fn","text":"<p>Symptom:</p> <pre><code>AttributeError: 'BackboneConstraints' object has no attribute 'loss_fn'\n</code></pre> <p>Cause: Not all extensions implement loss functions.</p> <p>Solution:</p> <pre><code># Check before calling\nif hasattr(extension, \"loss_fn\"):\n    loss = extension.loss_fn(batch, outputs)\nelse:\n    print(f\"{ext_name} has no loss function\")\n</code></pre>"},{"location":"examples/protein/protein-extensions-with-config/#summary","title":"Summary","text":"<p>In this example, you learned:</p> <ul> <li>\u2705 How protein extensions add modular, domain-specific capabilities to models</li> <li>\u2705 How to use Pydantic-based configurations for type safety and validation</li> <li>\u2705 How to load configurations from YAML files for version control</li> <li>\u2705 How extensions integrate with geometric models and contribute to losses</li> <li>\u2705 How the configuration system provides serialization and documentation</li> </ul> <p>Key Takeaways:</p> <ol> <li>Modularity: Extensions are composable and loosely coupled</li> <li>Type Safety: Pydantic validates configs automatically</li> <li>YAML Integration: Version control friendly configuration files</li> <li>Loss Composition: Extensions contribute domain-specific losses</li> </ol>"},{"location":"examples/protein/protein-extensions-with-config/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Protein Point Cloud</p> <p>Deep dive into protein point cloud modeling</p> <p> protein-point-cloud-example.md</p> </li> <li> <p> Protein with Modality</p> <p>Learn about the modality architecture</p> <p> protein-model-with-modality.md</p> </li> <li> <p> Configuration Guide</p> <p>Complete guide to Artifex's config system</p> <p> configuration-guide.md</p> </li> <li> <p> Custom Extensions</p> <p>Create your own domain-specific extensions</p> <p> custom-extensions.md</p> </li> </ul>"},{"location":"examples/protein/protein-extensions-with-config/#additional-resources","title":"Additional Resources","text":"<ul> <li>Artifex Documentation: Configuration System</li> <li>Artifex Documentation: Protein Extensions</li> <li>Pydantic Documentation: Models and Validation</li> <li>API Reference: ProteinExtensionConfig</li> <li>API Reference: create_protein_extensions</li> </ul>"},{"location":"examples/protein/protein-extensions-with-config/#related-examples","title":"Related Examples","text":"<ul> <li>Protein Point Cloud Example - Detailed protein geometric modeling</li> <li>Protein Model with Modality - Modality architecture integration</li> <li>Geometric Benchmark Demo - Evaluating geometric models</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/","title":"Protein-Ligand Co-Design Benchmark","text":"<p>Level: Advanced | Runtime: ~3-5 minutes (CPU) / ~1-2 minutes (GPU) | Format: Python + Jupyter</p> <p>Prerequisites: Understanding of protein-ligand interactions, drug discovery, and molecular modeling | Target Audience: Computational chemists and drug discovery researchers</p>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#overview","title":"Overview","text":"<p>This example demonstrates a comprehensive benchmark suite for evaluating protein-ligand co-design models. Learn how to use the CrossDocked2020 dataset, compute binding affinity predictions, assess molecular validity, evaluate drug-likeness, and systematically compare model architectures for computational drug discovery.</p>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> Molecular Modality</p> <p>Domain-specific framework for chemical structure representation</p> </li> <li> <p> CrossDocked2020</p> <p>Large-scale protein-ligand binding dataset with 22.5M complexes</p> </li> <li> <p> Binding Affinity</p> <p>Predict and evaluate protein-ligand binding energies (kcal/mol)</p> </li> <li> <p> Molecular Validity</p> <p>Assess chemical plausibility of generated structures</p> </li> <li> <p> Drug-likeness (QED)</p> <p>Quantify pharmaceutical potential using QED scores</p> </li> <li> <p> Benchmark Suite</p> <p>Systematically evaluate and compare model architectures</p> </li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#files","title":"Files","text":"<p>This example is available in two formats:</p> <ul> <li>Python Script: <code>protein_ligand_benchmark_demo.py</code></li> <li>Jupyter Notebook: <code>protein_ligand_benchmark_demo.ipynb</code></li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#quick-start","title":"Quick Start","text":""},{"location":"examples/protein/protein-ligand-benchmark-demo/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the complete demo\npython examples/generative_models/protein/protein_ligand_benchmark_demo.py\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/protein/protein_ligand_benchmark_demo.ipynb\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/protein/protein-ligand-benchmark-demo/#1-protein-ligand-co-design","title":"1. Protein-Ligand Co-Design","text":"<p>Simultaneously optimizing both the protein binding site and ligand molecule for strong, specific binding:</p> <pre><code>Protein Pocket + Ligand \u2192 Protein-Ligand Complex\n     \u2193               \u2193              \u2193\n  Flexibility    Chemistry    Binding Affinity\n  Specificity    Drug-like    Stability\n</code></pre> <p>Applications:</p> <ul> <li>De novo drug design</li> <li>Lead optimization</li> <li>Binding site engineering</li> <li>Personalized medicine</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#2-crossdocked2020-dataset","title":"2. CrossDocked2020 Dataset","text":"<p>Large-scale protein-ligand binding dataset:</p> <pre><code>from artifex.benchmarks.datasets.crossdocked import CrossDockedDataset\nfrom artifex.generative_models.core.configuration import DataConfig\n\n# Create dataset configuration\ndataset_config = DataConfig(\n    name=\"crossdocked_demo\",\n    dataset_name=\"crossdocked\",\n    metadata={\n        \"num_samples\": 50,\n        \"max_protein_atoms\": 200,\n        \"max_ligand_atoms\": 30,\n        \"pocket_radius\": 8.0,\n    },\n)\n\ndataset = CrossDockedDataset(\n    data_path=\"./data/crossdocked\",\n    config=dataset_config,\n    rngs=rngs,\n)\n\n# Get a sample\nsample = dataset[0]\n# sample = {\n#     \"protein_coords\": (200, 3),      # Protein atom coordinates\n#     \"protein_types\": (200,),         # Atom types (C, N, O, S, etc.)\n#     \"ligand_coords\": (30, 3),        # Ligand atom coordinates\n#     \"ligand_types\": (30,),           # Ligand atom types\n#     \"binding_affinity\": -8.5,        # In kcal/mol (lower = stronger)\n#     \"pocket_indices\": [12, 45, ...], # Binding pocket atom indices\n# }\n</code></pre> <p>Dataset Statistics:</p> <ul> <li>Total complexes: 22.5 million docked pairs</li> <li>Protein size: ~50-500 atoms (binding pocket)</li> <li>Ligand size: ~10-50 atoms (drug-like)</li> <li>Binding affinity range: -15 to 0 kcal/mol</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#3-molecular-modality-framework","title":"3. Molecular Modality Framework","text":"<p>Domain-specific functionality for chemical structures:</p> <pre><code>from artifex.generative_models.modalities.molecular import MolecularModality\n\nmodality = MolecularModality(rngs=rngs)\n\n# Chemical constraints\nconfig = ModalityConfiguration(\n    name=\"molecular_config\",\n    modality_name=\"molecular\",\n    metadata={\n        \"use_chemical_constraints\": True,\n        \"bond_length_weight\": 1.0,        # Enforce realistic bond lengths\n        \"bond_angle_weight\": 0.5,         # Enforce bond angles\n        \"use_pharmacophore_features\": True,\n        \"pharmacophore_types\": [\n            \"donor\",       # H-bond donors\n            \"acceptor\",    # H-bond acceptors\n            \"hydrophobic\"  # Hydrophobic regions\n        ],\n    }\n)\n\nextensions = modality.get_extensions(config, rngs=rngs)\n# extensions = {\n#     \"chemical_constraints\": &lt;ConstraintModule&gt;,\n#     \"pharmacophore_features\": &lt;PharmacophoreModule&gt;,\n# }\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#4-binding-affinity-metric","title":"4. Binding Affinity Metric","text":"<p>Evaluates binding affinity prediction accuracy:</p> \\[\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} (\\Delta G_{\\text{pred}}^i - \\Delta G_{\\text{true}}^i)^2}\\] <pre><code>from artifex.benchmarks.metrics.protein_ligand import BindingAffinityMetric\n\nmetric = BindingAffinityMetric(rngs=rngs)\n\n# True binding affinities (in kcal/mol)\ntrue_affinities = jnp.array([-8.2, -6.5, -9.1, -7.8])\n\n# Model predictions\npredictions = jnp.array([-8.5, -6.2, -8.9, -7.5])\n\nresults = metric.compute(predictions, true_affinities)\n# results = {\n#     \"rmse\": 0.32,          # Root Mean Square Error (kcal/mol)\n#     \"pearson_r\": 0.95,     # Correlation coefficient\n#     \"mae\": 0.28,           # Mean Absolute Error\n# }\n</code></pre> <p>Performance Targets:</p> <ul> <li>Excellent: RMSE &lt; 0.5 kcal/mol</li> <li>Good: RMSE &lt; 1.0 kcal/mol</li> <li>Acceptable: RMSE &lt; 1.5 kcal/mol</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#5-molecular-validity-metric","title":"5. Molecular Validity Metric","text":"<p>Checks chemical plausibility of generated molecules:</p> <pre><code>from artifex.benchmarks.metrics.protein_ligand import MolecularValidityMetric\n\nmetric = MolecularValidityMetric(rngs=rngs)\n\nresults = metric.compute(\n    coordinates=ligand_coords,  # (batch, num_atoms, 3)\n    atom_types=atom_types,      # (batch, num_atoms)\n    masks=atom_masks            # (batch, num_atoms)\n)\n# results = {\n#     \"validity_rate\": 0.96,      # Overall validity (target: &gt;0.95)\n#     \"bond_validity\": 0.98,      # Valid bond lengths\n#     \"clash_free\": 0.94,         # No atomic clashes\n#     \"connectivity\": 0.97,       # Proper atom connectivity\n# }\n</code></pre> <p>Validity Checks:</p> <ul> <li>Bond lengths: 1.2-2.0 \u00c5 for most bonds</li> <li>No clashes: Atoms &gt;1.0 \u00c5 apart (except bonded)</li> <li>Connectivity: All atoms form connected graph</li> <li>Valence: Atoms respect valence rules</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#6-drug-likeness-metric-qed","title":"6. Drug-likeness Metric (QED)","text":"<p>Quantitative Estimate of Drug-likeness:</p> \\[\\text{QED} = \\exp\\left(\\frac{1}{8}\\sum_{i=1}^{8} \\ln p_i\\right)\\] <p>where \\(p_i\\) are desirability functions for 8 molecular properties.</p> <pre><code>from artifex.benchmarks.metrics.protein_ligand import DrugLikenessMetric\n\nmetric = DrugLikenessMetric(rngs=rngs)\n\nresults = metric.compute(\n    coordinates=ligand_coords,\n    atom_types=atom_types,\n    masks=atom_masks\n)\n# results = {\n#     \"qed_score\": 0.75,             # Overall drug-likeness (target: &gt;0.7)\n#     \"lipinski_compliance\": 0.85,   # Lipinski's Rule of Five\n#     \"molecular_weight\": 385.4,     # Daltons (target: 180-500)\n#     \"logp\": 2.3,                   # Lipophilicity (target: 0-5)\n#     \"h_bond_donors\": 2,            # Target: \u22645\n#     \"h_bond_acceptors\": 4,         # Target: \u226410\n# }\n</code></pre> <p>Lipinski's Rule of Five:</p> <ul> <li>Molecular weight \u2264 500 Da</li> <li>LogP \u2264 5</li> <li>H-bond donors \u2264 5</li> <li>H-bond acceptors \u2264 10</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#7-benchmark-suite","title":"7. Benchmark Suite","text":"<p>Comprehensive evaluation across all metrics:</p> <pre><code>from artifex.benchmarks.suites.protein_ligand_suite import ProteinLigandBenchmarkSuite\n\nsuite = ProteinLigandBenchmarkSuite(\n    dataset_config={\n        \"num_samples\": 50,\n        \"max_protein_atoms\": 200,\n        \"max_ligand_atoms\": 30,\n    },\n    benchmark_config={\n        \"num_samples\": 20,\n        \"batch_size\": 4,\n    },\n    rngs=rngs\n)\n\n# Run evaluation\nresults = suite.run_all(model)\n# results = {\n#     \"binding_affinity\": {\n#         \"rmse\": 0.45,\n#         \"pearson_r\": 0.92,\n#     },\n#     \"molecular_validity\": {\n#         \"validity_rate\": 0.97,\n#         \"bond_validity\": 0.98,\n#     },\n#     \"drug_likeness\": {\n#         \"qed_score\": 0.78,\n#         \"lipinski_compliance\": 0.89,\n#     },\n# }\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#code-structure","title":"Code Structure","text":"<p>The example demonstrates six main components:</p> <ol> <li>Molecular Modality Framework - Chemical constraints and pharmacophore features</li> <li>CrossDocked2020 Dataset - Protein-ligand complex loading and statistics</li> <li>Binding Affinity Metric - RMSE evaluation for binding predictions</li> <li>Molecular Validity Metric - Chemical plausibility assessment</li> <li>Drug-likeness Metric - QED and Lipinski compliance</li> <li>Benchmark Suite - Comprehensive evaluation and model comparison</li> </ol>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>\u2705 Molecular modality with chemical constraints</li> <li>\u2705 CrossDocked2020 dataset with pocket extraction</li> <li>\u2705 Binding affinity prediction (RMSE, correlation)</li> <li>\u2705 Molecular validity checks (bonds, clashes, connectivity)</li> <li>\u2705 Drug-likeness evaluation (QED, Lipinski)</li> <li>\u2705 Complete benchmark suite execution</li> <li>\u2705 Model comparison across quality levels</li> <li>\u2705 Performance target assessment</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Adjust Model Quality</li> </ol> <pre><code>model = ExampleProteinLigandModel(rngs)\nmodel.model_quality = \"excellent\"  # Try \"poor\", \"good\", or \"excellent\"\nresults = suite.run_all(model)\n</code></pre> <ol> <li>Increase Dataset Size</li> </ol> <pre><code>dataset_config = {\n    \"num_samples\": 100,    # More samples\n    \"max_protein_atoms\": 300,\n    \"max_ligand_atoms\": 40,\n}\n</code></pre> <ol> <li>Custom Pocket Radius</li> </ol> <pre><code>dataset_config = DataConfig(\n    name=\"custom_pocket\",\n    dataset_name=\"crossdocked\",\n    metadata={\n        \"pocket_radius\": 10.0,  # Larger binding pocket\n        \"num_samples\": 50,\n    },\n)\ndataset = CrossDockedDataset(\n    data_path=\"./data/crossdocked\",\n    config=dataset_config,\n    rngs=rngs,\n)\n</code></pre> <ol> <li>Add Custom Metrics</li> </ol> <pre><code>class CustomMetric(nnx.Module):\n    def compute(self, predictions, targets):\n        # Your custom evaluation logic\n        return {\"custom_score\": score}\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Molecular Generation</p> <p>Generate novel drug-like molecules</p> <p> Molecule Generation</p> </li> <li> <p> Protein Folding</p> <p>Predict protein structures</p> <p> Protein Folding Demo</p> </li> <li> <p> Advanced Docking</p> <p>Learn molecular docking methods</p> <p> Docking Tutorial</p> </li> <li> <p> Framework Features</p> <p>Understand modality system</p> <p> Framework Demo</p> </li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/protein/protein-ligand-benchmark-demo/#importerror-for-molecular-modality","title":"ImportError for Molecular Modality","text":"<p>Symptom: Cannot import molecular modality classes</p> <p>Solution: Install molecular extras</p> <pre><code>uv sync --extra molecular\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#dataset-loading-too-slow","title":"Dataset Loading Too Slow","text":"<p>Symptom: Long wait times for dataset initialization</p> <p>Solution: Reduce number of samples</p> <pre><code>dataset_config = {\n    \"num_samples\": 20,  # Smaller dataset for faster loading\n}\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Symptom: GPU memory error during evaluation</p> <p>Solution: Reduce batch size</p> <pre><code>benchmark_config = {\n    \"batch_size\": 2,  # Smaller batches\n}\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#low-molecular-validity-rates","title":"Low Molecular Validity Rates","text":"<p>Symptom: Most generated molecules are invalid</p> <p>Cause: Incorrect coordinate scaling or atom types</p> <p>Solution: Check coordinate normalization</p> <pre><code># Ensure coordinates are in angstroms\ncoordinates = coordinates * coordinate_scale\n\n# Use realistic atom types (1-6 for C, N, O, S, P, F)\natom_types = jax.random.randint(key, (batch, num_atoms), 1, 7)\n</code></pre>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/protein/protein-ligand-benchmark-demo/#documentation","title":"Documentation","text":"<ul> <li>Molecular Modality Guide - Chemical structure representation</li> <li>Protein-Ligand Benchmarks - Complete benchmarking guide</li> <li>CrossDocked2020 Dataset - Dataset API reference</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#related-examples","title":"Related Examples","text":"<ul> <li>Geometric Benchmark Demo - 3D generation</li> <li>Loss Examples - Loss functions</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#papers-and-resources","title":"Papers and Resources","text":"<ul> <li>CrossDocked2020: \"Protein-Ligand Docking and Scoring with Deep Learning\" (Francoeur et al., 2020)</li> <li>QED: \"Quantifying the chemical beauty of drugs\" (Bickerton et al., 2012)</li> <li>Lipinski's Rule: \"Experimental and computational approaches to estimate solubility\" (Lipinski et al., 2001)</li> <li>Autodock Vina: Popular molecular docking software</li> </ul>"},{"location":"examples/protein/protein-ligand-benchmark-demo/#external-tools","title":"External Tools","text":"<ul> <li>RDKit: Open-source cheminformatics library</li> <li>Open Babel: Chemical toolbox for file conversion</li> <li>PyMOL: Molecular visualization</li> <li>Protein Data Bank (PDB): Protein structure database</li> </ul>"},{"location":"examples/protein/protein-model-extension/","title":"Protein Model Extensions Example","text":"<p>Demonstrate how to use protein-specific extensions with Artifex's geometric model framework, combining domain knowledge with general-purpose geometric models.</p>"},{"location":"examples/protein/protein-model-extension/#files","title":"Files","text":"<ul> <li>Python Script: <code>protein_model_extension.py</code></li> <li>Jupyter Notebook: <code>protein_model_extension.ipynb</code></li> </ul>"},{"location":"examples/protein/protein-model-extension/#quick-start","title":"Quick Start","text":"<pre><code># Run the Python script\nsource activate.sh\npython examples/generative_models/protein/protein_model_extension.py\n\n# Or use Jupyter notebook\njupyter notebook examples/generative_models/protein/protein_model_extension.ipynb\n</code></pre>"},{"location":"examples/protein/protein-model-extension/#overview","title":"Overview","text":"<p>This example shows how to enhance a point cloud model with protein-specific extensions. Extensions add domain knowledge about protein structure, chemistry, and geometry to improve model predictions and learning.</p>"},{"location":"examples/protein/protein-model-extension/#learning-objectives","title":"Learning Objectives","text":"<ul> <li> Understand the extension system in Artifex</li> <li> Learn how to create and configure protein-specific extensions</li> <li> See how extensions enhance model outputs with domain knowledge</li> <li> Understand extension contribution to loss calculations</li> <li> Learn to combine multiple extensions for complex domain knowledge</li> </ul>"},{"location":"examples/protein/protein-model-extension/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of protein structure (residues, backbone atoms)</li> <li>Familiarity with point cloud models</li> <li>Knowledge of Flax NNX modules</li> <li>Understanding of JAX random number generation</li> </ul>"},{"location":"examples/protein/protein-model-extension/#background-protein-structure-and-extensions","title":"Background: Protein Structure and Extensions","text":""},{"location":"examples/protein/protein-model-extension/#protein-basics","title":"Protein Basics","text":"<p>Proteins are polymers composed of amino acid residues connected by peptide bonds. The backbone consists of repeating units with four key atoms per residue:</p> <ul> <li>N (Nitrogen): Backbone nitrogen</li> <li>C\u03b1 (Alpha Carbon): Central carbon with side chain</li> <li>C (Carbonyl Carbon): Carbon of carbonyl group</li> <li>O (Oxygen): Carbonyl oxygen</li> </ul>"},{"location":"examples/protein/protein-model-extension/#geometric-constraints","title":"Geometric Constraints","text":"<p>Protein structures follow specific geometric constraints due to chemical bonding:</p> <p>Bond Lengths: Relatively fixed distances between bonded atoms</p> \\[d_{\\text{C-N}} \\approx 1.33\\text{\u00c5}, \\quad d_{\\text{N-C\u03b1}} \\approx 1.46\\text{\u00c5}, \\quad d_{\\text{C\u03b1-C}} \\approx 1.52\\text{\u00c5}\\] <p>Bond Angles: Preferred angles between consecutive bonds</p> \\[\\theta_{\\text{N-C\u03b1-C}} \\approx 110\u00b0, \\quad \\theta_{\\text{C\u03b1-C-N}} \\approx 120\u00b0\\] <p>Torsion Angles: Backbone flexibility through \u03c6 (phi) and \u03c8 (psi) angles</p> <p>These constraints make protein structure prediction a constrained optimization problem, which extensions help enforce.</p>"},{"location":"examples/protein/protein-model-extension/#extension-system","title":"Extension System","text":"<p>Artifex's extension system allows adding domain-specific knowledge to base models. For proteins, we have:</p> <ol> <li>Protein Mixin Extension: Integrates amino acid type information</li> <li>Protein Constraints Extension: Enforces backbone geometry</li> <li>Bond Length Extension: Monitors and penalizes bond violations</li> <li>Bond Angle Extension: Monitors and penalizes angle violations</li> </ol>"},{"location":"examples/protein/protein-model-extension/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/protein/protein-model-extension/#1-import-required-modules","title":"1. Import Required Modules","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration import (\n    PointCloudConfig,\n    PointCloudNetworkConfig,\n)\nfrom artifex.generative_models.extensions.base.extensions import ExtensionConfig\nfrom artifex.generative_models.extensions.protein import (\n    BondAngleExtension,\n    BondLengthExtension,\n    ProteinMixinExtension,\n)\nfrom artifex.generative_models.extensions.protein.constraints import (\n    ProteinBackboneConstraint,\n)\nfrom artifex.generative_models.models.geometric.point_cloud import (\n    PointCloudModel,\n)\n</code></pre> <p>We import:</p> <ul> <li>JAX for array operations and random number generation</li> <li>Flax NNX for neural network modules</li> <li>Artifex configuration and extension classes</li> <li>Protein-specific extensions</li> <li>Point cloud model</li> </ul>"},{"location":"examples/protein/protein-model-extension/#2-configure-protein-structure","title":"2. Configure Protein Structure","text":"<pre><code>num_residues = 10\natoms_per_residue = 4  # N, CA, C, O\nnum_points = num_residues * atoms_per_residue\nembedding_dim = 64\n\n# Create network config for point cloud processing\nnetwork_config = PointCloudNetworkConfig(\n    name=\"protein_network\",\n    hidden_dims=(embedding_dim,) * 2,  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=embedding_dim,\n    num_heads=4,\n    num_layers=2,\n    dropout_rate=0.1,\n)\n\n# Create point cloud config with nested network config\nmodel_config = PointCloudConfig(\n    name=\"protein_point_cloud\",\n    network=network_config,\n    num_points=num_points,\n    dropout_rate=0.1,\n)\n</code></pre> <p>Key configuration points:</p> <ul> <li>num_points: Must be specified to override default (1024)</li> <li>hidden_dims: List of hidden dimensions for each layer</li> <li>parameters: Additional model-specific parameters</li> <li>dropout_rate: Regularization during training</li> </ul>"},{"location":"examples/protein/protein-model-extension/#3-create-protein-specific-extensions","title":"3. Create Protein-Specific Extensions","text":""},{"location":"examples/protein/protein-model-extension/#31-protein-mixin-extension","title":"3.1 Protein Mixin Extension","text":"<pre><code>mixin_config = ExtensionConfig(\n    name=\"protein_mixin\",\n    weight=1.0,\n    enabled=True,\n    extensions={\n        \"embedding_dim\": embedding_dim,\n        \"num_aa_types\": 20,\n    },\n)\nextensions_dict[\"protein_mixin\"] = ProteinMixinExtension(\n    config=mixin_config,\n    rngs=nnx.Rngs(params=mixin_key),\n)\n</code></pre> <p>The mixin extension learns embeddings for all 20 standard amino acid types, allowing the model to incorporate sequence information.</p>"},{"location":"examples/protein/protein-model-extension/#32-protein-constraints-extension","title":"3.2 Protein Constraints Extension","text":"<pre><code>constraint_config = ExtensionConfig(\n    name=\"protein_constraints\",\n    weight=1.0,\n    enabled=True,\n    extensions={\n        \"num_residues\": num_residues,\n        \"backbone_indices\": [0, 1, 2, 3],\n    },\n)\nextensions_dict[\"protein_constraints\"] = ProteinBackboneConstraint(\n    config=constraint_config,\n    rngs=nnx.Rngs(params=constraint_key),\n)\n</code></pre> <p>This extension enforces geometric constraints on backbone atoms during generation.</p>"},{"location":"examples/protein/protein-model-extension/#33-bond-length-extension","title":"3.3 Bond Length Extension","text":"<pre><code>bond_length_config = ExtensionConfig(\n    name=\"bond_length\",\n    weight=1.0,\n    enabled=True,\n    extensions={\n        \"num_residues\": num_residues,\n        \"backbone_indices\": [0, 1, 2, 3],\n    },\n)\nextensions_dict[\"bond_length\"] = BondLengthExtension(\n    config=bond_length_config,\n    rngs=nnx.Rngs(params=constraint_key),\n)\n</code></pre> <p>Monitors bond lengths and calculates violations for use in the loss function.</p>"},{"location":"examples/protein/protein-model-extension/#34-bond-angle-extension","title":"3.4 Bond Angle Extension","text":"<pre><code>bond_angle_config = ExtensionConfig(\n    name=\"bond_angle\",\n    weight=0.5,  # Lower weight - angles are more flexible\n    enabled=True,\n    extensions={\n        \"num_residues\": num_residues,\n        \"backbone_indices\": [0, 1, 2, 3],\n    },\n)\nextensions_dict[\"bond_angle\"] = BondAngleExtension(\n    config=bond_angle_config,\n    rngs=nnx.Rngs(params=backbone_key),\n)\n</code></pre> <p>Note the lower weight (0.5) compared to bond lengths, reflecting the fact that bond angles are more flexible than bond lengths in real proteins.</p>"},{"location":"examples/protein/protein-model-extension/#4-wrap-extensions-in-nnxdict","title":"4. Wrap Extensions in nnx.Dict","text":"<pre><code>extensions = nnx.Dict(extensions_dict)\n</code></pre> <p>Flax NNX 0.12.0+ requires extensions to be wrapped in <code>nnx.Dict</code> for proper parameter tracking and serialization.</p>"},{"location":"examples/protein/protein-model-extension/#5-create-model-with-extensions","title":"5. Create Model with Extensions","text":"<pre><code>model = PointCloudModel(\n    model_config,\n    extensions=extensions,\n    rngs=nnx.Rngs(params=model_key)\n)\n</code></pre> <p>The point cloud model now has access to all four extensions and will use them during forward passes and loss calculation.</p>"},{"location":"examples/protein/protein-model-extension/#6-create-test-batch","title":"6. Create Test Batch","text":"<pre><code>batch = {\n    \"aatype\": aatype,        # Shape: (batch_size, num_residues)\n    \"positions\": coords,      # Shape: (batch_size, num_points, 3)\n    \"mask\": mask,            # Shape: (batch_size, num_points)\n}\n</code></pre> <p>The batch contains:</p> <ul> <li>aatype: Amino acid types (integers 0-19 for 20 amino acids)</li> <li>positions: 3D coordinates of all atoms</li> <li>mask: Binary mask indicating valid atoms</li> </ul>"},{"location":"examples/protein/protein-model-extension/#7-forward-pass-with-extensions","title":"7. Forward Pass with Extensions","text":"<pre><code>outputs = model(batch)\n</code></pre> <p>During the forward pass:</p> <ol> <li>Model processes input through transformer layers</li> <li>Each enabled extension runs on the intermediate representations</li> <li>Extension outputs are collected and returned alongside main output</li> </ol> <p>Extension outputs might include:</p> <ul> <li>Amino acid embeddings (from mixin)</li> <li>Constraint violation metrics</li> <li>Bond statistics</li> <li>Angle statistics</li> </ul>"},{"location":"examples/protein/protein-model-extension/#8-calculate-loss-with-extensions","title":"8. Calculate Loss with Extensions","text":"<pre><code>loss_fn = model.get_loss_fn()\nloss_outputs = loss_fn(batch, outputs)\n</code></pre> <p>The loss function combines:</p> \\[\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{MSE}} + \\sum_{i} w_i \\mathcal{L}_{\\text{ext}_i}\\] <p>Where:</p> <ul> <li>\\(\\mathcal{L}_{\\text{MSE}}\\): Main reconstruction loss</li> <li>\\(w_i\\): Extension weight</li> <li>\\(\\mathcal{L}_{\\text{ext}_i}\\): Extension-specific loss</li> </ul> <p>This multi-objective loss encourages the model to:</p> <ol> <li>Reconstruct input positions accurately</li> <li>Respect bond length constraints</li> <li>Maintain realistic bond angles</li> <li>Utilize amino acid type information</li> </ol>"},{"location":"examples/protein/protein-model-extension/#expected-output","title":"Expected Output","text":"<pre><code>Created extensions: protein_mixin, protein_constraints, bond_length, bond_angle\nCreated model: PointCloudModel\n\nModel outputs:\n- Main output shape: (2, 40, 3)\n- Extension outputs:\n  - protein_mixin\n  - protein_constraints\n  - bond_length\n  - bond_angle\n\nLoss calculation:\n- Available loss keys: ['total_loss', 'mse_loss', 'protein_mixin', 'protein_constraints', 'bond_length', 'bond_angle']\n- Total loss: 89.77\n- total_loss: 89.77\n- mse_loss: 87.01\n\nProtein model extension demo completed successfully!\n</code></pre>"},{"location":"examples/protein/protein-model-extension/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/protein/protein-model-extension/#extension-configuration","title":"Extension Configuration","text":"<p>Extensions use <code>ExtensionConfig</code> with:</p> <ul> <li>name: Identifier for the extension</li> <li>weight: Contribution to total loss (0-1 or higher)</li> <li>enabled: Whether extension is active</li> <li>extensions: Extension-specific parameters dict</li> </ul>"},{"location":"examples/protein/protein-model-extension/#extension-weights","title":"Extension Weights","text":"<p>Weights control the relative importance of different constraints:</p> <ul> <li>Bond lengths: Weight 1.0 (strict constraint)</li> <li>Bond angles: Weight 0.5 (more flexible)</li> <li>Constraints: Weight 1.0 (enforce geometry)</li> <li>Mixin: Weight 1.0 (sequence information)</li> </ul> <p>Adjust weights based on your application's priorities.</p>"},{"location":"examples/protein/protein-model-extension/#flax-nnx-0120-compatibility","title":"Flax NNX 0.12.0+ Compatibility","text":"<p>Always wrap extension dictionaries in <code>nnx.Dict</code>:</p> <pre><code># CORRECT\nextensions = nnx.Dict(extensions_dict)\n\n# WRONG (will fail in NNX 0.12.0+)\nextensions = extensions_dict\n</code></pre>"},{"location":"examples/protein/protein-model-extension/#random-number-generation","title":"Random Number Generation","text":"<p>Each extension receives its own RNG for parameter initialization:</p> <pre><code>key, mixin_key, constraint_key, backbone_key = jax.random.split(key, 4)\n\nextensions_dict[\"protein_mixin\"] = ProteinMixinExtension(\n    config=mixin_config,\n    rngs=nnx.Rngs(params=mixin_key),  # Separate key\n)\n</code></pre> <p>This ensures independent randomness across extensions.</p>"},{"location":"examples/protein/protein-model-extension/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Adjust Extension Weights</li> </ol> <pre><code># Emphasize bond lengths more\nbond_length_config.weight = 2.0\nbond_angle_config.weight = 0.1\n</code></pre> <ol> <li>Disable Specific Extensions</li> </ol> <pre><code># See impact of removing angle constraints\nbond_angle_config.enabled = False\n</code></pre> <ol> <li>Increase Protein Size</li> </ol> <pre><code>num_residues = 50  # Larger protein\natoms_per_residue = 4\n</code></pre> <ol> <li>Add Custom Extensions</li> </ol> <p>Create your own extension for other properties (e.g., secondary structure, hydrophobicity).</p> <ol> <li>Visualize Bond Statistics</li> </ol> <p>Extract and plot bond length/angle distributions from extension outputs.</p> <ol> <li>Compare With/Without Extensions</li> </ol> <p>Train two models (with and without extensions) and compare structure quality.</p>"},{"location":"examples/protein/protein-model-extension/#next-steps","title":"Next Steps","text":"<p>Explore related examples to deepen your understanding:</p> <ul> <li> <p> Protein Extensions Deep Dive</p> <p>Learn more about individual protein extensions and their implementation.</p> <p> protein_extensions_example.py</p> </li> <li> <p> Protein Point Cloud Model</p> <p>Explore the ProteinPointCloudModel that combines point clouds with protein constraints.</p> <p> protein_point_cloud_example.py</p> </li> <li> <p> Protein Diffusion</p> <p>Use diffusion models for protein structure generation with extensions.</p> <p> protein_diffusion_example.py</p> </li> <li> <p> Protein Benchmarks</p> <p>Evaluate protein models with domain-specific metrics.</p> <p> protein_ligand_benchmark_demo.py</p> </li> </ul>"},{"location":"examples/protein/protein-model-extension/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/protein/protein-model-extension/#extension-not-contributing-to-loss","title":"Extension Not Contributing to Loss","text":"<p>Problem: Extension appears in outputs but not in loss.</p> <p>Solution: Check that:</p> <ol> <li>Extension weight is non-zero</li> <li>Extension is enabled (<code>enabled=True</code>)</li> <li>Extension implements <code>compute_loss()</code> method</li> </ol>"},{"location":"examples/protein/protein-model-extension/#nnxdict-error","title":"nnx.Dict Error","text":"<p>Problem: <code>TypeError: extensions must be nnx.Dict</code></p> <p>Solution: Wrap your extensions dictionary:</p> <pre><code>extensions = nnx.Dict(extensions_dict)\n</code></pre>"},{"location":"examples/protein/protein-model-extension/#bond-violations-too-high","title":"Bond Violations Too High","text":"<p>Problem: Bond length/angle violations are unreasonably large.</p> <p>Solution:</p> <ol> <li>Check input coordinates are in correct units (Angstroms)</li> <li>Verify backbone indices match your atom ordering</li> <li>Increase extension weights to penalize violations more</li> </ol>"},{"location":"examples/protein/protein-model-extension/#out-of-memory","title":"Out of Memory","text":"<p>Problem: GPU runs out of memory with extensions.</p> <p>Solution:</p> <ol> <li>Reduce batch size</li> <li>Reduce number of residues</li> <li>Reduce embedding dimension</li> <li>Disable less critical extensions</li> </ol>"},{"location":"examples/protein/protein-model-extension/#additional-resources","title":"Additional Resources","text":"<ul> <li>Extension System Documentation</li> <li>Point Cloud Models Guide</li> <li>Protein Modeling Tutorial</li> <li>Flax NNX Documentation</li> <li>JAX Documentation</li> </ul>"},{"location":"examples/protein/protein-model-extension/#citation","title":"Citation","text":"<p>If you use this example in your research, please cite:</p> <pre><code>@software{artifex2025,\n  title={Artifex: Modular Generative Modeling Library},\n  author={Artifex Contributors},\n  year={2025},\n  url={https://github.com/avitai/artifex}\n}\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/","title":"Protein Models with Modality Architecture","text":"<p>Level: Intermediate | Runtime: ~10 seconds (CPU/GPU) | Format: Python + Jupyter</p>"},{"location":"examples/protein/protein-model-with-modality/#overview","title":"Overview","text":"<p>This example demonstrates Artifex's modality architecture for creating protein-specific generative models. The modality system provides a unified interface for working with different data types (image, text, protein, etc.) while maintaining domain-specific capabilities. You'll learn how to use the factory system to create protein models with automatic modality-specific enhancements.</p>"},{"location":"examples/protein/protein-model-with-modality/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Understand Artifex's modality architecture and its benefits</li> <li>Create protein models using the factory system with modalities</li> <li>Work with different model types (PointCloudModel, GeometricModel) for proteins</li> <li>Use full module paths when working with the factory system</li> <li>Apply domain-specific extensions through modality configuration</li> </ul>"},{"location":"examples/protein/protein-model-with-modality/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/protein/protein_model_with_modality.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/protein/protein_model_with_modality.ipynb</code></li> </ul>"},{"location":"examples/protein/protein-model-with-modality/#quick-start","title":"Quick Start","text":""},{"location":"examples/protein/protein-model-with-modality/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/protein/protein_model_with_modality.py\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/protein/protein_model_with_modality.ipynb\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/protein/protein-model-with-modality/#modality-architecture","title":"Modality Architecture","text":"<p>Artifex uses a modality-based design where each data type has its own modality class that handles:</p> <ul> <li>Domain-specific preprocessing: Convert protein data to appropriate representations</li> <li>Evaluation metrics: Compute relevant metrics for the domain (RMSD, TM-score, etc.)</li> <li>Model adaptations: Apply domain-specific enhancements and extensions</li> </ul> <p>Benefits:</p> <ul> <li>Consistent interface across different data types</li> <li>Automatic application of domain expertise</li> <li>Easy switching between different model architectures</li> <li>Built-in best practices for each domain</li> </ul>"},{"location":"examples/protein/protein-model-with-modality/#factory-system","title":"Factory System","text":"<p>The <code>create_model()</code> factory provides a unified way to instantiate models:</p> <pre><code>from artifex.generative_models.factory import create_model\n\nmodel = create_model(\n    config=model_config,\n    modality=\"protein\",  # Automatically applies protein-specific enhancements\n    rngs=rngs\n)\n</code></pre> <p>Important: When using the factory system, model classes must be specified with full module paths:</p> <ul> <li>\u2705 <code>\"artifex.generative_models.models.geometric.point_cloud.PointCloudModel\"</code></li> <li>\u274c <code>\"PointCloudModel\"</code> (will raise ValueError)</li> </ul>"},{"location":"examples/protein/protein-model-with-modality/#protein-data-structure","title":"Protein Data Structure","text":"<p>Protein models in Artifex expect input data with the following structure:</p> <pre><code>protein_data = {\n    \"aatype\": jnp.array,        # [batch, num_residues] - amino acid types\n    \"atom_positions\": jnp.array,  # [batch, num_residues, num_atoms, 3] - 3D coords\n    \"atom_mask\": jnp.array,      # [batch, num_residues, num_atoms] - presence mask\n}\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#code-structure","title":"Code Structure","text":"<p>The example demonstrates four major sections:</p> <ol> <li>Available Modalities: List all registered modalities in the system</li> <li>Model Configuration: Create configuration with full module paths</li> <li>Factory with Modality: Create protein models using the factory</li> <li>Model Usage: Perform inference with protein data</li> </ol>"},{"location":"examples/protein/protein-model-with-modality/#example-code","title":"Example Code","text":""},{"location":"examples/protein/protein-model-with-modality/#listing-available-modalities","title":"Listing Available Modalities","text":"<pre><code>from artifex.generative_models.modalities import list_modalities\n\n# See what modalities are available\nmodalities = list_modalities()\nfor name, cls in modalities.items():\n    print(f\"  - {name}: {cls.__name__}\")\n# Output:\n#   - protein: MolecularModality\n#   - image: ImageModality\n#   - text: TextModality\n#   ...\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#creating-model-configuration","title":"Creating Model Configuration","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    PointCloudConfig,\n    PointCloudNetworkConfig,\n)\n\n# Create network config for point cloud\nnetwork_config = PointCloudNetworkConfig(\n    name=\"protein_point_cloud_network\",\n    hidden_dims=(64, 64),  # Tuple for frozen dataclass\n    activation=\"gelu\",\n    embed_dim=64,\n    num_heads=4,\n    num_layers=3,\n    dropout_rate=0.1,\n)\n\n# Create PointCloudConfig with nested network config\nmodel_config = PointCloudConfig(\n    name=\"protein_point_cloud\",\n    network=network_config,\n    num_points=128,  # Total points (residues \u00d7 atoms per residue)\n    dropout_rate=0.1,\n)\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#factory-with-modality-parameter","title":"Factory with Modality Parameter","text":"<pre><code>from artifex.generative_models.factory import create_model\nfrom flax import nnx\n\n# Initialize RNG\nrngs = nnx.Rngs(params=42)\n\n# Create model with protein modality\nmodel = create_model(\n    config=model_config,\n    modality=\"protein\",  # Applies protein-specific enhancements\n    rngs=rngs,\n)\n\nprint(f\"Created model: {model.__class__.__name__}\")\n# Output: Created model: PointCloudModel\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#using-different-model-types","title":"Using Different Model Types","text":"<pre><code># Create configuration for GeometricModel instead\ngeometric_config = ModelConfig(\n    name=\"protein_geometric\",\n    # Different model class, same modality system\n    model_class=\"artifex.generative_models.models.geometric.base.GeometricModel\",\n    input_dim=128,\n    output_dim=128,\n    hidden_dims=[64, 64],\n    parameters=model_config.parameters,\n    metadata=model_config.metadata,\n)\n\n# Create with same modality\ngeometric_model = create_model(\n    config=geometric_config,\n    modality=\"protein\",\n    rngs=rngs,\n)\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#model-inference","title":"Model Inference","text":"<pre><code>import jax.numpy as jnp\n\n# Create dummy protein data\nnum_residues = 10\nnum_atoms = 4\nbatch_size = 2\n\nprotein_input = {\n    \"aatype\": jnp.full((batch_size, num_residues), 7),  # All glycine\n    \"atom_positions\": jnp.ones((batch_size, num_residues, num_atoms, 3)),\n    \"atom_mask\": jnp.ones((batch_size, num_residues, num_atoms)),\n}\n\n# Generate output\noutputs = model(protein_input, deterministic=True)\n\n# Inspect results\nfor key, value in outputs.items():\n    if hasattr(value, \"shape\"):\n        print(f\"{key}: {value.shape}\")\n# Output might include:\n#   coordinates: (2, 40, 3)\n#   bond_length: {...}\n#   bond_angle: {...}\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>Modality Registration: Automatic discovery of available modalities</li> <li>Factory Pattern: Unified model creation interface</li> <li>Full Module Paths: Required syntax for model class specification</li> <li>Domain Extensions: Protein-specific constraints and mixins</li> <li>Model Flexibility: Same modality works with different model architectures</li> <li>Type Safety: Configuration validation through Pydantic models</li> </ul>"},{"location":"examples/protein/protein-model-with-modality/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Explore Other Modalities: Try changing <code>modality=\"protein\"</code> to <code>modality=\"image\"</code> and see how the interface remains consistent</li> </ol> <pre><code>image_model = create_model(\n    config=image_config,\n    modality=\"image\",\n    rngs=rngs\n)\n</code></pre> <ol> <li>Modify Extension Weights: Change <code>bond_length_weight</code> and <code>bond_angle_weight</code> to see their effect on the model's constraint enforcement</li> </ol> <pre><code>metadata = {\n    \"extensions\": {\n        \"bond_length_weight\": 2.0,  # Increased from 1.0\n        \"bond_angle_weight\": 1.0,   # Increased from 0.5\n    }\n}\n</code></pre> <ol> <li>Scale to Larger Proteins: Increase <code>num_residues</code> and <code>num_atoms</code> to work with larger protein structures</li> </ol> <pre><code>num_residues = 100  # Larger protein\nnum_atoms = 14      # All heavy atoms\n</code></pre> <ol> <li>Add Training Loop: Extend this example to include a simple training loop using the model's loss function</li> </ol> <pre><code>loss_fn = model.get_loss_fn()\nfor batch in data_loader:\n    outputs = model(batch)\n    loss = loss_fn(batch, outputs)\n    # Update parameters\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/protein/protein-model-with-modality/#common-issues","title":"Common Issues","text":""},{"location":"examples/protein/protein-model-with-modality/#valueerror-invalid-model-class","title":"ValueError: Invalid model class","text":"<p>Symptom:</p> <pre><code>ValueError: model_class must be a fully qualified module path\n</code></pre> <p>Cause: Using short names like <code>\"PointCloudModel\"</code> instead of full module paths.</p> <p>Solution:</p> <pre><code># \u274c WRONG - Short name\nmodel_class=\"PointCloudModel\"\n\n# \u2705 CORRECT - Full module path\nmodel_class=\"artifex.generative_models.models.geometric.point_cloud.PointCloudModel\"\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#keyerror-sample","title":"KeyError: 'sample'","text":"<p>Symptom:</p> <pre><code>KeyError: 'sample'\n</code></pre> <p>Cause: Missing RNG keys for stochastic operations.</p> <p>Solution:</p> <pre><code># Ensure all required RNG keys are initialized\nrngs = nnx.Rngs(params=42, dropout=43, sample=44)\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#shape-mismatch-errors","title":"Shape mismatch errors","text":"<p>Symptom:</p> <pre><code>ValueError: Expected shape (batch, num_points, 3), got (batch, num_residues, num_atoms, 3)\n</code></pre> <p>Cause: <code>num_points</code> parameter doesn't match the actual number of points in input data.</p> <p>Solution:</p> <pre><code># Ensure num_points = num_residues \u00d7 num_atoms\nparameters = {\n    \"num_points\": num_residues * num_atoms,  # e.g., 10 \u00d7 4 = 40\n}\n</code></pre>"},{"location":"examples/protein/protein-model-with-modality/#summary","title":"Summary","text":"<p>In this example, you learned:</p> <ul> <li>\u2705 Artifex's modality architecture provides a unified interface for different data types</li> <li>\u2705 The factory system with <code>create_model()</code> simplifies model creation</li> <li>\u2705 Full module paths are required when specifying model classes</li> <li>\u2705 The same modality can work with different model architectures</li> <li>\u2705 Domain-specific extensions are automatically applied through modality configuration</li> </ul> <p>Key Takeaways:</p> <ol> <li>Modality System: Separates domain logic from model architecture</li> <li>Factory Pattern: Consistent API across all model types</li> <li>Full Paths Required: Always use complete module paths for <code>model_class</code></li> <li>Flexibility: Easy to switch between different model types while maintaining domain capabilities</li> </ol>"},{"location":"examples/protein/protein-model-with-modality/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Protein Point Cloud Models</p> <p>Learn more about protein-specific geometric models</p> <p> protein-point-cloud-example.py</p> </li> <li> <p> Protein Extensions</p> <p>Explore protein constraint extensions in detail</p> <p> protein-extensions-example.py</p> </li> <li> <p> Model Configuration</p> <p>Deep dive into configuration system and parameters</p> <p> Configuration Guide</p> </li> <li> <p> Factory System</p> <p>Comprehensive guide to the model factory</p> <p> Factory Guide</p> </li> </ul>"},{"location":"examples/protein/protein-model-with-modality/#additional-resources","title":"Additional Resources","text":"<ul> <li>Artifex Documentation: Modality System</li> <li>Artifex Documentation: Factory System</li> <li>Artifex Documentation: Protein Extensions</li> <li>API Reference: ModelConfig</li> <li>API Reference: create_model</li> </ul>"},{"location":"examples/protein/protein-model-with-modality/#related-examples","title":"Related Examples","text":"<ul> <li>Protein Point Cloud Example - Detailed protein geometric modeling</li> <li>Protein Extensions Example - Using protein-specific constraints</li> <li>Geometric Benchmark Demo - Evaluating geometric models</li> </ul>"},{"location":"examples/protein/protein-point-cloud-example/","title":"Protein Point Cloud Model Example","text":"<p>Level: Intermediate | Runtime: ~15 seconds (CPU) / ~5 seconds (GPU) | Format: Python + Jupyter</p>"},{"location":"examples/protein/protein-point-cloud-example/#overview","title":"Overview","text":"<p>This example demonstrates the ProteinPointCloudModel, a specialized geometric model designed for protein structure generation and refinement. It combines point cloud processing with protein-specific constraints (bond lengths, bond angles) to generate physically plausible protein structures. You'll learn how to work with backbone-only representations and apply geometric constraints to ensure chemical validity.</p>"},{"location":"examples/protein/protein-point-cloud-example/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Represent proteins as 3D point clouds (atoms as points in space)</li> <li>Configure and create ProteinPointCloudModel with attention mechanisms</li> <li>Generate synthetic protein data with alpha-helix geometry</li> <li>Apply geometric constraints (bond lengths, angles) during generation</li> <li>Evaluate model outputs using reconstruction and constraint losses</li> <li>Work with backbone-only protein representations (N, CA, C, O atoms)</li> </ul>"},{"location":"examples/protein/protein-point-cloud-example/#files","title":"Files","text":"<ul> <li>Python Script: <code>examples/generative_models/protein/protein_point_cloud_example.py</code></li> <li>Jupyter Notebook: <code>examples/generative_models/protein/protein_point_cloud_example.ipynb</code></li> </ul>"},{"location":"examples/protein/protein-point-cloud-example/#quick-start","title":"Quick Start","text":""},{"location":"examples/protein/protein-point-cloud-example/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the example\npython examples/generative_models/protein/protein_point_cloud_example.py\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/protein/protein_point_cloud_example.ipynb\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/protein/protein-point-cloud-example/#point-cloud-representation","title":"Point Cloud Representation","text":"<p>Proteins can be represented as sets of 3D points, where each point corresponds to an atom's position in space:</p> <pre><code>atom_positions: [num_residues, num_atoms, 3]  # 3D coordinates\natom_mask: [num_residues, num_atoms]           # Presence indicator\n</code></pre> <p>Benefits:</p> <ul> <li>Invariance: Naturally invariant to rotation and translation</li> <li>Flexibility: Can handle variable-length structures</li> <li>Geometric: Directly represents 3D spatial structure</li> </ul>"},{"location":"examples/protein/protein-point-cloud-example/#backbone-atoms","title":"Backbone Atoms","text":"<p>The protein backbone consists of four atoms present in every amino acid:</p> <ul> <li>N (Nitrogen): Index 0, forms peptide bond</li> <li>CA (Alpha Carbon): Index 1, central carbon atom</li> <li>C (Carbon): Index 2, carbonyl carbon</li> <li>O (Oxygen): Index 4, carbonyl oxygen</li> </ul> <p>These atoms determine the overall protein structure (folds and secondary structures).</p>"},{"location":"examples/protein/protein-point-cloud-example/#geometric-constraints","title":"Geometric Constraints","text":"<p>Physical constraints ensure generated structures are chemically valid:</p> <p>Bond Length Constraints: $\\(\\mathcal{L}_{\\text{bond}} = \\sum_{i} \\left| \\| p_i - p_{i+1} \\| - d_{\\text{ideal}} \\right|\\)$</p> <p>Where \\(p_i\\) are atomic positions and \\(d_{\\text{ideal}}\\) is the ideal bond length.</p> <p>Bond Angle Constraints: $\\(\\mathcal{L}_{\\text{angle}} = \\sum_{i} \\left| \\angle(p_{i-1}, p_i, p_{i+1}) - \\theta_{\\text{ideal}} \\right|\\)$</p> <p>These constraints penalize deviations from ideal bond geometry.</p>"},{"location":"examples/protein/protein-point-cloud-example/#attention-mechanisms","title":"Attention Mechanisms","text":"<p>The model uses multi-head attention to capture:</p> <ul> <li>Long-range interactions: Between distant amino acids</li> <li>Structural context: How each residue affects neighbors</li> <li>Folding patterns: Secondary and tertiary structure</li> </ul>"},{"location":"examples/protein/protein-point-cloud-example/#code-structure","title":"Code Structure","text":"<p>The example demonstrates eight major sections:</p> <ol> <li>Setup and Initialization: Import libraries and create RNG keys</li> <li>Model Configuration: Define architecture and constraint parameters</li> <li>Model Creation: Instantiate ProteinPointCloudModel</li> <li>Synthetic Data Generation: Create alpha-helix structures for testing</li> <li>Dataset Loading: Load protein data using ProteinDataset</li> <li>Forward Pass: Generate protein structures</li> <li>Loss Calculation: Compute reconstruction and constraint losses</li> <li>Summary: Key takeaways and experiments</li> </ol>"},{"location":"examples/protein/protein-point-cloud-example/#example-code","title":"Example Code","text":""},{"location":"examples/protein/protein-point-cloud-example/#model-configuration","title":"Model Configuration","text":"<pre><code>from artifex.generative_models.core.configuration import (\n    PointCloudNetworkConfig,\n    ProteinConstraintConfig,\n    ProteinPointCloudConfig,\n)\n\n# Create network config for point cloud processing\nnetwork_config = PointCloudNetworkConfig(\n    name=\"protein_network\",\n    hidden_dims=(128, 128, 128, 128),  # 4 layers for hierarchical processing\n    activation=\"gelu\",\n    embed_dim=128,      # Embedding dimension for each atom\n    num_heads=4,        # Number of attention heads\n    num_layers=4,       # Number of transformer layers\n    dropout_rate=0.1,   # Dropout rate for regularization\n)\n\n# Create constraint config for structural constraints\nconstraint_config = ProteinConstraintConfig(\n    bond_weight=1.0,    # Weight for bond length constraints\n    angle_weight=0.5,   # Weight for bond angle constraints\n)\n\n# Create protein point cloud config with nested configs\nconfig = ProteinPointCloudConfig(\n    name=\"protein_example\",\n    network=network_config,\n    num_points=128 * 4,  # num_residues \u00d7 num_atoms (flattened)\n    dropout_rate=0.1,\n    num_residues=128,    # Maximum number of residues\n    num_atoms_per_residue=4,  # Only backbone atoms (N, CA, C, O)\n    backbone_indices=(0, 1, 2, 3),  # Sequential indices for backbone-only view\n    use_constraints=True,\n    constraint_config=constraint_config,\n)\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#creating-the-model","title":"Creating the Model","text":"<pre><code>from artifex.generative_models.models.geometric.protein_point_cloud import (\n    ProteinPointCloudModel,\n)\nfrom flax import nnx\nimport jax\n\n# Initialize RNGs\nkey = jax.random.key(42)\nkey, params_key, dropout_key = jax.random.split(key, 3)\nrngs = nnx.Rngs(params=params_key, dropout=dropout_key)\n\n# Create model\nmodel = ProteinPointCloudModel(config, rngs=rngs)\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#generating-synthetic-data","title":"Generating Synthetic Data","text":"<pre><code>import numpy as np\n\n# Create alpha-helix geometry\nseq_length = 64\natom_positions = np.zeros((seq_length, num_atoms, 3))\n\nfor i in range(seq_length):\n    t = i * 0.5  # Helix parameter\n\n    # CA (alpha carbon) along helix\n    atom_positions[i, 1, 0] = 3.0 * np.sin(t)\n    atom_positions[i, 1, 1] = 3.0 * np.cos(t)\n    atom_positions[i, 1, 2] = 1.5 * t  # Rise along z-axis\n\n    # N (nitrogen) relative to CA\n    atom_positions[i, 0, :] = atom_positions[i, 1, :] + np.array([-1.45, 0, 0])\n\n    # C (carbon) relative to CA\n    atom_positions[i, 2, :] = atom_positions[i, 1, :] + np.array([1.52, 0, 0])\n\n    # O (oxygen) relative to C\n    atom_positions[i, 4, :] = atom_positions[i, 2, :] + np.array([0, 1.23, 0])\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#model-forward-pass","title":"Model Forward Pass","text":"<pre><code>from artifex.data.protein.dataset import ProteinDataset\n\n# Load dataset\ndataset = ProteinDataset(\"data/synthetic_proteins.pkl\", backbone_only=True)\nbatch = dataset.get_batch([0, 1, 2, 3])\n\n# Forward pass\noutputs = model(batch)\n\nprint(\"Outputs:\")\nfor key, value in outputs.items():\n    if hasattr(value, \"shape\"):\n        print(f\"  {key}: {value.shape}\")\n# Output:\n#   coordinates: (4, 512, 3)  # Predicted atom positions\n#   constraints: {...}         # Constraint violations\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#loss-calculation","title":"Loss Calculation","text":"<pre><code># Get loss function\nloss_fn = model.get_loss_fn()\n\n# Calculate losses\nloss_dict = loss_fn(batch, outputs)\n\nprint(\"Losses:\")\nfor key, value in loss_dict.items():\n    print(f\"  {key}: {value:.4f}\")\n# Output:\n#   total_loss: 2.5634\n#   reconstruction_loss: 2.1234\n#   bond_loss: 0.3200\n#   angle_loss: 0.1200\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>Point Cloud Processing: Representing proteins as unordered sets of 3D points</li> <li>Attention Mechanisms: Multi-head attention for capturing structural context</li> <li>Geometric Constraints: Enforcing physical validity through bond/angle constraints</li> <li>Backbone Representation: Working with minimal backbone atoms (N, CA, C, O)</li> <li>Synthetic Data: Generating alpha-helix structures for testing</li> <li>Loss Computation: Combining reconstruction and constraint losses</li> </ul>"},{"location":"examples/protein/protein-point-cloud-example/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Modify Constraint Weights: Adjust the balance between bond and angle constraints</li> </ol> <pre><code>config.parameters[\"constraint_config\"] = {\n    \"bond_weight\": 2.0,  # Stricter bond constraints\n    \"angle_weight\": 1.0,  # Stricter angle constraints\n}\n</code></pre> <p>Expected Effect: Stronger enforcement of ideal geometry, potentially slower convergence</p> <ol> <li>Change Architecture Size: Increase model capacity for larger proteins</li> </ol> <pre><code>config.parameters.update({\n    \"embed_dim\": 256,     # Increased from 128\n    \"num_layers\": 8,      # Increased from 4\n    \"num_heads\": 8,       # Increased from 4\n})\n</code></pre> <p>Expected Effect: Better capacity for complex structures, more parameters to train</p> <ol> <li>Generate Beta-Sheets: Modify synthetic data to create beta-sheet geometry</li> </ol> <pre><code># Extended strand geometry instead of helix\nfor i in range(seq_length):\n    # CA positions along extended strand\n    atom_positions[i, 1, 0] = i * 3.8  # Extended along x\n    atom_positions[i, 1, 1] = 0.0\n    atom_positions[i, 1, 2] = 0.0\n</code></pre> <p>Expected Effect: Test model's ability to handle different secondary structures</p> <ol> <li>Increase Protein Length: Scale to longer sequences</li> </ol> <pre><code>config.parameters[\"num_residues\"] = 256  # Increased from 128\nmax_seq_length = 256\n</code></pre> <p>Expected Effect: Test scaling behavior and memory requirements</p>"},{"location":"examples/protein/protein-point-cloud-example/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/protein/protein-point-cloud-example/#common-issues","title":"Common Issues","text":""},{"location":"examples/protein/protein-point-cloud-example/#shape-mismatch-error","title":"Shape mismatch error","text":"<p>Symptom:</p> <pre><code>ValueError: Expected shape (batch, 512, 3), got (batch, 256, 3)\n</code></pre> <p>Cause: Mismatch between <code>num_residues \u00d7 num_atoms</code> in config and actual data shape.</p> <p>Solution:</p> <pre><code># Ensure consistency\nnum_residues = 128\nnum_atoms = 4\nconfig.parameters[\"num_residues\"] = num_residues\nconfig.parameters[\"num_atoms\"] = num_atoms\nconfig.input_dim = num_residues * num_atoms\nconfig.output_dim = num_residues * num_atoms\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#oom-out-of-memory-error","title":"OOM (Out of Memory) error","text":"<p>Symptom:</p> <pre><code>jax.errors.OutOfMemoryError: RESOURCE_EXHAUSTED\n</code></pre> <p>Cause: Model or sequence length too large for available GPU memory.</p> <p>Solutions:</p> <ol> <li>Reduce sequence length:</li> </ol> <pre><code>config.parameters[\"num_residues\"] = 64  # Reduced from 128\n</code></pre> <ol> <li>Reduce model size:</li> </ol> <pre><code>config.parameters.update({\n    \"embed_dim\": 64,      # Reduced from 128\n    \"num_layers\": 2,      # Reduced from 4\n})\n</code></pre> <ol> <li>Use CPU instead of GPU:</li> </ol> <pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force CPU\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#constraint-loss-is-nan","title":"Constraint loss is NaN","text":"<p>Symptom:</p> <pre><code>bond_loss: nan\nangle_loss: nan\n</code></pre> <p>Cause: Invalid atom positions (NaN or Inf values) in data.</p> <p>Solution:</p> <pre><code># Validate data before training\nimport numpy as np\n\ndef validate_batch(batch):\n    for key, value in batch.items():\n        if not np.all(np.isfinite(value)):\n            raise ValueError(f\"{key} contains NaN or Inf values\")\n    return batch\n\nbatch = validate_batch(batch)\n</code></pre>"},{"location":"examples/protein/protein-point-cloud-example/#summary","title":"Summary","text":"<p>In this example, you learned:</p> <ul> <li>\u2705 How to represent proteins as 3D point clouds for geometric modeling</li> <li>\u2705 How to configure ProteinPointCloudModel with attention mechanisms</li> <li>\u2705 How to generate synthetic alpha-helix structures for testing</li> <li>\u2705 How to apply geometric constraints to ensure chemical validity</li> <li>\u2705 How to evaluate models using reconstruction and constraint losses</li> <li>\u2705 How to work with backbone-only protein representations</li> </ul> <p>Key Takeaways:</p> <ol> <li>Point Clouds: Effective representation for geometric protein modeling</li> <li>Attention: Captures long-range interactions in protein structures</li> <li>Constraints: Essential for generating physically plausible structures</li> <li>Backbone: Minimal representation sufficient for overall structure</li> </ol>"},{"location":"examples/protein/protein-point-cloud-example/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Protein Model with Modality</p> <p>Learn about the modality architecture for protein modeling</p> <p> protein-model-with-modality.md</p> </li> <li> <p> Protein Extensions</p> <p>Deep dive into protein-specific constraint extensions</p> <p> protein-extensions-example.py</p> </li> <li> <p>:material-benchmark: Geometric Benchmark</p> <p>Evaluate geometric models on protein datasets</p> <p> geometric-benchmark-demo.md</p> </li> <li> <p> Protein Ligand Benchmark</p> <p>Advanced protein-ligand interaction modeling</p> <p> protein-ligand-benchmark-demo.md</p> </li> </ul>"},{"location":"examples/protein/protein-point-cloud-example/#additional-resources","title":"Additional Resources","text":"<ul> <li>Artifex Documentation: Protein Modeling Guide</li> <li>Artifex Documentation: Geometric Models</li> <li>Artifex Documentation: Point Cloud Processing</li> <li>API Reference: ProteinPointCloudModel</li> <li>API Reference: ProteinDataset</li> <li>Paper: ProteinMPNN - Protein structure prediction with message passing</li> </ul>"},{"location":"examples/protein/protein-point-cloud-example/#related-examples","title":"Related Examples","text":"<ul> <li>Protein Model with Modality - Using the modality architecture</li> <li>Protein Extensions Example - Protein-specific constraints</li> <li>Geometric Benchmark Demo - Evaluating geometric models</li> </ul>"},{"location":"examples/sampling/blackjax-example/","title":"BlackJAX Integration Example","text":""},{"location":"examples/sampling/blackjax-example/#overview","title":"Overview","text":"<p>This example demonstrates how to use BlackJAX samplers with Artifex's distribution framework, comparing different MCMC algorithms on both multimodal distributions and Bayesian regression tasks.</p>"},{"location":"examples/sampling/blackjax-example/#files","title":"Files","text":"<ul> <li>Python script: <code>examples/generative_models/sampling/blackjax_example.py</code></li> <li>Jupyter notebook: <code>examples/generative_models/sampling/blackjax_example.ipynb</code></li> </ul>"},{"location":"examples/sampling/blackjax-example/#quick-start","title":"Quick Start","text":"Python ScriptJupyter Notebook <pre><code># Run the complete example\npython examples/generative_models/sampling/blackjax_example.py\n</code></pre> <pre><code># Launch Jupyter and open the notebook\njupyter notebook examples/generative_models/sampling/blackjax_example.ipynb\n</code></pre>"},{"location":"examples/sampling/blackjax-example/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this example, you will:</p> <ul> <li> Understand how to use BlackJAX samplers (HMC, NUTS, MALA) with Artifex</li> <li> Learn to sample from multimodal distributions using different MCMC methods</li> <li> Implement Bayesian regression using NUTS sampling</li> <li> Compare different sampling algorithms for the same problem</li> <li> Visualize and interpret MCMC sampling results</li> </ul>"},{"location":"examples/sampling/blackjax-example/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of MCMC sampling concepts</li> <li>Basic knowledge of Bayesian inference</li> <li>Familiarity with probability distributions</li> <li>Artifex core sampling module</li> </ul>"},{"location":"examples/sampling/blackjax-example/#what-is-blackjax","title":"What is BlackJAX?","text":"<p>BlackJAX is a library of samplers for JAX that provides state-of-the-art MCMC algorithms. Artifex integrates BlackJAX to offer advanced sampling capabilities.</p>"},{"location":"examples/sampling/blackjax-example/#supported-algorithms","title":"Supported Algorithms","text":"Algorithm Description Best For HMC Hamiltonian Monte Carlo Smooth, continuous distributions NUTS No-U-Turn Sampler Complex posteriors, automatic tuning MALA Metropolis-Adjusted Langevin Gradient-based sampling"},{"location":"examples/sampling/blackjax-example/#theory","title":"Theory","text":""},{"location":"examples/sampling/blackjax-example/#hamiltonian-monte-carlo-hmc","title":"Hamiltonian Monte Carlo (HMC)","text":"<p>HMC uses Hamiltonian dynamics to propose efficient moves in the parameter space:</p> \\[ H(q, p) = U(q) + K(p) \\] <p>where \\(U(q) = -\\log p(q)\\) is the potential energy and \\(K(p) = \\frac{1}{2}p^T M^{-1} p\\) is the kinetic energy.</p> <p>The algorithm simulates Hamiltonian dynamics using the leapfrog integrator:</p> \\[ \\begin{align} p_{i+\\frac{1}{2}} &amp;= p_i - \\frac{\\epsilon}{2} \\nabla U(q_i) \\\\ q_{i+1} &amp;= q_i + \\epsilon M^{-1} p_{i+\\frac{1}{2}} \\\\ p_{i+1} &amp;= p_{i+\\frac{1}{2}} - \\frac{\\epsilon}{2} \\nabla U(q_{i+1}) \\end{align} \\]"},{"location":"examples/sampling/blackjax-example/#no-u-turn-sampler-nuts","title":"No-U-Turn Sampler (NUTS)","text":"<p>NUTS automatically tunes the HMC trajectory length by building a tree of states until the trajectory makes a \"U-turn\". This eliminates the need to manually set the number of integration steps.</p>"},{"location":"examples/sampling/blackjax-example/#mala-metropolis-adjusted-langevin-algorithm","title":"MALA (Metropolis-Adjusted Langevin Algorithm)","text":"<p>MALA uses Langevin dynamics for proposals:</p> \\[ \\theta' = \\theta + \\frac{\\epsilon^2}{2} \\nabla \\log p(\\theta) + \\epsilon \\eta \\] <p>where \\(\\eta \\sim \\mathcal{N}(0, I)\\) is Gaussian noise.</p>"},{"location":"examples/sampling/blackjax-example/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/sampling/blackjax-example/#example-1-multimodal-distribution-sampling","title":"Example 1: Multimodal Distribution Sampling","text":"<p>The first example compares four sampling methods on a bimodal Gaussian mixture:</p> <pre><code># Define a bimodal log probability function\ndef log_prob_fn(x):\n    log_prob1 = -0.5 * ((x - 2.0) ** 2) / 0.5\n    log_prob2 = -0.5 * ((x + 2.0) ** 2) / 0.5\n    return jnp.logaddexp(log_prob1, log_prob2)\n\n# Sample using Metropolis-Hastings\nmh_samples = mcmc_sampling(\n    log_prob_fn=log_prob_fn,\n    init_state=init_state,\n    key=key,\n    n_samples=2000,\n    n_burnin=500,\n    step_size=0.5,\n)\n\n# Sample using HMC (BlackJAX)\nhmc_samples = hmc_sampling(\n    log_prob_fn=log_prob_fn,\n    init_state=init_state,\n    key=key,\n    n_samples=2000,\n    n_burnin=500,\n    step_size=0.1,\n    num_integration_steps=10,\n)\n</code></pre> <p>Key Points:</p> <ul> <li>The bimodal distribution has two modes at \\(x = -2\\) and \\(x = 2\\)</li> <li>HMC uses gradient information for more efficient exploration</li> <li>NUTS automatically tunes trajectory length</li> <li>MALA balances speed and efficiency</li> </ul>"},{"location":"examples/sampling/blackjax-example/#example-2-bayesian-linear-regression","title":"Example 2: Bayesian Linear Regression","text":"<p>The second example demonstrates Bayesian parameter estimation:</p> <pre><code># Define Bayesian regression model\ndef log_prob_fn(params):\n    beta = params[\"beta\"]\n    log_sigma = params[\"log_sigma\"]\n    sigma = jnp.exp(log_sigma)\n\n    # Prior\n    prior_beta = jnp.sum(jax.scipy.stats.norm.logpdf(beta, loc=0.0, scale=1.0))\n    prior_sigma = jax.scipy.stats.norm.logpdf(log_sigma, loc=-2.0, scale=1.0)\n\n    # Likelihood\n    y_pred = X @ beta\n    likelihood = jnp.sum(jax.scipy.stats.norm.logpdf(y, loc=y_pred, scale=sigma))\n\n    return prior_beta + prior_sigma + likelihood\n\n# Sample using NUTS\nnuts_samples = nuts_sampling(\n    log_prob_fn=log_prob_fn,\n    init_state=init_state,\n    key=key,\n    n_samples=2000,\n    n_burnin=1000,\n)\n</code></pre> <p>Key Points:</p> <ul> <li>NUTS is ideal for Bayesian inference with multiple parameters</li> <li>The model includes priors on coefficients and noise scale</li> <li>Posterior distributions recover true parameter values</li> <li>No manual tuning of trajectory length needed</li> </ul>"},{"location":"examples/sampling/blackjax-example/#expected-output","title":"Expected Output","text":""},{"location":"examples/sampling/blackjax-example/#multimodal-distribution","title":"Multimodal Distribution","text":"<p>The example generates comparison plots showing samples from each method:</p> <ul> <li>Metropolis-Hastings: Baseline performance</li> <li>HMC: Efficient exploration with gradient information</li> <li>NUTS: Similar to HMC but with automatic tuning</li> <li>MALA: Fast per-iteration sampling</li> </ul> <p>You should see all methods successfully sample from both modes of the distribution.</p>"},{"location":"examples/sampling/blackjax-example/#bayesian-regression","title":"Bayesian Regression","text":"<p>The regression example produces:</p> <ol> <li>Coefficient posteriors: Distributions for each \\(\\beta_i\\) parameter</li> <li>Noise posterior: Distribution for noise scale \\(\\sigma\\)</li> <li>Comparison with truth: True values marked on plots</li> </ol> <p>Expected results:</p> <ul> <li>Posterior means close to true parameter values</li> <li>Reasonable posterior uncertainty</li> <li>Successful convergence after burn-in</li> </ul>"},{"location":"examples/sampling/blackjax-example/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/sampling/blackjax-example/#computational-cost","title":"Computational Cost","text":"Method Time per Sample Effective Sample Size (ESS) Overall Efficiency MH Low Low Baseline HMC Medium High Good NUTS High Very High Excellent MALA Low-Medium Medium Good"},{"location":"examples/sampling/blackjax-example/#memory-usage","title":"Memory Usage","text":"<ul> <li>HMC/MALA: Moderate memory usage</li> <li>NUTS: Higher memory usage due to trajectory storage</li> <li>For memory-constrained systems, reduce <code>max_num_doublings</code> in NUTS</li> </ul>"},{"location":"examples/sampling/blackjax-example/#tuning-guidelines","title":"Tuning Guidelines","text":"<p>HMC:</p> <ul> <li><code>step_size</code>: Start with 0.1, adjust based on acceptance rate (target: 0.6-0.8)</li> <li><code>num_integration_steps</code>: Start with 10, increase for complex distributions</li> </ul> <p>MALA:</p> <ul> <li><code>step_size</code>: Start with 0.05, adjust based on acceptance rate (target: 0.5-0.7)</li> </ul> <p>NUTS:</p> <ul> <li><code>step_size</code>: Usually auto-tuned, but can be set manually</li> <li><code>max_num_doublings</code>: Controls trajectory length and memory usage (default: 10)</li> </ul>"},{"location":"examples/sampling/blackjax-example/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li> <p>Change the distribution: Modify the bimodal distribution to have three modes    or varying widths</p> </li> <li> <p>Tune hyperparameters: Experiment with different step sizes and integration    steps, observing effects on acceptance rate and mixing</p> </li> <li> <p>Compare convergence: Plot traces and autocorrelation to assess convergence    and mixing for each method</p> </li> <li> <p>Higher dimensions: Extend Bayesian regression to 20-50 features to see    how samplers scale</p> </li> <li> <p>Different priors: Try informative vs uninformative priors on regression    coefficients</p> </li> <li> <p>Visualize traces: Add MCMC trace plots to check convergence</p> </li> </ol>"},{"location":"examples/sampling/blackjax-example/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/sampling/blackjax-example/#low-acceptance-rate","title":"Low Acceptance Rate","text":"<p>Symptom: Acceptance rate below 0.5 for HMC/MALA</p> <p>Solution:</p> <ul> <li>Decrease <code>step_size</code> for HMC/MALA</li> <li>Check gradient computation (should not have NaNs)</li> <li>Verify log probability function is correct</li> </ul>"},{"location":"examples/sampling/blackjax-example/#poor-mixing","title":"Poor Mixing","text":"<p>Symptom: Samples stay in one mode of multimodal distribution</p> <p>Solution:</p> <ul> <li>Increase burn-in period</li> <li>Try different initialization points</li> <li>Consider tempering or parallel chains</li> </ul>"},{"location":"examples/sampling/blackjax-example/#nuts-memory-errors","title":"NUTS Memory Errors","text":"<p>Symptom: Out of memory errors with NUTS</p> <p>Solution:</p> <pre><code># Reduce max_num_doublings\nnuts_samples = nuts_sampling(\n    log_prob_fn=log_prob_fn,\n    init_state=init_state,\n    key=key,\n    n_samples=1000,  # Reduce number of samples\n    n_burnin=500,\n    max_num_doublings=5,  # Lower from default 10\n)\n</code></pre>"},{"location":"examples/sampling/blackjax-example/#divergent-transitions-nuts","title":"Divergent Transitions (NUTS)","text":"<p>Symptom: Warning about divergent transitions</p> <p>Solution:</p> <ul> <li>Decrease <code>step_size</code></li> <li>Reparameterize the model (e.g., use non-centered parameterization)</li> <li>Check for prior-likelihood conflicts</li> </ul>"},{"location":"examples/sampling/blackjax-example/#next-steps","title":"Next Steps","text":""},{"location":"examples/sampling/blackjax-example/#related-examples","title":"Related Examples","text":"<ul> <li> <p>BlackJAX Sampling Examples</p> <p>Explore more sampling algorithms and advanced usage patterns</p> <p>blackjax-sampling-examples.md</p> </li> <li> <p>BlackJAX Integration Examples</p> <p>Learn advanced integration with Artifex distributions</p> <p>blackjax-integration-examples.md</p> </li> </ul>"},{"location":"examples/sampling/blackjax-example/#further-learning","title":"Further Learning","text":"<ul> <li>BlackJAX Documentation</li> <li>MCMC Diagnostics</li> <li>HMC Tutorial</li> <li>Artifex Sampling Module Documentation</li> </ul>"},{"location":"examples/sampling/blackjax-example/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/sampling/blackjax-example/#papers","title":"Papers","text":"<ol> <li>Neal, R. M. (2011). \"MCMC using Hamiltonian dynamics\"</li> <li>Hoffman, M. D., &amp; Gelman, A. (2014). \"The No-U-Turn Sampler\"</li> <li>Roberts, G. O., &amp; Tweedie, R. L. (1996). \"Exponential convergence of Langevin    distributions and their discrete approximations\"</li> </ol>"},{"location":"examples/sampling/blackjax-example/#code-references","title":"Code References","text":"<ul> <li>Distribution creation: <code>artifex.generative_models.core.distributions</code></li> <li>Sampling functions: <code>artifex.generative_models.core.sampling</code></li> <li>BlackJAX integration: <code>artifex.generative_models.core.sampling.blackjax_samplers</code></li> </ul>"},{"location":"examples/sampling/blackjax-example/#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check that BlackJAX is installed: <code>pip install blackjax</code></li> <li>Verify JAX GPU/CPU setup is correct</li> <li>Review error messages for parameter constraints</li> <li>Consult Artifex documentation or open an issue</li> </ol> <p>Tags: #mcmc #blackjax #hmc #nuts #mala #bayesian #sampling</p> <p>Difficulty: Intermediate</p> <p>Estimated Time: 15-20 minutes</p>"},{"location":"examples/sampling/blackjax-integration-examples/","title":"BlackJAX Integration Examples","text":""},{"location":"examples/sampling/blackjax-integration-examples/#overview","title":"Overview","text":"<p>This example demonstrates advanced integration patterns between BlackJAX samplers and Artifex's distribution framework. It compares two approaches: using BlackJAX's API directly for maximum control and visual feedback, versus using Artifex's functional API for simplicity and maximum performance.</p>"},{"location":"examples/sampling/blackjax-integration-examples/#files","title":"Files","text":"<ul> <li>Python script: <code>examples/generative_models/sampling/blackjax_integration_examples.py</code></li> <li>Jupyter notebook: <code>examples/generative_models/sampling/blackjax_integration_examples.ipynb</code></li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#quick-start","title":"Quick Start","text":"Python ScriptJupyter Notebook <pre><code># Run the complete example\npython examples/generative_models/sampling/blackjax_integration_examples.py\n</code></pre> <pre><code># Launch Jupyter and open the notebook\njupyter notebook examples/generative_models/sampling/blackjax_integration_examples.ipynb\n</code></pre>"},{"location":"examples/sampling/blackjax-integration-examples/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this example, you will:</p> <ul> <li> Understand how to use BlackJAX sampler classes with Artifex distributions</li> <li> Learn to use both class-based and functional sampling APIs</li> <li> Apply samplers to Artifex distributions (Normal, Mixture)</li> <li> Compare class-based vs functional sampling approaches</li> <li> Handle memory constraints in NUTS sampling</li> <li> Sample from mixture distributions using MCMC</li> <li> Understand the trade-offs between direct API (progress bars) and functional API (speed)</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of MCMC sampling concepts</li> <li>Familiarity with Artifex distributions module</li> <li>Basic knowledge of HMC, MALA, and NUTS algorithms</li> <li>Completion of BlackJAX Integration Example</li> <li>Artifex core sampling and distributions modules</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#integration-approaches","title":"Integration Approaches","text":"<p>Artifex supports two ways to integrate with BlackJAX, each with distinct advantages:</p>"},{"location":"examples/sampling/blackjax-integration-examples/#1-direct-blackjax-api","title":"1. Direct BlackJAX API","text":"<p>Use BlackJAX's native API directly with Artifex distributions:</p> <p>Advantages:</p> <ul> <li>Full control over sampling parameters and state management</li> <li>Progress bars with <code>tqdm</code> for visual feedback</li> <li>Per-iteration monitoring and debugging</li> <li>Custom sampling logic and diagnostics</li> </ul> <p>Disadvantages:</p> <ul> <li>More verbose code</li> <li>Requires manual state management</li> <li>Must handle random key splitting manually</li> </ul> <p>When to use:</p> <ul> <li>Interactive development and exploration</li> <li>When you need visual feedback on long-running samples</li> <li>Debugging and monitoring sampling behavior</li> <li>Implementing custom sampling algorithms</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#2-artifex-functional-api","title":"2. Artifex Functional API","text":"<p>Use Artifex's convenience functions like <code>hmc_sampling()</code>, <code>mala_sampling()</code>:</p> <p>Advantages:</p> <ul> <li>Single function call for complete sampling workflow</li> <li>Automatic burn-in and state management</li> <li>Fully JIT-compiled for maximum performance</li> <li>Simplified interface for common use cases</li> <li>Cleanest, most concise code</li> </ul> <p>Disadvantages:</p> <ul> <li>No progress bars (due to JIT compilation)</li> <li>Less fine-grained control</li> <li>Limited customization options</li> </ul> <p>When to use:</p> <ul> <li>Production code requiring maximum performance</li> <li>Batch processing and automated workflows</li> <li>When simplicity is more important than monitoring</li> <li>Recommended for most applications</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#example-overview","title":"Example Overview","text":"<p>This example includes six demonstrations:</p> <ol> <li>Normal Distribution with Direct BlackJAX HMC: Full control with progress bars</li> <li>Normal Distribution with hmc_sampling: Simple, fast functional API</li> <li>Normal Distribution with Direct BlackJAX MALA: MALA sampler with monitoring</li> <li>Univariate Normal with Direct BlackJAX NUTS: Memory-aware NUTS implementation</li> <li>Multimodal Distribution Comparison: Teaching example comparing samplers</li> <li>5a: Mixture with MALA (Wide Separation): Demonstrates local sampler limitations</li> <li>5b: Mixture with NUTS (Moderate Separation): Shows Hamiltonian dynamics advantage</li> </ol>"},{"location":"examples/sampling/blackjax-integration-examples/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/sampling/blackjax-integration-examples/#example-1-direct-blackjax-hmc-with-progress-bars","title":"Example 1: Direct BlackJAX HMC with Progress Bars","text":"<p>This example demonstrates the direct API approach with full visual feedback:</p> <pre><code>import blackjax\nfrom artifex.generative_models.core.distributions import Normal\nfrom tqdm import tqdm\n\n# Create distribution\ntrue_mean = jnp.array([3.0, -2.0])\ntrue_scale = jnp.array([1.5, 0.8])\nnormal_dist = Normal(loc=true_mean, scale=true_scale)\n\n# Set up HMC sampler\ninverse_mass_matrix = jnp.eye(2)\nhmc = blackjax.hmc(\n    normal_dist.log_prob,\n    step_size=0.1,\n    inverse_mass_matrix=inverse_mass_matrix,\n    num_integration_steps=10,\n)\n\n# Initialize\ninit_position = jnp.zeros(2)\nstate = hmc.init(init_position)\nstep_fn = jax.jit(hmc.step)  # JIT compile step function\n\n# Burn-in with progress bar\nprint(\"Running burn-in...\")\nfor i in tqdm(range(n_burnin), desc=\"Burn-in\", ncols=80):\n    key = jax.random.fold_in(key, i)\n    state, _ = step_fn(key, state)\n\n# Sampling with progress bar\nsamples = jnp.zeros((n_samples, 2))\nprint(\"Sampling...\")\nfor i in tqdm(range(n_samples), desc=\"Sampling\", ncols=80):\n    key = jax.random.fold_in(key, n_burnin + i)\n    state, _ = step_fn(key, state)\n    samples = samples.at[i].set(state.position)\n</code></pre> <p>Key Points:</p> <ul> <li>JIT-compile the step function for performance: <code>step_fn = jax.jit(hmc.step)</code></li> <li>Use <code>tqdm</code> for visual feedback during long-running operations</li> <li>Manual state management provides full control</li> <li>Use <code>jax.random.fold_in()</code> for deterministic key splitting</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#example-2-functional-api-for-maximum-performance","title":"Example 2: Functional API for Maximum Performance","text":"<p>This example shows the simplified functional API:</p> <pre><code>from artifex.generative_models.core.sampling.blackjax_samplers import hmc_sampling\n\n# Create distribution (same as above)\nnormal_dist = Normal(loc=true_mean, scale=true_scale)\n\n# Single function call - fully JIT-compiled\nsamples = hmc_sampling(\n    normal_dist,\n    init_position,\n    key,\n    n_samples=1000,\n    n_burnin=500,\n    step_size=0.1,\n    num_integration_steps=10,\n)\n</code></pre> <p>Key Points:</p> <ul> <li>Single function call replaces ~20 lines of code</li> <li>Automatically JIT-compiled using <code>jax.lax.scan</code> internally</li> <li>No progress bars, but maximum performance</li> <li>Automatic state management and burn-in</li> <li>Recommended for production and batch processing</li> </ul> <p>Performance Comparison:</p> <ul> <li>Direct API with progress bars: ~2-3s for 1000 samples (with tqdm overhead)</li> <li>Functional API: ~1-2s for 1000 samples (fully optimized)</li> <li>Both use JIT compilation, but functional API has less Python overhead</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#example-3-direct-blackjax-mala","title":"Example 3: Direct BlackJAX MALA","text":"<p>MALA demonstrates faster per-iteration sampling:</p> <pre><code># Create MALA sampler\nmala = blackjax.mala(normal_dist.log_prob, step_size=0.05)\n\n# Initialize and run (similar pattern to HMC)\nstate = mala.init(init_position)\nstep_fn = jax.jit(mala.step)\n\n# Burn-in and sampling with progress bars\n# (same structure as HMC example)\n</code></pre> <p>Key Points:</p> <ul> <li>MALA uses smaller step sizes than HMC (typically 0.05 vs 0.1)</li> <li>Faster per-iteration, but may need more samples for same ESS</li> <li>Good for problems where gradient evaluation is cheap</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#example-4-nuts-with-memory-awareness","title":"Example 4: NUTS with Memory Awareness","text":"<p>NUTS requires special attention to memory constraints:</p> <pre><code># Use 1D distribution to reduce memory\ntrue_mean = jnp.array([2.0])\ntrue_scale = jnp.array([1.0])\nnormal_dist_1d = Normal(loc=true_mean, scale=true_scale)\n\n# Create NUTS sampler with memory constraints\ninverse_mass_matrix = jnp.array([1.0])\nnuts = blackjax.nuts(\n    normal_dist_1d.log_prob,\n    step_size=0.8,\n    inverse_mass_matrix=inverse_mass_matrix,\n    max_depth=5,  # Lower to reduce memory usage\n)\n</code></pre> <p>Key Points:</p> <ul> <li>NUTS stores trajectory information, requiring more memory</li> <li>Use <code>max_depth</code> parameter to control memory usage (default: 10)</li> <li>Start with lower-dimensional problems for testing</li> <li>Reduce <code>max_depth</code> if encountering memory errors</li> <li>For production, use smaller <code>n_samples</code> or increase system memory</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#example-5-multimodal-distribution-comparison","title":"Example 5: Multimodal Distribution Comparison","text":"<p>This teaching example demonstrates how different MCMC samplers handle multimodal distributions, comparing local samplers (MALA) with Hamiltonian samplers (NUTS).</p>"},{"location":"examples/sampling/blackjax-integration-examples/#example-5a-mala-on-widely-separated-mixture","title":"Example 5a: MALA on Widely-Separated Mixture","text":"<p>Demonstrates MALA's limitation with distant modes:</p> <pre><code>from artifex.generative_models.core.distributions import Mixture, Normal\nfrom artifex.generative_models.core.sampling.blackjax_samplers import mala_sampling\n\n# Create 1D mixture with modes 10 units apart\nweights = jnp.array([0.6, 0.4])\nmeans = jnp.array([[-2.0], [8.0]])  # Widely separated\nscales = jnp.array([[0.8], [0.8]])\n\ncomponents = [Normal(loc=means[0], scale=scales[0]),\n              Normal(loc=means[1], scale=scales[1])]\nmixture = Mixture(components, weights)\n\n# Sample with MALA\nsamples = mala_sampling(\n    mixture,\n    init_position=jnp.array([-2.0]),  # Start at first mode\n    key=key,\n    n_samples=10000,\n    n_burnin=5000,\n    step_size=0.05,\n)\n</code></pre> <p>Observation: MALA gets stuck at the starting mode due to small gradient-guided steps. With <code>step_size=0.05</code> and modes 10 units apart, the sampler cannot efficiently jump between modes.</p> <p>Key Teaching Points:</p> <ul> <li>MALA is a local sampler - takes small gradient-guided steps</li> <li>Struggles with modes separated by low-probability regions</li> <li>Step size trade-off: small = slow mixing, large = poor acceptance</li> <li>Demonstrates importance of algorithm selection for problem structure</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#example-5b-nuts-on-moderately-separated-mixture","title":"Example 5b: NUTS on Moderately-Separated Mixture","text":"<p>Shows NUTS's improved exploration:</p> <pre><code>from artifex.generative_models.core.sampling.blackjax_samplers import nuts_sampling\n\n# Create 1D mixture with modes 5 units apart (more moderate)\nweights = jnp.array([0.6, 0.4])\nmeans = jnp.array([[-2.0], [3.0]])  # Moderately separated\nscales = jnp.array([[0.8], [0.8]])\n\ncomponents = [Normal(loc=means[0], scale=scales[0]),\n              Normal(loc=means[1], scale=scales[1])]\nmixture = Mixture(components, weights)\n\n# Sample with NUTS\nsamples = nuts_sampling(\n    mixture,\n    init_position=jnp.array([-2.0]),\n    key=key,\n    n_samples=10000,\n    n_burnin=5000,\n    step_size=0.5,  # NUTS adapts this\n)\n</code></pre> <p>Observation: NUTS successfully explores both modes, achieving ~53%/47% occupancy (close to target 60%/40%). Hamiltonian dynamics enable long-range exploration.</p> <p>Key Teaching Points:</p> <ul> <li>NUTS uses Hamiltonian dynamics - momentum enables distant exploration</li> <li>Automatically adapts step size and trajectory length (no-U-turn criterion)</li> <li>Handles moderate multimodality better than local samplers</li> <li>Still faces challenges with very distant modes (energy conservation constraints)</li> <li>For extreme multimodality: need parallel tempering, SMC, or tempered transitions</li> </ul> <p>Comparison Summary:</p> Aspect MALA (5a) NUTS (5b) Separation 10 units (wide) 5 units (moderate) Result Stuck in one mode Both modes explored Mechanism Gradient-guided steps Hamiltonian dynamics Best for Unimodal/log-concave Moderate multimodality <p>Research Foundation:</p> <p>This comparison is supported by extensive MCMC research:</p> <ol> <li>Roberts &amp; Tweedie (1996) - \"Exponential convergence of Langevin diffusions and their discrete approximations\"</li> <li>Established MALA's limitations with multimodal distributions</li> <li> <p>Showed MALA struggles with modes separated by low-probability regions</p> </li> <li> <p>Neal (2011) - \"MCMC Using Hamiltonian Dynamics\" (Handbook of MCMC)</p> </li> <li>Comprehensive treatment of HMC advantages for exploration</li> <li> <p>Explains energy conservation constraints limiting extreme mode-switching</p> </li> <li> <p>Hoffman &amp; Gelman (2014) - \"The No-U-Turn Sampler\" (JMLR 15:1593-1623)</p> </li> <li>Introduced NUTS as adaptive HMC</li> <li>Demonstrated superior performance on complex posteriors</li> <li> <p>Note: Still faces challenges with very distant modes</p> </li> <li> <p>Betancourt (2017) - \"A Conceptual Introduction to Hamiltonian Monte Carlo\"</p> </li> <li>Explains why HMC/NUTS struggle with strongly multimodal distributions</li> <li>Maximum potential energy increase bounded by initial kinetic energy</li> <li>Recommends tempering for extreme multimodality</li> </ol>"},{"location":"examples/sampling/blackjax-integration-examples/#expected-output","title":"Expected Output","text":""},{"location":"examples/sampling/blackjax-integration-examples/#sample-plots","title":"Sample Plots","text":"<p>Examples generate visualizations showing sampling behavior:</p> <ul> <li>Normal distributions (Examples 1-4): Scatter plots (2D) or histograms (1D) centered at true parameters</li> <li>Mixture 5a (MALA): Histogram shows samples stuck near -2.0, very few near 8.0</li> <li>Mixture 5b (NUTS): Histogram shows clear bimodal structure with both modes explored</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#statistics-tables","title":"Statistics Tables","text":"<p>Examples print comparison tables showing true vs sampled statistics:</p> <pre><code>Statistic       True Value                     Sample Value\n----------------------------------------------------------------------\nMean            [ 3.0000, -2.0000]             [ 3.0123, -1.9987]\nStd             [ 1.5000,  0.8000]             [ 1.4987,  0.8012]\n\nTiming: 1.23s total (812.3 samples/sec)\n</code></pre>"},{"location":"examples/sampling/blackjax-integration-examples/#timing-information","title":"Timing Information","text":"<ul> <li>Direct API: Includes separate burn-in and sampling times</li> <li>Functional API: Reports total time (including JIT compilation on first call)</li> <li>Samples/sec: Measures sampling throughput (excluding burn-in)</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/sampling/blackjax-integration-examples/#api-comparison","title":"API Comparison","text":"Aspect Direct API Functional API Speed Fast (JIT-compiled steps) Fastest (fully JIT-compiled) Progress Bars \u2705 Yes \u274c No (JIT limitation) Code Complexity Medium (~30-40 lines) Low (~5-10 lines) Flexibility High (full control) Medium (common parameters) Memory Efficiency Good Excellent (optimized scan) Best For Development, debugging Production, batch jobs"},{"location":"examples/sampling/blackjax-integration-examples/#memory-usage","title":"Memory Usage","text":"<p>HMC/MALA:</p> <ul> <li>Memory scales with problem dimension and sample count</li> <li>Minimal overhead beyond sample storage</li> </ul> <p>NUTS:</p> <ul> <li>Stores trajectory tree: memory = \\(O(2^{\\text{max\\_depth}} \\times \\text{dimension})\\)</li> <li>Reduce <code>max_depth</code> from default 10 to 5-7 for memory-constrained systems</li> <li>Use smaller dimensions for testing (1D-5D)</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#tuning-recommendations","title":"Tuning Recommendations","text":"<p>For Direct API:</p> <ul> <li>JIT-compile step function: <code>step_fn = jax.jit(sampler.step)</code></li> <li>Use <code>jax.random.fold_in()</code> for deterministic key generation</li> <li>Add progress bars for long-running samples (burn-in &gt; 1000 or n_samples &gt; 5000)</li> </ul> <p>For Functional API:</p> <ul> <li>No additional tuning needed - already optimized</li> <li>First call includes JIT compilation time (~1-5s)</li> <li>Subsequent calls with same parameters are instant</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/sampling/blackjax-integration-examples/#slow-sampling-direct-api","title":"Slow Sampling (Direct API)","text":"<p>Symptom: Direct API examples taking too long</p> <p>Solution:</p> <pre><code># Always JIT-compile the step function\nstep_fn = jax.jit(hmc.step)  # DO THIS\n\n# Not this:\nstate, _ = hmc.step(key, state)  # Too slow!\n</code></pre>"},{"location":"examples/sampling/blackjax-integration-examples/#no-progress-bars-functional-api","title":"No Progress Bars (Functional API)","text":"<p>Symptom: Functional API appears to hang with no feedback</p> <p>Solution:</p> <ul> <li>This is expected behavior - functional API is fully JIT-compiled</li> <li>First call takes longer (JIT compilation)</li> <li>No progress bars due to JIT compilation</li> <li>If you need progress bars, use the Direct API approach</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#nuts-memory-errors","title":"NUTS Memory Errors","text":"<p>Symptom: Out of memory when using NUTS</p> <p>Solution:</p> <pre><code># Reduce max_depth\nnuts = blackjax.nuts(\n    log_prob_fn,\n    step_size=0.8,\n    inverse_mass_matrix=mass_matrix,\n    max_depth=5,  # Lower from default 10\n)\n\n# Or reduce problem dimension for testing\n# Or use fewer samples\nn_samples = 500  # Instead of 2000\n</code></pre>"},{"location":"examples/sampling/blackjax-integration-examples/#poor-mixing-on-mixture","title":"Poor Mixing on Mixture","text":"<p>Symptom: Samples stuck in one mode of mixture distribution</p> <p>Solution:</p> <pre><code># Increase burn-in significantly\nn_burnin = 2000  # Or more\n\n# Try different initialization\ninit_position = jnp.array([3.0, 3.0])  # Start near one mode\n\n# Use longer sampling\nn_samples = 5000\n\n# Consider HMC instead of MALA for better mode exploration\n</code></pre>"},{"location":"examples/sampling/blackjax-integration-examples/#design-patterns","title":"Design Patterns","text":""},{"location":"examples/sampling/blackjax-integration-examples/#when-to-use-each-approach","title":"When to Use Each Approach","text":"<p>Use Direct API when:</p> <ul> <li>Developing and debugging new sampling strategies</li> <li>Need visual feedback on long-running operations</li> <li>Implementing custom diagnostics or monitoring</li> <li>Interactive exploration in Jupyter notebooks</li> <li>Learning and understanding MCMC behavior</li> </ul> <p>Use Functional API when:</p> <ul> <li>Running production inference pipelines</li> <li>Batch processing many sampling tasks</li> <li>Maximizing performance is critical</li> <li>Code simplicity is valued</li> <li>You're confident in the sampling parameters</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#hybrid-approach","title":"Hybrid Approach","text":"<p>For best of both worlds, use this pattern:</p> <pre><code># Development: Use Direct API with progress bars\nif __name__ == \"__main__\":\n    # Interactive development with monitoring\n    samples = sample_with_progress_bars(...)\n\n# Production: Switch to functional API\ndef production_inference(data):\n    # Fast, JIT-compiled sampling\n    return hmc_sampling(...)\n</code></pre>"},{"location":"examples/sampling/blackjax-integration-examples/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li> <p>Compare timing: Measure Direct API vs Functional API performance on same problem</p> </li> <li> <p>Memory profiling: Test NUTS with different <code>max_depth</code> values and monitor memory usage</p> </li> <li> <p>Multimodal exploration: Visualize how different samplers explore the mixture distribution</p> </li> <li> <p>Scaling experiment: Test both APIs with increasing problem dimensions (2D, 10D, 50D, 100D)</p> </li> <li> <p>Thinning effects: Experiment with <code>thinning</code> parameter in functional API to reduce autocorrelation</p> </li> <li> <p>Acceptance rates: Track acceptance rates in Direct API examples to optimize step sizes</p> </li> </ol>"},{"location":"examples/sampling/blackjax-integration-examples/#next-steps","title":"Next Steps","text":""},{"location":"examples/sampling/blackjax-integration-examples/#related-examples","title":"Related Examples","text":"<ul> <li> <p>BlackJAX Integration Example</p> <p>Start with the basics of BlackJAX integration</p> <p>blackjax-example.md</p> </li> <li> <p>BlackJAX Sampling Examples</p> <p>Compare different sampling algorithms</p> <p>blackjax-sampling-examples.md</p> </li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#further-learning","title":"Further Learning","text":"<ul> <li>BlackJAX Documentation</li> <li>BlackJAX Sampling Book</li> <li>JAX Documentation on Scan</li> <li>Artifex Distributions Module Documentation</li> <li>Artifex Sampling Module Documentation</li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/sampling/blackjax-integration-examples/#papers","title":"Papers","text":"<ol> <li> <p>HMC Performance: Betancourt, M. (2017). \"A Conceptual Introduction to Hamiltonian Monte Carlo\"</p> </li> <li> <p>NUTS Algorithm: Hoffman, M. D., &amp; Gelman, A. (2014). \"The No-U-Turn Sampler\"</p> </li> <li> <p>JAX for MCMC: Lao, J., et al. (2020). \"tfp.mcmc: Modern Markov Chain Monte Carlo Tools Built for Modern Hardware\"</p> </li> </ol>"},{"location":"examples/sampling/blackjax-integration-examples/#code-references","title":"Code References","text":"<ul> <li>Distribution classes: <code>artifex.generative_models.core.distributions</code></li> <li>Functional samplers: <code>artifex.generative_models.core.sampling.blackjax_samplers</code></li> <li>Direct BlackJAX API: <code>blackjax.hmc</code>, <code>blackjax.nuts</code>, <code>blackjax.mala</code></li> <li>JAX primitives: <code>jax.lax.scan</code>, <code>jax.lax.fori_loop</code></li> </ul>"},{"location":"examples/sampling/blackjax-integration-examples/#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check that BlackJAX is installed: <code>pip install blackjax</code></li> <li>Verify JAX GPU/CPU setup is correct</li> <li>For memory errors, reduce <code>max_depth</code> in NUTS or problem dimension</li> <li>For slow performance, ensure step functions are JIT-compiled</li> <li>Check progress bar behavior matches API expectations</li> <li>Consult Artifex documentation or open an issue</li> </ol> <p>Tags: #mcmc #blackjax #hmc #nuts #mala #integration #advanced #performance</p> <p>Difficulty: Advanced</p> <p>Estimated Time: 25-35 minutes</p>"},{"location":"examples/sampling/blackjax-sampling-examples/","title":"BlackJAX Sampling Examples","text":""},{"location":"examples/sampling/blackjax-sampling-examples/#overview","title":"Overview","text":"<p>This example provides a comprehensive exploration of BlackJAX samplers integrated with Artifex's distribution framework. It compares four different approaches to MCMC sampling: Artifex's HMC wrapper, Artifex's MALA wrapper, Artifex's NUTS wrapper, and direct BlackJAX API usage.</p>"},{"location":"examples/sampling/blackjax-sampling-examples/#files","title":"Files","text":"<ul> <li>Python script: <code>examples/generative_models/sampling/blackjax_sampling_examples.py</code></li> <li>Jupyter notebook: <code>examples/generative_models/sampling/blackjax_sampling_examples.ipynb</code></li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#quick-start","title":"Quick Start","text":"Python ScriptJupyter Notebook <pre><code># Run the complete example\npython examples/generative_models/sampling/blackjax_sampling_examples.py\n</code></pre> <pre><code># Launch Jupyter and open the notebook\njupyter notebook examples/generative_models/sampling/blackjax_sampling_examples.ipynb\n</code></pre>"},{"location":"examples/sampling/blackjax-sampling-examples/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this example, you will:</p> <ul> <li> Understand different MCMC sampling algorithms (HMC, MALA, NUTS)</li> <li> Learn to use Artifex's BlackJAX integration API</li> <li> Compare Artifex's sampler wrappers with direct BlackJAX usage</li> <li> Apply MCMC sampling to mixture distributions</li> <li> Visualize and interpret sampling results</li> <li> Handle memory constraints in NUTS sampling</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of MCMC sampling fundamentals</li> <li>Familiarity with probability distributions</li> <li>Basic knowledge of Hamiltonian Monte Carlo</li> <li>Completion of BlackJAX Integration Example</li> <li>Artifex core sampling module</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#mcmc-algorithms-overview","title":"MCMC Algorithms Overview","text":"<p>This example demonstrates three state-of-the-art MCMC algorithms from the BlackJAX library.</p>"},{"location":"examples/sampling/blackjax-sampling-examples/#hamiltonian-monte-carlo-hmc","title":"Hamiltonian Monte Carlo (HMC)","text":"<p>HMC uses gradient information to propose efficient moves in parameter space by simulating Hamiltonian dynamics.</p> <p>Key Characteristics:</p> <ul> <li>Uses gradient information for exploration</li> <li>Requires tuning of step size and integration steps</li> <li>Excellent for smooth, continuous distributions</li> <li>Higher computational cost per iteration than MH</li> </ul> <p>Mathematical Formulation:</p> <p>The Hamiltonian system is defined as: $$ H(q, p) = U(q) + K(p) $$</p> <p>where:</p> <ul> <li>\\(U(q) = -\\log p(q)\\) is the potential energy</li> <li>\\(K(p) = \\frac{1}{2}p^T M^{-1} p\\) is the kinetic energy</li> <li>\\(M\\) is the mass matrix</li> </ul> <p>The leapfrog integrator updates positions and momenta:</p> \\[ \\begin{align} p_{i+\\frac{1}{2}} &amp;= p_i - \\frac{\\epsilon}{2} \\nabla U(q_i) \\\\ q_{i+1} &amp;= q_i + \\epsilon M^{-1} p_{i+\\frac{1}{2}} \\\\ p_{i+1} &amp;= p_{i+\\frac{1}{2}} - \\frac{\\epsilon}{2} \\nabla U(q_{i+1}) \\end{align} \\]"},{"location":"examples/sampling/blackjax-sampling-examples/#metropolis-adjusted-langevin-algorithm-mala","title":"Metropolis-Adjusted Langevin Algorithm (MALA)","text":"<p>MALA is a gradient-based Metropolis method that uses Langevin dynamics for proposals.</p> <p>Key Characteristics:</p> <ul> <li>Single step per iteration (faster than HMC)</li> <li>Gradient-based proposals for efficiency</li> <li>Good for smooth posteriors with strong gradients</li> <li>Lower acceptance rate than HMC typically</li> </ul> <p>Mathematical Formulation:</p> <p>The proposal distribution is:</p> \\[ \\theta' = \\theta + \\frac{\\epsilon^2}{2} \\nabla \\log p(\\theta) + \\epsilon \\eta \\] <p>where \\(\\eta \\sim \\mathcal{N}(0, I)\\) is Gaussian noise.</p> <p>The acceptance probability follows the Metropolis-Hastings rule:</p> \\[ \\alpha(\\theta' | \\theta) = \\min\\left(1, \\frac{p(\\theta') q(\\theta | \\theta')}{p(\\theta) q(\\theta' | \\theta)}\\right) \\]"},{"location":"examples/sampling/blackjax-sampling-examples/#no-u-turn-sampler-nuts","title":"No-U-Turn Sampler (NUTS)","text":"<p>NUTS automatically tunes the HMC trajectory length by building a tree of states until the trajectory makes a \"U-turn\".</p> <p>Key Characteristics:</p> <ul> <li>No manual tuning of integration steps needed</li> <li>Adaptive step size selection</li> <li>State-of-the-art for Bayesian inference</li> <li>Higher memory usage due to trajectory storage</li> <li>Excellent for complex, high-dimensional posteriors</li> </ul> <p>Algorithm Overview:</p> <p>NUTS builds a balanced binary tree of trajectory states by recursively doubling until:</p> <ol> <li>The trajectory makes a U-turn (forward/backward directions oppose)</li> <li>Maximum tree depth is reached (<code>max_num_doublings</code>)</li> </ol> <p>The U-turn criterion is:</p> \\[ (\\theta^+ - \\theta^-) \\cdot p^- &lt; 0 \\quad \\text{or} \\quad (\\theta^+ - \\theta^-) \\cdot p^+ &lt; 0 \\] <p>where \\(\\theta^+, p^+\\) are the forward endpoint and \\(\\theta^-, p^-\\) are the backward endpoint.</p>"},{"location":"examples/sampling/blackjax-sampling-examples/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/sampling/blackjax-sampling-examples/#example-1-artifex-hmc-sampling","title":"Example 1: Artifex HMC Sampling","text":"<p>This example uses Artifex's HMC wrapper to sample from a bimodal mixture of Gaussians:</p> <pre><code># Create a 2D mixture of Gaussians\ndef create_mixture_logprob():\n    mean1 = jnp.array([3.0, 3.0])\n    mean2 = jnp.array([-3.0, -3.0])\n\n    def log_prob_fn(x):\n        dist1 = Normal(loc=mean1, scale=jnp.array([1.0, 1.0]))\n        dist2 = Normal(loc=mean2, scale=jnp.array([1.0, 1.0]))\n\n        log_prob1 = jnp.sum(dist1.log_prob(x))\n        log_prob2 = jnp.sum(dist2.log_prob(x))\n\n        # Equal mixture weights\n        return jnp.logaddexp(log_prob1, log_prob2) - jnp.log(2.0)\n\n    return log_prob_fn\n\n# Sample using Artifex's HMC wrapper\nmixture_logprob = create_mixture_logprob()\ninit_state = jnp.zeros(2)\n\nhmc_samples = hmc_sampling(\n    mixture_logprob,\n    init_state,\n    key,\n    n_samples=1000,\n    n_burnin=500,\n    step_size=0.1,\n    num_integration_steps=10,\n)\n</code></pre> <p>Key Points:</p> <ul> <li>The mixture has two well-separated modes at [3, 3] and [-3, -3]</li> <li>HMC explores both modes efficiently using gradient information</li> <li>Artifex wrapper handles initialization and sampling loop</li> <li>Returns array of samples with shape <code>[n_samples, 2]</code></li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#example-2-artifex-mala-sampling","title":"Example 2: Artifex MALA Sampling","text":"<p>This example demonstrates MALA on the same bimodal distribution:</p> <pre><code>mala_samples = mala_sampling(\n    mixture_logprob,\n    init_state,\n    key,\n    n_samples=1000,\n    n_burnin=500,\n    step_size=0.05,  # Smaller step size than HMC\n)\n</code></pre> <p>Key Points:</p> <ul> <li>MALA uses smaller step sizes than HMC (typically 0.05 vs 0.1)</li> <li>Single Langevin step per iteration makes it faster per sample</li> <li>May need more samples to achieve same effective sample size as HMC</li> <li>Good for problems where gradient evaluation is cheap</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#example-3-artifex-nuts-sampling","title":"Example 3: Artifex NUTS Sampling","text":"<p>NUTS automatically tunes trajectory length, eliminating manual tuning:</p> <pre><code># Use simpler distribution to avoid memory issues\nsimple_logprob = create_normal_logprob()\n\nnuts_samples = nuts_sampling(\n    simple_logprob,\n    init_state,\n    key,\n    n_samples=500,  # Fewer samples due to memory\n    n_burnin=200,\n    step_size=0.8,\n    max_num_doublings=5,  # Control memory usage\n)\n</code></pre> <p>Key Points:</p> <ul> <li>NUTS is memory-intensive due to trajectory tree storage</li> <li>Use <code>max_num_doublings</code> to control memory usage (default: 10)</li> <li>Excellent for complex posteriors where tuning is difficult</li> <li>This example uses a simpler distribution to demonstrate the API</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#example-4-direct-blackjax-hmc","title":"Example 4: Direct BlackJAX HMC","text":"<p>This example shows how to use BlackJAX's API directly without Artifex wrappers:</p> <pre><code>import blackjax\n\n# Initialize the HMC algorithm\ninverse_mass_matrix = jnp.eye(2)\nhmc = blackjax.hmc(\n    mixture_logprob,\n    step_size=0.1,\n    inverse_mass_matrix=inverse_mass_matrix,\n    num_integration_steps=10,\n)\n\n# Initialize sampling state\ninitial_state = hmc.init(init_state)\n\n# Define one step function\n@nnx.jit\ndef one_step(state, key):\n    state, _ = hmc.step(key, state)\n    return state, state\n\n# Burn-in phase\nstate = initial_state\nfor _ in range(n_burnin):\n    key, subkey = jax.random.split(key)\n    state, _ = one_step(state, subkey)\n\n# Collect samples\nkey, subkey = jax.random.split(key)\nstate, samples = jax.lax.scan(\n    one_step,\n    state,\n    jax.random.split(subkey, n_samples)\n)\nsamples = samples.position\n</code></pre> <p>Key Points:</p> <ul> <li>Direct API provides fine-grained control over sampling</li> <li>Must manually manage state and random keys</li> <li>Use <code>jax.lax.scan</code> for efficient sample collection</li> <li>JIT compilation improves performance significantly</li> <li>Useful when implementing custom sampling logic</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#expected-output","title":"Expected Output","text":""},{"location":"examples/sampling/blackjax-sampling-examples/#sample-plots","title":"Sample Plots","text":"<p>Each example generates a scatter plot showing the samples in 2D space:</p> <ul> <li>HMC samples: Should show clear exploration of both modes</li> <li>MALA samples: Similar coverage but potentially more concentrated</li> <li>NUTS samples: For the normal distribution, centered at origin</li> <li>Direct API samples: Should match Artifex HMC results</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#statistics","title":"Statistics","text":"<p>Each example prints sample statistics:</p> <pre><code>Sample Statistics:\nMean: [ 0.1234 -0.2345]\nStd: [2.9876  2.8765]\n</code></pre> <p>For the bimodal mixture, expect:</p> <ul> <li>Mean near [0, 0] (average of two modes)</li> <li>Large standard deviation (reflecting mode separation)</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#performance-comparison","title":"Performance Comparison","text":""},{"location":"examples/sampling/blackjax-sampling-examples/#computational-cost","title":"Computational Cost","text":"Method Time per Sample ESS per Sample Memory Usage Tuning Required HMC Medium High Low Yes (step size, steps) MALA Low Medium Low Yes (step size) NUTS High Very High High Minimal (auto-tuning) Direct API Medium High Low Yes (same as HMC)"},{"location":"examples/sampling/blackjax-sampling-examples/#when-to-use-each-method","title":"When to Use Each Method","text":"<p>Use HMC when:</p> <ul> <li>You have smooth, continuous target distributions</li> <li>You can afford moderate computational cost</li> <li>You want efficient exploration with gradients</li> </ul> <p>Use MALA when:</p> <ul> <li>Gradient evaluation is cheap</li> <li>You need many samples quickly</li> <li>Target has strong gradients</li> </ul> <p>Use NUTS when:</p> <ul> <li>You have complex, high-dimensional posteriors</li> <li>You can afford higher memory usage</li> <li>You want to avoid manual tuning</li> <li>You need robust inference</li> </ul> <p>Use Direct API when:</p> <ul> <li>You need custom sampling logic</li> <li>You want fine-grained control</li> <li>You're implementing advanced algorithms</li> <li>Artifex wrappers don't fit your use case</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#tuning-guidelines","title":"Tuning Guidelines","text":""},{"location":"examples/sampling/blackjax-sampling-examples/#hmc-tuning","title":"HMC Tuning","text":"<p>Step Size (<code>step_size</code>):</p> <ul> <li>Start with 0.1</li> <li>Target acceptance rate: 0.6-0.8</li> <li>Too large: low acceptance rate</li> <li>Too small: slow mixing</li> </ul> <p>Integration Steps (<code>num_integration_steps</code>):</p> <ul> <li>Start with 10</li> <li>Increase for better exploration</li> <li>Higher values increase cost per sample</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#mala-tuning","title":"MALA Tuning","text":"<p>Step Size (<code>step_size</code>):</p> <ul> <li>Start with 0.05 (smaller than HMC)</li> <li>Target acceptance rate: 0.5-0.7</li> <li>Adjust based on acceptance diagnostics</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#nuts-tuning","title":"NUTS Tuning","text":"<p>Step Size (<code>step_size</code>):</p> <ul> <li>Often auto-tuned during warmup</li> <li>Can set manually if needed</li> <li>Usually between 0.1-1.0</li> </ul> <p>Max Doublings (<code>max_num_doublings</code>):</p> <ul> <li>Controls trajectory length and memory</li> <li>Default: 10 (max trajectory length = 2^10 = 1024)</li> <li>Reduce if encountering memory errors</li> <li>Values 5-7 often sufficient</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li> <p>Compare mixing: Plot trace plots and autocorrelation for each sampler to assess mixing quality</p> </li> <li> <p>Tune hyperparameters: Systematically vary step sizes and integration steps, tracking acceptance rates and ESS</p> </li> <li> <p>Higher dimensions: Extend the mixture to 10D or 20D to see how samplers scale</p> </li> <li> <p>Different targets: Try non-Gaussian distributions like:</p> </li> <li>Rosenbrock's banana-shaped distribution</li> <li>Neal's funnel distribution</li> <li> <p>Mixture of many components</p> </li> <li> <p>Effective sample size: Compute ESS using <code>arviz</code> or similar tools to measure sampling efficiency</p> </li> <li> <p>Warmup strategies: Experiment with different warmup lengths and adaptive schemes</p> </li> <li> <p>Parallel chains: Run multiple chains and assess convergence using R-hat</p> </li> </ol>"},{"location":"examples/sampling/blackjax-sampling-examples/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/sampling/blackjax-sampling-examples/#low-acceptance-rate","title":"Low Acceptance Rate","text":"<p>Symptom: Acceptance rate below 0.5</p> <p>Solution:</p> <ul> <li>Reduce <code>step_size</code> by factor of 2</li> <li>Check gradient computation (no NaNs)</li> <li>Verify log probability is correct</li> <li>Try simpler test distribution first</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#poor-mixing","title":"Poor Mixing","text":"<p>Symptom: Samples stuck in one mode of multimodal distribution</p> <p>Solution:</p> <ul> <li>Increase burn-in period (try 2x-5x current)</li> <li>Try different initialization points</li> <li>Consider parallel tempering for multimodal targets</li> <li>Increase <code>num_integration_steps</code> for HMC</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#nuts-memory-errors","title":"NUTS Memory Errors","text":"<p>Symptom: Out of memory errors with NUTS</p> <p>Solution:</p> <pre><code># Reduce memory usage\nnuts_samples = nuts_sampling(\n    log_prob_fn,\n    init_state,\n    key,\n    n_samples=500,  # Reduce sample count\n    n_burnin=200,\n    max_num_doublings=5,  # Lower from default 10\n)\n</code></pre>"},{"location":"examples/sampling/blackjax-sampling-examples/#divergent-transitions-nuts","title":"Divergent Transitions (NUTS)","text":"<p>Symptom: Warning about divergent transitions</p> <p>Solution:</p> <ul> <li>Decrease <code>step_size</code> (try 0.5x current)</li> <li>Reparameterize the model (e.g., non-centered parameterization)</li> <li>Check for prior-likelihood conflicts</li> <li>Increase warmup period</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#slow-performance","title":"Slow Performance","text":"<p>Symptom: Sampling taking too long</p> <p>Solution:</p> <ul> <li>Ensure JIT compilation is used (<code>@nnx.jit</code> or <code>@jax.jit</code>)</li> <li>Check if GPU is available and being used</li> <li>Use Direct API with <code>jax.lax.scan</code> for efficient loops</li> <li>Reduce sample count for testing</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#next-steps","title":"Next Steps","text":""},{"location":"examples/sampling/blackjax-sampling-examples/#related-examples","title":"Related Examples","text":"<ul> <li> <p>BlackJAX Integration Example</p> <p>Learn the basics of BlackJAX with Artifex</p> <p>blackjax-example.md</p> </li> <li> <p>BlackJAX Integration Examples</p> <p>Advanced integration patterns and production use cases</p> <p>blackjax-integration-examples.md</p> </li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#further-learning","title":"Further Learning","text":"<ul> <li>BlackJAX Documentation</li> <li>BlackJAX Sampling Book</li> <li>MCMC Diagnostics Guide</li> <li>HMC Tutorial by Betancourt</li> <li>NUTS Paper</li> <li>Artifex Sampling Module Documentation</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/sampling/blackjax-sampling-examples/#papers","title":"Papers","text":"<ol> <li> <p>HMC: Neal, R. M. (2011). \"MCMC using Hamiltonian dynamics\". Handbook of Markov Chain Monte Carlo.</p> </li> <li> <p>NUTS: Hoffman, M. D., &amp; Gelman, A. (2014). \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo\". Journal of Machine Learning Research.</p> </li> <li> <p>MALA: Roberts, G. O., &amp; Tweedie, R. L. (1996). \"Exponential convergence of Langevin distributions and their discrete approximations\". Bernoulli.</p> </li> <li> <p>Convergence Diagnostics: Vehtari, A., et al. (2021). \"Rank-Normalization, Folding, and Localization: An Improved R-hat for Assessing Convergence of MCMC\". Bayesian Analysis.</p> </li> </ol>"},{"location":"examples/sampling/blackjax-sampling-examples/#code-references","title":"Code References","text":"<ul> <li>Distribution creation: <code>artifex.generative_models.core.distributions</code></li> <li>Sampling functions: <code>artifex.generative_models.core.sampling</code></li> <li>BlackJAX wrappers: <code>artifex.generative_models.core.sampling.blackjax_samplers</code></li> <li>Direct BlackJAX API: <code>blackjax.hmc</code>, <code>blackjax.nuts</code>, <code>blackjax.mala</code></li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#diagnostic-tools","title":"Diagnostic Tools","text":"<ul> <li>ArviZ: Python package for MCMC diagnostics and visualization</li> <li>PyStan: Stan interface with excellent diagnostics</li> <li>PyMC: Bayesian modeling with built-in diagnostics</li> </ul>"},{"location":"examples/sampling/blackjax-sampling-examples/#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check that BlackJAX is installed: <code>pip install blackjax</code></li> <li>Verify JAX GPU/CPU setup is correct</li> <li>Review error messages for parameter constraints</li> <li>Check BlackJAX documentation for API changes</li> <li>Consult Artifex documentation or open an issue</li> </ol> <p>Tags: #mcmc #blackjax #hmc #nuts #mala #sampling #comparison #advanced</p> <p>Difficulty: Advanced</p> <p>Estimated Time: 20-30 minutes</p>"},{"location":"examples/text/simple-text-generation/","title":"Simple Text Generation with Character-Level Models","text":"Beginner Runtime: ~5min \ud83d\udcd3 Dual Format"},{"location":"examples/text/simple-text-generation/#files","title":"Files","text":"<ul> <li>Python Script: <code>simple_text_generation.py</code></li> <li>Jupyter Notebook: <code>simple_text_generation.ipynb</code></li> </ul>"},{"location":"examples/text/simple-text-generation/#quick-start","title":"Quick Start","text":"<pre><code># Run the Python script\nuv run python examples/generative_models/text/simple_text_generation.py\n\n# Or open the Jupyter notebook\njupyter lab examples/generative_models/text/simple_text_generation.ipynb\n</code></pre>"},{"location":"examples/text/simple-text-generation/#overview","title":"Overview","text":"<p>This example demonstrates fundamental text generation using character-level language modeling. Learn how to build a simple recurrent text generator that processes sequences one character at a time, implementing the basic building blocks of more sophisticated language models.</p>"},{"location":"examples/text/simple-text-generation/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this example, you will understand:</p> <ul> <li> Character-level text generation fundamentals</li> <li> How to implement embedding layers for character representations</li> <li> Building recurrent-style networks with sequential processing</li> <li> Temperature-based sampling for generation diversity</li> <li> Handling variable-length sequence generation</li> </ul>"},{"location":"examples/text/simple-text-generation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of neural networks</li> <li>Familiarity with sequence modeling concepts</li> <li>Knowledge of JAX/Flax NNX module patterns</li> <li>Understanding of text tokenization basics</li> </ul>"},{"location":"examples/text/simple-text-generation/#theory","title":"Theory","text":""},{"location":"examples/text/simple-text-generation/#character-level-language-modeling","title":"Character-Level Language Modeling","text":"<p>Character-level language models process text one character at a time, making them simpler but often less powerful than word-level or subword models. However, they offer several advantages:</p> <ol> <li>Small Vocabulary: Only need to represent ~128 ASCII characters</li> <li>No Unknown Tokens: Can process any text without special handling</li> <li>Morphology Learning: Can learn word formation patterns</li> <li>Simplicity: Easy to implement and understand</li> </ol> <p>The model learns the conditional probability distribution:</p> \\[P(x_t | x_{&lt;t}) = \\text{softmax}(f_\\theta(x_{&lt;t}))\\] <p>where \\(x_t\\) is the character at position \\(t\\) and \\(f_\\theta\\) is the neural network that encodes the context \\(x_{&lt;t}\\).</p>"},{"location":"examples/text/simple-text-generation/#model-architecture","title":"Model Architecture","text":"<p>The <code>SimpleTextGenerator</code> consists of three main components:</p> <ol> <li>Embedding Layer: Maps discrete character IDs to continuous vector representations</li> <li>Input: Character IDs \\(\\in \\{0, ..., 127\\}\\)</li> <li> <p>Output: Dense vectors \\(\\in \\mathbb{R}^{d}\\)</p> </li> <li> <p>Recurrent Network: Processes sequences position-by-position</p> </li> <li>Uses a simple feedforward architecture for demonstration</li> <li>In practice, LSTM/GRU layers would be more effective</li> <li> <p>Maintains hidden states across sequence positions</p> </li> <li> <p>Output Projection: Maps hidden states to vocabulary logits</p> </li> <li>Produces probability distribution over next characters</li> <li>Uses softmax activation for normalization</li> </ol>"},{"location":"examples/text/simple-text-generation/#temperature-sampling","title":"Temperature Sampling","text":"<p>Temperature is a hyperparameter that controls generation diversity:</p> \\[P_T(x_t) = \\frac{\\exp(z_t / T)}{\\sum_i \\exp(z_i / T)}\\] <p>where \\(z_t\\) are the logits and \\(T\\) is the temperature.</p> <ul> <li>Low temperature (\\(T &lt; 1\\)): Sharpens distribution, more deterministic</li> <li>Temperature = 1: Standard softmax distribution</li> <li>High temperature (\\(T &gt; 1\\)): Flattens distribution, more random</li> </ul>"},{"location":"examples/text/simple-text-generation/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"examples/text/simple-text-generation/#1-model-definition","title":"1. Model Definition","text":"<p>The <code>SimpleTextGenerator</code> class implements the core architecture:</p> <pre><code>class SimpleTextGenerator(nnx.Module):\n    \"\"\"Simple character-level text generator.\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int = 128,\n        embed_dim: int = 64,\n        hidden_dim: int = 128,\n        seq_length: int = 32,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        # Embedding layer maps character IDs to vectors\n        self.embedding = nnx.Embed(\n            num_embeddings=vocab_size,\n            features=embed_dim,\n            rngs=rngs\n        )\n\n        # Simple RNN-like network\n        self.rnn = nnx.Sequential(\n            nnx.Linear(embed_dim, hidden_dim, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(hidden_dim, vocab_size, rngs=rngs),\n        )\n\n        # Output projection\n        self.output_layer = nnx.Linear(\n            vocab_size, vocab_size, rngs=rngs\n        )\n</code></pre> <p>Key implementation details:</p> <ul> <li>Uses <code>nnx.Embed</code> for efficient character embedding lookup</li> <li><code>nnx.Sequential</code> chains layers for compact definition</li> <li>ReLU activations introduce non-linearity</li> <li>All layers require <code>rngs</code> parameter for initialization</li> </ul>"},{"location":"examples/text/simple-text-generation/#2-forward-pass","title":"2. Forward Pass","text":"<p>The forward pass processes input sequences:</p> <pre><code>def __call__(self, input_ids):\n    # Embed input tokens\n    x = self.embedding(input_ids)  # [batch, seq_len, embed_dim]\n\n    # Process through RNN-like network\n    batch_size, seq_len, _ = x.shape\n    outputs = []\n\n    for i in range(seq_len):\n        # Process each position\n        h = self.rnn(x[:, i, :])  # [batch, vocab_size]\n        outputs.append(h)\n\n    # Stack outputs\n    logits = jnp.stack(outputs, axis=1)  # [batch, seq_len, vocab_size]\n\n    # Apply output layer\n    logits = self.output_layer(logits)\n\n    return logits\n</code></pre> <p>This implementation:</p> <ul> <li>Processes each position independently (simplified RNN)</li> <li>Accumulates outputs across sequence length</li> <li>Returns logits for next-character prediction at each position</li> </ul>"},{"location":"examples/text/simple-text-generation/#3-text-generation","title":"3. Text Generation","text":"<p>The generation method implements autoregressive sampling:</p> <pre><code>def generate(\n    self,\n    prompt: str = \"\",\n    max_length: int = 100,\n    temperature: float = 1.0,\n    *,\n    rngs: nnx.Rngs\n):\n    # Convert prompt to token IDs\n    if prompt:\n        input_ids = jnp.array([ord(c) % self.vocab_size for c in prompt])\n    else:\n        # Start with random character\n        key = rngs.sample()\n        input_ids = jax.random.randint(key, (1,), 0, self.vocab_size)\n\n    generated = list(input_ids)\n\n    for _ in range(max_length - len(generated)):\n        # Prepare input (pad or truncate to seq_length)\n        current_seq = jnp.array(generated[-self.seq_length:])\n        if len(current_seq) &lt; self.seq_length:\n            padding = jnp.zeros(\n                self.seq_length - len(current_seq),\n                dtype=jnp.int32\n            )\n            current_seq = jnp.concatenate([padding, current_seq])\n\n        # Get predictions\n        logits = self(current_seq[None, :])\n\n        # Sample next token with temperature\n        next_logits = logits[0, -1, :] / temperature\n        key = rngs.sample()\n        next_token = jax.random.categorical(key, next_logits)\n\n        generated.append(int(next_token))\n\n    # Convert back to text\n    text = \"\".join([chr(t % 128) for t in generated])\n    return text\n</code></pre> <p>Key features:</p> <ul> <li>Handles variable-length prompts with padding</li> <li>Uses sliding window of last <code>seq_length</code> characters</li> <li>Temperature scaling before sampling</li> <li>Autoregressive generation (feeds outputs back as inputs)</li> </ul>"},{"location":"examples/text/simple-text-generation/#4-training-data-creation","title":"4. Training Data Creation","text":"<p>Creates simple patterns for demonstration:</p> <pre><code>def create_training_data():\n    patterns = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"Hello world! This is a text generation example.\",\n        \"JAX and Flax make neural networks easy.\",\n        \"Machine learning with Python is fun.\",\n        \"Generative models can create text.\",\n    ]\n\n    # Repeat patterns to create more data\n    text = \" \".join(patterns * 10)\n\n    # Convert to token IDs (simple ASCII encoding)\n    token_ids = jnp.array([ord(c) % 128 for c in text])\n\n    return text, token_ids\n</code></pre>"},{"location":"examples/text/simple-text-generation/#5-demonstration","title":"5. Demonstration","text":"<p>The main demonstration shows:</p> <ol> <li>Model Creation: Initialize with RNG handling</li> <li>Forward Pass Testing: Batch processing of sequences</li> <li>Temperature Sampling: Generate with different temperatures</li> <li>Batch Processing: Handle multiple prompts efficiently</li> </ol>"},{"location":"examples/text/simple-text-generation/#experiments-to-try","title":"Experiments to Try","text":""},{"location":"examples/text/simple-text-generation/#1-architecture-modifications","title":"1. Architecture Modifications","text":"<p>Increase Model Capacity:</p> <pre><code>generator = SimpleTextGenerator(\n    vocab_size=128,\n    embed_dim=128,      # Increased from 64\n    hidden_dim=256,     # Increased from 128\n    seq_length=64,      # Increased from 32\n    rngs=rngs\n)\n</code></pre> <p>Add More Layers:</p> <ul> <li>Stack multiple RNN layers</li> <li>Implement LSTM or GRU cells</li> <li>Add residual connections</li> </ul>"},{"location":"examples/text/simple-text-generation/#2-training-improvements","title":"2. Training Improvements","text":"<p>Implement Proper Training:</p> <pre><code># Add optimizer (wrt=nnx.Param required in NNX 0.11.0+)\noptimizer = nnx.Optimizer(generator, optax.adam(1e-3), wrt=nnx.Param)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in data_loader:\n        def loss_fn(generator):\n            # Forward pass\n            logits = generator(batch['input_ids'])\n\n            # Compute loss\n            return optax.softmax_cross_entropy_with_integer_labels(\n                logits[:, :-1, :],\n                batch['target_ids'][:, 1:]\n            ).mean()\n\n        # Compute gradients and update\n        loss, grads = nnx.value_and_grad(loss_fn)(generator)\n        optimizer.update(generator, grads)\n</code></pre> <p>Data Augmentation:</p> <ul> <li>Use real text corpora (Wikipedia, books)</li> <li>Implement dynamic sequence length</li> <li>Add noise for robustness</li> </ul>"},{"location":"examples/text/simple-text-generation/#3-generation-techniques","title":"3. Generation Techniques","text":"<p>Top-k Sampling:</p> <pre><code>def top_k_sample(logits, k=5):\n    top_k_logits, top_k_indices = jax.lax.top_k(logits, k)\n    probs = jax.nn.softmax(top_k_logits)\n    sampled_idx = jax.random.categorical(key, jnp.log(probs))\n    return top_k_indices[sampled_idx]\n</code></pre> <p>Nucleus (Top-p) Sampling:</p> <pre><code>def nucleus_sample(logits, p=0.9):\n    probs = jax.nn.softmax(logits)\n    sorted_probs = jnp.sort(probs)[::-1]\n    cumsum = jnp.cumsum(sorted_probs)\n    cutoff = sorted_probs[jnp.searchsorted(cumsum, p)]\n    # Sample from probs &gt;= cutoff\n</code></pre> <p>Beam Search:</p> <ul> <li>Maintain top-k hypotheses</li> <li>Score based on cumulative probability</li> <li>Return highest-scoring sequence</li> </ul>"},{"location":"examples/text/simple-text-generation/#4-advanced-features","title":"4. Advanced Features","text":"<p>Conditional Generation:</p> <pre><code>class ConditionalTextGenerator(nnx.Module):\n    def __init__(self, num_conditions, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.condition_embed = nnx.Embed(\n            num_embeddings=num_conditions,\n            features=self.hidden_dim,\n            rngs=kwargs['rngs']\n        )\n\n    def __call__(self, input_ids, condition):\n        x = self.embedding(input_ids)\n        c = self.condition_embed(condition)\n        # Combine input and condition\n        ...\n</code></pre> <p>Byte-Pair Encoding:</p> <ul> <li>Use subword tokenization for better vocabulary</li> <li>Implement BPE/WordPiece tokenizer</li> <li>Balance vocabulary size and coverage</li> </ul>"},{"location":"examples/text/simple-text-generation/#next-steps","title":"Next Steps","text":"<ul> <li> <p>:material-transformer: Transformer Models</p> <p>Explore modern attention-based architectures that revolutionized NLP</p> <p> Transformer Examples</p> </li> <li> <p> Text Compression</p> <p>Learn information-theoretic approaches to text modeling</p> <p> Compression Examples</p> </li> <li> <p> Sequence-to-Sequence</p> <p>Build models for translation, summarization, and other seq2seq tasks</p> <p> Seq2Seq Examples</p> </li> <li> <p> Multimodal Models</p> <p>Combine text with images for richer representations</p> <p> Multimodal Examples</p> </li> </ul>"},{"location":"examples/text/simple-text-generation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/text/simple-text-generation/#common-issues","title":"Common Issues","text":"<p>Out of Memory:</p> <ul> <li>Reduce <code>seq_length</code> or <code>batch_size</code></li> <li>Use gradient accumulation</li> <li>Enable mixed precision training</li> </ul> <p>Poor Generation Quality:</p> <ul> <li>Increase training data size</li> <li>Add more model capacity</li> <li>Tune temperature and sampling parameters</li> <li>Implement better training procedures</li> </ul> <p>Slow Generation:</p> <ul> <li>Use JIT compilation: <code>@jax.jit</code></li> <li>Implement caching for KV pairs</li> <li>Reduce sequence length</li> <li>Use beam search pruning</li> </ul>"},{"location":"examples/text/simple-text-generation/#performance-tips","title":"Performance Tips","text":"<ol> <li>JIT Compilation:</li> </ol> <pre><code>@jax.jit\ndef generate_step(model, input_seq, key):\n    logits = model(input_seq)\n    return jax.random.categorical(key, logits[0, -1, :])\n</code></pre> <ol> <li> <p>Batched Generation:</p> </li> <li> <p>Process multiple prompts in parallel</p> </li> <li>Use vectorized operations</li> <li> <p>Minimize Python loops</p> </li> <li> <p>Caching:</p> </li> <li> <p>Cache intermediate hidden states</p> </li> <li>Reuse computation from previous steps</li> <li>Implement incremental decoding</li> </ol>"},{"location":"examples/text/simple-text-generation/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/text/simple-text-generation/#documentation","title":"Documentation","text":"<ul> <li>Flax NNX Guide</li> <li>JAX Random Numbers</li> <li>Language Modeling Tutorial</li> </ul>"},{"location":"examples/text/simple-text-generation/#research-papers","title":"Research Papers","text":"<ul> <li>Character-Aware Neural Language Models - Kim et al., 2015</li> <li>Generating Sequences With Recurrent Neural Networks - Graves, 2013</li> <li>The Curious Case of Neural Text Degeneration - Holtzman et al., 2019</li> </ul>"},{"location":"examples/text/simple-text-generation/#related-examples","title":"Related Examples","text":"<ul> <li>EBM Text Modeling: Energy-based approaches to language modeling</li> <li>Autoregressive Transformers: Self-attention for sequence modeling</li> <li>VAE for Text: Variational autoencoders for text generation</li> </ul> <p>Author: Artifex Team Last Updated: 2025-10-22 Difficulty: Beginner Time to Complete: ~30 minutes</p>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/","title":"Multi-\u03b2 VAE Controllable Generation Benchmark","text":"<p>Level: Intermediate | Runtime: ~2-3 minutes (CPU) / ~1 minute (GPU) | Format: Python + Jupyter</p> <p>Prerequisites: Understanding of VAEs and disentangled representations | Target Audience: Researchers in controllable generation and representation learning</p>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#overview","title":"Overview","text":"<p>This example demonstrates how to benchmark \u03b2-VAE models for controllable generation using disentanglement and image quality metrics. Learn how the \u03b2 parameter affects the trade-off between disentanglement and reconstruction quality, and how to systematically evaluate models using MIG, FID, LPIPS, and SSIM metrics.</p>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li> <p> \u03b2-VAE Framework</p> <p>Understand how \u03b2 controls disentanglement vs reconstruction trade-off</p> </li> <li> <p> MIG Score</p> <p>Measure disentanglement using Mutual Information Gap</p> </li> <li> <p> Image Quality Metrics</p> <p>Evaluate generation quality with FID, LPIPS, and SSIM</p> </li> <li> <p> Quality Trade-offs</p> <p>Balance disentanglement, reconstruction, and training time</p> </li> <li> <p> Model Comparison</p> <p>Systematically compare different model configurations</p> </li> <li> <p>:material-benchmark: Benchmark Suite</p> <p>Comprehensive evaluation across multiple metrics</p> </li> </ul>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#files","title":"Files","text":"<p>This example is available in two formats:</p> <ul> <li>Python Script: <code>multi_beta_vae_benchmark_demo.py</code></li> <li>Jupyter Notebook: <code>multi_beta_vae_benchmark_demo.ipynb</code></li> </ul>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#quick-start","title":"Quick Start","text":""},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#run-the-python-script","title":"Run the Python Script","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Run the benchmark demo\npython examples/generative_models/vae/multi_beta_vae_benchmark_demo.py\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#run-the-jupyter-notebook","title":"Run the Jupyter Notebook","text":"<pre><code># Activate environment\nsource activate.sh\n\n# Launch Jupyter\njupyter lab examples/generative_models/vae/multi_beta_vae_benchmark_demo.ipynb\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#1-vae-framework","title":"1. \u03b2-VAE Framework","text":"<p>\u03b2-VAE modifies the standard VAE loss by adding a weight \u03b2 to the KL divergence term:</p> \\[\\mathcal{L}_{\\beta\\text{-VAE}} = \\underbrace{\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{Reconstruction}} - \\underbrace{\\beta \\cdot \\text{KL}(q(z|x) \\| p(z))}_{\\text{Regularization}}\\] <p>\u03b2 Parameter Effects:</p> <ul> <li>\u03b2 = 1: Standard VAE</li> <li>\u03b2 &gt; 1: Encourages disentanglement, may reduce reconstruction quality</li> <li>\u03b2 &lt; 1: Prioritizes reconstruction, may reduce disentanglement</li> </ul> <pre><code># Different \u03b2 values for different goals\nbeta_configs = {\n    \"reconstruction_focused\": 0.5,   # Better reconstruction\n    \"balanced\": 1.0,                 # Standard VAE\n    \"disentanglement_focused\": 4.0,  # Better disentanglement\n}\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#2-disentanglement","title":"2. Disentanglement","text":"<p>Disentangled representations have independent latent dimensions that each capture a single factor of variation:</p> <pre><code>Disentangled Latent Space:\nz[0] \u2192 Controls rotation\nz[1] \u2192 Controls size\nz[2] \u2192 Controls color\n...\n\nEntangled Latent Space:\nz[0] \u2192 Affects rotation AND size\nz[1] \u2192 Affects size AND color\nz[2] \u2192 Affects rotation AND color\n</code></pre> <p>Why Disentanglement Matters:</p> <ul> <li>Better interpretability</li> <li>More controllable generation</li> <li>Improved generalization</li> <li>Easier downstream task learning</li> </ul>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#3-mig-score-mutual-information-gap","title":"3. MIG Score (Mutual Information Gap)","text":"<p>MIG measures how much each latent dimension encodes a single ground-truth factor:</p> \\[\\text{MIG} = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{I(z_j^{(k)}; v_k) - I(z_j^{(k-1)}; v_k)}{H(v_k)}\\] <p>where \\(I\\) is mutual information, \\(v_k\\) are ground-truth factors, and \\(j^{(k)}\\) is the latent dimension with highest MI for factor \\(k\\).</p> <pre><code>from artifex.benchmarks.metrics.disentanglement import MIGMetric\n\n# Compute MIG score\nmig_metric = MIGMetric(rngs=rngs)\nmig_score = mig_metric.compute(\n    latent_codes=z,          # (batch, latent_dim)\n    ground_truth_factors=factors  # (batch, num_factors)\n)\n# mig_score: 0-1 (higher is better)\n# &gt;0.3: Good disentanglement\n# &gt;0.5: Excellent disentanglement\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#4-fid-score-frechet-inception-distance","title":"4. FID Score (Fr\u00e9chet Inception Distance)","text":"<p>Measures distribution distance between real and generated images in feature space:</p> \\[\\text{FID} = \\|\\mu_r - \\mu_g\\|^2 + \\text{Tr}(\\Sigma_r + \\Sigma_g - 2\\sqrt{\\Sigma_r \\Sigma_g})\\] <pre><code>from artifex.benchmarks.metrics.image import FIDMetric\n\nfid_metric = FIDMetric(rngs=rngs)\nfid_score = fid_metric.compute(\n    real_images=real_imgs,\n    generated_images=gen_imgs\n)\n# fid_score: 0-\u221e (lower is better)\n# &lt;30: Excellent quality\n# &lt;50: Good quality\n# &lt;100: Acceptable quality\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#5-lpips-learned-perceptual-image-patch-similarity","title":"5. LPIPS (Learned Perceptual Image Patch Similarity)","text":"<p>Uses deep features to measure perceptual similarity:</p> <pre><code>from artifex.benchmarks.metrics.image import LPIPSMetric\n\nlpips_metric = LPIPSMetric(rngs=rngs)\nlpips_score = lpips_metric.compute(\n    images1=original_imgs,\n    images2=reconstructed_imgs\n)\n# lpips_score: 0-1 (lower is better)\n# &lt;0.1: Excellent perceptual quality\n# &lt;0.2: Good perceptual quality\n# &lt;0.3: Acceptable perceptual quality\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#6-ssim-structural-similarity-index","title":"6. SSIM (Structural Similarity Index)","text":"<p>Measures structural similarity between images:</p> \\[\\text{SSIM}(x, y) = \\frac{(2\\mu_x\\mu_y + c_1)(2\\sigma_{xy} + c_2)}{(\\mu_x^2 + \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)}\\] <pre><code>from artifex.benchmarks.metrics.image import SSIMMetric\n\nssim_metric = SSIMMetric(rngs=rngs)\nssim_score = ssim_metric.compute(\n    images1=original_imgs,\n    images2=reconstructed_imgs\n)\n# ssim_score: 0-1 (higher is better)\n# &gt;0.9: Excellent structural similarity\n# &gt;0.8: Good structural similarity\n# &gt;0.7: Acceptable structural similarity\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#7-multi-vae-benchmark-suite","title":"7. Multi-\u03b2 VAE Benchmark Suite","text":"<p>Comprehensive evaluation across all metrics:</p> <pre><code>from artifex.benchmarks.suites.multi_beta_vae_suite import MultiBetaVAEBenchmarkSuite\n\nsuite = MultiBetaVAEBenchmarkSuite(\n    dataset_config={\n        \"num_samples\": 100,\n        \"image_size\": 64,\n        \"include_attributes\": True,  # For disentanglement metrics\n    },\n    benchmark_config={\n        \"num_samples\": 50,\n        \"batch_size\": 10,\n    },\n    rngs=rngs\n)\n\n# Run evaluation\nresults = suite.run_all(model)\n# results = {\n#     \"multi_beta_vae_benchmark\": {\n#         \"mig_score\": 0.35,\n#         \"fid_score\": 42.3,\n#         \"lpips_score\": 0.18,\n#         \"ssim_score\": 0.84,\n#         \"training_time_per_epoch\": 7.5,\n#     }\n# }\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#code-structure","title":"Code Structure","text":"<p>The example consists of three main components:</p> <ol> <li>MockMultiBetaVAE - Simulates \u03b2-VAE with controllable quality levels</li> <li>Three quality modes: low, medium, high</li> <li>Demonstrates encode-decode-generate pipeline</li> <li> <p>Shows proper RNG handling patterns</p> </li> <li> <p>Benchmark Suite - Comprehensive evaluation system</p> </li> <li>Disentanglement metrics (MIG)</li> <li>Image quality metrics (FID, LPIPS, SSIM)</li> <li> <p>Training efficiency metrics</p> </li> <li> <p>Model Comparison - Systematic evaluation</p> </li> <li>Compare across quality levels</li> <li>Analyze trade-offs</li> <li>Performance targets</li> </ol>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#features-demonstrated","title":"Features Demonstrated","text":"<ul> <li>\u2705 \u03b2-VAE framework for controllable generation</li> <li>\u2705 Disentanglement evaluation (MIG score)</li> <li>\u2705 Image quality assessment (FID, LPIPS, SSIM)</li> <li>\u2705 Model comparison across quality levels</li> <li>\u2705 Trade-off analysis (disentanglement vs quality vs training time)</li> <li>\u2705 Comprehensive benchmark suite</li> <li>\u2705 Performance target assessment</li> </ul>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#experiments-to-try","title":"Experiments to Try","text":"<ol> <li>Adjust Latent Dimensionality</li> </ol> <pre><code>model = MockMultiBetaVAE(\n    latent_dim=256,  # Try different sizes: 16, 32, 64, 128, 256\n    image_size=64,\n    quality_level=\"high\",\n    rngs=rngs\n)\n</code></pre> <ol> <li>Vary \u03b2 Values</li> </ol> <pre><code># In real \u03b2-VAE training\nbeta_values = [0.5, 1.0, 2.0, 4.0, 8.0]\nfor beta in beta_values:\n    model = BetaVAE(latent_dim=64, beta=beta, rngs=rngs)\n    # Train and evaluate\n</code></pre> <ol> <li>Change Dataset Size</li> </ol> <pre><code>suite = MultiBetaVAEBenchmarkSuite(\n    dataset_config={\n        \"num_samples\": 500,  # More samples for stable metrics\n        \"image_size\": 128,   # Higher resolution\n    },\n    # ...\n)\n</code></pre> <ol> <li>Custom Quality Configurations</li> </ol> <pre><code>model.quality_params[\"custom\"] = {\n    \"mig_score\": 0.40,\n    \"fid_score\": 35.0,\n    \"lpips_score\": 0.12,\n    \"ssim_score\": 0.88,\n}\nmodel.quality_level = \"custom\"\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#next-steps","title":"Next Steps","text":"<ul> <li> <p> VAE Training</p> <p>Train \u03b2-VAE on real datasets</p> <p> VAE MNIST Tutorial</p> </li> <li> <p> Advanced VAE</p> <p>Explore FactorVAE and \u03b2-TCVAE</p> <p> Advanced VAE</p> </li> <li> <p> Latent Space Analysis</p> <p>Visualize and interpret disentangled representations</p> <p> Latent Space Tutorial</p> </li> <li> <p> Loss Functions</p> <p>Understand VAE loss components</p> <p> Loss Examples</p> </li> </ul>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#benchmark-runs-slowly","title":"Benchmark Runs Slowly","text":"<p>Symptom: Evaluation takes too long</p> <p>Solution: Reduce dataset or batch size</p> <pre><code>dataset_config = {\n    \"num_samples\": 50,  # Smaller dataset\n}\nbenchmark_config = {\n    \"batch_size\": 5,    # Smaller batches\n}\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#models-dont-meet-targets","title":"Models Don't Meet Targets","text":"<p>Symptom: All models fail to meet performance targets</p> <p>Cause: Insufficient model capacity or training</p> <p>Solution: Increase latent dimensionality or improve quality</p> <pre><code>model = MockMultiBetaVAE(\n    latent_dim=128,      # Larger capacity\n    quality_level=\"high\", # Better quality\n    rngs=rngs\n)\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#high-memory-usage","title":"High Memory Usage","text":"<p>Symptom: Out of memory errors during evaluation</p> <p>Solution: Reduce image size or batch size</p> <pre><code>dataset_config = {\n    \"image_size\": 32,   # Smaller images\n}\nbenchmark_config = {\n    \"batch_size\": 4,    # Smaller batches\n}\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#inconsistent-mig-scores","title":"Inconsistent MIG Scores","text":"<p>Symptom: MIG scores vary significantly between runs</p> <p>Cause: Too few samples for stable metric computation</p> <p>Solution: Increase number of evaluation samples</p> <pre><code>benchmark_config = {\n    \"num_samples\": 100,  # More samples for stability\n}\n</code></pre>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#additional-resources","title":"Additional Resources","text":""},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#documentation","title":"Documentation","text":"<ul> <li>VAE Guide - Complete VAE documentation</li> <li>Benchmark Suite - Benchmarking guide</li> </ul>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#related-examples","title":"Related Examples","text":"<ul> <li>VAE MNIST Tutorial - Basic VAE training</li> <li>Advanced VAE - Advanced VAE variants</li> <li>Loss Examples - VAE loss functions</li> <li>Framework Features Demo - Configuration system</li> </ul>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#papers-and-resources","title":"Papers and Resources","text":"<ul> <li>\u03b2-VAE: \"\u03b2-VAE: Learning Basic Visual Concepts with a Constrained VAE\" (Higgins et al., 2017)</li> <li>FactorVAE: \"Disentangling by Factorising\" (Kim &amp; Mnih, 2018)</li> <li>\u03b2-TCVAE: \"Isolating Sources of Disentanglement in VAEs\" (Chen et al., 2018)</li> <li>MIG: \"A Framework for the Quantitative Evaluation of Disentangled Representations\" (Chen et al., 2018)</li> <li>FID: \"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\" (Heusel et al., 2017)</li> <li>LPIPS: \"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\" (Zhang et al., 2018)</li> </ul>"},{"location":"examples/vae/multi-beta-vae-benchmark-demo/#external-libraries","title":"External Libraries","text":"<ul> <li>disentanglement_lib: Google's disentanglement benchmark</li> <li>pytorch-fid: FID score implementation</li> <li>LPIPS: Official LPIPS implementation</li> </ul>"},{"location":"extensions/","title":"Extensions","text":"<p>Extensions in Artifex provide domain-specific functionality that enhances base generative models with specialized knowledge and constraints. This modular system allows you to add protein modeling, NLP preprocessing, audio processing, and vision augmentation capabilities to any model.</p>"},{"location":"extensions/#overview","title":"Overview","text":"<ul> <li> <p> Protein Extensions</p> <p>Add protein-specific constraints, backbone geometry, and amino acid features to geometric models</p> <p> Protein Extensions</p> </li> <li> <p> NLP Extensions</p> <p>Tokenization, embeddings, and text preprocessing for language models</p> <p> NLP Extensions</p> </li> <li> <p> Audio Extensions</p> <p>Spectral processing and temporal features for audio generation</p> <p> Audio Extensions</p> </li> <li> <p> Vision Extensions</p> <p>Image augmentation and preprocessing for visual models</p> <p> Vision Extensions</p> </li> </ul>"},{"location":"extensions/#quick-start","title":"Quick Start","text":"<p>Extensions integrate seamlessly with Artifex models through the extension system:</p> <pre><code>import jax\nfrom flax import nnx\nfrom artifex.generative_models.core.configuration import (\n    ProteinExtensionConfig,\n    ProteinMixinConfig,\n)\nfrom artifex.generative_models.extensions.protein import (\n    BondLengthExtension,\n    BondAngleExtension,\n    ProteinMixinExtension,\n)\nfrom artifex.generative_models.models.geometric.point_cloud import PointCloudModel\n\n# Configure extensions using frozen dataclass configs\nmixin_config = ProteinMixinConfig(\n    name=\"protein_mixin\",\n    weight=1.0,\n    enabled=True,\n    embedding_dim=64,\n    num_aa_types=20,\n)\n\n# Create extension instance\nkey = jax.random.key(42)\nprotein_mixin = ProteinMixinExtension(\n    config=mixin_config,\n    rngs=nnx.Rngs(params=key),\n)\n\n# Wrap in nnx.Dict for NNX compatibility\nextensions = nnx.Dict({\"protein_mixin\": protein_mixin})\n\n# Create model with extensions\nmodel = PointCloudModel(model_config, extensions=extensions, rngs=nnx.Rngs(params=key))\n</code></pre>"},{"location":"extensions/#protein-extensions","title":"Protein Extensions","text":"<p>Protein extensions add domain knowledge about molecular structure to geometric models, enabling physically realistic protein generation.</p>"},{"location":"extensions/#available-extensions","title":"Available Extensions","text":"Extension Description Key Features ProteinMixinExtension Amino acid integration 20 AA type embeddings, residue features ProteinBackboneConstraint Backbone geometry N, CA, C, O atom indices, geometric constraints BondLengthExtension Bond distance monitoring Violation detection, loss contribution BondAngleExtension Bond angle monitoring Peptide bond angles, backbone geometry"},{"location":"extensions/#usage-example","title":"Usage Example","text":"<pre><code>import jax\nfrom flax import nnx\nfrom artifex.generative_models.core.configuration import (\n    ProteinExtensionConfig,\n    ProteinMixinConfig,\n)\nfrom artifex.generative_models.extensions.protein import (\n    BondLengthExtension,\n    BondAngleExtension,\n    ProteinMixinExtension,\n)\nfrom artifex.generative_models.extensions.protein.constraints import (\n    ProteinBackboneConstraint,\n)\n\nkey = jax.random.key(42)\n\n# Create multiple protein extensions\nextensions_dict = {}\n\n# Amino acid type integration with frozen dataclass config\nextensions_dict[\"protein_mixin\"] = ProteinMixinExtension(\n    config=ProteinMixinConfig(\n        name=\"protein_mixin\",\n        weight=1.0,\n        enabled=True,\n        embedding_dim=64,\n        num_aa_types=20,\n    ),\n    rngs=nnx.Rngs(params=key),\n)\n\n# Backbone constraints with explicit fields\nextensions_dict[\"backbone\"] = ProteinBackboneConstraint(\n    config=ProteinExtensionConfig(\n        name=\"backbone\",\n        weight=1.0,\n        enabled=True,\n        bond_length_weight=1.0,\n        bond_angle_weight=0.5,\n    ),\n    rngs=nnx.Rngs(params=key),\n)\n\n# Bond length monitoring\nextensions_dict[\"bond_length\"] = BondLengthExtension(\n    config=ProteinExtensionConfig(\n        name=\"bond_length\",\n        weight=1.0,\n        enabled=True,\n        bond_length_weight=1.0,\n        ideal_bond_lengths={\"N-CA\": 1.45, \"CA-C\": 1.52, \"C-N\": 1.33},\n    ),\n    rngs=nnx.Rngs(params=key),\n)\n\n# Wrap for NNX\nextensions = nnx.Dict(extensions_dict)\n</code></pre>"},{"location":"extensions/#documentation","title":"Documentation","text":"<ul> <li>Backbone Extension - Backbone atom handling</li> <li>Constraints - Geometric constraints</li> <li>Mixin Extension - Amino acid integration</li> <li>Utilities - Protein utility functions</li> </ul>"},{"location":"extensions/#nlp-extensions","title":"NLP Extensions","text":"<p>NLP extensions provide text processing capabilities for language models and multimodal systems.</p>"},{"location":"extensions/#available-extensions_1","title":"Available Extensions","text":"Extension Description Key Features Tokenization Text tokenization BPE, SentencePiece, character-level Embeddings Token embeddings Positional encoding, learned embeddings"},{"location":"extensions/#documentation_1","title":"Documentation","text":"<ul> <li>Tokenization - Tokenization methods</li> <li>Embeddings - Embedding systems</li> </ul>"},{"location":"extensions/#audio-extensions","title":"Audio Extensions","text":"<p>Audio extensions add signal processing capabilities for audio generation models.</p>"},{"location":"extensions/#available-extensions_2","title":"Available Extensions","text":"Extension Description Key Features Spectral Frequency analysis STFT, mel-spectrograms, spectrogram inversion Temporal Time-domain features Envelope extraction, onset detection"},{"location":"extensions/#documentation_2","title":"Documentation","text":"<ul> <li>Spectral Processing - Frequency domain operations</li> <li>Temporal Features - Time domain processing</li> </ul>"},{"location":"extensions/#vision-extensions","title":"Vision Extensions","text":"<p>Vision extensions provide image preprocessing and augmentation for visual models.</p>"},{"location":"extensions/#available-extensions_3","title":"Available Extensions","text":"Extension Description Key Features Augmentation Data augmentation Flips, rotations, color jitter, cutout"},{"location":"extensions/#documentation_3","title":"Documentation","text":"<ul> <li>Augmentation - Image augmentation methods</li> </ul>"},{"location":"extensions/#extension-architecture","title":"Extension Architecture","text":""},{"location":"extensions/#configuration-classes","title":"Configuration Classes","text":"<p>Extensions use frozen dataclass configurations from <code>core.configuration</code>:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    ExtensionConfig,           # Base extension config\n    ConstraintExtensionConfig, # For constraint extensions\n    ProteinExtensionConfig,    # Protein-specific constraints\n    ProteinMixinConfig,        # Protein amino acid features\n    ChemicalConstraintConfig,  # Chemical/molecular constraints\n    ImageAugmentationConfig,   # Vision augmentation\n    AudioSpectralConfig,       # Audio spectral processing\n    TextEmbeddingConfig,       # NLP embeddings\n)\n\n# Base ExtensionConfig for simple extensions\nconfig = ExtensionConfig(\n    name=\"my_extension\",      # Unique identifier\n    weight=1.0,               # Loss contribution weight\n    enabled=True,             # Enable/disable toggle\n)\n\n# Domain-specific configs have explicit fields (no extensions dict)\nprotein_config = ProteinExtensionConfig(\n    name=\"backbone\",\n    weight=1.0,\n    enabled=True,\n    bond_length_weight=1.0,   # Explicit field, not in extensions dict\n    bond_angle_weight=0.5,\n    ideal_bond_lengths={\"N-CA\": 1.45, \"CA-C\": 1.52},\n)\n</code></pre>"},{"location":"extensions/#extension-registry","title":"Extension Registry","text":"<p>Extensions can be registered and discovered through the registry:</p> <pre><code>from artifex.generative_models.extensions.registry import (\n    register_extension,\n    get_extension,\n    list_extensions,\n)\n\n# List available extensions\navailable = list_extensions()\nprint(f\"Available extensions: {available}\")\n\n# Get extension by name\nExtensionClass = get_extension(\"protein_mixin\")\n</code></pre>"},{"location":"extensions/#documentation_4","title":"Documentation","text":"<ul> <li>Extensions Base - Base extension classes</li> <li>Registry - Extension registration system</li> <li>Features - Feature extraction utilities</li> </ul>"},{"location":"extensions/#creating-custom-extensions","title":"Creating Custom Extensions","text":"<p>You can create custom extensions by inheriting from the base extension class:</p> <pre><code>import dataclasses\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.core.configuration import ExtensionConfig\nfrom artifex.generative_models.extensions.base import ModelExtension\n\n\n# Define a custom frozen dataclass config for your extension\n@dataclasses.dataclass(frozen=True)\nclass MyExtensionConfig(ExtensionConfig):\n    \"\"\"Custom extension configuration.\"\"\"\n    my_param: float = 1.0\n    another_param: int = 10\n\n\nclass MyCustomExtension(ModelExtension):\n    \"\"\"Custom extension for domain-specific processing.\"\"\"\n\n    def __init__(\n        self,\n        config: MyExtensionConfig | ExtensionConfig,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(config, rngs=rngs)\n        # Use explicit config fields (frozen dataclass pattern)\n        if isinstance(config, MyExtensionConfig):\n            self.my_param = config.my_param\n            self.another_param = config.another_param\n        else:\n            # Defaults for base ExtensionConfig\n            self.my_param = 1.0\n            self.another_param = 10\n\n    def __call__(self, inputs, model_outputs, **kwargs) -&gt; dict:\n        \"\"\"Process inputs and model outputs.\n\n        Args:\n            inputs: Input batch dictionary\n            model_outputs: Model predictions\n            **kwargs: Additional arguments\n\n        Returns:\n            Extension outputs dictionary\n        \"\"\"\n        if not self.enabled:\n            return {\"extension_type\": \"my_custom\"}\n\n        # Implement extension logic\n        result = self._process(inputs, model_outputs)\n        return {\"my_output\": result, \"extension_type\": \"my_custom\"}\n\n    def loss_fn(self, batch: dict, model_outputs, **kwargs) -&gt; jax.Array:\n        \"\"\"Compute extension-specific loss.\n\n        Args:\n            batch: Input batch\n            model_outputs: Model outputs\n\n        Returns:\n            Loss value (scalar JAX array)\n        \"\"\"\n        if not self.enabled:\n            return jnp.array(0.0)\n\n        # Implement loss computation using pure JAX operations\n        return self._compute_loss(batch, model_outputs)\n</code></pre>"},{"location":"extensions/#best-practices","title":"Best Practices","text":"<p>DO</p> <ul> <li>Use frozen dataclass configs from <code>core.configuration</code></li> <li>Use domain-specific configs (e.g., <code>ProteinExtensionConfig</code>) with explicit fields</li> <li>Wrap extensions in <code>nnx.Dict</code> for NNX compatibility</li> <li>Set appropriate weights for multi-extension setups</li> <li>Disable unused extensions for efficiency</li> <li>Use pure JAX operations in <code>loss_fn</code> for JIT compatibility</li> </ul> <p>DON'T</p> <ul> <li>Don't use <code>extensions={}</code> dict pattern (old Pydantic style)</li> <li>Don't use raw dictionaries instead of <code>nnx.Dict</code></li> <li>Don't forget to pass <code>rngs</code> to extension constructors</li> <li>Don't use conflicting extension names</li> <li>Don't enable extensions without proper configuration</li> <li>Don't mutate RNGs inside traced functions (JIT/grad)</li> </ul>"},{"location":"extensions/#summary","title":"Summary","text":"<p>Extensions provide a modular way to add domain-specific functionality:</p> <ul> <li>Protein: Physical constraints and amino acid features</li> <li>NLP: Tokenization and text embeddings</li> <li>Audio: Spectral and temporal processing</li> <li>Vision: Image augmentation</li> </ul> <p>All extensions follow consistent patterns for configuration, registration, and integration with Artifex models.</p>"},{"location":"extensions/augmentation/","title":"Augmentation","text":"<p>Module: <code>generative_models.extensions.vision.augmentation</code></p> <p>Source: <code>generative_models/extensions/vision/augmentation.py</code></p>"},{"location":"extensions/augmentation/#overview","title":"Overview","text":"<p>Advanced image augmentation for generative models.</p> <p>This module provides JAX-compatible image augmentation techniques for training robust generative models.</p>"},{"location":"extensions/augmentation/#classes","title":"Classes","text":""},{"location":"extensions/augmentation/#advancedimageaugmentation","title":"AdvancedImageAugmentation","text":"<pre><code>class AdvancedImageAugmentation\n</code></pre>"},{"location":"extensions/augmentation/#functions","title":"Functions","text":""},{"location":"extensions/augmentation/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/augmentation/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/augmentation/#apply_cutout","title":"apply_cutout","text":"<pre><code>def apply_cutout()\n</code></pre>"},{"location":"extensions/augmentation/#apply_horizontal_flip","title":"apply_horizontal_flip","text":"<pre><code>def apply_horizontal_flip()\n</code></pre>"},{"location":"extensions/augmentation/#create_augmentation_sequence","title":"create_augmentation_sequence","text":"<pre><code>def create_augmentation_sequence()\n</code></pre>"},{"location":"extensions/augmentation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 5</li> <li>Imports: 5</li> </ul>"},{"location":"extensions/backbone/","title":"Backbone","text":"<p>Module: <code>generative_models.extensions.protein.backbone</code></p> <p>Source: <code>generative_models/extensions/protein/backbone.py</code></p>"},{"location":"extensions/backbone/#overview","title":"Overview","text":"<p>Protein backbone extensions for generative models.</p> <p>This module implements extensions for adding protein-specific backbone functionality to geometric models without modifying core implementations.</p>"},{"location":"extensions/backbone/#classes","title":"Classes","text":""},{"location":"extensions/backbone/#bondangleextension","title":"BondAngleExtension","text":"<pre><code>class BondAngleExtension\n</code></pre>"},{"location":"extensions/backbone/#bondlengthextension","title":"BondLengthExtension","text":"<pre><code>class BondLengthExtension\n</code></pre>"},{"location":"extensions/backbone/#functions","title":"Functions","text":""},{"location":"extensions/backbone/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/backbone/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/backbone/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/backbone/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/backbone/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"extensions/backbone/#loss_fn_1","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"extensions/backbone/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 6</li> <li>Imports: 5</li> </ul>"},{"location":"extensions/constraints/","title":"Constraints","text":"<p>Module: <code>generative_models.extensions.protein.constraints</code></p> <p>Source: <code>generative_models/extensions/protein/constraints.py</code></p>"},{"location":"extensions/constraints/#overview","title":"Overview","text":"<p>Protein geometric constraints.</p> <p>This module defines constraints specific to protein structure generation.</p>"},{"location":"extensions/constraints/#classes","title":"Classes","text":""},{"location":"extensions/constraints/#proteinbackboneconstraint","title":"ProteinBackboneConstraint","text":"<pre><code>class ProteinBackboneConstraint\n</code></pre>"},{"location":"extensions/constraints/#proteindihedralconstraint","title":"ProteinDihedralConstraint","text":"<pre><code>class ProteinDihedralConstraint\n</code></pre>"},{"location":"extensions/constraints/#functions","title":"Functions","text":""},{"location":"extensions/constraints/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/constraints/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/constraints/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/constraints/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/constraints/#calculate_bond_angles","title":"calculate_bond_angles","text":"<pre><code>def calculate_bond_angles()\n</code></pre>"},{"location":"extensions/constraints/#calculate_bond_lengths","title":"calculate_bond_lengths","text":"<pre><code>def calculate_bond_lengths()\n</code></pre>"},{"location":"extensions/constraints/#calculate_dihedral_angles","title":"calculate_dihedral_angles","text":"<pre><code>def calculate_dihedral_angles()\n</code></pre>"},{"location":"extensions/constraints/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"extensions/constraints/#loss_fn_1","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"extensions/constraints/#validate","title":"validate","text":"<pre><code>def validate()\n</code></pre>"},{"location":"extensions/constraints/#validate_1","title":"validate","text":"<pre><code>def validate()\n</code></pre>"},{"location":"extensions/constraints/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 11</li> <li>Imports: 6</li> </ul>"},{"location":"extensions/embeddings/","title":"Embeddings","text":"<p>Module: <code>generative_models.extensions.nlp.embeddings</code></p> <p>Source: <code>generative_models/extensions/nlp/embeddings.py</code></p>"},{"location":"extensions/embeddings/#overview","title":"Overview","text":"<p>Text embedding utilities for generation tasks.</p> <p>This module provides text embedding and representation learning utilities for generative models working with textual data.</p>"},{"location":"extensions/embeddings/#classes","title":"Classes","text":""},{"location":"extensions/embeddings/#textembeddings","title":"TextEmbeddings","text":"<pre><code>class TextEmbeddings\n</code></pre>"},{"location":"extensions/embeddings/#functions","title":"Functions","text":""},{"location":"extensions/embeddings/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/embeddings/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/embeddings/#compute_attention_weights","title":"compute_attention_weights","text":"<pre><code>def compute_attention_weights()\n</code></pre>"},{"location":"extensions/embeddings/#compute_similarity","title":"compute_similarity","text":"<pre><code>def compute_similarity()\n</code></pre>"},{"location":"extensions/embeddings/#create_contextual_embeddings","title":"create_contextual_embeddings","text":"<pre><code>def create_contextual_embeddings()\n</code></pre>"},{"location":"extensions/embeddings/#embed","title":"embed","text":"<pre><code>def embed()\n</code></pre>"},{"location":"extensions/embeddings/#extract_sentence_embedding","title":"extract_sentence_embedding","text":"<pre><code>def extract_sentence_embedding()\n</code></pre>"},{"location":"extensions/embeddings/#get_embedding_statistics","title":"get_embedding_statistics","text":"<pre><code>def get_embedding_statistics()\n</code></pre>"},{"location":"extensions/embeddings/#get_token_embeddings","title":"get_token_embeddings","text":"<pre><code>def get_token_embeddings()\n</code></pre>"},{"location":"extensions/embeddings/#interpolate_embeddings","title":"interpolate_embeddings","text":"<pre><code>def interpolate_embeddings()\n</code></pre>"},{"location":"extensions/embeddings/#project_to_vocabulary","title":"project_to_vocabulary","text":"<pre><code>def project_to_vocabulary()\n</code></pre>"},{"location":"extensions/embeddings/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 11</li> <li>Imports: 5</li> </ul>"},{"location":"extensions/extensions/","title":"Extensions","text":"<p>Module: <code>generative_models.extensions.base.extensions</code></p> <p>Source: <code>generative_models/extensions/base/extensions.py</code></p>"},{"location":"extensions/extensions/#overview","title":"Overview","text":"<p>Base classes for model extensions.</p> <p>This module provides the core extension interfaces used across different domains to enable extension of model functionality without modifying core implementations.</p>"},{"location":"extensions/extensions/#classes","title":"Classes","text":""},{"location":"extensions/extensions/#constraintextension","title":"ConstraintExtension","text":"<pre><code>class ConstraintExtension\n</code></pre>"},{"location":"extensions/extensions/#extensionconfig","title":"ExtensionConfig","text":"<pre><code>class ExtensionConfig\n</code></pre>"},{"location":"extensions/extensions/#modelextension","title":"ModelExtension","text":"<pre><code>class ModelExtension\n</code></pre>"},{"location":"extensions/extensions/#functions","title":"Functions","text":""},{"location":"extensions/extensions/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/extensions/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/extensions/#is_enabled","title":"is_enabled","text":"<pre><code>def is_enabled()\n</code></pre>"},{"location":"extensions/extensions/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"extensions/extensions/#project","title":"project","text":"<pre><code>def project()\n</code></pre>"},{"location":"extensions/extensions/#validate","title":"validate","text":"<pre><code>def validate()\n</code></pre>"},{"location":"extensions/extensions/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 6</li> <li>Imports: 5</li> </ul>"},{"location":"extensions/features/","title":"Features","text":"<p>Module: <code>generative_models.extensions.chemical.features</code></p> <p>Source: <code>generative_models/extensions/chemical/features.py</code></p>"},{"location":"extensions/features/#overview","title":"Overview","text":"<p>Molecular feature computation and extraction.</p> <p>This module provides molecular descriptor computation and feature extraction for chemical property prediction and drug-likeness assessment.</p>"},{"location":"extensions/features/#classes","title":"Classes","text":""},{"location":"extensions/features/#molecularfeatures","title":"MolecularFeatures","text":"<pre><code>class MolecularFeatures\n</code></pre>"},{"location":"extensions/features/#functions","title":"Functions","text":""},{"location":"extensions/features/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/features/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/features/#compute_descriptors","title":"compute_descriptors","text":"<pre><code>def compute_descriptors()\n</code></pre>"},{"location":"extensions/features/#compute_drug_likeness_score","title":"compute_drug_likeness_score","text":"<pre><code>def compute_drug_likeness_score()\n</code></pre>"},{"location":"extensions/features/#extract_fingerprint","title":"extract_fingerprint","text":"<pre><code>def extract_fingerprint()\n</code></pre>"},{"location":"extensions/features/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 5</li> <li>Imports: 5</li> </ul>"},{"location":"extensions/mixin/","title":"Mixin","text":"<p>Module: <code>generative_models.extensions.protein.mixin</code></p> <p>Source: <code>generative_models/extensions/protein/mixin.py</code></p>"},{"location":"extensions/mixin/#overview","title":"Overview","text":"<p>Protein mixin extensions for generative models.</p> <p>This module implements protein-specific mixin extensions that add protein functionality to models such as amino acid type handling.</p>"},{"location":"extensions/mixin/#classes","title":"Classes","text":""},{"location":"extensions/mixin/#proteinmixinextension","title":"ProteinMixinExtension","text":"<pre><code>class ProteinMixinExtension\n</code></pre>"},{"location":"extensions/mixin/#functions","title":"Functions","text":""},{"location":"extensions/mixin/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/mixin/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/mixin/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 2</li> <li>Imports: 4</li> </ul>"},{"location":"extensions/registry/","title":"Registry","text":"<p>Module: <code>generative_models.extensions.registry</code></p> <p>Source: <code>generative_models/extensions/registry.py</code></p>"},{"location":"extensions/registry/#overview","title":"Overview","text":"<p>Central registry for modality-specific extensions.</p> <p>This module provides a unified registry system for managing and discovering extensions across different modalities in the generative models framework.</p>"},{"location":"extensions/registry/#classes","title":"Classes","text":""},{"location":"extensions/registry/#extensionsregistry","title":"ExtensionsRegistry","text":"<pre><code>class ExtensionsRegistry\n</code></pre>"},{"location":"extensions/registry/#functions","title":"Functions","text":""},{"location":"extensions/registry/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/registry/#create_extension","title":"create_extension","text":"<pre><code>def create_extension()\n</code></pre>"},{"location":"extensions/registry/#create_extension_pipeline","title":"create_extension_pipeline","text":"<pre><code>def create_extension_pipeline()\n</code></pre>"},{"location":"extensions/registry/#get_available_capabilities","title":"get_available_capabilities","text":"<pre><code>def get_available_capabilities()\n</code></pre>"},{"location":"extensions/registry/#get_available_modalities","title":"get_available_modalities","text":"<pre><code>def get_available_modalities()\n</code></pre>"},{"location":"extensions/registry/#get_extension_info","title":"get_extension_info","text":"<pre><code>def get_extension_info()\n</code></pre>"},{"location":"extensions/registry/#get_extensions_by_capability","title":"get_extensions_by_capability","text":"<pre><code>def get_extensions_by_capability()\n</code></pre>"},{"location":"extensions/registry/#get_extensions_for_modality","title":"get_extensions_for_modality","text":"<pre><code>def get_extensions_for_modality()\n</code></pre>"},{"location":"extensions/registry/#get_extensions_registry","title":"get_extensions_registry","text":"<pre><code>def get_extensions_registry()\n</code></pre>"},{"location":"extensions/registry/#list_all_extensions","title":"list_all_extensions","text":"<pre><code>def list_all_extensions()\n</code></pre>"},{"location":"extensions/registry/#register_extension","title":"register_extension","text":"<pre><code>def register_extension()\n</code></pre>"},{"location":"extensions/registry/#search_extensions","title":"search_extensions","text":"<pre><code>def search_extensions()\n</code></pre>"},{"location":"extensions/registry/#validate_extension_compatibility","title":"validate_extension_compatibility","text":"<pre><code>def validate_extension_compatibility()\n</code></pre>"},{"location":"extensions/registry/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 13</li> <li>Imports: 11</li> </ul>"},{"location":"extensions/spectral/","title":"Spectral","text":"<p>Module: <code>generative_models.extensions.audio_processing.spectral</code></p> <p>Source: <code>generative_models/extensions/audio_processing/spectral.py</code></p>"},{"location":"extensions/spectral/#overview","title":"Overview","text":"<p>Spectral analysis and feature extraction for audio processing.</p> <p>This module provides JAX-compatible spectral analysis tools for audio generation and processing tasks.</p>"},{"location":"extensions/spectral/#classes","title":"Classes","text":""},{"location":"extensions/spectral/#spectralanalysis","title":"SpectralAnalysis","text":"<pre><code>class SpectralAnalysis\n</code></pre>"},{"location":"extensions/spectral/#functions","title":"Functions","text":""},{"location":"extensions/spectral/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/spectral/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/spectral/#compute_log_mel_spectrogram","title":"compute_log_mel_spectrogram","text":"<pre><code>def compute_log_mel_spectrogram()\n</code></pre>"},{"location":"extensions/spectral/#compute_mel_spectrogram","title":"compute_mel_spectrogram","text":"<pre><code>def compute_mel_spectrogram()\n</code></pre>"},{"location":"extensions/spectral/#compute_mfcc","title":"compute_mfcc","text":"<pre><code>def compute_mfcc()\n</code></pre>"},{"location":"extensions/spectral/#compute_spectral_bandwidth","title":"compute_spectral_bandwidth","text":"<pre><code>def compute_spectral_bandwidth()\n</code></pre>"},{"location":"extensions/spectral/#compute_spectral_centroid","title":"compute_spectral_centroid","text":"<pre><code>def compute_spectral_centroid()\n</code></pre>"},{"location":"extensions/spectral/#compute_spectral_rolloff","title":"compute_spectral_rolloff","text":"<pre><code>def compute_spectral_rolloff()\n</code></pre>"},{"location":"extensions/spectral/#compute_spectrogram","title":"compute_spectrogram","text":"<pre><code>def compute_spectrogram()\n</code></pre>"},{"location":"extensions/spectral/#compute_stft","title":"compute_stft","text":"<pre><code>def compute_stft()\n</code></pre>"},{"location":"extensions/spectral/#extract_spectral_features","title":"extract_spectral_features","text":"<pre><code>def extract_spectral_features()\n</code></pre>"},{"location":"extensions/spectral/#hz_to_mel","title":"hz_to_mel","text":"<pre><code>def hz_to_mel()\n</code></pre>"},{"location":"extensions/spectral/#inverse_mel_spectrogram","title":"inverse_mel_spectrogram","text":"<pre><code>def inverse_mel_spectrogram()\n</code></pre>"},{"location":"extensions/spectral/#mel_to_hz","title":"mel_to_hz","text":"<pre><code>def mel_to_hz()\n</code></pre>"},{"location":"extensions/spectral/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 14</li> <li>Imports: 5</li> </ul>"},{"location":"extensions/temporal/","title":"Temporal","text":"<p>Module: <code>generative_models.extensions.audio_processing.temporal</code></p> <p>Source: <code>generative_models/extensions/audio_processing/temporal.py</code></p>"},{"location":"extensions/temporal/#overview","title":"Overview","text":"<p>Temporal analysis and feature extraction for audio processing.</p> <p>This module provides temporal pattern analysis for audio generation tasks, including rhythm, tempo, and time-domain feature extraction.</p>"},{"location":"extensions/temporal/#classes","title":"Classes","text":""},{"location":"extensions/temporal/#temporalanalysis","title":"TemporalAnalysis","text":"<pre><code>class TemporalAnalysis\n</code></pre>"},{"location":"extensions/temporal/#functions","title":"Functions","text":""},{"location":"extensions/temporal/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/temporal/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/temporal/#compute_energy","title":"compute_energy","text":"<pre><code>def compute_energy()\n</code></pre>"},{"location":"extensions/temporal/#compute_onset_strength","title":"compute_onset_strength","text":"<pre><code>def compute_onset_strength()\n</code></pre>"},{"location":"extensions/temporal/#compute_pulse_clarity","title":"compute_pulse_clarity","text":"<pre><code>def compute_pulse_clarity()\n</code></pre>"},{"location":"extensions/temporal/#compute_rhythm_features","title":"compute_rhythm_features","text":"<pre><code>def compute_rhythm_features()\n</code></pre>"},{"location":"extensions/temporal/#compute_rms","title":"compute_rms","text":"<pre><code>def compute_rms()\n</code></pre>"},{"location":"extensions/temporal/#compute_temporal_features","title":"compute_temporal_features","text":"<pre><code>def compute_temporal_features()\n</code></pre>"},{"location":"extensions/temporal/#compute_zero_crossing_rate","title":"compute_zero_crossing_rate","text":"<pre><code>def compute_zero_crossing_rate()\n</code></pre>"},{"location":"extensions/temporal/#detect_beats","title":"detect_beats","text":"<pre><code>def detect_beats()\n</code></pre>"},{"location":"extensions/temporal/#estimate_tempo","title":"estimate_tempo","text":"<pre><code>def estimate_tempo()\n</code></pre>"},{"location":"extensions/temporal/#segment_by_energy","title":"segment_by_energy","text":"<pre><code>def segment_by_energy()\n</code></pre>"},{"location":"extensions/temporal/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 12</li> <li>Imports: 5</li> </ul>"},{"location":"extensions/tokenization/","title":"Tokenization","text":"<p>Module: <code>generative_models.extensions.nlp.tokenization</code></p> <p>Source: <code>generative_models/extensions/nlp/tokenization.py</code></p>"},{"location":"extensions/tokenization/#overview","title":"Overview","text":"<p>Advanced tokenization for text generation tasks.</p> <p>This module provides JAX-compatible tokenization utilities for text processing and generation tasks.</p>"},{"location":"extensions/tokenization/#classes","title":"Classes","text":""},{"location":"extensions/tokenization/#advancedtokenization","title":"AdvancedTokenization","text":"<pre><code>class AdvancedTokenization\n</code></pre>"},{"location":"extensions/tokenization/#functions","title":"Functions","text":""},{"location":"extensions/tokenization/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"extensions/tokenization/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"extensions/tokenization/#add_special_tokens","title":"add_special_tokens","text":"<pre><code>def add_special_tokens()\n</code></pre>"},{"location":"extensions/tokenization/#apply_masking","title":"apply_masking","text":"<pre><code>def apply_masking()\n</code></pre>"},{"location":"extensions/tokenization/#compute_token_frequencies","title":"compute_token_frequencies","text":"<pre><code>def compute_token_frequencies()\n</code></pre>"},{"location":"extensions/tokenization/#create_attention_mask","title":"create_attention_mask","text":"<pre><code>def create_attention_mask()\n</code></pre>"},{"location":"extensions/tokenization/#create_position_ids","title":"create_position_ids","text":"<pre><code>def create_position_ids()\n</code></pre>"},{"location":"extensions/tokenization/#decode_batch","title":"decode_batch","text":"<pre><code>def decode_batch()\n</code></pre>"},{"location":"extensions/tokenization/#detokenize","title":"detokenize","text":"<pre><code>def detokenize()\n</code></pre>"},{"location":"extensions/tokenization/#encode_batch","title":"encode_batch","text":"<pre><code>def encode_batch()\n</code></pre>"},{"location":"extensions/tokenization/#get_vocabulary_info","title":"get_vocabulary_info","text":"<pre><code>def get_vocabulary_info()\n</code></pre>"},{"location":"extensions/tokenization/#tokenize","title":"tokenize","text":"<pre><code>def tokenize()\n</code></pre>"},{"location":"extensions/tokenization/#truncate_sequences","title":"truncate_sequences","text":"<pre><code>def truncate_sequences()\n</code></pre>"},{"location":"extensions/tokenization/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 13</li> <li>Imports: 5</li> </ul>"},{"location":"extensions/utils/","title":"Utils","text":"<p>Module: <code>generative_models.extensions.protein.utils</code></p> <p>Source: <code>generative_models/extensions/protein/utils.py</code></p>"},{"location":"extensions/utils/#overview","title":"Overview","text":"<p>Utility functions for protein extensions.</p> <p>This module provides utility functions for creating and managing protein extensions in a composable way.</p>"},{"location":"extensions/utils/#functions","title":"Functions","text":""},{"location":"extensions/utils/#create_protein_extensions","title":"create_protein_extensions","text":"<pre><code>def create_protein_extensions()\n</code></pre>"},{"location":"extensions/utils/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 1</li> <li>Imports: 6</li> </ul>"},{"location":"factory/","title":"Model Factory","text":"<p>The factory module provides a centralized, type-safe interface for creating generative models in Artifex. It uses dataclass-based configurations to determine model type automatically, eliminating the need for string-based model class specifications.</p>"},{"location":"factory/#overview","title":"Overview","text":"<ul> <li> <p> Unified Interface</p> <p>Single <code>create_model()</code> function for all model types</p> </li> <li> <p> Type-Safe</p> <p>Dataclass configs with automatic validation</p> </li> <li> <p> Extensible</p> <p>Register custom builders for new model types</p> </li> <li> <p> Modality Support</p> <p>Optional modality adaptation for domain-specific models</p> </li> </ul>"},{"location":"factory/#quick-start","title":"Quick Start","text":""},{"location":"factory/#basic-model-creation","title":"Basic Model Creation","text":"<pre><code>from artifex.generative_models.factory import create_model\nfrom artifex.generative_models.core.configuration import (\n    VAEConfig,\n    EncoderConfig,\n    DecoderConfig,\n)\nfrom flax import nnx\n\n# Create configuration\nencoder = EncoderConfig(\n    name=\"encoder\",\n    input_shape=(28, 28, 1),\n    latent_dim=32,\n    hidden_dims=(256, 128),\n    activation=\"relu\",\n)\ndecoder = DecoderConfig(\n    name=\"decoder\",\n    output_shape=(28, 28, 1),\n    latent_dim=32,\n    hidden_dims=(128, 256),\n    activation=\"relu\",\n)\nconfig = VAEConfig(\n    name=\"my_vae\",\n    encoder=encoder,\n    decoder=decoder,\n    kl_weight=1.0,\n)\n\n# Create model - type is inferred from config\nrngs = nnx.Rngs(params=42, dropout=43, sample=44)\nmodel = create_model(config, rngs=rngs)\n</code></pre>"},{"location":"factory/#model-type-inference","title":"Model Type Inference","text":"<p>The factory automatically infers model type from the configuration class:</p> Config Class Model Type Created Model <code>VAEConfig</code>, <code>BetaVAEConfig</code>, <code>ConditionalVAEConfig</code>, <code>VQVAEConfig</code> <code>vae</code> VAE variants <code>GANConfig</code>, <code>DCGANConfig</code>, <code>WGANConfig</code>, <code>LSGANConfig</code> <code>gan</code> GAN variants <code>DiffusionConfig</code>, <code>DDPMConfig</code>, <code>ScoreDiffusionConfig</code> <code>diffusion</code> Diffusion models <code>EBMConfig</code>, <code>DeepEBMConfig</code> <code>ebm</code> Energy-based models <code>FlowConfig</code> <code>flow</code> Normalizing flows <code>AutoregressiveConfig</code>, <code>TransformerConfig</code>, <code>PixelCNNConfig</code>, <code>WaveNetConfig</code> <code>autoregressive</code> Autoregressive models <code>GeometricConfig</code>, <code>PointCloudConfig</code>, <code>MeshConfig</code>, <code>VoxelConfig</code>, <code>GraphConfig</code> <code>geometric</code> Geometric models"},{"location":"factory/#api-reference","title":"API Reference","text":""},{"location":"factory/#create_model","title":"<code>create_model</code>","text":"<p>The main function for model creation.</p> <pre><code>def create_model(\n    config: DataclassConfig,\n    *,\n    modality: str | None = None,\n    rngs: nnx.Rngs,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Create a model from configuration.\n\n    Args:\n        config: Dataclass model configuration (DDPMConfig, VAEConfig, etc.)\n        modality: Optional modality for adaptation ('image', 'text', 'audio', etc.)\n        rngs: Random number generators\n        **kwargs: Additional arguments passed to the builder\n\n    Returns:\n        Created model instance\n\n    Raises:\n        TypeError: If config is not a supported dataclass config\n        ValueError: If builder not found for model type\n    \"\"\"\n</code></pre> <p>Example:</p> <pre><code>from artifex.generative_models.factory import create_model\nfrom artifex.generative_models.core.configuration import DDPMConfig, UNetBackboneConfig, NoiseScheduleConfig\n\n# Create diffusion model config\nbackbone = UNetBackboneConfig(\n    name=\"unet\",\n    in_channels=3,\n    out_channels=3,\n    base_channels=64,\n    channel_mults=(1, 2, 4),\n)\nnoise_schedule = NoiseScheduleConfig(\n    name=\"schedule\",\n    schedule_type=\"linear\",\n    num_timesteps=1000,\n    beta_start=1e-4,\n    beta_end=2e-2,\n)\nconfig = DDPMConfig(\n    name=\"ddpm\",\n    input_shape=(3, 32, 32),\n    backbone=backbone,\n    noise_schedule=noise_schedule,\n)\n\n# Create model\nmodel = create_model(config, rngs=rngs)\n</code></pre>"},{"location":"factory/#create_model_with_extensions","title":"<code>create_model_with_extensions</code>","text":"<p>Create a model with extensions for enhanced functionality.</p> <pre><code>def create_model_with_extensions(\n    config: DataclassConfig,\n    *,\n    extensions_config: dict[str, ExtensionConfig] | None = None,\n    modality: str | None = None,\n    rngs: nnx.Rngs,\n    **kwargs,\n) -&gt; tuple[Any, dict[str, ModelExtension]]:\n    \"\"\"Create a model and its extensions from configuration.\n\n    Returns:\n        Tuple of (model, extensions_dict)\n    \"\"\"\n</code></pre> <p>Example:</p> <pre><code>from artifex.generative_models.factory import create_model_with_extensions\n\n# Create model with extensions\nmodel, extensions = create_model_with_extensions(\n    config,\n    extensions_config={\n        \"augmentation\": augmentation_config,\n        \"regularization\": reg_config,\n    },\n    rngs=rngs,\n)\n</code></pre>"},{"location":"factory/#modelfactory","title":"<code>ModelFactory</code>","text":"<p>The underlying factory class for advanced usage.</p> <pre><code>class ModelFactory:\n    \"\"\"Centralized factory for all generative models.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with default builders.\"\"\"\n\n    def create(\n        self,\n        config: DataclassConfig,\n        *,\n        modality: str | None = None,\n        rngs: nnx.Rngs,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"Create a model from dataclass configuration.\"\"\"\n</code></pre>"},{"location":"factory/#builders","title":"Builders","text":"<p>Each model family has a dedicated builder that handles model instantiation:</p>"},{"location":"factory/#vae-builder","title":"VAE Builder","text":"<p>Creates VAE variants based on configuration type:</p> <ul> <li><code>VAEConfig</code> \u2192 <code>VAE</code></li> <li><code>BetaVAEConfig</code> \u2192 <code>BetaVAE</code></li> <li><code>ConditionalVAEConfig</code> \u2192 <code>ConditionalVAE</code></li> <li><code>VQVAEConfig</code> \u2192 <code>VQVAE</code></li> </ul> <p> VAE Builder Reference</p>"},{"location":"factory/#gan-builder","title":"GAN Builder","text":"<p>Creates GAN variants:</p> <ul> <li><code>GANConfig</code> \u2192 <code>GAN</code></li> <li><code>DCGANConfig</code> \u2192 <code>DCGAN</code></li> <li><code>WGANConfig</code> \u2192 <code>WGAN</code></li> <li><code>LSGANConfig</code> \u2192 <code>LSGAN</code></li> </ul> <p> GAN Builder Reference</p>"},{"location":"factory/#diffusion-builder","title":"Diffusion Builder","text":"<p>Creates diffusion models:</p> <ul> <li><code>DDPMConfig</code> \u2192 <code>DDPMModel</code></li> <li><code>ScoreDiffusionConfig</code> \u2192 <code>ScoreDiffusionModel</code></li> <li><code>DiffusionConfig</code> \u2192 <code>DiffusionModel</code></li> </ul> <p> Diffusion Builder Reference</p>"},{"location":"factory/#flow-builder","title":"Flow Builder","text":"<p>Creates normalizing flows:</p> <ul> <li><code>FlowConfig</code> \u2192 <code>NormalizingFlow</code></li> </ul> <p> Flow Builder Reference</p>"},{"location":"factory/#ebm-builder","title":"EBM Builder","text":"<p>Creates energy-based models:</p> <ul> <li><code>EBMConfig</code> \u2192 <code>EBM</code></li> <li><code>DeepEBMConfig</code> \u2192 <code>DeepEBM</code></li> </ul> <p> EBM Builder Reference</p>"},{"location":"factory/#autoregressive-builder","title":"Autoregressive Builder","text":"<p>Creates autoregressive models:</p> <ul> <li><code>TransformerConfig</code> \u2192 <code>Transformer</code></li> <li><code>PixelCNNConfig</code> \u2192 <code>PixelCNN</code></li> <li><code>WaveNetConfig</code> \u2192 <code>WaveNet</code></li> </ul> <p> Autoregressive Builder Reference</p>"},{"location":"factory/#geometric-builder","title":"Geometric Builder","text":"<p>Creates geometric models:</p> <ul> <li><code>PointCloudConfig</code> \u2192 <code>PointCloudModel</code></li> <li><code>MeshConfig</code> \u2192 <code>MeshModel</code></li> <li><code>VoxelConfig</code> \u2192 <code>VoxelModel</code></li> <li><code>GraphConfig</code> \u2192 <code>GraphModel</code></li> </ul> <p> Geometric Builder Reference</p>"},{"location":"factory/#registry","title":"Registry","text":"<p>The model type registry manages builder registration:</p> <pre><code>from artifex.generative_models.factory.registry import ModelTypeRegistry\n\n# Create custom registry\nregistry = ModelTypeRegistry()\n\n# Register custom builder\nregistry.register(\"custom_type\", CustomBuilder())\n\n# Get builder\nbuilder = registry.get_builder(\"custom_type\")\n</code></pre> <p> Registry Reference</p>"},{"location":"factory/#modality-adaptation","title":"Modality Adaptation","text":"<p>The factory supports optional modality adaptation for domain-specific models:</p> <pre><code># Create image-adapted model\nmodel = create_model(config, modality=\"image\", rngs=rngs)\n\n# Create text-adapted model\nmodel = create_model(config, modality=\"text\", rngs=rngs)\n\n# Create audio-adapted model\nmodel = create_model(config, modality=\"audio\", rngs=rngs)\n</code></pre> <p>Available Modalities:</p> <ul> <li><code>image</code>: Convolutional layers, FID/IS metrics</li> <li><code>text</code>: Tokenization, perplexity metrics</li> <li><code>audio</code>: Spectrograms, MFCCs</li> <li><code>protein</code>: Structure prediction, sequence modeling</li> <li><code>geometric</code>: Point clouds, meshes</li> </ul>"},{"location":"factory/#custom-builders","title":"Custom Builders","text":"<p>Create custom builders for new model types:</p> <pre><code>from artifex.generative_models.factory.registry import ModelBuilder\nfrom flax import nnx\n\nclass CustomBuilder(ModelBuilder):\n    \"\"\"Builder for custom model type.\"\"\"\n\n    def build(self, config, *, rngs: nnx.Rngs, **kwargs):\n        \"\"\"Build the model from configuration.\"\"\"\n        return CustomModel(config, rngs=rngs, **kwargs)\n\n# Register with factory\nfrom artifex.generative_models.factory import ModelFactory\n\nfactory = ModelFactory()\nfactory.registry.register(\"custom\", CustomBuilder())\n</code></pre>"},{"location":"factory/#best-practices","title":"Best Practices","text":""},{"location":"factory/#do","title":"DO","text":"<ul> <li>\u2705 Use dataclass configs instead of dictionaries</li> <li>\u2705 Validate configs before passing to factory</li> <li>\u2705 Use type hints for better IDE support</li> <li>\u2705 Pass all required RNG streams to <code>nnx.Rngs</code></li> </ul>"},{"location":"factory/#dont","title":"DON'T","text":"<ul> <li>\u274c Pass dictionary configs (will raise <code>TypeError</code>)</li> <li>\u274c Use string-based model class specifications</li> <li>\u274c Forget to provide <code>rngs</code> parameter</li> </ul>"},{"location":"factory/#error-handling","title":"Error Handling","text":"<p>The factory provides clear error messages:</p> <pre><code># TypeError: Dictionary configs not supported\ncreate_model({\"model_class\": \"vae\"}, rngs=rngs)\n# Raises: TypeError: Expected dataclass config, got dict.\n\n# TypeError: Unknown config type\ncreate_model(UnknownConfig(), rngs=rngs)\n# Raises: TypeError: Unknown config type: UnknownConfig\n\n# ValueError: Builder not found\n# (Only possible with custom registries)\n</code></pre>"},{"location":"factory/#module-reference","title":"Module Reference","text":"Module Description core Core factory implementation and <code>create_model</code> function registry Model type registry and builder base class vae VAE model builder gan GAN model builder diffusion Diffusion model builder flow Normalizing flow builder ebm Energy-based model builder autoregressive Autoregressive model builder geometric Geometric model builder"},{"location":"factory/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration System - Understanding dataclass configs</li> <li>Model Gallery - Available model implementations</li> <li>Factory Guide - Detailed factory usage guide</li> </ul>"},{"location":"factory/autoregressive/","title":"Autoregressive","text":"<p>Module: <code>generative_models.factory.builders.autoregressive</code></p> <p>Source: <code>generative_models/factory/builders/autoregressive.py</code></p>"},{"location":"factory/autoregressive/#overview","title":"Overview","text":"<p>Autoregressive model builder.</p>"},{"location":"factory/autoregressive/#classes","title":"Classes","text":""},{"location":"factory/autoregressive/#autoregressivebuilder","title":"AutoregressiveBuilder","text":"<pre><code>class AutoregressiveBuilder\n</code></pre>"},{"location":"factory/autoregressive/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 0</li> <li>Imports: 8</li> </ul>"},{"location":"factory/core/","title":"Core","text":"<p>Module: <code>generative_models.factory.core</code></p> <p>Source: <code>generative_models/factory/core.py</code></p>"},{"location":"factory/core/#overview","title":"Overview","text":"<p>Core factory implementation.</p>"},{"location":"factory/core/#classes","title":"Classes","text":""},{"location":"factory/core/#modelfactory","title":"ModelFactory","text":"<pre><code>class ModelFactory\n</code></pre>"},{"location":"factory/core/#functions","title":"Functions","text":""},{"location":"factory/core/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"factory/core/#create","title":"create","text":"<pre><code>def create()\n</code></pre>"},{"location":"factory/core/#create_model","title":"create_model","text":"<pre><code>def create_model()\n</code></pre>"},{"location":"factory/core/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 3</li> <li>Imports: 12</li> </ul>"},{"location":"factory/diffusion/","title":"Diffusion","text":"<p>Module: <code>generative_models.factory.builders.diffusion</code></p> <p>Source: <code>generative_models/factory/builders/diffusion.py</code></p>"},{"location":"factory/diffusion/#overview","title":"Overview","text":"<p>Diffusion model builder.</p>"},{"location":"factory/diffusion/#classes","title":"Classes","text":""},{"location":"factory/diffusion/#diffusionbuilder","title":"DiffusionBuilder","text":"<pre><code>class DiffusionBuilder\n</code></pre>"},{"location":"factory/diffusion/#functions","title":"Functions","text":""},{"location":"factory/diffusion/#create_backbone","title":"create_backbone","text":"<pre><code>def create_backbone()\n</code></pre>"},{"location":"factory/diffusion/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 1</li> <li>Imports: 5</li> </ul>"},{"location":"factory/ebm/","title":"Ebm","text":"<p>Module: <code>generative_models.factory.builders.ebm</code></p> <p>Source: <code>generative_models/factory/builders/ebm.py</code></p>"},{"location":"factory/ebm/#overview","title":"Overview","text":"<p>Energy-based model builder.</p>"},{"location":"factory/ebm/#classes","title":"Classes","text":""},{"location":"factory/ebm/#ebmbuilder","title":"EBMBuilder","text":"<pre><code>class EBMBuilder\n</code></pre>"},{"location":"factory/ebm/#functions","title":"Functions","text":""},{"location":"factory/ebm/#build","title":"build","text":"<pre><code>def build()\n</code></pre>"},{"location":"factory/ebm/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 1</li> <li>Imports: 6</li> </ul>"},{"location":"factory/flow/","title":"Flow","text":"<p>Module: <code>generative_models.factory.builders.flow</code></p> <p>Source: <code>generative_models/factory/builders/flow.py</code></p>"},{"location":"factory/flow/#overview","title":"Overview","text":"<p>Normalizing flow model builder.</p>"},{"location":"factory/flow/#classes","title":"Classes","text":""},{"location":"factory/flow/#flowbuilder","title":"FlowBuilder","text":"<pre><code>class FlowBuilder\n</code></pre>"},{"location":"factory/flow/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 0</li> <li>Imports: 4</li> </ul>"},{"location":"factory/gan/","title":"Gan","text":"<p>Module: <code>generative_models.factory.builders.gan</code></p> <p>Source: <code>generative_models/factory/builders/gan.py</code></p>"},{"location":"factory/gan/#overview","title":"Overview","text":"<p>GAN model builder.</p>"},{"location":"factory/gan/#classes","title":"Classes","text":""},{"location":"factory/gan/#ganbuilder","title":"GANBuilder","text":"<pre><code>class GANBuilder\n</code></pre>"},{"location":"factory/gan/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 0</li> <li>Imports: 4</li> </ul>"},{"location":"factory/geometric/","title":"Geometric","text":"<p>Module: <code>generative_models.factory.builders.geometric</code></p> <p>Source: <code>generative_models/factory/builders/geometric.py</code></p>"},{"location":"factory/geometric/#overview","title":"Overview","text":"<p>Geometric model builder.</p>"},{"location":"factory/geometric/#classes","title":"Classes","text":""},{"location":"factory/geometric/#geometricbuilder","title":"GeometricBuilder","text":"<pre><code>class GeometricBuilder\n</code></pre>"},{"location":"factory/geometric/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 0</li> <li>Imports: 8</li> </ul>"},{"location":"factory/registry/","title":"Registry","text":"<p>Module: <code>generative_models.factory.registry</code></p> <p>Source: <code>generative_models/factory/registry.py</code></p>"},{"location":"factory/registry/#overview","title":"Overview","text":"<p>Model type registry for factory system.</p>"},{"location":"factory/registry/#classes","title":"Classes","text":""},{"location":"factory/registry/#basemodelbuilder","title":"BaseModelBuilder","text":"<pre><code>class BaseModelBuilder\n</code></pre>"},{"location":"factory/registry/#buildernotfounderror","title":"BuilderNotFoundError","text":"<pre><code>class BuilderNotFoundError\n</code></pre>"},{"location":"factory/registry/#duplicatebuildererror","title":"DuplicateBuilderError","text":"<pre><code>class DuplicateBuilderError\n</code></pre>"},{"location":"factory/registry/#modelbuilder","title":"ModelBuilder","text":"<pre><code>class ModelBuilder\n</code></pre>"},{"location":"factory/registry/#modeltyperegistry","title":"ModelTypeRegistry","text":"<pre><code>class ModelTypeRegistry\n</code></pre>"},{"location":"factory/registry/#functions","title":"Functions","text":""},{"location":"factory/registry/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"factory/registry/#build","title":"build","text":"<pre><code>def build()\n</code></pre>"},{"location":"factory/registry/#build_1","title":"build","text":"<pre><code>def build()\n</code></pre>"},{"location":"factory/registry/#clear","title":"clear","text":"<pre><code>def clear()\n</code></pre>"},{"location":"factory/registry/#discover_builders","title":"discover_builders","text":"<pre><code>def discover_builders()\n</code></pre>"},{"location":"factory/registry/#get_builder","title":"get_builder","text":"<pre><code>def get_builder()\n</code></pre>"},{"location":"factory/registry/#list_builders","title":"list_builders","text":"<pre><code>def list_builders()\n</code></pre>"},{"location":"factory/registry/#register","title":"register","text":"<pre><code>def register()\n</code></pre>"},{"location":"factory/registry/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 5</li> <li>Functions: 8</li> <li>Imports: 6</li> </ul>"},{"location":"factory/vae/","title":"Vae","text":"<p>Module: <code>generative_models.factory.builders.vae</code></p> <p>Source: <code>generative_models/factory/builders/vae.py</code></p>"},{"location":"factory/vae/#overview","title":"Overview","text":"<p>VAE model builder.</p>"},{"location":"factory/vae/#classes","title":"Classes","text":""},{"location":"factory/vae/#vaebuilder","title":"VAEBuilder","text":"<pre><code>class VAEBuilder\n</code></pre>"},{"location":"factory/vae/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 0</li> <li>Imports: 7</li> </ul>"},{"location":"fine_tuning/","title":"Fine-Tuning","text":"<p>Comprehensive fine-tuning infrastructure for adapting pre-trained generative models, including parameter-efficient methods, knowledge distillation, and reinforcement learning from human feedback.</p>"},{"location":"fine_tuning/#overview","title":"Overview","text":"<ul> <li> <p> Parameter-Efficient Adapters</p> <p>LoRA, Prefix Tuning, and Prompt Tuning for efficient adaptation</p> </li> <li> <p> Knowledge Distillation</p> <p>Transfer knowledge from large to small models</p> </li> <li> <p> Reinforcement Learning</p> <p>RLHF, DPO, and PPO for alignment training</p> </li> <li> <p> Transfer Learning</p> <p>Few-shot and domain adaptation techniques</p> </li> </ul>"},{"location":"fine_tuning/#quick-start","title":"Quick Start","text":""},{"location":"fine_tuning/#lora-fine-tuning","title":"LoRA Fine-Tuning","text":"<pre><code>from artifex.fine_tuning.adapters import LoRAAdapter\n\n# Create LoRA adapter\nadapter = LoRAAdapter(\n    rank=8,\n    alpha=16,\n    dropout=0.1,\n    target_modules=[\"query\", \"value\"],\n)\n\n# Apply to model\nadapted_model = adapter.apply(pretrained_model)\n\n# Train with frozen base weights\ntrainer.train(adapted_model, train_data)\n</code></pre>"},{"location":"fine_tuning/#dpo-training","title":"DPO Training","text":"<pre><code>from artifex.fine_tuning.rl import DPOTrainer\n\n# Create DPO trainer\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=reference_model,\n    beta=0.1,\n)\n\n# Train on preference data\ntrainer.train(preference_dataset)\n</code></pre>"},{"location":"fine_tuning/#parameter-efficient-adapters","title":"Parameter-Efficient Adapters","text":""},{"location":"fine_tuning/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<p>Efficient fine-tuning with low-rank matrix decomposition.</p> <pre><code>from artifex.fine_tuning.adapters import LoRAAdapter, LoRAConfig\n\nconfig = LoRAConfig(\n    rank=8,           # Rank of low-rank matrices\n    alpha=16,         # Scaling factor\n    dropout=0.1,      # Dropout rate\n    target_modules=[  # Modules to adapt\n        \"attention.query\",\n        \"attention.value\",\n        \"mlp.dense\",\n    ],\n)\n\nadapter = LoRAAdapter(config)\nadapted_model = adapter.apply(model)\n\n# Get trainable parameters only\ntrainable_params = adapter.get_trainable_params()\nprint(f\"Trainable: {trainable_params:,} params\")\n</code></pre> <p> LoRA Reference</p>"},{"location":"fine_tuning/#prefix-tuning","title":"Prefix Tuning","text":"<p>Learn continuous task-specific prefixes.</p> <pre><code>from artifex.fine_tuning.adapters import PrefixTuning, PrefixConfig\n\nconfig = PrefixConfig(\n    prefix_length=20,      # Number of prefix tokens\n    num_layers=12,         # Layers to add prefixes\n    hidden_dim=768,        # Prefix hidden dimension\n    init_method=\"random\",  # Initialization method\n)\n\nprefix_tuner = PrefixTuning(config)\nadapted_model = prefix_tuner.apply(model)\n</code></pre> <p> Prefix Tuning Reference</p>"},{"location":"fine_tuning/#prompt-tuning","title":"Prompt Tuning","text":"<p>Learn soft prompts for task adaptation.</p> <pre><code>from artifex.fine_tuning.adapters import PromptTuning, PromptConfig\n\nconfig = PromptConfig(\n    num_tokens=10,           # Number of learnable tokens\n    init_from_vocab=True,    # Initialize from vocabulary\n    init_text=\"Generate:\",   # Text for initialization\n)\n\nprompt_tuner = PromptTuning(config)\nadapted_model = prompt_tuner.apply(model)\n</code></pre> <p> Prompt Tuning Reference</p>"},{"location":"fine_tuning/#fine-tuning-methods","title":"Fine-Tuning Methods","text":""},{"location":"fine_tuning/#knowledge-distillation","title":"Knowledge Distillation","text":"<p>Transfer knowledge from teacher to student models.</p> <pre><code>from artifex.fine_tuning import DistillationTrainer, DistillationConfig\n\nconfig = DistillationConfig(\n    temperature=4.0,       # Softmax temperature\n    alpha=0.5,             # Balance between hard/soft labels\n    loss_type=\"kl_div\",    # Distillation loss type\n)\n\ntrainer = DistillationTrainer(\n    student=small_model,\n    teacher=large_model,\n    config=config,\n)\n\ntrainer.train(train_data)\n</code></pre> <p> Distillation Reference</p>"},{"location":"fine_tuning/#few-shot-learning","title":"Few-Shot Learning","text":"<p>Adapt models with limited examples.</p> <pre><code>from artifex.fine_tuning import FewShotTrainer, FewShotConfig\n\nconfig = FewShotConfig(\n    n_ways=5,              # Number of classes\n    n_shots=5,             # Examples per class\n    n_queries=15,          # Query examples per class\n    meta_batch_size=4,     # Tasks per batch\n)\n\ntrainer = FewShotTrainer(model, config)\ntrainer.train(support_set, query_set)\n</code></pre> <p> Few-Shot Reference</p>"},{"location":"fine_tuning/#transfer-learning","title":"Transfer Learning","text":"<p>Transfer pre-trained models to new domains.</p> <pre><code>from artifex.fine_tuning import TransferTrainer, TransferConfig\n\nconfig = TransferConfig(\n    freeze_encoder=True,   # Freeze feature extractor\n    new_head=True,         # Add new classification head\n    layer_wise_lr={        # Layer-wise learning rates\n        \"encoder\": 1e-5,\n        \"decoder\": 1e-4,\n        \"head\": 1e-3,\n    },\n)\n\ntrainer = TransferTrainer(pretrained_model, config)\ntrainer.train(target_dataset)\n</code></pre> <p> Transfer Learning Reference</p>"},{"location":"fine_tuning/#reinforcement-learning","title":"Reinforcement Learning","text":""},{"location":"fine_tuning/#rlhf-reinforcement-learning-from-human-feedback","title":"RLHF (Reinforcement Learning from Human Feedback)","text":"<p>Align models with human preferences.</p> <pre><code>from artifex.fine_tuning.rl import RLHFTrainer, RLHFConfig\n\nconfig = RLHFConfig(\n    reward_model_path=\"reward_model.ckpt\",\n    kl_coef=0.1,           # KL divergence coefficient\n    clip_range=0.2,        # PPO clip range\n    value_loss_coef=0.5,   # Value function loss weight\n)\n\ntrainer = RLHFTrainer(\n    policy_model=model,\n    reward_model=reward_model,\n    config=config,\n)\n\ntrainer.train(prompts_dataset)\n</code></pre> <p> RLHF Reference</p>"},{"location":"fine_tuning/#dpo-direct-preference-optimization","title":"DPO (Direct Preference Optimization)","text":"<p>Direct optimization on preference pairs without reward model.</p> <pre><code>from artifex.fine_tuning.rl import DPOTrainer, DPOConfig\n\nconfig = DPOConfig(\n    beta=0.1,              # Temperature parameter\n    label_smoothing=0.0,   # Label smoothing\n    loss_type=\"sigmoid\",   # Loss function type\n)\n\ntrainer = DPOTrainer(\n    model=model,\n    ref_model=reference_model,\n    config=config,\n)\n\n# Preference data format: (prompt, chosen, rejected)\ntrainer.train(preference_dataset)\n</code></pre> <p> DPO Reference</p>"},{"location":"fine_tuning/#ppo-proximal-policy-optimization","title":"PPO (Proximal Policy Optimization)","text":"<p>Policy gradient with clipped surrogate objective.</p> <pre><code>from artifex.fine_tuning.rl import PPOTrainer, PPOConfig\n\nconfig = PPOConfig(\n    clip_range=0.2,        # Policy clip range\n    clip_range_vf=0.2,     # Value clip range\n    entropy_coef=0.01,     # Entropy bonus\n    vf_coef=0.5,           # Value function coefficient\n    max_grad_norm=0.5,     # Gradient clipping\n    n_epochs=4,            # PPO epochs per batch\n)\n\ntrainer = PPOTrainer(\n    model=model,\n    reward_model=reward_model,\n    config=config,\n)\n\ntrainer.train(prompts_dataset)\n</code></pre> <p> PPO Reference</p>"},{"location":"fine_tuning/#best-practices","title":"Best Practices","text":""},{"location":"fine_tuning/#choosing-an-adapter","title":"Choosing an Adapter","text":"Method Parameters Memory Best For LoRA ~0.1-1% Low General fine-tuning Prefix Tuning ~0.1% Low Sequence tasks Prompt Tuning ~0.01% Very Low Few-shot adaptation"},{"location":"fine_tuning/#training-tips","title":"Training Tips","text":"<pre><code># 1. Start with smaller rank for LoRA\nlora_config = LoRAConfig(rank=4, alpha=8)\n\n# 2. Use learning rate warmup\nscheduler = create_scheduler(\n    \"cosine\",\n    warmup_steps=100,\n    total_steps=10000,\n)\n\n# 3. Monitor KL divergence in RLHF\ncallbacks = [\n    KLDivergenceCallback(threshold=0.1),\n]\n\n# 4. Save adapter weights separately\nadapter.save_weights(\"adapter_weights.ckpt\")\n</code></pre>"},{"location":"fine_tuning/#module-reference","title":"Module Reference","text":"Category Modules Adapters lora, prefix_tuning, prompt_tuning Fine-Tuning distillation, few_shot, transfer RL dpo, ppo, rlhf"},{"location":"fine_tuning/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Guide - Complete training guide</li> <li>RL Training Guide - RL fine-tuning details</li> <li>Training Systems - Training infrastructure</li> </ul>"},{"location":"fine_tuning/distillation/","title":"Distillation","text":"<p>Module: <code>generative_models.fine_tuning.distillation</code></p> <p>Source: <code>generative_models/fine_tuning/distillation.py</code></p>"},{"location":"fine_tuning/distillation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"fine_tuning/dpo/","title":"Dpo","text":"<p>Module: <code>generative_models.fine_tuning.rl.dpo</code></p> <p>Source: <code>generative_models/fine_tuning/rl/dpo.py</code></p>"},{"location":"fine_tuning/dpo/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"fine_tuning/few_shot/","title":"Few Shot","text":"<p>Module: <code>generative_models.fine_tuning.few_shot</code></p> <p>Source: <code>generative_models/fine_tuning/few_shot.py</code></p>"},{"location":"fine_tuning/few_shot/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"fine_tuning/lora/","title":"Lora","text":"<p>Module: <code>generative_models.fine_tuning.adapters.lora</code></p> <p>Source: <code>generative_models/fine_tuning/adapters/lora.py</code></p>"},{"location":"fine_tuning/lora/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"fine_tuning/ppo/","title":"Ppo","text":"<p>Module: <code>generative_models.fine_tuning.rl.ppo</code></p> <p>Source: <code>generative_models/fine_tuning/rl/ppo.py</code></p>"},{"location":"fine_tuning/ppo/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"fine_tuning/prefix_tuning/","title":"Prefix Tuning","text":"<p>Module: <code>generative_models.fine_tuning.adapters.prefix_tuning</code></p> <p>Source: <code>generative_models/fine_tuning/adapters/prefix_tuning.py</code></p>"},{"location":"fine_tuning/prefix_tuning/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"fine_tuning/prompt_tuning/","title":"Prompt Tuning","text":"<p>Module: <code>generative_models.fine_tuning.adapters.prompt_tuning</code></p> <p>Source: <code>generative_models/fine_tuning/adapters/prompt_tuning.py</code></p>"},{"location":"fine_tuning/prompt_tuning/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"fine_tuning/rlhf/","title":"Rlhf","text":"<p>Module: <code>generative_models.fine_tuning.rl.rlhf</code></p> <p>Source: <code>generative_models/fine_tuning/rl/rlhf.py</code></p>"},{"location":"fine_tuning/rlhf/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"fine_tuning/transfer/","title":"Transfer","text":"<p>Module: <code>generative_models.fine_tuning.transfer</code></p> <p>Source: <code>generative_models/fine_tuning/transfer.py</code></p>"},{"location":"fine_tuning/transfer/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"generative_models/","title":"Generative Models","text":"<p>The main module containing all generative model implementations, core infrastructure, modalities, extensions, and training systems in Artifex.</p>"},{"location":"generative_models/#overview","title":"Overview","text":"<ul> <li> <p> Model Architectures</p> <p>VAE, GAN, Diffusion, Flow, EBM, and Autoregressive models</p> </li> <li> <p> Core Infrastructure</p> <p>Configuration, losses, distributions, sampling, and metrics</p> </li> <li> <p> Modalities</p> <p>Image, text, audio, protein, and multimodal support</p> </li> <li> <p> Extensions</p> <p>Domain-specific extensions for proteins, geometric data, and more</p> </li> </ul>"},{"location":"generative_models/#quick-start","title":"Quick Start","text":""},{"location":"generative_models/#creating-a-model","title":"Creating a Model","text":"<pre><code>from artifex.generative_models.factories import create_model\nfrom artifex.generative_models.core.configuration import VAEConfig\nfrom flax import nnx\n\n# Create VAE model\nrngs = nnx.Rngs(0)\nconfig = VAEConfig(\n    name=\"my_vae\",\n    latent_dim=64,\n    encoder_hidden_dims=[256, 128],\n    decoder_hidden_dims=[128, 256],\n)\n\nmodel = create_model(\"vae\", config=config, rngs=rngs)\n</code></pre>"},{"location":"generative_models/#training-a-model","title":"Training a Model","text":"<pre><code>from artifex.generative_models.training import VAETrainer\n\ntrainer = VAETrainer(\n    model=model,\n    config=training_config,\n    train_dataset=train_data,\n)\n\ntrainer.train()\n</code></pre>"},{"location":"generative_models/#generating-samples","title":"Generating Samples","text":"<pre><code>from artifex.inference import InferencePipeline\n\npipeline = InferencePipeline(model)\nsamples = pipeline.generate(num_samples=16)\n</code></pre>"},{"location":"generative_models/#module-structure","title":"Module Structure","text":"<p>The <code>generative_models</code> package is organized into the following submodules:</p>"},{"location":"generative_models/#models","title":"Models","text":"<p>Implementation of all generative model architectures.</p> Model Type Description VAE Variational Autoencoders GAN Generative Adversarial Networks Diffusion Denoising Diffusion Models Flow Normalizing Flow Models EBM Energy-Based Models Autoregressive Autoregressive Models <p> Models Reference</p>"},{"location":"generative_models/#core","title":"Core","text":"<p>Foundational abstractions and utilities.</p> Component Description Configuration Unified configuration system Losses Loss functions Distributions Probability distributions Sampling Sampling methods Metrics Evaluation metrics Layers Neural network layers <p> Core Reference</p>"},{"location":"generative_models/#modalities","title":"Modalities","text":"<p>Data modality implementations.</p> Modality Description Base Base modality classes Adapters Model adapters for modalities Evaluation Modality evaluation metrics Datasets Dataset utilities Representations Feature representations <p> Modalities Reference</p>"},{"location":"generative_models/#training","title":"Training","text":"<p>Training infrastructure and utilities.</p> Component Description VAE Trainer VAE model trainer GAN Trainer GAN model trainer Diffusion Trainer Diffusion model trainer Checkpoint Training checkpointing Data Parallel Multi-device training AdamW Optimization algorithms Scheduler Learning rate schedules <p> Training Reference</p>"},{"location":"generative_models/#extensions","title":"Extensions","text":"<p>Domain-specific extensions.</p> Extension Description Extensions Extension system overview Backbone Model backbones Embeddings Embedding layers Features Feature processing Registry Extension registry <p> Extensions Reference</p>"},{"location":"generative_models/#factories","title":"Factories","text":"<p>Model creation and registration.</p> <pre><code>from artifex.generative_models.factories import (\n    create_model,\n    register_model,\n    list_models,\n)\n\n# List available models\navailable = list_models()\n# ['vae', 'beta_vae', 'vq_vae', 'gan', 'wgan', ...]\n\n# Create model by name\nmodel = create_model(\"vae\", config=config, rngs=rngs)\n\n# Register custom model\nregister_model(\"my_model\", MyModelClass)\n</code></pre> <p> Factory Reference</p> <p>See also: VAE Factory | GAN Factory | Diffusion Factory | Flow Factory</p>"},{"location":"generative_models/#architecture","title":"Architecture","text":"<pre><code>generative_models/\n\u251c\u2500\u2500 core/                 # Core infrastructure\n\u2502   \u251c\u2500\u2500 configuration/    # Configuration system\n\u2502   \u251c\u2500\u2500 losses/           # Loss functions\n\u2502   \u251c\u2500\u2500 distributions/    # Probability distributions\n\u2502   \u251c\u2500\u2500 sampling/         # Sampling methods\n\u2502   \u251c\u2500\u2500 metrics/          # Evaluation metrics\n\u2502   \u2514\u2500\u2500 layers/           # Neural network layers\n\u251c\u2500\u2500 models/               # Model implementations\n\u2502   \u251c\u2500\u2500 vae/              # VAE variants\n\u2502   \u251c\u2500\u2500 gan/              # GAN variants\n\u2502   \u251c\u2500\u2500 diffusion/        # Diffusion models\n\u2502   \u251c\u2500\u2500 flow/             # Flow models\n\u2502   \u251c\u2500\u2500 ebm/              # Energy-based models\n\u2502   \u2514\u2500\u2500 autoregressive/   # Autoregressive models\n\u251c\u2500\u2500 modalities/           # Data modality support\n\u2502   \u251c\u2500\u2500 image/            # Image modality\n\u2502   \u251c\u2500\u2500 text/             # Text modality\n\u2502   \u251c\u2500\u2500 audio/            # Audio modality\n\u2502   \u251c\u2500\u2500 protein/          # Protein modality\n\u2502   \u2514\u2500\u2500 multi_modal/      # Multimodal support\n\u251c\u2500\u2500 training/             # Training infrastructure\n\u2502   \u251c\u2500\u2500 trainers/         # Model trainers\n\u2502   \u251c\u2500\u2500 callbacks/        # Training callbacks\n\u2502   \u251c\u2500\u2500 distributed/      # Distributed training\n\u2502   \u2514\u2500\u2500 optimizers/       # Optimizers and schedulers\n\u251c\u2500\u2500 extensions/           # Domain extensions\n\u2502   \u251c\u2500\u2500 protein/          # Protein modeling\n\u2502   \u2514\u2500\u2500 geometric/        # Geometric deep learning\n\u2514\u2500\u2500 factories/            # Model creation\n    \u251c\u2500\u2500 model_factory.py  # Model factory\n    \u2514\u2500\u2500 registry.py       # Model registry\n</code></pre>"},{"location":"generative_models/#design-principles","title":"Design Principles","text":""},{"location":"generative_models/#1-protocol-based-interfaces","title":"1. Protocol-Based Interfaces","text":"<p>All components use Python Protocols for type-safe interfaces:</p> <pre><code>from artifex.generative_models.core.protocols import GenerativeModel\n\nclass MyModel(GenerativeModel):\n    def generate(self, num_samples: int, **kwargs) -&gt; jax.Array:\n        ...\n\n    def loss_fn(self, batch: jax.Array, **kwargs) -&gt; jax.Array:\n        ...\n</code></pre>"},{"location":"generative_models/#2-unified-configuration","title":"2. Unified Configuration","text":"<p>All models use the unified configuration system:</p> <pre><code>from artifex.generative_models.core.configuration import VAEConfig\n\nconfig = VAEConfig(\n    name=\"my_vae\",\n    latent_dim=64,\n    # Type-safe, validated configuration\n)\n</code></pre>"},{"location":"generative_models/#3-modality-agnostic-design","title":"3. Modality-Agnostic Design","text":"<p>Models work with any data modality through adapters:</p> <pre><code>from artifex.generative_models.modalities import get_modality\n\n# Get modality handler\nimage_modality = get_modality(\"image\", rngs=rngs)\n\n# Adapt model for modality\nadapted_model = image_modality.get_adapter(\"vae\").adapt(model)\n</code></pre>"},{"location":"generative_models/#4-hardware-aware","title":"4. Hardware-Aware","text":"<p>All components are hardware-aware with automatic device management:</p> <pre><code>from artifex.generative_models.core import DeviceManager\n\ndevice_manager = DeviceManager()\ndevice = device_manager.get_device()  # Auto-selects GPU/CPU\n</code></pre>"},{"location":"generative_models/#related-documentation","title":"Related Documentation","text":"<ul> <li>Models Reference - Model implementations</li> <li>Core Reference - Core infrastructure</li> <li>Modalities Reference - Data modalities</li> <li>Training Reference - Training systems</li> <li>Factory Reference - Model creation factories</li> </ul>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>This guide introduces the fundamental concepts behind Artifex and generative modeling. Understanding these concepts will help you make the most of the library.</p>"},{"location":"getting-started/core-concepts/#what-is-generative-modeling","title":"What is Generative Modeling?","text":"<p>Generative modeling is about learning probability distributions from data and generating new samples from those distributions.</p>"},{"location":"getting-started/core-concepts/#the-core-problem","title":"The Core Problem","text":"<p>Given a dataset \\(\\mathcal{D} = \\{x_1, x_2, ..., x_N\\}\\), we want to:</p> <ol> <li>Learn the underlying data distribution \\(p(x)\\)</li> <li>Generate new samples \\(\\tilde{x} \\sim p(x)\\) that look like the training data</li> <li>Evaluate the quality of generated samples</li> </ol>"},{"location":"getting-started/core-concepts/#why-generative-models","title":"Why Generative Models?","text":"<ul> <li> <p> Image Generation</p> <p>Create realistic images, art, faces, or any visual content</p> </li> <li> <p> Data Augmentation</p> <p>Generate synthetic training data to improve discriminative models</p> </li> <li> <p> Representation Learning</p> <p>Learn meaningful latent representations of data</p> </li> <li> <p> Anomaly Detection</p> <p>Identify outliers by measuring likelihood under the learned distribution</p> </li> <li> <p> Content Creation</p> <p>Generate text, audio, video, 3D shapes, and more</p> </li> <li> <p> Scientific Discovery</p> <p>Generate molecules, proteins, materials for drug design and research</p> </li> </ul>"},{"location":"getting-started/core-concepts/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/core-concepts/#1-probability-distribution","title":"1. Probability Distribution","text":"<p>A probability distribution \\(p(x)\\) assigns probabilities to different outcomes:</p> <ul> <li>Discrete: \\(p(x) \\in [0, 1]\\) for each \\(x\\), \\(\\sum_x p(x) = 1\\)</li> <li>Continuous: \\(p(x) \\geq 0\\), \\(\\int p(x) dx = 1\\)</li> </ul> <p>Goal: Learn \\(p(x)\\) from data so we can sample new \\(x \\sim p(x)\\).</p>"},{"location":"getting-started/core-concepts/#2-likelihood","title":"2. Likelihood","text":"<p>The likelihood \\(p(x|\\theta)\\) measures how probable data \\(x\\) is under model parameters \\(\\theta\\).</p> <p>Maximum Likelihood Estimation (MLE):</p> \\[ \\theta^* = \\arg\\max_\\theta \\sum_{i=1}^N \\log p(x_i | \\theta) \\] <p>Models with tractable likelihoods (e.g., Flows, Autoregressive) directly optimize this.</p>"},{"location":"getting-started/core-concepts/#3-latent-variables","title":"3. Latent Variables","text":"<p>Latent variables \\(z\\) are unobserved variables that capture underlying structure:</p> \\[ p(x) = \\int p(x|z) p(z) dz \\] <ul> <li>\\(p(z)\\): Prior distribution (usually standard normal)</li> <li>\\(p(x|z)\\): Likelihood (decoder/generator)</li> <li>\\(p(z|x)\\): Posterior (encoder/inference network)</li> </ul> <p>Examples:</p> <ul> <li>VAE: Continuous latent space</li> <li>VQ-VAE: Discrete latent codebook</li> <li>Diffusion: Noisy latent trajectory</li> </ul>"},{"location":"getting-started/core-concepts/#4-sampling","title":"4. Sampling","text":"<p>Generating new data from the learned distribution:</p> <p>Ancestral Sampling: Sample from the prior \\(z \\sim p(z)\\), then generate \\(x \\sim p(x|z)\\)</p> <p>MCMC Sampling: Use Markov chains to sample from complex distributions</p> <p>Diffusion Sampling: Iteratively denoise from pure noise to data</p>"},{"location":"getting-started/core-concepts/#5-encoder-decoder-architecture","title":"5. Encoder-Decoder Architecture","text":"<pre><code>graph LR\n    X[Data x] --&gt;|Encode| Z[Latent z]\n    Z --&gt;|Decode| XR[Reconstructed x']\n\n    style X fill:#e1f5ff\n    style Z fill:#fff4e1\n    style XR fill:#e8f5e9</code></pre> <ul> <li>Encoder: Maps data to latent space \\(q(z|x)\\)</li> <li>Decoder: Maps latent to data space \\(p(x|z)\\)</li> <li>Latent Space: Compressed, structured representation</li> </ul>"},{"location":"getting-started/core-concepts/#generative-model-types","title":"Generative Model Types","text":"<p>Artifex supports six main types of generative models, each with different trade-offs:</p>"},{"location":"getting-started/core-concepts/#1-variational-autoencoders-vae","title":"1. Variational Autoencoders (VAE)","text":"<p>Idea: Learn latent representations with probabilistic encoding/decoding</p> <p>Key Equation: Evidence Lower Bound (ELBO)</p> \\[ \\mathcal{L}_{ELBO} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\text{KL}(q(z|x) || p(z)) \\] <p>Architecture:</p> <pre><code>graph TD\n    X[Input x] --&gt; E[Encoder q]\n    E --&gt; M[Mean \u03bc]\n    E --&gt; S[Std \u03c3]\n    M --&gt; R[Reparameterize]\n    S --&gt; R\n    R --&gt; Z[Latent z]\n    Z --&gt; D[Decoder p]\n    D --&gt; XR[Output x']</code></pre> <p>Pros:</p> <ul> <li>Stable training</li> <li>Interpretable latent space</li> <li>Fast sampling</li> </ul> <p>Cons:</p> <ul> <li>Lower sample quality compared to GANs/Diffusion</li> <li>Posterior approximation may be limited</li> </ul> <p>Use Cases: Data compression, latent space exploration, representation learning</p> <p>Artifex Example:</p> <pre><code>from flax import nnx\nfrom artifex.generative_models.models.vae import VAE\nfrom artifex.generative_models.core.configuration import VAEConfig, EncoderConfig, DecoderConfig\n\nencoder = EncoderConfig(name=\"encoder\", input_shape=(28, 28, 1), latent_dim=128, hidden_dims=(256, 128), activation=\"relu\")\ndecoder = DecoderConfig(name=\"decoder\", latent_dim=128, output_shape=(28, 28, 1), hidden_dims=(128, 256), activation=\"relu\")\nconfig = VAEConfig(name=\"beta_vae\", encoder=encoder, decoder=decoder, encoder_type=\"dense\", kl_weight=1.0)\nrngs = nnx.Rngs(0)\nmodel = VAE(config, rngs=rngs)\n</code></pre>"},{"location":"getting-started/core-concepts/#2-generative-adversarial-networks-gan","title":"2. Generative Adversarial Networks (GAN)","text":"<p>Idea: Train generator and discriminator in adversarial game</p> <p>Key Equation: Minimax objective</p> \\[ \\min_G \\max_D \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1 - D(G(z)))] \\] <p>Architecture:</p> <pre><code>graph LR\n    Z[Noise z] --&gt; G[Generator G]\n    G --&gt; XF[Fake x']\n    XR[Real x] --&gt; D[Discriminator D]\n    XF --&gt; D\n    D --&gt; R[Real/Fake]\n\n    style G fill:#fff4e1\n    style D fill:#ffe1f5</code></pre> <p>Pros:</p> <ul> <li>Highest sample quality</li> <li>No explicit likelihood needed</li> <li>Mode coverage (with proper training)</li> </ul> <p>Cons:</p> <ul> <li>Training instability (mode collapse, vanishing gradients)</li> <li>Hyperparameter sensitive</li> <li>No direct likelihood evaluation</li> </ul> <p>Use Cases: High-quality image generation, style transfer, image-to-image translation</p> <p>Artifex Example:</p> <pre><code>from flax import nnx\nfrom artifex.generative_models.models.gan import DCGAN\nfrom artifex.generative_models.core.configuration import (\n    DCGANConfig, ConvGeneratorConfig, ConvDiscriminatorConfig\n)\n\ngenerator = ConvGeneratorConfig(\n    name=\"generator\", latent_dim=100, hidden_dims=(512, 256, 128, 64),\n    output_shape=(1, 28, 28), kernel_size=(4, 4), stride=(2, 2), padding=\"SAME\",\n    activation=\"relu\"\n)\ndiscriminator = ConvDiscriminatorConfig(\n    name=\"discriminator\", hidden_dims=(64, 128, 256, 512),\n    input_shape=(1, 28, 28), kernel_size=(4, 4), stride=(2, 2), padding=\"SAME\",\n    activation=\"leaky_relu\"\n)\nconfig = DCGANConfig(name=\"dcgan\", generator=generator, discriminator=discriminator)\nrngs = nnx.Rngs(params=0, dropout=1, sample=2)\nmodel = DCGAN(config, rngs=rngs)\n</code></pre>"},{"location":"getting-started/core-concepts/#3-diffusion-models","title":"3. Diffusion Models","text":"<p>Idea: Learn to denoise data through iterative refinement</p> <p>Forward Process: Add noise gradually</p> \\[ q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I) \\] <p>Reverse Process: Learn to denoise</p> \\[ p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) \\] <p>Architecture:</p> <pre><code>graph LR\n    X0[Clean x\u2080] --&gt;|Add noise| X1[Noisy x\u2081]\n    X1 --&gt;|Add noise| X2[Noisy x\u2082]\n    X2 --&gt;|...| XT[Pure noise x\u209c]\n    XT --&gt;|Denoise| X2R[x\u2082]\n    X2R --&gt;|Denoise| X1R[x\u2081]\n    X1R --&gt;|Denoise| X0R[Clean x\u2080]\n\n    style X0 fill:#e8f5e9\n    style XT fill:#ffebee\n    style X0R fill:#e8f5e9</code></pre> <p>Pros:</p> <ul> <li>State-of-the-art sample quality</li> <li>Stable training</li> <li>Flexible conditioning and guidance</li> </ul> <p>Cons:</p> <ul> <li>Slow sampling (many steps)</li> <li>Memory intensive</li> <li>Training time</li> </ul> <p>Use Cases: Image/audio generation, inpainting, super-resolution, conditional generation</p> <p>Artifex Example:</p> <pre><code>from flax import nnx\nfrom artifex.generative_models.models.diffusion import DDPMModel\nfrom artifex.generative_models.core.configuration import DDPMConfig, UNetBackboneConfig, NoiseScheduleConfig\n\nbackbone = UNetBackboneConfig(name=\"backbone\", in_channels=1, out_channels=1, hidden_dims=(64, 128), channel_mult=(1, 2), activation=\"silu\")\nnoise_schedule = NoiseScheduleConfig(name=\"schedule\", schedule_type=\"cosine\", num_timesteps=1000)\nconfig = DDPMConfig(name=\"ddpm\", input_shape=(28, 28, 1), backbone=backbone, noise_schedule=noise_schedule)\nrngs = nnx.Rngs(0)\nmodel = DDPMModel(config, rngs=rngs)\n</code></pre>"},{"location":"getting-started/core-concepts/#4-normalizing-flows","title":"4. Normalizing Flows","text":"<p>Idea: Use invertible transformations with tractable Jacobians</p> <p>Key Property: Change of variables</p> \\[ p_X(x) = p_Z(f^{-1}(x)) \\left| \\det \\frac{\\partial f^{-1}}{\\partial x} \\right| \\] <p>Architecture:</p> <pre><code>graph LR\n    Z[Simple z] --&gt;|f| X[Complex x]\n    X --&gt;|f\u207b\u00b9| Z2[Simple z]\n\n    style Z fill:#e8f5e9\n    style X fill:#e1f5ff</code></pre> <p>Pros:</p> <ul> <li>Exact likelihood computation</li> <li>Invertible (can go both ways)</li> <li>Stable training</li> </ul> <p>Cons:</p> <ul> <li>Architecture constraints (must be invertible)</li> <li>May require many layers for expressiveness</li> </ul> <p>Use Cases: Density estimation, exact inference, anomaly detection</p> <p>Artifex Example:</p> <pre><code>from flax import nnx\nfrom artifex.generative_models.models.flow import RealNVP\nfrom artifex.generative_models.core.configuration import RealNVPConfig, CouplingNetworkConfig\n\ncoupling = CouplingNetworkConfig(name=\"coupling\", hidden_dims=(256, 256), activation=\"relu\")\nconfig = RealNVPConfig(name=\"realnvp\", input_dim=784, num_coupling_layers=8, coupling_network=coupling)\nrngs = nnx.Rngs(0)\nmodel = RealNVP(config, rngs=rngs)\n</code></pre>"},{"location":"getting-started/core-concepts/#5-energy-based-models-ebm","title":"5. Energy-Based Models (EBM)","text":"<p>Idea: Learn energy function, sample with MCMC</p> <p>Key Equation: Gibbs distribution</p> \\[ p(x) = \\frac{1}{Z} \\exp(-E(x)) \\] <p>where</p> \\[Z = \\int \\exp(-E(x)) dx\\] <p>is the partition function.</p> <p>Pros:</p> <ul> <li>Flexible, can model any distribution</li> <li>Composable (combine multiple EBMs)</li> </ul> <p>Cons:</p> <ul> <li>Expensive sampling (MCMC)</li> <li>Training complexity (contrastive divergence)</li> </ul> <p>Use Cases: Compositional generation, constraint satisfaction, hybrid models</p> <p>Artifex Example:</p> <pre><code>from flax import nnx\nfrom artifex.generative_models.models.energy import EBM\nfrom artifex.generative_models.core.configuration import EBMConfig, EnergyNetworkConfig, MCMCConfig\nfrom artifex.generative_models.core.configuration.energy_config import SampleBufferConfig\n\nenergy_network = EnergyNetworkConfig(name=\"energy_net\", hidden_dims=(256, 256), activation=\"swish\")\nmcmc = MCMCConfig(name=\"mcmc\", n_steps=60, step_size=0.01)\nsample_buffer = SampleBufferConfig(name=\"buffer\", capacity=10000)\nconfig = EBMConfig(name=\"ebm\", input_dim=784, energy_network=energy_network, mcmc=mcmc, sample_buffer=sample_buffer)\nrngs = nnx.Rngs(0)\nmodel = EBM(config, rngs=rngs)\n</code></pre>"},{"location":"getting-started/core-concepts/#6-autoregressive-models","title":"6. Autoregressive Models","text":"<p>Idea: Model distribution as product of conditionals</p> \\[ p(x) = \\prod_{i=1}^n p(x_i | x_{&lt;i}) \\] <p>Architecture:</p> <pre><code>graph LR\n    X1[x\u2081] --&gt;|p| X2[x\u2082]\n    X2 --&gt;|p| X3[x\u2083]\n    X3 --&gt;|...| XN[x\u2099]</code></pre> <p>Pros:</p> <ul> <li>Explicit likelihood</li> <li>Flexible architectures (Transformers, CNNs)</li> <li>Strong theoretical foundation</li> </ul> <p>Cons:</p> <ul> <li>Sequential generation (slow)</li> <li>Fixed ordering required</li> </ul> <p>Use Cases: Text generation, ordered sequences, explicit probability modeling</p> <p>Artifex Example:</p> <pre><code>from flax import nnx\nfrom artifex.generative_models.models.autoregressive import PixelCNN\nfrom artifex.generative_models.core.configuration import PixelCNNConfig\n\nconfig = PixelCNNConfig(name=\"pixelcnn\", image_shape=(28, 28, 1), hidden_channels=64, num_layers=8)\nrngs = nnx.Rngs(0)\nmodel = PixelCNN(config, rngs=rngs)\n</code></pre>"},{"location":"getting-started/core-concepts/#model-comparison-matrix","title":"Model Comparison Matrix","text":"Feature VAE GAN Diffusion Flow EBM Autoregressive Sample Quality \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Training Stability \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Sampling Speed \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50 Exact Likelihood \u274c \u274c \u274c \u2705 \u274c* \u2705 Latent Space \u2705 \u2705 \u274c \u2705 \u274c \u274c Mode Coverage \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 <p>*EBM has exact likelihood but intractable partition function</p>"},{"location":"getting-started/core-concepts/#artifex-architecture","title":"Artifex Architecture","text":""},{"location":"getting-started/core-concepts/#high-level-design","title":"High-Level Design","text":"<p>Artifex follows a modular, protocol-based design:</p> <pre><code>graph TB\n    subgraph User[\"User Interface\"]\n        Config[Configuration Classes]\n    end\n\n    subgraph Core[\"Core Components\"]\n        Protocols[Protocols &amp; Interfaces]\n        Device[Device Manager]\n        Loss[Loss Functions]\n    end\n\n    subgraph Models[\"Generative Models\"]\n        VAE[VAE]\n        GAN[GAN]\n        Diff[Diffusion]\n        Flow[Flow]\n        EBM[EBM]\n        AR[Autoregressive]\n    end\n\n    subgraph Training[\"Training System\"]\n        Trainer[Trainers]\n        Opt[Optimizers]\n        Callbacks[Callbacks]\n    end\n\n    Config --&gt; Models\n    Models --&gt; Training\n    Core --&gt; Models\n    Core --&gt; Training</code></pre>"},{"location":"getting-started/core-concepts/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Protocol-Based: Type-safe interfaces using Python Protocols</li> <li>Configuration-Driven: Pydantic-based unified configuration system</li> <li>Factory Pattern: Centralized model creation</li> <li>Hardware-Aware: Automatic GPU/CPU/TPU detection and optimization</li> <li>Modular: Composable components for flexibility</li> </ol>"},{"location":"getting-started/core-concepts/#configuration-system","title":"Configuration System","text":"<p>All models use a unified configuration class:</p> <pre><code>from artifex.generative_models.core.configuration import ModelConfig\n\nconfig = ModelConfig(\n    # Required fields\n    name=\"vae_experiment\",\n    model_class=\"artifex.generative_models.models.vae.VAE\",  # Fully qualified class name\n    input_dim=(28, 28, 1),         # Input dimensions (int or tuple)\n\n    # Architecture\n    hidden_dims=(256, 128, 64),    # Hidden layer sizes\n    output_dim=32,                 # Output/latent dimensions\n    activation=\"gelu\",             # Activation function\n\n    # Model-specific parameters (functional hyperparameters)\n    parameters={\n        \"beta\": 1.0,                # VAE-specific: \u03b2-VAE weight\n        \"kl_weight\": 1.0,           # KL divergence weight\n        \"reconstruction_loss\": \"mse\" # Reconstruction loss type\n    },\n\n    # Optional metadata (non-functional tracking info)\n    metadata={\n        \"experiment_id\": \"vae_001\",\n        \"dataset\": \"mnist\"\n    }\n)\n</code></pre> <p>Benefits:</p> <ul> <li>Type-safe with Pydantic validation</li> <li>Serializable (save/load configurations)</li> <li>Versioned for reproducibility</li> <li>Extensible for custom models</li> </ul>"},{"location":"getting-started/core-concepts/#device-management","title":"Device Management","text":"<p>Artifex automatically handles GPU/CPU/TPU:</p> <pre><code>from artifex.generative_models.core.device_manager import get_device_manager\nimport jax\n\n# Automatic device detection\nmanager = get_device_manager()\ninfo = manager.get_device_info()\nprint(f\"Using: {info['backend']}\")  # gpu, cpu, or tpu\nprint(f\"Device count: {len(jax.devices())}\")\n\n# Explicit configuration\nfrom artifex.generative_models.core.device_manager import configure_for_generative_models, MemoryStrategy\n\nmanager = configure_for_generative_models(\n    memory_strategy=MemoryStrategy.BALANCED,  # 75% GPU memory\n    enable_mixed_precision=True                # BF16/FP16\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#protocol-system","title":"Protocol System","text":"<p>Models implement the <code>GenerativeModelProtocol</code>:</p> <pre><code>from typing import Any, Protocol\nfrom flax import nnx\nimport jax\n\nclass GenerativeModelProtocol(Protocol):\n    \"\"\"Base protocol for all generative models.\"\"\"\n\n    def __call__(self, x: Any, *, rngs: nnx.Rngs | None = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Forward pass.\"\"\"\n        ...\n\n    def generate(self, n_samples: int = 1, *, rngs: nnx.Rngs | None = None, **kwargs) -&gt; jax.Array:\n        \"\"\"Generate samples from the model.\"\"\"\n        ...\n\n    def loss_fn(self, batch: Any, model_outputs: dict[str, Any], *, rngs: nnx.Rngs | None = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Compute loss.\"\"\"\n        ...\n\n    def sample(self, num_samples: int, **kwargs: Any) -&gt; jax.Array:\n        \"\"\"Generate samples (alias for generate).\"\"\"\n        ...\n</code></pre> <p>This enables:</p> <ul> <li>Type checking at development time</li> <li>Generic training loops that work with any model</li> <li>Consistent interfaces across model types</li> </ul>"},{"location":"getting-started/core-concepts/#jax-and-flax-nnx-basics","title":"JAX and Flax NNX Basics","text":"<p>Artifex is built on JAX and Flax NNX. Here are the key concepts:</p>"},{"location":"getting-started/core-concepts/#jax-functional-programming","title":"JAX: Functional Programming","text":"<p>JAX provides functional transformations:</p> <pre><code>import jax\nimport jax.numpy as jnp\n\n# JIT compilation for speed\n@jax.jit\ndef fast_function(x):\n    return jnp.sum(x ** 2)\n\n# Automatic differentiation\ndef loss_fn(params, x):\n    return jnp.sum((params['w'] * x) ** 2)\n\ngrad_fn = jax.grad(loss_fn)  # Get gradient function\ngradients = grad_fn(params, x)  # Compute gradients\n\n# Vectorization\nbatch_fn = jax.vmap(fast_function)  # Apply to batches\n</code></pre>"},{"location":"getting-started/core-concepts/#flax-nnx-object-oriented-neural-networks","title":"Flax NNX: Object-Oriented Neural Networks","text":"<p>Flax NNX provides a Pythonic API for neural networks:</p> <pre><code>from flax import nnx\n\nclass MyModel(nnx.Module):\n    def __init__(self, features: int, *, rngs: nnx.Rngs):\n        super().__init__()\n        self.dense1 = nnx.Linear(784, features, rngs=rngs)\n        self.dense2 = nnx.Linear(features, 10, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        x = self.dense1(x)\n        x = nnx.relu(x)\n        x = self.dense2(x)\n        return x\n\n# Create model\nrngs = nnx.Rngs(0)\nmodel = MyModel(features=128, rngs=rngs)\n\n# Use model\nx = jax.random.normal(jax.random.PRNGKey(0), (32, 784))\ny = model(x)\n</code></pre>"},{"location":"getting-started/core-concepts/#random-number-generation","title":"Random Number Generation","text":"<p>JAX requires explicit RNG management:</p> <pre><code>from flax import nnx\n\n# Create RNG\nrngs = nnx.Rngs(seed=42)\n\n# Use for model initialization\nmodel = MyModel(rngs=rngs)\n\n# Use for sampling\nsamples = jax.random.normal(rngs.sample(), (10, 784))\n</code></pre>"},{"location":"getting-started/core-concepts/#multi-modal-support","title":"Multi-Modal Support","text":"<p>Artifex supports multiple data modalities. Some datasets are currently implemented while others are planned for future releases:</p>"},{"location":"getting-started/core-concepts/#image","title":"Image","text":"<p>Coming Soon</p> <p>CIFAR10Dataset is planned but not yet implemented.</p> <pre><code># Planned API (not yet available)\nfrom artifex.data.datasets.image import CIFAR10Dataset\n\ndataset = CIFAR10Dataset(root='./data', train=True)\n</code></pre>"},{"location":"getting-started/core-concepts/#text","title":"Text","text":"<p>Coming Soon</p> <p>WikipediaDataset is planned but not yet implemented.</p> <pre><code># Planned API (not yet available)\nfrom artifex.data.datasets.text import WikipediaDataset\n\ndataset = WikipediaDataset(\n    tokenizer='bpe',\n    max_length=512\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#audio","title":"Audio","text":"<p>Coming Soon</p> <p>LibriSpeechDataset is planned but not yet implemented.</p> <pre><code># Planned API (not yet available)\nfrom artifex.data.datasets.audio import LibriSpeechDataset\n\ndataset = LibriSpeechDataset(\n    root='./data',\n    sample_rate=16000\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#protein","title":"Protein","text":"<pre><code>from artifex.data.protein_dataset import ProteinDataset\n\ndataset = ProteinDataset(\n    pdb_dir='./data/pdb',\n    with_constraints=True\n)\n</code></pre>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<p>Now that you understand the core concepts:</p> <ul> <li> <p> Quickstart Guide</p> <p>Train your first VAE model with Artifex in minutes</p> <p> Quickstart</p> </li> <li> <p> Explore Model Guides</p> <p>Deep dives into each model type with examples</p> <p> VAE Guide  Model Implementations</p> </li> <li> <p> Check API Reference</p> <p>Complete API documentation for all components</p> <p> Core API</p> </li> <li> <p> Learn Training</p> <p>Training workflows, optimization, and distributed training</p> <p> Training Guide</p> </li> </ul>"},{"location":"getting-started/core-concepts/#further-reading","title":"Further Reading","text":""},{"location":"getting-started/core-concepts/#generative-models","title":"Generative Models","text":"<ul> <li>VAE: Kingma &amp; Welling (2013) - Auto-Encoding Variational Bayes</li> <li>GAN: Goodfellow et al. (2014) - Generative Adversarial Networks</li> <li>Diffusion: Ho et al. (2020) - Denoising Diffusion Probabilistic Models</li> <li>Flow: Dinh et al. (2016) - Density Estimation using Real NVP</li> </ul>"},{"location":"getting-started/core-concepts/#jax-and-flax","title":"JAX and Flax","text":"<ul> <li>JAX Documentation</li> <li>Flax NNX Documentation</li> <li>JAX 101 Tutorial</li> </ul> <p>Last Updated: 2026-02-02</p>"},{"location":"getting-started/hardware-setup-guide/","title":"Hardware Setup Guide: Customizing for Your GPU/TPU","text":"<p>This comprehensive guide explains how Artifex's setup system works and how to customize it for different hardware configurations including various NVIDIA GPUs, AMD GPUs, Apple Silicon, and TPUs.</p>"},{"location":"getting-started/hardware-setup-guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Architecture &amp; Philosophy</li> <li>The Setup Pipeline</li> <li>Customizing for Different Hardware</li> <li>Multi-GPU Configurations</li> <li>Memory Management</li> <li>TPU Setup</li> <li>AMD GPU Setup (ROCm)</li> <li>Apple Silicon (M1/M2/M3)</li> <li>Troubleshooting</li> </ul>"},{"location":"getting-started/hardware-setup-guide/#overview","title":"Overview","text":"<p>Artifex uses a three-component setup system:</p> <ol> <li><code>setup.sh</code>: Main orchestration script that detects hardware and creates the environment</li> <li><code>.env.example</code>: Template for <code>.env</code> file with GPU-specific configurations</li> <li><code>activate.sh</code>: Generated activation script that loads the environment</li> </ol>"},{"location":"getting-started/hardware-setup-guide/#quick-facts","title":"Quick Facts","text":"<ul> <li>Hardware Agnostic: Despite being tested on RTX 4090, the setup is designed for ANY NVIDIA GPU</li> <li>No Hardware-Specific Code: The system auto-detects and configures for your hardware</li> <li>CUDA Version Independent: Uses CUDA libraries installed in your virtual environment</li> <li>Extensible: Easy to customize for TPUs, AMD GPUs, or custom hardware</li> </ul>"},{"location":"getting-started/hardware-setup-guide/#architecture-philosophy","title":"Architecture Philosophy","text":""},{"location":"getting-started/hardware-setup-guide/#design-principles","title":"Design Principles","text":""},{"location":"getting-started/hardware-setup-guide/#1-virtual-environment-isolation","title":"1. Virtual Environment Isolation","text":"<p>All CUDA libraries are installed inside the virtual environment, not system-wide:</p> <pre><code>.venv/\n\u2514\u2500\u2500 lib/\n    \u2514\u2500\u2500 python3.*/\n        \u2514\u2500\u2500 site-packages/\n            \u2514\u2500\u2500 nvidia/\n                \u251c\u2500\u2500 cublas/lib/\n                \u251c\u2500\u2500 cudnn/lib/\n                \u251c\u2500\u2500 cufft/lib/\n                \u2514\u2500\u2500 ... (all CUDA libraries)\n</code></pre> <p>Why? This allows:</p> <ul> <li>Multiple projects with different CUDA versions</li> <li>No system-wide CUDA installation required</li> <li>Clean, reproducible environments</li> <li>Easy cleanup (just delete <code>.venv/</code>)</li> </ul>"},{"location":"getting-started/hardware-setup-guide/#2-dynamic-hardware-detection","title":"2. Dynamic Hardware Detection","text":"<p>The setup script detects your hardware at runtime:</p> <pre><code># From setup.sh\ndetect_cuda_support() {\n    if command -v nvidia-smi &amp;&gt; /dev/null; then\n        gpu_info=$(nvidia-smi --query-gpu=name --format=csv,noheader 2&gt;/dev/null | head -1)\n        if [ -n \"$gpu_info\" ]; then\n            echo \"\u2705 NVIDIA GPU detected: $gpu_info\"\n            return 0\n        fi\n    fi\n    return 1  # No GPU found\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#3-template-based-configuration","title":"3. Template-Based Configuration","text":"<p>The <code>.env.example</code> provides a base configuration that gets customized during setup:</p> <ul> <li>Generic Base: Works for any NVIDIA GPU</li> <li>Customizable: Easy to modify for specific needs</li> <li>Version-Independent: Auto-detects Python version</li> </ul>"},{"location":"getting-started/hardware-setup-guide/#the-setup-pipeline","title":"The Setup Pipeline","text":""},{"location":"getting-started/hardware-setup-guide/#step-by-step-execution","title":"Step-by-Step Execution","text":"<p>When you run <code>./setup.sh</code>, here's what happens:</p>"},{"location":"getting-started/hardware-setup-guide/#1-pre-flight-checks","title":"1. Pre-flight Checks","text":"<pre><code># Checks and installs uv if needed\nensure_uv_installed()\n\n# Detects GPU hardware\ndetect_cuda_support()  # Returns true for GPU, false for CPU\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#2-environment-cleaning-if-needed","title":"2. Environment Cleaning (if needed)","text":"<pre><code>perform_cleaning()\n# - Removes old .venv/\n# - Clears caches\n# - Removes old config files\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#3-environment-file-creation","title":"3. Environment File Creation","text":"<pre><code>create_env_file() {\n    local has_cuda=$1  # true or false from detection\n\n    if [ \"$has_cuda\" = true ]; then\n        # Use .env.example if it exists\n        if [ -f \".env.example\" ]; then\n            # Copy and customize template\n            sed \"s|PROJECT_DIR=\\\"\\$(pwd)\\\"|PROJECT_DIR=\\\"$(pwd)\\\"|g\" \\\n                .env.example &gt; .env\n        else\n            # Use embedded fallback template\n            # (same as .env.example)\n        fi\n    else\n        # Create CPU-only .env\n        # (simpler configuration)\n    fi\n}\n</code></pre> <p>Key Point: The template is hardware-agnostic. It doesn't know about RTX 4090 vs A100 vs V100.</p>"},{"location":"getting-started/hardware-setup-guide/#4-virtual-environment-creation","title":"4. Virtual Environment Creation","text":"<pre><code>setup_environment() {\n    uv venv\n    source .venv/bin/activate\n    source .env\n\n    if [ \"$has_cuda\" = true ]; then\n        # Install GPU packages\n        uv sync --extra all\n\n        # Install matching CUDA plugins for JAX\n        JAX_VERSION=$(python -c \"import jax; print(jax.__version__)\")\n        uv pip install --force-reinstall \\\n            \"jax-cuda12-pjrt==$JAX_VERSION\" \\\n            \"jax-cuda12-plugin==$JAX_VERSION\"\n    else\n        # CPU-only installation\n        uv sync --extra dev\n    fi\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#5-activation-script-generation","title":"5. Activation Script Generation","text":"<pre><code>create_activation_script() {\n    cat &gt; activate.sh &lt;&lt; 'EOF'\n#!/bin/bash\n# 1. Activates .venv\n# 2. Sources .env (loads environment variables)\n# 3. Runs JAX verification tests\n# 4. Displays system information\nEOF\n    chmod +x activate.sh\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#6-verification","title":"6. Verification","text":"<pre><code>verify_installation() {\n    # Tests JAX import\n    # Tests GPU detection (if applicable)\n    # Runs simple computations\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#understanding-envexample","title":"Understanding <code>.env.example</code>","text":""},{"location":"getting-started/hardware-setup-guide/#template-structure","title":"Template Structure","text":"<p>The template has several key sections:</p>"},{"location":"getting-started/hardware-setup-guide/#section-1-project-path-detection","title":"Section 1: Project Path Detection","text":"<pre><code># Use absolute path for the project directory (will be replaced during setup)\nPROJECT_DIR=\"/path/to/artifex\"  # \u2190 Replaced by setup.sh with actual path\n\n# Dynamically detect Python version\nif [ -f \"${PROJECT_DIR}/.venv/bin/python\" ]; then\n    PYTHON_VERSION=$(\"${PROJECT_DIR}/.venv/bin/python\" -c \"import sys; print(f'python{sys.version_info.major}.{sys.version_info.minor}')\")\n    # ... fallback detection\nfi\nVENV_CUDA_BASE=\"${PROJECT_DIR}/.venv/lib/${PYTHON_VERSION}/site-packages/nvidia\"\n</code></pre> <p>Purpose: Locate CUDA libraries in the virtual environment, regardless of Python version.</p>"},{"location":"getting-started/hardware-setup-guide/#section-2-path-filtering","title":"Section 2: Path Filtering","text":"<pre><code># Filter out old CUDA paths from existing LD_LIBRARY_PATH\nif [ -n \"$LD_LIBRARY_PATH\" ]; then\n    FILTERED_LD_PATH=$(echo \"$LD_LIBRARY_PATH\" | tr ':' '\\n' | \\\n        grep -v -E '(nvidia|cuda|cudnn|nccl|...)' | \\\n        tr '\\n' ':' | sed 's/:$//')\nfi\n</code></pre> <p>Purpose: Remove system-wide CUDA paths to avoid version conflicts. Preserve non-CUDA paths.</p>"},{"location":"getting-started/hardware-setup-guide/#section-3-cuda-library-paths","title":"Section 3: CUDA Library Paths","text":"<pre><code># Include ALL CUDA libraries installed by jax-cuda12-plugin\nNEW_CUDA_PATHS=\"${VENV_CUDA_BASE}/cublas/lib:${VENV_CUDA_BASE}/cusolver/lib:...\"\n\nexport LD_LIBRARY_PATH=\"${NEW_CUDA_PATHS}:${FILTERED_LD_PATH}\"\nexport CUDA_HOME=\"${VENV_CUDA_BASE}\"\nexport PATH=\"${VENV_CUDA_BASE}/cuda_nvcc/bin:${PATH}\"\n</code></pre> <p>Purpose: Make JAX use CUDA libraries from venv, not system.</p>"},{"location":"getting-started/hardware-setup-guide/#section-4-jax-configuration","title":"Section 4: JAX Configuration","text":"<pre><code>export JAX_PLATFORMS=\"cuda,cpu\"\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.9\"\nexport XLA_FLAGS=\"--xla_gpu_strict_conv_algorithm_picker=false\"\n</code></pre> <p>Purpose: Configure JAX for optimal GPU performance.</p>"},{"location":"getting-started/hardware-setup-guide/#section-5-development-settings","title":"Section 5: Development Settings","text":"<pre><code>export PYTHONPATH=\"${PYTHONPATH:+${PYTHONPATH}:}${PROJECT_DIR}\"\nexport PYTEST_CUDA_ENABLED=\"true\"\nexport PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=\"python\"\n</code></pre> <p>Purpose: Project-specific development configuration.</p>"},{"location":"getting-started/hardware-setup-guide/#why-this-template-works-for-all-nvidia-gpus","title":"Why This Template Works for All NVIDIA GPUs","text":"<p>The template is hardware-agnostic because:</p> <ol> <li>No GPU-Specific Settings: It doesn't configure compute capability, SM count, or GPU-specific features</li> <li>CUDA Auto-Detection: JAX and XLA handle GPU-specific optimizations automatically</li> <li>Dynamic Library Loading: Libraries are loaded based on what's installed, not hardcoded</li> <li>Memory Fraction: Uses 90% by default, works on any GPU size</li> </ol>"},{"location":"getting-started/hardware-setup-guide/#customizing-for-different-hardware","title":"Customizing for Different Hardware","text":""},{"location":"getting-started/hardware-setup-guide/#nvidia-gpu-variations","title":"NVIDIA GPU Variations","text":"<p>The default template works for all NVIDIA GPUs (V100, A100, RTX 3090, RTX 4090, H100, etc.), but you can optimize for specific use cases.</p>"},{"location":"getting-started/hardware-setup-guide/#consumer-gpus-rtx-3060-3090-4090","title":"Consumer GPUs (RTX 3060, 3090, 4090)","text":"<p>Typical Configuration (already optimal in template):</p> <pre><code># For 8-24GB GPUs\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.9\"\nexport CUDA_MODULE_LOADING=\"LAZY\"\n</code></pre> <p>For Lower Memory GPUs (6-8GB):</p> <p>Edit <code>.env.example</code>:</p> <pre><code># Conservative memory usage for RTX 3060, GTX 1660, etc.\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.7\"  # Use 70% of GPU memory\nexport XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\"  # Better memory management\nexport TF_FORCE_GPU_ALLOW_GROWTH=\"true\"  # Gradual memory allocation\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#data-center-gpus-v100-a100-h100","title":"Data Center GPUs (V100, A100, H100)","text":"<p>For A100 (40GB/80GB):</p> <pre><code># Aggressive memory usage for large GPU\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.95\"  # Use 95% of memory\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\nexport NCCL_DEBUG=\"INFO\"  # Enable NCCL logging for multi-GPU\nexport NCCL_IB_DISABLE=\"0\"  # Enable InfiniBand if available\nexport NCCL_NET_GDR_LEVEL=\"5\"  # Enable GPUDirect RDMA\n</code></pre> <p>For H100:</p> <pre><code># H100-specific optimizations\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.95\"\nexport XLA_FLAGS=\"--xla_gpu_enable_async_all_reduce=true --xla_gpu_enable_latency_hiding_scheduler=true\"\nexport CUDA_MODULE_LOADING=\"EAGER\"  # H100 benefits from eager loading\nexport NCCL_PROTO=\"Simple\"  # Optimal for H100 NVLink\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#professional-gpus-quadro-tesla","title":"Professional GPUs (Quadro, Tesla)","text":"<pre><code># Similar to consumer but may benefit from ECC\nexport CUDA_FORCE_PTX_JIT=\"1\"  # Force PTX JIT compilation\nexport XLA_FLAGS=\"--xla_gpu_cuda_graph_level=3\"  # Aggressive CUDA graph optimizations\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#custom-template-creation","title":"Custom Template Creation","text":"<p>Create a custom template for your hardware:</p> <pre><code># 1. Copy the original template\ncp .env.example .env.example.rtx3060\n\n# 2. Edit for your GPU\nnano .env.example.rtx3060\n\n# 3. Modify the create_env_file function in setup.sh to use your template\n# Edit setup.sh, line ~274:\nif [ -f \".env.example.rtx3060\" ]; then\n    sed \"s|PROJECT_DIR=\\\"\\$(pwd)\\\"|PROJECT_DIR=\\\"$(pwd)\\\"|g\" \\\n        .env.example.rtx3060 &gt; .env\nfi\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#example-rtx-3060-12gb-optimized-template","title":"Example: RTX 3060 (12GB) Optimized Template","text":"<p>Create <code>.env.example.rtx3060</code>:</p> <pre><code># Artifex Environment Configuration - RTX 3060 Optimized\n# 12GB VRAM, consumer GPU optimizations\n\nPROJECT_DIR=\"$(pwd)\"\n\n# [... Python version detection same as original ...]\n\n# CUDA paths configuration\n# [... same as original until JAX Configuration ...]\n\n# JAX Configuration for RTX 3060 (12GB VRAM)\nexport JAX_PLATFORMS=\"cuda,cpu\"\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.75\"  # Conservative for 12GB\nexport XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\"  # Better memory management\nexport XLA_FLAGS=\"--xla_gpu_strict_conv_algorithm_picker=false --xla_gpu_force_compilation_parallelism=1\"\n\n# Memory optimization for consumer GPU\nexport CUDA_MODULE_LOADING=\"LAZY\"\nexport CUDA_CACHE_MAXSIZE=\"268435456\"  # 256MB cache\nexport TF_FORCE_GPU_ALLOW_GROWTH=\"true\"\n\n# JAX CUDA Plugin Configuration\nexport JAX_CUDA_PLUGIN_VERIFY=\"false\"\n\n# Reduce CUDA warnings\nexport TF_CPP_MIN_LOG_LEVEL=\"1\"\n\n# Performance settings\nexport JAX_ENABLE_X64=\"0\"  # Keep 32-bit for better performance\n\n# Development settings\nexport PYTHONPATH=\"${PYTHONPATH:+${PYTHONPATH}:}${PROJECT_DIR}\"\nexport PYTEST_CUDA_ENABLED=\"true\"\nexport PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=\"python\"\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#multi-gpu-configurations","title":"Multi-GPU Configurations","text":""},{"location":"getting-started/hardware-setup-guide/#modifying-for-multiple-gpus","title":"Modifying for Multiple GPUs","text":""},{"location":"getting-started/hardware-setup-guide/#1-update-envexample-for-multi-gpu","title":"1. Update <code>.env.example</code> for Multi-GPU","text":"<p>Add to the template:</p> <pre><code># Multi-GPU Configuration\n# Set which GPUs to use (0,1,2,3 for 4 GPUs)\nexport CUDA_VISIBLE_DEVICES=\"0,1,2,3\"  # Use all 4 GPUs\n# Or: export CUDA_VISIBLE_DEVICES=\"0,1\"  # Use only first 2 GPUs\n\n# NCCL Configuration for Multi-GPU\nexport NCCL_DEBUG=\"INFO\"  # Enable NCCL logging\nexport NCCL_DEBUG_SUBSYS=\"ALL\"  # Debug all subsystems\nexport NCCL_IB_DISABLE=\"0\"  # Enable InfiniBand (if available)\nexport NCCL_SOCKET_IFNAME=\"eth0\"  # Network interface for NCCL\nexport NCCL_P2P_LEVEL=\"LOC\"  # Enable peer-to-peer transfers\n\n# XLA Multi-GPU Settings\nexport XLA_FLAGS=\"--xla_gpu_enable_async_all_reduce=true \\\n                  --xla_gpu_enable_async_all_gather=true \\\n                  --xla_gpu_all_reduce_combine_threshold_bytes=134217728 \\\n                  --xla_gpu_enable_nccl_comm_splitting=true\"\n\n# Memory per GPU (adjust based on total GPU memory)\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.9\"  # 90% per GPU\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#2-modify-setupsh-for-multi-gpu-detection","title":"2. Modify <code>setup.sh</code> for Multi-GPU Detection","text":"<p>Add GPU counting to <code>detect_cuda_support()</code>:</p> <pre><code>detect_cuda_support() {\n    if [ \"$CPU_ONLY\" = true ]; then\n        return 1\n    fi\n\n    if command -v nvidia-smi &amp;&gt; /dev/null; then\n        # Get all GPU names\n        gpu_info=$(nvidia-smi --query-gpu=name --format=csv,noheader 2&gt;/dev/null)\n        gpu_count=$(echo \"$gpu_info\" | wc -l)\n\n        if [ -n \"$gpu_info\" ] &amp;&amp; [ $gpu_count -gt 0 ]; then\n            log_success \"NVIDIA GPU(s) detected: $gpu_count GPU(s)\"\n            echo \"$gpu_info\" | nl -w2 -s'. '  # Number and list GPUs\n\n            # Store GPU count for later use\n            export DETECTED_GPU_COUNT=$gpu_count\n            return 0\n        fi\n    fi\n\n    return 1\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#3-dynamic-multi-gpu-configuration","title":"3. Dynamic Multi-GPU Configuration","text":"<p>Modify <code>create_env_file()</code> to use GPU count:</p> <pre><code>create_env_file() {\n    local has_cuda=$1\n\n    if [ \"$has_cuda\" = true ]; then\n        # Copy template\n        sed \"s|PROJECT_DIR=\\\"\\$(pwd)\\\"|PROJECT_DIR=\\\"$(pwd)\\\"|g\" \\\n            .env.example &gt; .env\n\n        # Add multi-GPU settings if multiple GPUs detected\n        if [ \"${DETECTED_GPU_COUNT:-1}\" -gt 1 ]; then\n            cat &gt;&gt; .env &lt;&lt; EOF\n\n# Auto-detected Multi-GPU Configuration ($DETECTED_GPU_COUNT GPUs)\nexport CUDA_VISIBLE_DEVICES=\"$(seq -s, 0 $((DETECTED_GPU_COUNT-1)))\"\nexport NCCL_DEBUG=\"WARN\"\nexport XLA_FLAGS=\"\\${XLA_FLAGS} --xla_gpu_enable_async_all_reduce=true\"\nEOF\n            log_success \"Multi-GPU configuration added for $DETECTED_GPU_COUNT GPUs\"\n        fi\n    fi\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#testing-multi-gpu-setup","title":"Testing Multi-GPU Setup","text":"<p>After setup, verify multi-GPU:</p> <pre><code>import jax\nprint(f\"Number of devices: {len(jax.devices())}\")\nprint(f\"Devices: {jax.devices()}\")\n\n# Test multi-device computation\nimport jax.numpy as jnp\nx = jnp.arange(1000)\n# Data parallel computation across all GPUs\nresults = jax.pmap(lambda x: x ** 2)(x.reshape(len(jax.devices()), -1))\nprint(f\"Results shape: {results.shape}\")\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#memory-management","title":"Memory Management","text":""},{"location":"getting-started/hardware-setup-guide/#understanding-memory-fractions","title":"Understanding Memory Fractions","text":"<p><code>XLA_PYTHON_CLIENT_MEM_FRACTION</code> controls GPU memory allocation:</p> <pre><code># Formula: usable_memory = total_memory * fraction\n# RTX 4090 (24GB): 24GB * 0.9 = 21.6GB available to JAX\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.9\"\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#gpu-specific-memory-recommendations","title":"GPU-Specific Memory Recommendations","text":"GPU Model VRAM Recommended Fraction Reasoning RTX 3060 12GB 0.7-0.75 Leave room for OS/display RTX 3080 10GB 0.75-0.8 Same as above RTX 3090 24GB 0.85-0.9 More memory available RTX 4090 24GB 0.85-0.9 Plenty of memory A100 (40GB) 40GB 0.9-0.95 Server GPU, no display A100 (80GB) 80GB 0.9-0.95 Maximize usage H100 80GB 0.95 Maximum usage V100 (16GB) 16GB 0.8-0.85 Balanced V100 (32GB) 32GB 0.85-0.9 More room T4 16GB 0.75-0.8 Shared environments"},{"location":"getting-started/hardware-setup-guide/#dynamic-memory-allocation","title":"Dynamic Memory Allocation","text":"<p>For variable workloads, use dynamic allocation:</p> <pre><code># Add to .env.example\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"  # Don't preallocate\nexport XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\"  # Use platform allocator\nexport TF_FORCE_GPU_ALLOW_GROWTH=\"true\"  # Gradual allocation\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#out-of-memory-handling","title":"Out-of-Memory Handling","text":"<p>Add to your template:</p> <pre><code># OOM handling\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.7\"  # Conservative\nexport JAX_PLATFORMS=\"cuda,cpu\"  # Fallback to CPU if GPU OOM\nexport JAX_DEBUG_NANS=\"false\"  # Disable NaN checking to save memory\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#tpu-setup","title":"TPU Setup","text":""},{"location":"getting-started/hardware-setup-guide/#creating-a-tpu-specific-template","title":"Creating a TPU-Specific Template","text":"<p>Create <code>.env.example.tpu</code>:</p> <pre><code># Artifex Environment Configuration - TPU Optimized\n# Google Cloud TPU configuration\n\nPROJECT_DIR=\"$(pwd)\"\n\n# JAX Configuration for TPU\nexport JAX_PLATFORMS=\"tpu,cpu\"  # TPU first, CPU fallback\nexport TPU_CHIPS_PER_HOST=\"8\"  # Standard TPU v3/v4 configuration\nexport TPU_NAME=\"local\"  # Or your TPU name\n\n# XLA TPU Flags\nexport XLA_FLAGS=\"--xla_tpu_enable_data_parallel_all_reduce_opt=true \\\n                  --xla_tpu_data_parallel_opt_different_sized_ops=true \\\n                  --xla_tpu_enable_async_collective_fusion=true \\\n                  --xla_tpu_enable_async_collective_fusion_multiple_steps=true\"\n\n# TPU Performance Settings\nexport JAX_ENABLE_X64=\"0\"  # TPUs work better with 32-bit\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"true\"  # TPUs benefit from preallocation\nexport XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\"\n\n# Development settings\nexport PYTHONPATH=\"${PYTHONPATH:+${PYTHONPATH}:}${PROJECT_DIR}\"\nexport PYTEST_CUDA_ENABLED=\"false\"\nexport PYTEST_TPU_ENABLED=\"true\"\n\n# Reduce verbosity\nexport TF_CPP_MIN_LOG_LEVEL=\"1\"\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#modifying-setupsh-for-tpu-detection","title":"Modifying <code>setup.sh</code> for TPU Detection","text":"<p>Add TPU detection:</p> <pre><code># Add to setup.sh\ndetect_tpu_support() {\n    if [ \"$CPU_ONLY\" = true ]; then\n        return 1\n    fi\n\n    # Check for TPU via environment variable\n    if [ -n \"$TPU_NAME\" ]; then\n        log_success \"TPU detected: $TPU_NAME\"\n        return 0\n    fi\n\n    # Check for TPU via gcloud\n    if command -v gcloud &amp;&gt; /dev/null; then\n        if gcloud compute tpus list 2&gt;/dev/null | grep -q \"READY\"; then\n            log_success \"TPU available via gcloud\"\n            return 0\n        fi\n    fi\n\n    log_info \"No TPU detected\"\n    return 1\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#tpu-specific-installation","title":"TPU-Specific Installation","text":"<p>Modify <code>setup_environment()</code>:</p> <pre><code>setup_environment() {\n    local has_cuda=$1\n    local has_tpu=$2  # Add TPU parameter\n\n    uv venv\n    source .venv/bin/activate\n\n    if [ \"$has_tpu\" = true ]; then\n        log_info \"Installing with TPU support...\"\n        uv sync --extra dev\n\n        # Install JAX with TPU support\n        pip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n\n        log_success \"TPU installation successful\"\n    elif [ \"$has_cuda\" = true ]; then\n        # GPU installation (as before)\n        # ...\n    fi\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#using-custom-tpu-template","title":"Using Custom TPU Template","text":"<pre><code># Run setup with TPU template\nTPU_NAME=\"your-tpu-name\" ./setup.sh\n\n# Or modify setup.sh to detect and use TPU template\n# In create_env_file():\nif [ -f \".env.example.tpu\" ]; then\n    sed \"s|PROJECT_DIR=\\\"\\$(pwd)\\\"|PROJECT_DIR=\\\"$(pwd)\\\"|g\" \\\n        .env.example.tpu &gt; .env\nfi\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#amd-gpu-setup-rocm","title":"AMD GPU Setup (ROCm)","text":""},{"location":"getting-started/hardware-setup-guide/#creating-amdrocm-template","title":"Creating AMD/ROCm Template","text":"<p>Create <code>.env.example.rocm</code>:</p> <pre><code># Artifex Environment Configuration - AMD ROCm\n# AMD GPU configuration using ROCm\n\nPROJECT_DIR=\"$(pwd)\"\n\n# ROCm Configuration\nexport ROCM_PATH=\"/opt/rocm\"  # Default ROCm installation\nexport HIP_VISIBLE_DEVICES=\"0\"  # GPU to use\n\n# Add ROCm to paths\nexport PATH=\"${ROCM_PATH}/bin:${PATH}\"\nexport LD_LIBRARY_PATH=\"${ROCM_PATH}/lib:${LD_LIBRARY_PATH}\"\n\n# JAX Configuration for ROCm\nexport JAX_PLATFORMS=\"rocm,cpu\"\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.9\"\n\n# ROCm-specific settings\nexport HSA_OVERRIDE_GFX_VERSION=\"10.3.0\"  # Adjust for your GPU\nexport GPU_DEVICE_ORDINAL=\"0\"\n\n# Development settings\nexport PYTHONPATH=\"${PYTHONPATH:+${PYTHONPATH}:}${PROJECT_DIR}\"\nexport PYTEST_CUDA_ENABLED=\"false\"\nexport PYTEST_ROCM_ENABLED=\"true\"\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#amd-gpu-detection","title":"AMD GPU Detection","text":"<p>Add to <code>setup.sh</code>:</p> <pre><code>detect_rocm_support() {\n    if [ \"$CPU_ONLY\" = true ]; then\n        return 1\n    fi\n\n    if command -v rocm-smi &amp;&gt; /dev/null; then\n        gpu_info=$(rocm-smi --showproductname 2&gt;/dev/null | grep \"Card series\" | head -1)\n        if [ -n \"$gpu_info\" ]; then\n            log_success \"AMD GPU detected: $gpu_info\"\n            return 0\n        fi\n    fi\n\n    log_info \"No AMD GPU detected\"\n    return 1\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#rocm-installation","title":"ROCm Installation","text":"<pre><code>setup_environment() {\n    # ... (existing code)\n\n    if [ \"$has_rocm\" = true ]; then\n        log_info \"Installing with ROCm support...\"\n        uv sync --extra dev\n\n        # Install JAX with ROCm (if available)\n        # Note: JAX ROCm support is experimental\n        pip install jax-rocm\n\n        log_success \"ROCm installation complete\"\n    fi\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#apple-silicon-m1m2m3","title":"Apple Silicon (M1/M2/M3)","text":""},{"location":"getting-started/hardware-setup-guide/#creating-apple-silicon-template","title":"Creating Apple Silicon Template","text":"<p>Create <code>.env.example.metal</code>:</p> <pre><code># Artifex Environment Configuration - Apple Silicon\n# M1/M2/M3 GPU configuration using Metal\n\nPROJECT_DIR=\"$(pwd)\"\n\n# JAX Configuration for Metal\nexport JAX_PLATFORMS=\"METAL,cpu\"  # Note: Experimental\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\n\n# Metal Performance Settings\nexport JAX_ENABLE_X64=\"0\"  # Metal works better with 32-bit\n\n# Apple Neural Engine (if supported)\n# export USE_ANE=\"1\"  # Experimental\n\n# Development settings\nexport PYTHONPATH=\"${PYTHONPATH:+${PYTHONPATH}:}${PROJECT_DIR}\"\nexport PYTEST_CUDA_ENABLED=\"false\"\nexport PYTEST_METAL_ENABLED=\"true\"\n\n# Reduce verbosity\nexport TF_CPP_MIN_LOG_LEVEL=\"1\"\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#apple-silicon-detection","title":"Apple Silicon Detection","text":"<pre><code>detect_apple_silicon() {\n    if [ \"$(uname)\" = \"Darwin\" ]; then\n        # Check for Apple Silicon\n        if [ \"$(uname -m)\" = \"arm64\" ]; then\n            chip_info=$(sysctl -n machdep.cpu.brand_string)\n            log_success \"Apple Silicon detected: $chip_info\"\n            return 0\n        fi\n    fi\n    return 1\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#metal-installation","title":"Metal Installation","text":"<pre><code>setup_environment() {\n    # ... (existing code)\n\n    if [ \"$has_metal\" = true ]; then\n        log_info \"Installing with Metal support...\"\n        uv sync --extra dev\n\n        # Install JAX with Metal support (experimental)\n        pip install jax-metal\n\n        log_success \"Metal installation complete\"\n    fi\n}\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#advanced-customization-examples","title":"Advanced Customization Examples","text":""},{"location":"getting-started/hardware-setup-guide/#example-1-multi-node-multi-gpu-cluster","title":"Example 1: Multi-Node Multi-GPU Cluster","text":"<p>For distributed training across multiple nodes:</p> <pre><code># .env.example.cluster\n# Multi-node configuration\n\nPROJECT_DIR=\"$(pwd)\"\n\n# [... standard CUDA paths ...]\n\n# Multi-Node Configuration\nexport MASTER_ADDR=\"192.168.1.100\"  # Head node IP\nexport MASTER_PORT=\"29500\"\nexport WORLD_SIZE=\"8\"  # Total number of GPUs across all nodes\nexport NODE_RANK=\"0\"  # This node's rank (0 for head, 1, 2, ... for workers)\nexport NPROC_PER_NODE=\"4\"  # GPUs per node\n\n# NCCL Multi-Node Settings\nexport NCCL_SOCKET_IFNAME=\"eth0\"  # Network interface\nexport NCCL_IB_DISABLE=\"0\"  # Enable InfiniBand\nexport NCCL_IB_HCA=\"mlx5_0\"  # InfiniBand adapter\nexport NCCL_DEBUG=\"INFO\"\nexport NCCL_ALGO=\"Ring\"  # Or \"Tree\" depending on topology\n\n# XLA Distributed Settings\nexport JAX_COORDINATOR_ADDRESS=\"${MASTER_ADDR}:${MASTER_PORT}\"\nexport JAX_NUM_PROCESSES=\"${WORLD_SIZE}\"\nexport JAX_PROCESS_INDEX=\"${NODE_RANK}\"\n\n# Memory settings for large scale\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.95\"\nexport XLA_FLAGS=\"--xla_gpu_enable_async_all_reduce=true \\\n                  --xla_gpu_enable_async_all_gather=true \\\n                  --xla_gpu_all_reduce_combine_threshold_bytes=268435456\"\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#example-2-low-poweredge-gpu-configuration","title":"Example 2: Low-Power/Edge GPU Configuration","text":"<p>For Jetson or other edge devices:</p> <pre><code># .env.example.edge\n# Edge device configuration (Jetson, etc.)\n\nPROJECT_DIR=\"$(pwd)\"\n\n# Conservative memory usage\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.6\"  # Leave room for system\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\nexport XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\"\n\n# Power-efficient settings\nexport CUDA_MODULE_LOADING=\"LAZY\"\nexport JAX_ENABLE_X64=\"0\"  # 32-bit for efficiency\nexport TF_FORCE_GPU_ALLOW_GROWTH=\"true\"\n\n# Reduce parallelism to save resources\nexport XLA_FLAGS=\"--xla_gpu_force_compilation_parallelism=1\"\n\n# Thermal management (if applicable)\n# export CUDA_CACHE_DISABLE=\"1\"  # Reduce disk I/O\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#example-3-mixed-precision-training","title":"Example 3: Mixed Precision Training","text":"<p>For automatic mixed precision:</p> <pre><code># Add to .env.example\n# Mixed Precision Training Configuration\nexport JAX_ENABLE_X64=\"0\"  # Force 32-bit\nexport XLA_FLAGS=\"${XLA_FLAGS} --xla_gpu_enable_xla_runtime_executable=true\"\n\n# For BF16 on supported GPUs (Ampere and newer)\nexport JAX_DEFAULT_DTYPE_BITS=\"32\"\nexport JAX_ENABLE_BFLOAT16=\"true\"  # If using BF16 explicitly\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/hardware-setup-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"getting-started/hardware-setup-guide/#issue-1-gpu-not-detected-after-setup","title":"Issue 1: GPU Not Detected After Setup","text":"<p>Problem: <code>activate.sh</code> shows CPU-only despite having GPU.</p> <p>Solution:</p> <pre><code># Check NVIDIA driver\nnvidia-smi\n\n# If driver is fine, check library paths\nsource .env\necho $LD_LIBRARY_PATH | tr ':' '\\n' | grep nvidia\n\n# Verify JAX can see GPU\npython -c \"import jax; print(jax.devices())\"\n\n# If still not working, force GPU reinstall\n./setup.sh --force\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#issue-2-cuda-version-mismatch","title":"Issue 2: CUDA Version Mismatch","text":"<p>Problem: JAX complains about CUDA version.</p> <p>Solution:</p> <pre><code># Check CUDA versions\nnvcc --version  # System CUDA\npython -c \"import jax; print(jax.lib.xla_bridge.get_backend().platform_version)\"  # JAX CUDA\n\n# Reinstall matching versions\npip uninstall jax jaxlib jax-cuda12-plugin jax-cuda12-pjrt\npip install \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#issue-3-out-of-memory-errors","title":"Issue 3: Out of Memory Errors","text":"<p>Problem: Training crashes with OOM.</p> <p>Solution:</p> <p>Edit <code>.env</code>:</p> <pre><code># Reduce memory fraction\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.6\"  # Was 0.9\n\n# Enable dynamic allocation\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\nexport TF_FORCE_GPU_ALLOW_GROWTH=\"true\"\n\n# Reload environment\ndeactivate\nsource ./activate.sh\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#issue-4-slow-training-on-multi-gpu","title":"Issue 4: Slow Training on Multi-GPU","text":"<p>Problem: Multi-GPU training slower than expected.</p> <p>Solution:</p> <p>Edit <code>.env</code>:</p> <pre><code># Enable NCCL optimizations\nexport NCCL_DEBUG=\"INFO\"  # Check for errors\nexport NCCL_IB_DISABLE=\"0\"  # Enable InfiniBand if available\nexport NCCL_P2P_LEVEL=\"NVL\"  # Use NVLink if available\n\n# Enable XLA optimizations\nexport XLA_FLAGS=\"--xla_gpu_enable_async_all_reduce=true \\\n                  --xla_gpu_enable_latency_hiding_scheduler=true \\\n                  --xla_gpu_enable_highest_priority_async_stream=true\"\n\n# Check actual NCCL performance\nNCCL_DEBUG=INFO python your_training_script.py 2&gt;&amp;1 | grep -i \"nccl\"\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#issue-5-template-not-being-used","title":"Issue 5: Template Not Being Used","text":"<p>Problem: Custom template not applied.</p> <p>Solution:</p> <pre><code># Check if template exists\nls -la .env.example*\n\n# Verify setup.sh uses your template\ngrep \".env.example\" setup.sh\n\n# Manually create .env from template\nsed \"s|PROJECT_DIR=\\\"\\$(pwd)\\\"|PROJECT_DIR=\\\"$(pwd)\\\"|g\" \\\n    .env.example.custom &gt; .env\n\n# Source it\nsource .env\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#testing-your-configuration","title":"Testing Your Configuration","text":""},{"location":"getting-started/hardware-setup-guide/#validation-script","title":"Validation Script","text":"<p>Create <code>test_hardware_setup.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Test hardware setup and configuration.\"\"\"\n\nimport jax\nimport jax.numpy as jnp\nimport os\nimport sys\n\ndef test_jax_installation():\n    \"\"\"Test JAX installation.\"\"\"\n    print(f\"\u2705 JAX version: {jax.__version__}\")\n    print(f\"\u2705 JAX backend: {jax.default_backend()}\")\n\n\ndef test_devices():\n    \"\"\"Test device detection.\"\"\"\n    devices = jax.devices()\n    print(f\"\\n\ud83d\udcf1 Available devices: {len(devices)}\")\n\n    for i, device in enumerate(devices):\n        print(f\"  {i+1}. {device}\")\n        print(f\"     Platform: {device.platform}\")\n        print(f\"     Device kind: {device.device_kind}\")\n\n    return len(devices) &gt; 0\n\ndef test_computation():\n    \"\"\"Test basic computation.\"\"\"\n    print(\"\\n\ud83e\uddee Testing computation...\")\n\n    try:\n        x = jnp.arange(1000)\n        y = jnp.sum(x ** 2)\n        print(f\"\u2705 Computation successful: {float(y)}\")\n        return True\n    except Exception as e:\n        print(f\"\u274c Computation failed: {e}\")\n        return False\n\ndef test_gpu_specific():\n    \"\"\"Test GPU-specific features.\"\"\"\n    devices = jax.devices()\n    gpu_devices = [d for d in devices if d.platform == 'gpu']\n\n    if not gpu_devices:\n        print(\"\\n\ud83d\udcbb No GPU devices (CPU-only mode)\")\n        return True\n\n    print(f\"\\n\ud83c\udfae Testing {len(gpu_devices)} GPU device(s)...\")\n\n    try:\n        # Test each GPU\n        for i, gpu in enumerate(gpu_devices):\n            with jax.default_device(gpu):\n                x = jax.random.normal(jax.random.PRNGKey(0), (1000, 1000))\n                y = jnp.dot(x, x.T)\n                print(f\"\u2705 GPU {i} test passed: {y.shape}\")\n\n        # Test multi-GPU if available\n        if len(gpu_devices) &gt; 1:\n            print(f\"\\n\ud83d\udd04 Testing multi-GPU with {len(gpu_devices)} devices...\")\n            x = jnp.arange(len(gpu_devices) * 100).reshape(len(gpu_devices), -1)\n            result = jax.pmap(lambda x: x ** 2)(x)\n            print(f\"\u2705 Multi-GPU test passed: {result.shape}\")\n\n        return True\n    except Exception as e:\n        print(f\"\u274c GPU test failed: {e}\")\n        return False\n\ndef test_environment_variables():\n    \"\"\"Test environment variables.\"\"\"\n    print(\"\\n\ud83d\udccb Environment Variables:\")\n\n    important_vars = [\n        'JAX_PLATFORMS',\n        'XLA_PYTHON_CLIENT_MEM_FRACTION',\n        'XLA_PYTHON_CLIENT_PREALLOCATE',\n        'LD_LIBRARY_PATH',\n        'CUDA_VISIBLE_DEVICES',\n        'CUDA_HOME',\n    ]\n\n    for var in important_vars:\n        value = os.environ.get(var, 'Not set')\n        # Truncate long values\n        if len(str(value)) &gt; 80:\n            value = str(value)[:77] + \"...\"\n        print(f\"  {var}: {value}\")\n\ndef main():\n    \"\"\"Run all tests.\"\"\"\n    print(\"=\" * 60)\n    print(\"Hardware Setup Validation\")\n    print(\"=\" * 60)\n\n    test_jax_installation()\n    devices_ok = test_devices()\n    compute_ok = test_computation()\n    gpu_ok = test_gpu_specific()\n    test_environment_variables()\n\n    print(\"\\n\" + \"=\" * 60)\n    if devices_ok and compute_ok and gpu_ok:\n        print(\"\u2705 All tests passed! Hardware setup is correct.\")\n        print(\"=\" * 60)\n        sys.exit(0)\n    else:\n        print(\"\u274c Some tests failed. Check configuration.\")\n        print(\"=\" * 60)\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run validation:</p> <pre><code># After setup and activation\npython test_hardware_setup.py\n</code></pre>"},{"location":"getting-started/hardware-setup-guide/#summary","title":"Summary","text":""},{"location":"getting-started/hardware-setup-guide/#quick-reference","title":"Quick Reference","text":"<p>For NVIDIA GPUs (any model):</p> <ul> <li>Use default <code>.env.example</code> (works for all)</li> <li>Run <code>./setup.sh</code></li> <li>Adjust <code>XLA_PYTHON_CLIENT_MEM_FRACTION</code> based on VRAM</li> </ul> <p>For Multi-GPU:</p> <ul> <li>Set <code>CUDA_VISIBLE_DEVICES</code></li> <li>Add NCCL configuration</li> <li>Enable async all-reduce in XLA_FLAGS</li> </ul> <p>For TPU:</p> <ul> <li>Create <code>.env.example.tpu</code></li> <li>Install JAX with TPU support</li> <li>Set <code>JAX_PLATFORMS=\"tpu,cpu\"</code></li> </ul> <p>For AMD ROCm:</p> <ul> <li>Create <code>.env.example.rocm</code></li> <li>Install JAX with ROCm (if available)</li> <li>Set <code>JAX_PLATFORMS=\"rocm,cpu\"</code></li> </ul> <p>For Apple Silicon:</p> <ul> <li>Create <code>.env.example.metal</code></li> <li>Install jax-metal</li> <li>Set <code>JAX_PLATFORMS=\"METAL,cpu\"</code></li> </ul>"},{"location":"getting-started/hardware-setup-guide/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>The template is hardware-agnostic - it works for any NVIDIA GPU by default</li> <li>Customization is optional - only needed for edge cases or optimizations</li> <li>Memory fraction is the main tunable - adjust based on your GPU VRAM</li> <li>Multi-GPU requires explicit configuration - add NCCL and XLA settings</li> <li>Test your setup - run validation script after customization</li> </ol>"},{"location":"getting-started/hardware-setup-guide/#getting-help","title":"Getting Help","text":"<p>If you have issues:</p> <ol> <li>Check this guide's troubleshooting section</li> <li>Run the validation script</li> <li>Check environment variables: <code>source .env &amp;&amp; env | grep -E '(JAX|XLA|CUDA)'</code></li> <li>Open an issue with your hardware specs and error messages</li> </ol> <p>Last Updated: 2025-10-15</p> <p>Maintainer: Artifex Team</p> <p>Feedback: Open an issue at github.com/avitai/artifex</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide provides comprehensive instructions for installing Artifex on various platforms and configurations.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Python: 3.10 or higher</li> <li>RAM: 8GB (16GB recommended)</li> <li>Disk Space: 2GB for installation, 10GB+ for datasets</li> <li>Operating System: Linux (Ubuntu 20.04+), macOS (10.15+), Windows 10/11 (via WSL2)</li> </ul>"},{"location":"getting-started/installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>Python: 3.10 or 3.11 (tested and recommended)</li> <li>RAM: 16GB+ for training models</li> <li>GPU: NVIDIA GPU with 8GB+ VRAM (for GPU acceleration)</li> <li>Compute Capability 7.0+ (V100, RTX 20xx, RTX 30xx, RTX 40xx, A100)</li> <li>CUDA 12.0 or higher</li> <li>Disk Space: 50GB+ for large-scale experiments</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":"<p>Artifex provides multiple installation methods to suit different use cases:</p> PyPI (Recommended for Users)uv (Recommended for Developers)pip (Traditional)Docker <p>The simplest way to get started:</p> <pre><code># CPU-only installation\npip install artifex\n\n# With GPU support\npip install artifex[cuda]\n\n# With all extras (documentation, development tools)\npip install artifex[all]\n</code></pre> <p>Note: PyPI package coming soon. For now, install from source.</p> <p>uv is a fast Python package manager. Artifex provides an automated setup script that handles everything:</p> <p>Quick Setup (Recommended):</p> <pre><code># Clone the repository\ngit clone https://github.com/avitai/artifex.git\ncd artifex\n\n# Run unified setup script (auto-detects GPU)\n./setup.sh\n\n# Activate environment\nsource ./activate.sh\n</code></pre> <p>What <code>setup.sh</code> does:</p> <ul> <li>Installs <code>uv</code> package manager if not present</li> <li>Detects GPU/CUDA availability automatically</li> <li>Creates <code>.venv</code> virtual environment</li> <li>Installs all dependencies (<code>uv sync --extra all</code> for GPU or <code>--extra dev</code> for CPU)</li> <li>Creates <code>.env</code> file with:<ul> <li>CUDA library paths (GPU mode)</li> <li>JAX platform configuration</li> <li>Environment variables for optimal performance</li> </ul> </li> <li>Generates <code>activate.sh</code> script for easy activation</li> <li>Verifies installation with JAX GPU tests</li> </ul> <p>What <code>activate.sh</code> does:</p> <ul> <li>Activates the <code>.venv</code> virtual environment</li> <li>Loads environment variables from <code>.env</code></li> <li>Configures CUDA paths (GPU mode)</li> <li>Verifies JAX installation and GPU detection</li> <li>Displays environment status and helpful commands</li> </ul> <p>Setup Options:</p> <pre><code># CPU-only setup (skip GPU detection)\n./setup.sh --cpu-only\n\n# Clean setup (removes caches)\n./setup.sh --deep-clean\n\n# Force reinstall\n./setup.sh --force\n\n# Verbose output\n./setup.sh --verbose\n</code></pre> <p>Manual Setup (Advanced):</p> <pre><code># Install uv if needed\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install Artifex with all dependencies\nuv sync --all-extras\n\n# Or install specific extras\nuv sync --extra cuda-dev  # CUDA development environment\nuv sync --extra dev       # Development tools only\n</code></pre> <p>Using standard pip with Artifex's setup script:</p> <p>Quick Setup with Scripts:</p> <pre><code># Clone the repository\ngit clone https://github.com/avitai/artifex.git\ncd artifex\n\n# Run setup script (installs uv automatically)\n./setup.sh\n\n# Activate environment\nsource ./activate.sh\n</code></pre> <p>The setup script works with both <code>uv</code> and <code>pip</code>. It will: - Auto-detect and install <code>uv</code> if needed - Create virtual environment and configure CUDA - Generate activation script with proper environment setup</p> <p>Manual pip Installation:</p> <pre><code># Clone the repository\ngit clone https://github.com/avitai/artifex.git\ncd artifex\n\n# Create and activate virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Or with extras\npip install -e '.[dev]'        # Development tools\npip install -e '.[cuda]'       # CUDA support\npip install -e '.[all]'        # Everything\n\n# For GPU support, manually configure environment:\nexport LD_LIBRARY_PATH=$PWD/.venv/lib/python3.*/site-packages/nvidia/*/lib:$LD_LIBRARY_PATH\nexport JAX_PLATFORMS=\"cuda,cpu\"\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\n</code></pre> <p>Note: Using <code>./setup.sh</code> is recommended even for pip users as it properly configures CUDA paths and environment variables automatically.</p> <p>For containerized deployment:</p> <pre><code># Pull the latest image\ndocker pull ghcr.io/avitai/artifex:latest\n\n# Run with GPU support\ndocker run --gpus all -it ghcr.io/avitai/artifex:latest\n\n# Run with volume mount for data\ndocker run --gpus all -v $(pwd)/data:/workspace/data \\\n  -it ghcr.io/avitai/artifex:latest\n\n# Or build locally\ndocker build -t artifex:local .\ndocker run --gpus all -it artifex:local\n</code></pre>"},{"location":"getting-started/installation/#local-virtual-environment-setup","title":"Local Virtual Environment Setup","text":"<p>Artifex's <code>setup.sh</code> and <code>activate.sh</code> scripts provide automated local development environment configuration with GPU support.</p> <p>Need Hardware-Specific Configuration?</p> <p>For detailed information on customizing the setup for different GPUs (NVIDIA, AMD), TPUs, Apple Silicon, or multi-GPU systems, see the Hardware Setup Guide. This comprehensive guide explains how <code>setup.sh</code> and <code>.env.example</code> work and how to customize them for your specific hardware.</p>"},{"location":"getting-started/installation/#understanding-the-setup-process","title":"Understanding the Setup Process","text":"<p>When you run <code>./setup.sh</code>, it creates a complete, self-contained development environment:</p> <p>Files Created:</p> <pre><code>artifex/\n\u251c\u2500\u2500 .venv/                    # Virtual environment\n\u2502   \u251c\u2500\u2500 bin/activate         # Standard venv activation\n\u2502   \u251c\u2500\u2500 lib/python3.*/       # Python packages\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 .env                      # Environment configuration\n\u251c\u2500\u2500 activate.sh              # Unified activation script\n\u2514\u2500\u2500 uv.lock                  # Dependency lock file\n</code></pre> <p>The <code>.env</code> File (GPU Mode):</p> <pre><code># CUDA library paths (from local venv installation)\nexport LD_LIBRARY_PATH=\".venv/lib/python3.*/site-packages/nvidia/*/lib:$LD_LIBRARY_PATH\"\n\n# JAX GPU configuration\nexport JAX_PLATFORMS=\"cuda,cpu\"\nexport XLA_PYTHON_CLIENT_PREALLOCATE=\"false\"\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.8\"\n\n# Project paths\nexport PYTHONPATH=\"${PYTHONPATH:+${PYTHONPATH}:}$(pwd)\"\nexport PYTEST_CUDA_ENABLED=\"true\"\n</code></pre> <p>The <code>.env</code> File (CPU Mode):</p> <pre><code># JAX CPU configuration\nexport JAX_PLATFORMS=\"cpu\"\nexport JAX_ENABLE_X64=\"0\"\n\n# Project paths\nexport PYTHONPATH=\"${PYTHONPATH:+${PYTHONPATH}:}$(pwd)\"\nexport PYTEST_CUDA_ENABLED=\"false\"\n</code></pre>"},{"location":"getting-started/installation/#using-the-environment","title":"Using the Environment","text":"<p>First Time Setup:</p> <pre><code># Run setup once\n./setup.sh\n\n# Activate environment\nsource ./activate.sh\n</code></pre> <p>Daily Workflow:</p> <pre><code># Simply activate the environment\nsource ./activate.sh\n\n# Your environment is now ready with:\n# - Virtual environment activated\n# - CUDA paths configured (if GPU)\n# - JAX optimally configured\n# - Project in PYTHONPATH\n\n# Work on your code...\nuv run pytest tests/\npython your_script.py\n\n# Deactivate when done\ndeactivate\n</code></pre>"},{"location":"getting-started/installation/#activation-script-features","title":"Activation Script Features","text":"<p>The <code>activate.sh</code> script provides:</p> <ol> <li>Smart Process Detection: Checks for running processes before deactivation</li> <li>GPU Verification: Tests GPU availability and displays device info</li> <li>Environment Status: Shows Python version, JAX backend, available devices</li> <li>Helpful Commands: Displays common development commands</li> <li>Error Handling: Provides clear messages if setup is incomplete</li> </ol> <p>Example activation output:</p> <pre><code>\ud83d\ude80 Activating Artifex Development Environment\n=============================================\n\u2705 Virtual environment activated\n\u2705 Environment configuration loaded\n   \ud83c\udfae GPU Mode: CUDA enabled\n\n\ud83d\udd0d Environment Status:\n   Python: Python 3.11.5\n   Virtual Environment: /path/to/artifex/.venv\n\n\ud83e\uddea JAX Configuration:\n   JAX version: 0.4.35\n   Default backend: gpu\n   Available devices: 1 total\n   \ud83c\udf89 GPU devices: 1 (['cuda(id=0)'])\n   \u2705 CUDA acceleration ready!\n\n\ud83d\ude80 Ready for Development!\n</code></pre>"},{"location":"getting-started/installation/#setup-script-options","title":"Setup Script Options","text":"<p>Customize setup for different scenarios:</p> <pre><code># Standard setup (auto-detects GPU)\n./setup.sh\n\n# CPU-only setup (laptop/CI)\n./setup.sh --cpu-only\n\n# Clean installation (remove all caches)\n./setup.sh --deep-clean\n\n# Force reinstall over existing environment\n./setup.sh --force\n\n# Verbose output for debugging\n./setup.sh --verbose\n\n# Combine options\n./setup.sh --force --deep-clean --verbose\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting-local-setup","title":"Troubleshooting Local Setup","text":"<p>Problem: <code>./setup.sh: Permission denied</code></p> <pre><code># Make script executable\nchmod +x setup.sh activate.sh\n./setup.sh\n</code></pre> <p>Problem: GPU not detected after setup</p> <pre><code># Check NVIDIA drivers\nnvidia-smi\n\n# Re-run setup with force\n./setup.sh --force\n\n# Check activation output for GPU status\nsource ./activate.sh\n</code></pre> <p>Problem: Environment variables not loaded</p> <pre><code># Verify .env file exists\ncat .env\n\n# Use 'source' not 'bash'\nsource ./activate.sh  # \u2705 Correct\nbash activate.sh       # \u274c Won't load environment\n</code></pre>"},{"location":"getting-started/installation/#gpu-setup","title":"GPU Setup","text":""},{"location":"getting-started/installation/#cuda-installation","title":"CUDA Installation","text":"<p>Artifex requires CUDA 12.0+ for GPU acceleration.</p>"},{"location":"getting-started/installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Check if CUDA is already installed\nnvcc --version\nnvidia-smi\n\n# If not installed, download CUDA Toolkit 12.0+\n# Visit: https://developer.nvidia.com/cuda-downloads\n\n# Example for Ubuntu 22.04\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cuda-toolkit-12-0\n\n# Add CUDA to PATH\necho 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<p>CUDA is not supported on macOS. Use Metal backend (experimental) or CPU mode.</p>"},{"location":"getting-started/installation/#windows-wsl2","title":"Windows (WSL2)","text":"<p>Windows users should use WSL2 with Ubuntu:</p> <pre><code># In PowerShell (Admin)\nwsl --install\n\n# Follow Ubuntu installation steps inside WSL2\n</code></pre>"},{"location":"getting-started/installation/#automated-gpu-setup-linux","title":"Automated GPU Setup (Linux)","text":"<p>Artifex provides automated CUDA setup through the main setup script:</p> <pre><code># Complete environment setup with CUDA (recommended)\n./setup.sh\n\n# This automatically:\n# - Detects GPU and CUDA availability\n# - Installs JAX with CUDA support\n# - Configures CUDA library paths\n# - Sets up JAX environment variables\n# - Creates activation script with GPU verification\n# - Tests GPU functionality\n\n# For CPU-only systems:\n./setup.sh --cpu-only\n</code></pre> <p>What gets configured automatically:</p> <ul> <li><code>LD_LIBRARY_PATH</code>: Points to CUDA libraries in <code>.venv/lib/python3.*/site-packages/nvidia/*/lib</code></li> <li><code>JAX_PLATFORMS</code>: Set to <code>\"cuda,cpu\"</code> for GPU or <code>\"cpu\"</code> for CPU-only</li> <li><code>XLA_PYTHON_CLIENT_PREALLOCATE</code>: Disabled for dynamic memory allocation</li> <li><code>XLA_PYTHON_CLIENT_MEM_FRACTION</code>: Set to 0.8 (use 80% of GPU memory)</li> <li>JAX CUDA plugins: Automatically matched to JAX version</li> </ul> <p>See Local Virtual Environment Setup for detailed explanation.</p>"},{"location":"getting-started/installation/#jax-gpu-installation","title":"JAX GPU Installation","text":"<p>After CUDA is installed:</p> <pre><code># Install JAX with CUDA support\npip install \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n# Verify GPU is detected\npython -c \"import jax; print(jax.devices())\"\n# Should print: [cuda(id=0)] or similar\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting-gpu-installation","title":"Troubleshooting GPU Installation","text":"<p>Problem: <code>jax.devices()</code> returns <code>[cpu(id=0)]</code> instead of GPU</p> <p>Solutions:</p> <ol> <li>Set LD_LIBRARY_PATH:</li> </ol> <pre><code>export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n</code></pre> <ol> <li>Use the setup script:</li> </ol> <pre><code>./scripts/fresh_cuda_setup.sh\n</code></pre> <ol> <li>Reinstall JAX with CUDA:</li> </ol> <pre><code>pip uninstall jax jaxlib\npip install \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre> <ol> <li>Check CUDA installation:</li> </ol> <pre><code>nvcc --version          # Should show CUDA version\nnvidia-smi              # Should show GPU info\npython -c \"import jax; print(jax.lib.xla_bridge.get_backend().platform)\"  # Should print 'gpu'\n</code></pre> <p>Problem: CUDA out of memory</p> <p>Solutions:</p> <ul> <li>Reduce batch size</li> <li>Enable mixed precision training (BF16/FP16)</li> <li>Use gradient accumulation</li> <li>Clear GPU cache: <code>jax.clear_caches()</code></li> </ul> <p>Problem: Slow training on GPU</p> <p>Solutions:</p> <ul> <li>Ensure XLA is enabled (automatic with JAX)</li> <li>Use JIT compilation: <code>@jax.jit</code></li> <li>Check GPU utilization: <code>nvidia-smi dmon</code></li> <li>Increase batch size for better GPU utilization</li> </ul>"},{"location":"getting-started/installation/#tpu-setup","title":"TPU Setup","text":"<p>For Google Cloud TPU:</p> <pre><code># Install JAX with TPU support\npip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n\n# Verify TPU is detected\npython -c \"import jax; print(jax.devices())\"\n# Should print TPU devices\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify your setup:</p> <pre><code># test_installation.py\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.models.vae import VAE\nfrom artifex.generative_models.core.configuration import (\n    VAEConfig,\n    EncoderConfig,\n    DecoderConfig,\n)\n\nprint(\"JAX version:\", jax.__version__)\nprint(\"JAX backend:\", jax.default_backend())\nprint(\"Available devices:\", jax.devices())\n\n# Test simple computation\nx = jnp.array([1.0, 2.0, 3.0])\nprint(\"JAX computation test:\", jnp.sum(x))\n\n# Test Artifex imports\nencoder = EncoderConfig(\n    name=\"test_encoder\",\n    input_shape=(28, 28, 1),\n    latent_dim=32,\n    hidden_dims=(64, 128),\n    activation=\"relu\",\n)\ndecoder = DecoderConfig(\n    name=\"test_decoder\",\n    latent_dim=32,\n    output_shape=(28, 28, 1),\n    hidden_dims=(128, 64),\n    activation=\"relu\",\n)\nconfig = VAEConfig(\n    name=\"test_vae\",\n    encoder=encoder,\n    decoder=decoder,\n    encoder_type=\"dense\",\n    kl_weight=1.0,\n)\nrngs = nnx.Rngs(0)\nmodel = VAE(config, rngs=rngs)\nprint(\"Artifex model created successfully!\")\n\n# Test forward pass (model uses internal RNGs)\nbatch = jax.random.normal(jax.random.PRNGKey(0), (4, 28, 28, 1))\noutputs = model(batch)\nprint(\"Forward pass successful!\")\nprint(\"Output keys:\", list(outputs.keys()))\n</code></pre> <p>Run the verification:</p> <pre><code>python test_installation.py\n</code></pre> <p>Expected output:</p> <pre><code>JAX version: 0.8.2\nJAX backend: gpu (or cpu)\nAvailable devices: [CudaDevice(id=0)] (or [CpuDevice(id=0)])\nJAX computation test: 6.0\nArtifex model created successfully!\nForward pass successful!\nOutput keys: ['reconstructed', 'mean', 'log_var', 'z']\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to Artifex:</p> <p>Quick Development Setup:</p> <pre><code># Clone repository\ngit clone https://github.com/avitai/artifex.git\ncd artifex\n\n# Run setup script (includes dev tools)\n./setup.sh\n\n# Activate environment\nsource ./activate.sh\n\n# Install pre-commit hooks\nuv run pre-commit install\n\n# Verify development setup\nuv run pytest tests/ -x              # Run tests\nuv run ruff check src/               # Linting\nuv run ruff format src/              # Formatting\nuv run pyright src/                  # Type checking\n</code></pre> <p>What's included in development setup:</p> <ul> <li>All core dependencies (<code>uv sync --extra all</code> or <code>--extra dev</code>)</li> <li>Development tools: pytest, ruff, pyright, pre-commit</li> <li>GPU support (if available)</li> <li>Documentation tools (mkdocs, mkdocstrings)</li> <li>Benchmarking utilities</li> <li>Test coverage tools</li> </ul> <p>Daily development workflow:</p> <pre><code># Start your work session\nsource ./activate.sh\n\n# Make changes to code\n# ...\n\n# Run tests before committing\nuv run pytest tests/ -x\n\n# Run pre-commit checks\nuv run pre-commit run --all-files\n\n# Commit your changes\ngit add .\ngit commit -m \"Your commit message\"\n\n# Deactivate when done\ndeactivate\n</code></pre>"},{"location":"getting-started/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"getting-started/installation/#linux","title":"Linux","text":"<ul> <li>Recommended: Ubuntu 20.04 LTS or Ubuntu 22.04 LTS</li> <li>Ensure NVIDIA drivers are up-to-date: <code>sudo ubuntu-drivers autoinstall</code></li> <li>For multi-GPU: Set <code>CUDA_VISIBLE_DEVICES=\"0,1\"</code></li> </ul>"},{"location":"getting-started/installation/#macos_1","title":"macOS","text":"<ul> <li>Apple Silicon (M1/M2/M3): JAX has experimental support</li> </ul> <pre><code>pip install jax-metal  # For Apple M1/M2/M3\n</code></pre> <ul> <li>Intel Macs: CPU-only mode is fully supported</li> <li>XCode Command Line Tools required: <code>xcode-select --install</code></li> </ul>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<ul> <li>WSL2 Required: Native Windows support is limited</li> <li>Install Ubuntu 22.04 from Microsoft Store</li> <li>Follow Linux installation inside WSL2</li> <li>GPU support requires Windows 11 + CUDA in WSL2</li> </ul>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Configure Artifex behavior with environment variables:</p> <pre><code># JAX Configuration\nexport JAX_PLATFORMS=gpu              # or 'cpu', 'tpu'\nexport XLA_PYTHON_CLIENT_PREALLOCATE=false  # Disable memory preallocation\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=0.75  # Use 75% of GPU memory\n\n# Artifex Configuration\nexport ARTIFEX_CACHE_DIR=~/.cache/artifex  # Cache directory\nexport ARTIFEX_DATA_DIR=~/artifex_data     # Data directory\nexport ARTIFEX_LOG_LEVEL=INFO               # Logging level\n\n# Development\nexport ENABLE_BLACKJAX_TESTS=1      # Enable expensive BlackJAX tests\n</code></pre> <p>Add to <code>~/.bashrc</code> or <code>~/.zshrc</code> for persistence.</p>"},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'artifex'</code></p> <p>Solutions:</p> <pre><code># Verify installation\npip list | grep artifex\n\n# Reinstall\npip install -e .\n\n# Check Python path\npython -c \"import sys; print(sys.path)\"\n</code></pre>"},{"location":"getting-started/installation/#version-conflicts","title":"Version Conflicts","text":"<p>Problem: Dependency version conflicts</p> <p>Solutions:</p> <pre><code># Clean install\ndeactivate\nrm -rf .venv uv.lock\nuv venv\nsource .venv/bin/activate\nuv sync --all-extras\n</code></pre>"},{"location":"getting-started/installation/#memory-issues","title":"Memory Issues","text":"<p>Problem: Out of memory during installation</p> <p>Solutions:</p> <pre><code># Increase pip timeout and disable parallel builds\npip install --no-cache-dir -e .\n\n# Or use uv which is more memory efficient\nuv sync --all-extras\n</code></pre>"},{"location":"getting-started/installation/#updating-artifex","title":"Updating Artifex","text":"<p>Keep Artifex up-to-date:</p> <pre><code># From PyPI (when available)\npip install --upgrade artifex\n\n# From source\ncd artifex\ngit pull origin main\nuv sync --all-extras  # or: pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#uninstallation","title":"Uninstallation","text":"<p>Remove Artifex completely:</p> <pre><code># If installed from PyPI\npip uninstall artifex\n\n# If installed from source\npip uninstall artifex\n\n# Remove cache and data (optional)\nrm -rf ~/.cache/artifex\nrm -rf ~/artifex_data\n\n# Remove virtual environment\ndeactivate\nrm -rf .venv\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Quick Start: Follow the Quickstart Guide to train your first model</li> <li>Core Concepts: Learn about Artifex architecture</li> <li>Examples: Explore ready-to-run Examples</li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ul> <li>Documentation: Check this guide and the documentation index</li> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Ask questions</li> <li>Discord: Join our community (coming soon)</li> </ul>"},{"location":"getting-started/installation/#hardware-recommendations","title":"Hardware Recommendations","text":""},{"location":"getting-started/installation/#for-researchdevelopment","title":"For Research/Development","text":"<ul> <li>CPU: 8+ cores (Intel i7/i9 or AMD Ryzen 7/9)</li> <li>RAM: 16-32GB</li> <li>GPU: NVIDIA RTX 3060 (12GB) or better</li> <li>Storage: SSD with 100GB+ free space</li> </ul>"},{"location":"getting-started/installation/#for-production","title":"For Production","text":"<ul> <li>CPU: 16+ cores (Intel Xeon or AMD EPYC)</li> <li>RAM: 64GB+</li> <li>GPU: NVIDIA A100 (40/80GB) or H100</li> <li>Storage: NVMe SSD with 500GB+ free space</li> </ul>"},{"location":"getting-started/installation/#for-large-scale-training","title":"For Large-Scale Training","text":"<ul> <li>Multi-GPU: 4-8x NVIDIA A100 or H100</li> <li>RAM: 256GB+</li> <li>Network: High-speed interconnect (NVLink, InfiniBand)</li> <li>Storage: Parallel file system (Lustre, GPFS)</li> </ul> <p>Last Updated: 2025-10-13</p> <p>Installation Support: For installation help, open an issue.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get started with Artifex in 5 minutes! This guide walks you through installing Artifex and training your first generative model.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>8GB RAM (16GB recommended)</li> <li>Optional: NVIDIA GPU with CUDA 12.0+ for faster training</li> </ul>"},{"location":"getting-started/quickstart/#step-1-install-artifex","title":"Step 1: Install Artifex","text":"<p>Choose your preferred installation method:</p> From Source (Recommended)PyPI (Coming Soon) <pre><code># Clone repository\ngit clone https://github.com/avitai/artifex.git\ncd artifex\n\n# Install with uv (fastest)\nuv venv &amp;&amp; source .venv/bin/activate\nuv sync --all-extras\n\n# Or with pip\npython -m venv .venv &amp;&amp; source .venv/bin/activate\npip install -e '.[dev]'\n</code></pre> <pre><code>pip install artifex\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import jax; print(f'JAX backend: {jax.default_backend()}')\"\n# Should print: JAX backend: gpu (or cpu)\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-train-your-first-vae","title":"Step 2: Train Your First VAE","text":"<p>Create a new Python file <code>train_vae.py</code>:</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\nfrom datarax.sources import TFDSEagerSource\nfrom datarax.sources.tfds_source import TFDSEagerConfig\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration import (\n    DecoderConfig,\n    EncoderConfig,\n    VAEConfig,\n)\nfrom artifex.generative_models.models.vae import VAE\nfrom artifex.generative_models.training import train_epoch_staged\nfrom artifex.generative_models.training.trainers import VAETrainer, VAETrainingConfig\n\n# 1. Load MNIST with TFDSEagerSource (pure JAX, no TF during training)\nprint(\"Loading MNIST...\")\ntfds_config = TFDSEagerConfig(name=\"mnist\", split=\"train\", shuffle=True, seed=42)\nmnist_source = TFDSEagerSource(tfds_config, rngs=nnx.Rngs(0))\n\n# Get images as JAX array and normalize to [0, 1]\nimages = mnist_source.data[\"image\"].astype(jnp.float32) / 255.0\nnum_samples = len(mnist_source)\nprint(f\"Loaded {num_samples} images, shape: {images.shape}\")\n\n# 2. Configure the model - CNN architecture for better image quality\nencoder = EncoderConfig(\n    name=\"mnist_cnn_encoder\",\n    input_shape=(28, 28, 1),\n    latent_dim=20,\n    hidden_dims=(32, 64, 128),\n    activation=\"relu\",\n    use_batch_norm=False,\n)\n\ndecoder = DecoderConfig(\n    name=\"mnist_cnn_decoder\",\n    latent_dim=20,\n    output_shape=(28, 28, 1),\n    hidden_dims=(32, 64, 128),\n    activation=\"relu\",\n    batch_norm=False,\n)\n\nmodel_config = VAEConfig(\n    name=\"mnist_cnn_vae\",\n    encoder=encoder,\n    decoder=decoder,\n    encoder_type=\"cnn\",\n    kl_weight=1.0,\n)\n\n# 3. Create model, optimizer, and trainer\nmodel = VAE(model_config, rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(2e-3), wrt=nnx.Param)\n\n# VAETrainer with KL annealing for stable training\ntrainer = VAETrainer(\n    VAETrainingConfig(\n        kl_annealing=\"linear\",\n        kl_warmup_steps=2000,\n        beta=1.0,\n    )\n)\n\nstate_leaves = jax.tree.leaves(nnx.state(model))\nparam_count = sum(p.size for p in state_leaves if hasattr(p, \"size\"))\nprint(f\"Model created with ~{param_count/1e3:.1f}K parameters\")\n\n# 4. Stage data on GPU and train with JIT-compiled loop\nprint(\"Staging data on GPU...\")\nstaged_data = jax.device_put(images)\n\nNUM_EPOCHS = 20\nBATCH_SIZE = 128\n\n# Warmup JIT compilation\nwarmup_rng = jax.random.key(999)\nloss_fn = trainer.create_loss_fn(step=0, loss_type=\"bce\")\n_ = train_epoch_staged(\n    model, optimizer, staged_data[:256],\n    batch_size=128, rng=warmup_rng, loss_fn=loss_fn,\n)\nprint(\"JIT warmup complete.\")\n\n# Training loop\nprint(f\"Training for {NUM_EPOCHS} epochs...\")\nstep = 0\nfor epoch in range(NUM_EPOCHS):\n    rng = jax.random.key(epoch)\n    loss_fn = trainer.create_loss_fn(step=step, loss_type=\"bce\")\n\n    step, metrics = train_epoch_staged(\n        model, optimizer, staged_data,\n        batch_size=BATCH_SIZE, rng=rng, loss_fn=loss_fn, base_step=step,\n    )\n    print(f\"Epoch {epoch + 1:2d}/{NUM_EPOCHS} | Loss: {metrics['loss']:7.2f}\")\n\nprint(\"Training complete!\")\n\n# 5. Generate samples and reconstruct\nsamples = model.sample(n_samples=16)\nprint(f\"Generated {samples.shape[0]} samples\")\n\ntest_images = jnp.array(images[:8])\nreconstructed = model.reconstruct(test_images, deterministic=True)\nprint(f\"Reconstructed {reconstructed.shape[0]} images\")\n\nprint(\"Success! You've trained your first VAE with Artifex!\")\n</code></pre> <p>Run the script:</p> <pre><code>python train_vae.py\n</code></pre> <p>Expected output:</p> <pre><code>Loading MNIST...\nLoaded 60000 images, shape: (60000, 28, 28, 1)\nModel created with ~314.9K parameters\nStaging data on GPU...\nJIT warmup complete.\nTraining for 20 epochs...\nEpoch  1/20 | Loss:  111.04\nEpoch  2/20 | Loss:   86.44\nEpoch  3/20 | Loss:   89.81\n...\nEpoch 20/20 | Loss:   95.31\nTraining complete!\nGenerated 16 samples\nReconstructed 8 images\nSuccess! You've trained your first VAE with Artifex!\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-visualize-results-optional","title":"Step 3: Visualize Results (Optional)","text":"<p>Add visualization to your script:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Visualize generated samples (4x4 grid)\nfig, axes = plt.subplots(4, 4, figsize=(8, 8))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(samples[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n    ax.axis('off')\nplt.suptitle('Generated Samples from VAE')\nplt.tight_layout()\nplt.savefig('vae_samples.png', dpi=150)\nprint(\"Saved samples to vae_samples.png\")\n\n# Visualize reconstructions (original vs reconstructed)\nfig, axes = plt.subplots(2, 8, figsize=(16, 4))\nfor i in range(8):\n    axes[0, i].imshow(test_images[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n    axes[0, i].axis('off')\n    axes[1, i].imshow(reconstructed[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n    axes[1, i].axis('off')\naxes[0, 0].set_title('Original')\naxes[1, 0].set_title('Reconstructed')\nplt.tight_layout()\nplt.savefig('vae_reconstruction.png', dpi=150)\nprint(\"Saved reconstruction to vae_reconstruction.png\")\n</code></pre> <p>Generated VAE Samples:</p> <p></p> <p>Original vs Reconstructed:</p> <p></p>"},{"location":"getting-started/quickstart/#what-you-just-did","title":"What You Just Did","text":"<p>In just a few minutes, you:</p> <ol> <li>Installed Artifex - Set up the complete environment</li> <li>Loaded data with TFDSEagerSource - Pure JAX data loading, no TensorFlow overhead during training</li> <li>Created a CNN-based VAE - Built a 3-layer convolutional variational autoencoder</li> <li>Used high-performance training - JIT-compiled training loops with GPU-staged data</li> <li>Trained for 20 epochs - Achieved fast training with the staged training loop</li> <li>Generated samples - Created new digit images from the learned distribution</li> <li>Reconstructed images - Verified high-quality encoder-decoder reconstruction</li> </ol>"},{"location":"getting-started/quickstart/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/quickstart/#high-performance-data-loading","title":"High-Performance Data Loading","text":"<pre><code>from datarax.sources import TFDSEagerSource\nfrom datarax.sources.tfds_source import TFDSEagerConfig\n\n# TFDSEagerSource loads ALL data to JAX arrays at initialization\ntfds_config = TFDSEagerConfig(name=\"mnist\", split=\"train\", shuffle=True, seed=42)\nmnist_source = TFDSEagerSource(tfds_config, rngs=nnx.Rngs(0))\n\n# Pure JAX from here - no TensorFlow during training\nimages = mnist_source.data[\"image\"].astype(jnp.float32) / 255.0\n</code></pre> <p>This approach eliminates TensorFlow's data loading overhead during training, resulting in much faster iteration.</p>"},{"location":"getting-started/quickstart/#configuration-system","title":"Configuration System","text":"<pre><code># Nested configuration for encoder and decoder\nencoder = EncoderConfig(\n    input_shape=(28, 28, 1),\n    latent_dim=20,\n    hidden_dims=(32, 64, 128),  # CNN layer channels\n    activation=\"relu\",\n)\ndecoder = DecoderConfig(\n    latent_dim=20,\n    output_shape=(28, 28, 1),\n    hidden_dims=(32, 64, 128),  # Symmetric to encoder\n)\nconfig = VAEConfig(\n    encoder=encoder,\n    decoder=decoder,\n    encoder_type=\"cnn\",  # \"dense\", \"cnn\", or \"resnet\"\n    kl_weight=1.0,\n)\n</code></pre> <p>Artifex uses a unified configuration system based on frozen dataclasses for type-safe, validated configurations.</p>"},{"location":"getting-started/quickstart/#jit-compiled-training-loops","title":"JIT-Compiled Training Loops","text":"<pre><code>from artifex.generative_models.training import train_epoch_staged\n\n# Stage data on GPU for maximum performance\nstaged_data = jax.device_put(images)\n\n# JIT-compiled training loop - 100-500x faster than Python loops\nstep, metrics = train_epoch_staged(\n    model, optimizer, staged_data,\n    batch_size=128, rng=rng, loss_fn=loss_fn,\n)\n</code></pre> <p>The <code>train_epoch_staged</code> function:</p> <ul> <li>Pre-stages data on GPU with <code>jax.device_put()</code></li> <li>Uses JIT-compiled training steps</li> <li>Achieves 100-500x speedup over naive Python loops</li> </ul>"},{"location":"getting-started/quickstart/#vaetrainer-with-kl-annealing","title":"VAETrainer with KL Annealing","text":"<pre><code>from artifex.generative_models.training.trainers import VAETrainer, VAETrainingConfig\n\n# Configure trainer with KL annealing for stable training\ntrainer = VAETrainer(\n    VAETrainingConfig(\n        kl_annealing=\"linear\",  # \"none\", \"linear\", \"sigmoid\", \"cyclical\"\n        kl_warmup_steps=2000,   # Steps to reach full KL weight\n        beta=1.0,               # Beta-VAE weight (&gt;1 for disentanglement)\n    )\n)\n\n# Create loss function with current step for annealing\nloss_fn = trainer.create_loss_fn(step=step, loss_type=\"bce\")\n</code></pre> <p>KL Annealing: Gradually increases the KL term weight during training, allowing the model to learn good reconstructions first before regularization kicks in.</p> <p>Supported Loss Types:</p> <ul> <li><code>\"bce\"</code>: Binary cross-entropy (for normalized [0,1] images)</li> <li><code>\"mse\"</code>: Mean squared error (for continuous data)</li> </ul>"},{"location":"getting-started/quickstart/#try-different-models","title":"Try Different Models","text":""},{"location":"getting-started/quickstart/#train-a-diffusion-model","title":"Train a Diffusion Model","text":"<pre><code>import jax\nimport jax.numpy as jnp\nimport optax\nfrom datarax import from_source\nfrom datarax.core.config import ElementOperatorConfig\nfrom datarax.dag.nodes import OperatorNode\nfrom datarax.operators import ElementOperator\nfrom datarax.sources import TfdsDataSourceConfig, TFDSSource\nfrom flax import nnx\n\nfrom artifex.generative_models.models.diffusion import DDPMModel\nfrom artifex.generative_models.core.configuration import (\n    DDPMConfig,\n    UNetBackboneConfig,\n    NoiseScheduleConfig,\n)\nfrom artifex.generative_models.training.trainers import (\n    DiffusionTrainer,\n    DiffusionTrainingConfig,\n)\n\n# 1. Load Fashion-MNIST with datarax\ndef normalize(element, _key):\n    \"\"\"Normalize images to [-1, 1] for diffusion models.\"\"\"\n    image = element.data[\"image\"].astype(jnp.float32) / 127.5 - 1.0\n    return element.replace(data={**element.data, \"image\": image})\n\nsource = TFDSSource(\n    TfdsDataSourceConfig(name=\"fashion_mnist\", split=\"train\", shuffle=True),\n    rngs=nnx.Rngs(0),\n)\nnormalize_op = ElementOperator(\n    ElementOperatorConfig(stochastic=False), fn=normalize, rngs=nnx.Rngs(1)\n)\npipeline = from_source(source, batch_size=64) &gt;&gt; OperatorNode(normalize_op)\n\n# 2. Create DDPM configuration\nbackbone = UNetBackboneConfig(\n    name=\"unet_backbone\",\n    in_channels=1,\n    out_channels=1,\n    hidden_dims=(32, 64, 128),\n    channel_mult=(1, 2, 4),\n    activation=\"silu\",\n)\n\nnoise_schedule = NoiseScheduleConfig(\n    name=\"cosine_schedule\",\n    schedule_type=\"cosine\",\n    num_timesteps=1000,\n    beta_start=1e-4,\n    beta_end=2e-2,\n)\n\nconfig = DDPMConfig(\n    name=\"fashion_ddpm\",\n    input_shape=(28, 28, 1),  # HWC format\n    backbone=backbone,\n    noise_schedule=noise_schedule,\n)\n\n# 3. Create model and optimizer\nrngs = nnx.Rngs(42)\nmodel = DDPMModel(config, rngs=rngs)\noptimizer = nnx.Optimizer(model, optax.adamw(1e-4), wrt=nnx.Param)\n\n# 4. Configure trainer with SOTA techniques (min-SNR weighting, EMA)\ntrainer = DiffusionTrainer(\n    noise_schedule=model.noise_schedule,\n    config=DiffusionTrainingConfig(\n        loss_weighting=\"min_snr\",  # Min-SNR weighting for faster convergence\n        snr_gamma=5.0,\n        ema_decay=0.9999,\n    ),\n)\n\n# JIT-compile the train_step for performance\njit_train_step = nnx.jit(trainer.train_step)\n\n# 5. Training loop\nrng = jax.random.PRNGKey(0)\nstep = 0\n\nfor batch in pipeline:\n    rng, step_rng = jax.random.split(rng)\n    _, metrics = jit_train_step(model, optimizer, {\"image\": batch[\"image\"]}, step_rng)\n    trainer.update_ema(model)  # EMA updates outside JIT\n\n    if step % 100 == 0:\n        print(f\"Step {step}: loss={metrics['loss']:.4f}\")\n    step += 1\n\n# 6. Generate samples\nsamples = model.sample(n_samples_or_shape=8, steps=100)\nprint(f\"Generated samples shape: {samples.shape}\")\n</code></pre>"},{"location":"getting-started/quickstart/#train-a-gan","title":"Train a GAN","text":"<pre><code>from flax import nnx\nfrom artifex.generative_models.models.gan import DCGAN\nfrom artifex.generative_models.core.configuration import (\n    DCGANConfig,\n    ConvGeneratorConfig,\n    ConvDiscriminatorConfig,\n)\n\n# Create DCGAN configuration with convolutional networks\ngenerator = ConvGeneratorConfig(\n    name=\"dcgan_generator\",\n    latent_dim=100,\n    hidden_dims=(512, 256, 128, 64),\n    output_shape=(1, 28, 28),  # CHW format\n    activation=\"relu\",\n    batch_norm=True,\n    kernel_size=(4, 4),\n    stride=(2, 2),\n    padding=\"SAME\",\n)\n\ndiscriminator = ConvDiscriminatorConfig(\n    name=\"dcgan_discriminator\",\n    hidden_dims=(64, 128, 256, 512),\n    input_shape=(1, 28, 28),  # CHW format\n    activation=\"leaky_relu\",\n    leaky_relu_slope=0.2,\n    batch_norm=True,\n    kernel_size=(4, 4),\n    stride=(2, 2),\n    padding=\"SAME\",\n)\n\nconfig = DCGANConfig(\n    name=\"mnist_dcgan\",\n    generator=generator,\n    discriminator=discriminator,\n)\n\nrngs = nnx.Rngs(params=0, dropout=1, sample=2)\nmodel = DCGAN(config, rngs=rngs)\nprint(f\"DCGAN created with latent_dim={config.generator.latent_dim}\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have a working setup, explore more:</p> <ul> <li> <p> Learn Core Concepts</p> <p>Understand generative modeling fundamentals and Artifex architecture</p> <p> Core Concepts</p> </li> <li> <p> Build Your First Model</p> </li> <li> <p> Explore Model Guides</p> <p>Deep dive into VAEs, GANs, Diffusion, Flows, and more</p> <p> VAE Guide  Model Implementations</p> </li> <li> <p> Check Examples</p> <p>Ready-to-run examples for various models and use cases</p> <p> Examples</p> </li> </ul>"},{"location":"getting-started/quickstart/#common-next-questions","title":"Common Next Questions","text":""},{"location":"getting-started/quickstart/#how-do-i-use-real-data","title":"How do I use real data?","text":"<p>See the Data Pipeline Guide for loading CIFAR-10, ImageNet, and custom datasets.</p>"},{"location":"getting-started/quickstart/#how-do-i-save-and-load-models","title":"How do I save and load models?","text":"<pre><code># Save\nfrom flax.training import checkpoints\ncheckpoints.save_checkpoint('checkpoints/', model, step=100)\n\n# Load\nmodel = checkpoints.restore_checkpoint('checkpoints/', model)\n</code></pre> <p>See Training Guide for details on checkpointing.</p>"},{"location":"getting-started/quickstart/#how-do-i-train-on-multiple-gpus","title":"How do I train on multiple GPUs?","text":"<p>Artifex supports distributed training out of the box. See Distributed Training Guide.</p>"},{"location":"getting-started/quickstart/#what-if-i-get-errors","title":"What if I get errors?","text":"<p>If you encounter issues, open an issue on GitHub.</p>"},{"location":"getting-started/quickstart/#quick-reference","title":"Quick Reference","text":""},{"location":"getting-started/quickstart/#model-types","title":"Model Types","text":"Type Model Class Config Class Use Case VAE <code>VAE</code> <code>VAEConfig</code> Latent representations, data compression GAN <code>DCGAN</code>, <code>GAN</code> <code>DCGANConfig</code>, <code>GANConfig</code> High-quality image generation Diffusion <code>DDPMModel</code> <code>DDPMConfig</code> State-of-the-art generation, controllable Flow <code>FlowModel</code> <code>FlowConfig</code> Exact likelihood, invertible transformations EBM <code>EnergyBasedModel</code> <code>EBMConfig</code> Energy-based modeling, composable"},{"location":"getting-started/quickstart/#key-commands","title":"Key Commands","text":"<pre><code># Install\nuv sync --all-extras\n\n# Run tests\npytest tests/ -v\n\n# Format code\nruff format src/\n\n# Type check\npyright src/\n\n# Build docs\nmkdocs serve\n</code></pre>"},{"location":"getting-started/quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Comprehensive guides and API reference</li> <li>Examples: Ready-to-run code in <code>examples/</code></li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul> <p>Congratulations! You've completed the quickstart guide. You're now ready to build more sophisticated generative models with Artifex!</p> <p>Next recommended step: Core Concepts to understand the architecture better, or explore the VAE Guide for advanced VAE techniques.</p>"},{"location":"guides/configuration/","title":"Configuration Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive configuration documentation.</p>"},{"location":"guides/configuration/#overview","title":"Overview","text":"<p>Learn how to configure Artifex models and training:</p> <ul> <li>Model configuration classes</li> <li>Training configuration</li> <li>Data configuration</li> <li>Nested and composed configurations</li> </ul>"},{"location":"guides/configuration/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Configuration</li> <li>Factory Guide</li> <li>Extensions Guide</li> </ul>"},{"location":"guides/configuration/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"guides/configuration/#frozen-dataclasses","title":"Frozen Dataclasses","text":"<p>All configurations use frozen dataclasses for immutability.</p>"},{"location":"guides/configuration/#nested-configuration","title":"Nested Configuration","text":"<p>Complex models use nested configuration objects.</p>"},{"location":"guides/configuration/#validation","title":"Validation","text":"<p>Configurations are validated at creation time using Pydantic.</p>"},{"location":"guides/custom-extensions/","title":"Custom Extensions Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive custom extension documentation.</p>"},{"location":"guides/custom-extensions/#overview","title":"Overview","text":"<p>Learn how to create custom extensions:</p> <ul> <li>Creating custom model types</li> <li>Implementing custom losses</li> <li>Building custom samplers</li> <li>Registering with factories</li> </ul>"},{"location":"guides/custom-extensions/#related-documentation","title":"Related Documentation","text":"<ul> <li>Extensions Guide</li> <li>Factory Guide</li> <li>Configuration Guide</li> </ul>"},{"location":"guides/custom-extensions/#extension-patterns","title":"Extension Patterns","text":""},{"location":"guides/custom-extensions/#model-extension","title":"Model Extension","text":"<p>Inherit from base classes and implement required methods.</p>"},{"location":"guides/custom-extensions/#loss-extension","title":"Loss Extension","text":"<p>Create callable loss functions with standard interface.</p>"},{"location":"guides/custom-extensions/#sampler-extension","title":"Sampler Extension","text":"<p>Implement sampling protocol for custom generation.</p>"},{"location":"guides/extensions/","title":"Extensions Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive extension documentation.</p>"},{"location":"guides/extensions/#overview","title":"Overview","text":"<p>Learn how to extend Artifex with custom functionality:</p> <ul> <li>Creating custom model architectures</li> <li>Adding new loss functions</li> <li>Implementing custom samplers</li> <li>Building domain-specific extensions</li> </ul>"},{"location":"guides/extensions/#related-documentation","title":"Related Documentation","text":"<ul> <li>Custom Extensions</li> <li>Configuration Guide</li> <li>Factory Guide</li> </ul>"},{"location":"guides/extensions/#extension-types","title":"Extension Types","text":""},{"location":"guides/extensions/#model-extensions","title":"Model Extensions","text":"<p>Extend base model classes with new capabilities.</p>"},{"location":"guides/extensions/#loss-extensions","title":"Loss Extensions","text":"<p>Create custom loss functions for specialized training objectives.</p>"},{"location":"guides/extensions/#sampler-extensions","title":"Sampler Extensions","text":"<p>Implement custom sampling strategies for generation.</p>"},{"location":"guides/extensions/#modality-extensions","title":"Modality Extensions","text":"<p>Add support for new data modalities.</p>"},{"location":"guides/factory/","title":"Factory Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive factory pattern documentation.</p>"},{"location":"guides/factory/#overview","title":"Overview","text":"<p>Learn about Artifex's factory pattern:</p> <ul> <li>Model factory for creating models from config</li> <li>Trainer factory for training pipelines</li> <li>Dataset factory for data loading</li> <li>Custom factory registration</li> </ul>"},{"location":"guides/factory/#related-documentation","title":"Related Documentation","text":"<ul> <li>Configuration Guide</li> <li>Extensions Guide</li> <li>Factory API</li> </ul>"},{"location":"guides/factory/#key-concepts","title":"Key Concepts","text":""},{"location":"guides/factory/#factory-pattern","title":"Factory Pattern","text":"<p>Centralized model creation from configuration objects.</p>"},{"location":"guides/factory/#registration","title":"Registration","text":"<p>Registering custom models with the factory.</p>"},{"location":"guides/factory/#validation","title":"Validation","text":"<p>Configuration validation before instantiation.</p>"},{"location":"guides/geometric-models/","title":"Geometric Models Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive geometric modeling documentation.</p>"},{"location":"guides/geometric-models/#overview","title":"Overview","text":"<p>Learn about geometric deep learning in Artifex:</p> <ul> <li>Point cloud processing</li> <li>Graph neural networks</li> <li>SE(3) equivariant networks</li> <li>Mesh generation</li> </ul>"},{"location":"guides/geometric-models/#related-documentation","title":"Related Documentation","text":"<ul> <li>Point Clouds</li> <li>Protein Modeling</li> <li>Extensions Guide</li> </ul>"},{"location":"guides/geometric-models/#key-concepts","title":"Key Concepts","text":""},{"location":"guides/geometric-models/#point-clouds","title":"Point Clouds","text":"<p>Unordered sets of 3D points with optional features.</p>"},{"location":"guides/geometric-models/#equivariance","title":"Equivariance","text":"<p>Networks that respect geometric symmetries.</p>"},{"location":"guides/geometric-models/#graph-networks","title":"Graph Networks","text":"<p>Message passing on molecular and structural graphs.</p>"},{"location":"guides/modalities/","title":"Modalities Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive modality documentation.</p>"},{"location":"guides/modalities/#overview","title":"Overview","text":"<p>Learn about Artifex's modality system:</p> <ul> <li>Image modality features</li> <li>Text modality features</li> <li>Audio modality features</li> <li>Protein and geometric modalities</li> </ul>"},{"location":"guides/modalities/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling</li> <li>Geometric Models</li> <li>Extensions Guide</li> </ul>"},{"location":"guides/modalities/#available-modalities","title":"Available Modalities","text":""},{"location":"guides/modalities/#image","title":"Image","text":"<p>Convolutional architectures, FID/IS metrics, image-specific augmentations.</p>"},{"location":"guides/modalities/#text","title":"Text","text":"<p>Tokenization, transformer architectures, perplexity metrics.</p>"},{"location":"guides/modalities/#audio","title":"Audio","text":"<p>Spectrograms, MFCCs, audio-specific preprocessing.</p>"},{"location":"guides/modalities/#protein","title":"Protein","text":"<p>Sequence encoding, structure handling, biological metrics.</p>"},{"location":"guides/modalities/#geometric","title":"Geometric","text":"<p>Point clouds, meshes, SE(3) equivariant processing.</p>"},{"location":"guides/point-clouds/","title":"Point Clouds Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive point cloud documentation.</p>"},{"location":"guides/point-clouds/#overview","title":"Overview","text":"<p>Learn about point cloud processing in Artifex:</p> <ul> <li>Point cloud representation</li> <li>PointNet and PointNet++ architectures</li> <li>Point cloud generation</li> <li>3D shape completion</li> </ul>"},{"location":"guides/point-clouds/#related-documentation","title":"Related Documentation","text":"<ul> <li>Geometric Models</li> <li>Protein Modeling</li> <li>Extensions Guide</li> </ul>"},{"location":"guides/point-clouds/#key-concepts","title":"Key Concepts","text":""},{"location":"guides/point-clouds/#point-cloud-format","title":"Point Cloud Format","text":"<p>Unordered sets of (x, y, z) coordinates with optional features.</p>"},{"location":"guides/point-clouds/#permutation-invariance","title":"Permutation Invariance","text":"<p>Networks that handle arbitrary point orderings.</p>"},{"location":"guides/point-clouds/#local-and-global-features","title":"Local and Global Features","text":"<p>Combining point-level and cloud-level information.</p>"},{"location":"guides/protein-extensions/","title":"Protein Extensions Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive protein extension documentation.</p>"},{"location":"guides/protein-extensions/#overview","title":"Overview","text":"<p>Learn about protein-specific extensions:</p> <ul> <li>Protein sequence encoders</li> <li>Structure prediction modules</li> <li>SE(3) equivariant layers</li> <li>Biological metrics</li> </ul>"},{"location":"guides/protein-extensions/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling</li> <li>Geometric Models</li> <li>Extensions Guide</li> </ul>"},{"location":"guides/protein-extensions/#key-components","title":"Key Components","text":""},{"location":"guides/protein-extensions/#sequence-encoders","title":"Sequence Encoders","text":"<p>Encoding amino acid sequences for model input.</p>"},{"location":"guides/protein-extensions/#structure-modules","title":"Structure Modules","text":"<p>Predicting and generating 3D protein structures.</p>"},{"location":"guides/protein-extensions/#equivariant-layers","title":"Equivariant Layers","text":"<p>SE(3) equivariant operations for proper coordinate handling.</p>"},{"location":"guides/protein-modeling/","title":"Protein Modeling Guide","text":"<p>Coming Soon</p> <p>This guide is being developed. Check back for comprehensive protein modeling documentation.</p>"},{"location":"guides/protein-modeling/#overview","title":"Overview","text":"<p>Learn about protein structure prediction and generation:</p> <ul> <li>Protein sequence encoding</li> <li>Structure prediction models</li> <li>SE(3) equivariant architectures</li> <li>Protein folding with generative models</li> </ul>"},{"location":"guides/protein-modeling/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Extensions</li> <li>Geometric Models</li> <li>Extensions Guide</li> </ul>"},{"location":"guides/protein-modeling/#key-concepts","title":"Key Concepts","text":""},{"location":"guides/protein-modeling/#sequence-representation","title":"Sequence Representation","text":"<p>Amino acid sequences and their embeddings.</p>"},{"location":"guides/protein-modeling/#structure-prediction","title":"Structure Prediction","text":"<p>Predicting 3D coordinates from sequences.</p>"},{"location":"guides/protein-modeling/#equivariance","title":"Equivariance","text":"<p>SE(3) equivariant operations for proper geometric handling.</p>"},{"location":"guides/benchmarks/protein-ligand/","title":"Protein-Ligand Benchmark Guide","text":"<p>Coming Soon</p> <p>This page is under development. Check back for protein-ligand benchmark documentation.</p>"},{"location":"guides/benchmarks/protein-ligand/#overview","title":"Overview","text":"<p>Guide for benchmarking protein-ligand binding models.</p>"},{"location":"guides/benchmarks/protein-ligand/#benchmarks","title":"Benchmarks","text":"<ul> <li>Binding affinity prediction</li> <li>Docking accuracy</li> <li>Pose generation quality</li> </ul>"},{"location":"guides/benchmarks/protein-ligand/#related-documentation","title":"Related Documentation","text":"<ul> <li>Benchmarks</li> <li>Protein Modeling</li> </ul>"},{"location":"guides/modalities/molecular/","title":"Molecular Modality Guide","text":"<p>Coming Soon</p> <p>This page is under development. Check back for molecular modality documentation.</p>"},{"location":"guides/modalities/molecular/#overview","title":"Overview","text":"<p>Guide for working with molecular data in Artifex.</p>"},{"location":"guides/modalities/molecular/#topics","title":"Topics","text":"<ul> <li>Molecular representations</li> <li>SMILES encoding</li> <li>3D coordinate handling</li> <li>Molecular graphs</li> </ul>"},{"location":"guides/modalities/molecular/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling</li> <li>Geometric Models</li> </ul>"},{"location":"inference/","title":"Inference Pipeline","text":"<p>Production-ready inference infrastructure for generative models, including optimized generators, sampling strategies, batching, model optimization, and serving endpoints.</p>"},{"location":"inference/#overview","title":"Overview","text":"<ul> <li> <p> Generators</p> <p>Model-specific generators for VAE, GAN, Diffusion, Flow, EBM, and Autoregressive</p> </li> <li> <p> Optimization</p> <p>Quantization, pruning, compilation, and caching for faster inference</p> </li> <li> <p> Serving</p> <p>REST, gRPC, and streaming endpoints for production deployment</p> </li> <li> <p> Sampling</p> <p>Temperature, top-k, nucleus, beam search, and classifier-free guidance</p> </li> </ul>"},{"location":"inference/#quick-start","title":"Quick Start","text":""},{"location":"inference/#basic-generation","title":"Basic Generation","text":"<pre><code>from artifex.inference import InferencePipeline\n\n# Create inference pipeline\npipeline = InferencePipeline.from_pretrained(\"model_checkpoint\")\n\n# Generate samples\nsamples = pipeline.generate(\n    num_samples=16,\n    seed=42,\n)\n</code></pre>"},{"location":"inference/#optimized-inference","title":"Optimized Inference","text":"<pre><code>from artifex.inference import InferencePipeline\nfrom artifex.inference.optimization import quantize, compile_model\n\n# Load and optimize model\npipeline = InferencePipeline.from_pretrained(\"model_checkpoint\")\npipeline.model = quantize(pipeline.model, dtype=\"int8\")\npipeline.model = compile_model(pipeline.model)\n\n# Fast inference\nsamples = pipeline.generate(num_samples=64)\n</code></pre>"},{"location":"inference/#generators","title":"Generators","text":"<p>Model-specific generators optimized for each architecture.</p>"},{"location":"inference/#vae-generator","title":"VAE Generator","text":"<pre><code>from artifex.inference.generators import VAEGenerator\n\ngenerator = VAEGenerator(\n    model=vae_model,\n    latent_dim=64,\n)\n\n# Generate from random latents\nsamples = generator.generate(num_samples=16)\n\n# Generate from specific latent\nsamples = generator.decode(latent_vectors)\n\n# Interpolate between samples\ninterpolated = generator.interpolate(z1, z2, steps=10)\n</code></pre> <p> VAE Generator</p>"},{"location":"inference/#gan-generator","title":"GAN Generator","text":"<pre><code>from artifex.inference.generators import GANGenerator\n\ngenerator = GANGenerator(\n    model=gan_model,\n    latent_dim=128,\n)\n\n# Generate samples\nsamples = generator.generate(num_samples=16)\n\n# Conditional generation\nsamples = generator.generate(num_samples=16, labels=class_labels)\n\n# Truncation trick for quality\nsamples = generator.generate(num_samples=16, truncation=0.7)\n</code></pre> <p> GAN Generator</p>"},{"location":"inference/#diffusion-generator","title":"Diffusion Generator","text":"<pre><code>from artifex.inference.generators import DiffusionGenerator\n\ngenerator = DiffusionGenerator(\n    model=diffusion_model,\n    num_steps=50,           # Sampling steps\n    sampler=\"ddim\",         # DDIM, DDPM, DPM++\n)\n\n# Generate samples\nsamples = generator.generate(num_samples=16)\n\n# With classifier-free guidance\nsamples = generator.generate(\n    num_samples=16,\n    prompt_embeds=text_embeddings,\n    guidance_scale=7.5,\n)\n</code></pre> <p> Diffusion Generator</p>"},{"location":"inference/#flow-generator","title":"Flow Generator","text":"<pre><code>from artifex.inference.generators import FlowGenerator\n\ngenerator = FlowGenerator(\n    model=flow_model,\n    num_steps=100,\n)\n\n# Generate samples\nsamples = generator.generate(num_samples=16)\n\n# Compute log-likelihood\nlog_prob = generator.log_prob(data)\n</code></pre> <p> Flow Generator</p>"},{"location":"inference/#energy-generator","title":"Energy Generator","text":"<pre><code>from artifex.inference.generators import EnergyGenerator\n\ngenerator = EnergyGenerator(\n    model=ebm_model,\n    sampler=\"langevin\",\n    num_steps=100,\n    step_size=0.01,\n)\n\n# Generate via MCMC\nsamples = generator.generate(num_samples=16)\n</code></pre> <p> Energy Generator</p>"},{"location":"inference/#autoregressive-generator","title":"Autoregressive Generator","text":"<pre><code>from artifex.inference.generators import AutoregressiveGenerator\n\ngenerator = AutoregressiveGenerator(\n    model=ar_model,\n    max_length=512,\n    temperature=0.8,\n)\n\n# Generate sequences\nsamples = generator.generate(\n    prompts=input_tokens,\n    num_samples=4,\n)\n</code></pre> <p> Autoregressive Generator</p>"},{"location":"inference/#sampling-strategies","title":"Sampling Strategies","text":""},{"location":"inference/#temperature-scaling","title":"Temperature Scaling","text":"<pre><code>from artifex.inference.sampling import TemperatureSampler\n\nsampler = TemperatureSampler(temperature=0.7)\nsamples = sampler.sample(logits)\n</code></pre> <p> Temperature</p>"},{"location":"inference/#top-k-sampling","title":"Top-K Sampling","text":"<pre><code>from artifex.inference.sampling import TopKSampler\n\nsampler = TopKSampler(k=50)\nsamples = sampler.sample(logits)\n</code></pre> <p> Top-K</p>"},{"location":"inference/#nucleus-top-p-sampling","title":"Nucleus (Top-P) Sampling","text":"<pre><code>from artifex.inference.sampling import NucleusSampler\n\nsampler = NucleusSampler(p=0.9)\nsamples = sampler.sample(logits)\n</code></pre> <p> Nucleus</p>"},{"location":"inference/#beam-search","title":"Beam Search","text":"<pre><code>from artifex.inference.sampling import BeamSearchSampler\n\nsampler = BeamSearchSampler(\n    beam_width=5,\n    length_penalty=0.6,\n)\nsamples = sampler.sample(model, input_ids)\n</code></pre> <p> Beam Search</p>"},{"location":"inference/#ancestral-sampling","title":"Ancestral Sampling","text":"<pre><code>from artifex.inference.sampling import AncestralSampler\n\nsampler = AncestralSampler()\nsamples = sampler.sample(diffusion_model, noise)\n</code></pre> <p> Ancestral</p>"},{"location":"inference/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<pre><code>from artifex.inference.sampling import ClassifierFreeGuidance\n\ncfg = ClassifierFreeGuidance(guidance_scale=7.5)\nsamples = cfg.sample(\n    model=diffusion_model,\n    cond_embeddings=text_embeddings,\n    uncond_embeddings=null_embeddings,\n)\n</code></pre> <p> Classifier-Free</p>"},{"location":"inference/#optimization","title":"Optimization","text":""},{"location":"inference/#quantization","title":"Quantization","text":"<pre><code>from artifex.inference.optimization import quantize\n\n# INT8 quantization\nquantized_model = quantize(\n    model,\n    dtype=\"int8\",\n    calibration_data=calibration_set,\n)\n\n# Dynamic quantization\nquantized_model = quantize(model, dtype=\"int8\", dynamic=True)\n</code></pre> <p> Quantization</p>"},{"location":"inference/#pruning","title":"Pruning","text":"<pre><code>from artifex.inference.optimization import prune\n\n# Magnitude-based pruning\npruned_model = prune(\n    model,\n    sparsity=0.5,\n    method=\"magnitude\",\n)\n\n# Structured pruning\npruned_model = prune(\n    model,\n    sparsity=0.3,\n    method=\"structured\",\n    granularity=\"channel\",\n)\n</code></pre> <p> Pruning</p>"},{"location":"inference/#compilation","title":"Compilation","text":"<pre><code>from artifex.inference.optimization import compile_model\n\n# JIT compilation\ncompiled_model = compile_model(model, backend=\"jax\")\n\n# XLA compilation\ncompiled_model = compile_model(model, backend=\"xla\")\n</code></pre> <p> Compilation</p>"},{"location":"inference/#caching","title":"Caching","text":"<pre><code>from artifex.inference.optimization import KVCache\n\n# Key-value caching for transformers\ncache = KVCache(\n    max_length=2048,\n    num_layers=24,\n    num_heads=16,\n)\n\noutput, new_cache = model(input, cache=cache)\n</code></pre> <p> Caching</p>"},{"location":"inference/#knowledge-distillation","title":"Knowledge Distillation","text":"<pre><code>from artifex.inference.optimization import distill\n\n# Distill to smaller model\nstudent_model = distill(\n    teacher=large_model,\n    student=small_model,\n    train_data=train_data,\n    temperature=4.0,\n)\n</code></pre> <p> Distillation</p>"},{"location":"inference/#batching","title":"Batching","text":""},{"location":"inference/#dynamic-batching","title":"Dynamic Batching","text":"<pre><code>from artifex.inference.batching import DynamicBatcher\n\nbatcher = DynamicBatcher(\n    max_batch_size=32,\n    max_wait_time=0.1,  # seconds\n)\n\nasync def handle_request(request):\n    return await batcher.process(request)\n</code></pre> <p> Dynamic Batching</p>"},{"location":"inference/#padding-strategies","title":"Padding Strategies","text":"<pre><code>from artifex.inference.batching import PaddedBatcher\n\nbatcher = PaddedBatcher(\n    pad_token_id=0,\n    max_length=512,\n    padding_side=\"right\",\n)\n</code></pre> <p> Padding</p>"},{"location":"inference/#streaming","title":"Streaming","text":"<pre><code>from artifex.inference.batching import StreamingBatcher\n\nbatcher = StreamingBatcher(\n    chunk_size=16,\n    overlap=2,\n)\n\nasync for chunk in batcher.stream(long_input):\n    process(chunk)\n</code></pre> <p> Streaming</p>"},{"location":"inference/#model-conversion","title":"Model Conversion","text":""},{"location":"inference/#onnx-export","title":"ONNX Export","text":"<pre><code>from artifex.inference.conversion import export_onnx\n\nexport_onnx(\n    model,\n    output_path=\"model.onnx\",\n    input_shape=(1, 3, 256, 256),\n    opset_version=17,\n)\n</code></pre> <p> ONNX</p>"},{"location":"inference/#tensorrt","title":"TensorRT","text":"<pre><code>from artifex.inference.conversion import export_tensorrt\n\nexport_tensorrt(\n    model,\n    output_path=\"model.trt\",\n    precision=\"fp16\",\n    max_batch_size=32,\n)\n</code></pre> <p> TensorRT</p>"},{"location":"inference/#tensorflowjs","title":"TensorFlow.js","text":"<pre><code>from artifex.inference.conversion import export_tfjs\n\nexport_tfjs(\n    model,\n    output_dir=\"tfjs_model/\",\n    quantization=\"float16\",\n)\n</code></pre> <p> TensorFlow.js</p>"},{"location":"inference/#serving","title":"Serving","text":""},{"location":"inference/#rest-api","title":"REST API","text":"<pre><code>from artifex.inference.serving import RESTServer\n\nserver = RESTServer(\n    model=model,\n    host=\"0.0.0.0\",\n    port=8080,\n)\n\nserver.run()\n</code></pre> <p> REST</p>"},{"location":"inference/#grpc","title":"gRPC","text":"<pre><code>from artifex.inference.serving import GRPCServer\n\nserver = GRPCServer(\n    model=model,\n    port=50051,\n    max_workers=10,\n)\n\nserver.run()\n</code></pre> <p> gRPC</p>"},{"location":"inference/#middleware","title":"Middleware","text":"<pre><code>from artifex.inference.serving import (\n    RateLimiter,\n    AuthMiddleware,\n    LoggingMiddleware,\n)\n\nserver.add_middleware(RateLimiter(requests_per_minute=100))\nserver.add_middleware(AuthMiddleware(api_key_header=\"X-API-Key\"))\nserver.add_middleware(LoggingMiddleware())\n</code></pre> <p> Middleware</p>"},{"location":"inference/#inference-metrics","title":"Inference Metrics","text":""},{"location":"inference/#latency-tracking","title":"Latency Tracking","text":"<pre><code>from artifex.inference.metrics import LatencyTracker\n\ntracker = LatencyTracker()\n\nwith tracker.measure(\"generation\"):\n    samples = generator.generate(num_samples=16)\n\nprint(f\"P50 latency: {tracker.percentile(50):.2f}ms\")\nprint(f\"P99 latency: {tracker.percentile(99):.2f}ms\")\n</code></pre> <p> Latency</p>"},{"location":"inference/#memory-monitoring","title":"Memory Monitoring","text":"<pre><code>from artifex.inference.metrics import MemoryMonitor\n\nmonitor = MemoryMonitor()\n\nwith monitor.track():\n    samples = generator.generate(num_samples=64)\n\nprint(f\"Peak memory: {monitor.peak_memory_mb:.2f} MB\")\n</code></pre> <p> Memory</p>"},{"location":"inference/#throughput","title":"Throughput","text":"<pre><code>from artifex.inference.metrics import ThroughputTracker\n\ntracker = ThroughputTracker()\n\ntracker.record(batch_size=32, latency_ms=50)\nprint(f\"Throughput: {tracker.samples_per_second:.2f} samples/s\")\n</code></pre> <p> Throughput</p>"},{"location":"inference/#module-reference","title":"Module Reference","text":"Category Modules Inference base, pipeline Generators autoregressive_generator, diffusion_generator, energy_generator, flow_generator, gan_generator, vae_generator Sampling ancestral, beam_search, classifier_free, nucleus, temperature, top_k Optimization caching, compilation, distillation, production, pruning, quantization Batching dynamic, padding, streaming Conversion onnx, tensorrt, tfjs Serving grpc, middleware, rest, stateless Metrics latency, memory, throughput Adaptation adapter, lora, prefix_tuning, prompt_tuning"},{"location":"inference/#related-documentation","title":"Related Documentation","text":"<ul> <li>CLI Commands - Command-line generation</li> <li>Distributed Training - Multi-device setup</li> <li>Parallelism Guide - Parallel inference</li> <li>Checkpointing - Model checkpoints</li> </ul>"},{"location":"inference/adapter/","title":"Adapter","text":"<p>Module: <code>generative_models.inference.adaptation.adapter</code></p> <p>Source: <code>generative_models/inference/adaptation/adapter.py</code></p>"},{"location":"inference/adapter/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/ancestral/","title":"Ancestral","text":"<p>Module: <code>generative_models.inference.sampling.ancestral</code></p> <p>Source: <code>generative_models/inference/sampling/ancestral.py</code></p>"},{"location":"inference/ancestral/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/autoregressive_generator/","title":"Autoregressive Generator","text":"<p>Module: <code>generative_models.inference.generators.autoregressive_generator</code></p> <p>Source: <code>generative_models/inference/generators/autoregressive_generator.py</code></p>"},{"location":"inference/autoregressive_generator/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/base/","title":"Base","text":"<p>Module: <code>generative_models.inference.base</code></p> <p>Source: <code>generative_models/inference/base.py</code></p>"},{"location":"inference/base/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/beam_search/","title":"Beam Search","text":"<p>Module: <code>generative_models.inference.sampling.beam_search</code></p> <p>Source: <code>generative_models/inference/sampling/beam_search.py</code></p>"},{"location":"inference/beam_search/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/caching/","title":"Caching","text":"<p>Module: <code>generative_models.inference.optimization.caching</code></p> <p>Source: <code>generative_models/inference/optimization/caching.py</code></p>"},{"location":"inference/caching/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/classifier_free/","title":"Classifier Free","text":"<p>Module: <code>generative_models.inference.sampling.classifier_free</code></p> <p>Source: <code>generative_models/inference/sampling/classifier_free.py</code></p>"},{"location":"inference/classifier_free/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/compilation/","title":"Compilation","text":"<p>Module: <code>generative_models.inference.optimization.compilation</code></p> <p>Source: <code>generative_models/inference/optimization/compilation.py</code></p>"},{"location":"inference/compilation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/diffusion_generator/","title":"Diffusion Generator","text":"<p>Module: <code>generative_models.inference.generators.diffusion_generator</code></p> <p>Source: <code>generative_models/inference/generators/diffusion_generator.py</code></p>"},{"location":"inference/diffusion_generator/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/distillation/","title":"Distillation","text":"<p>Module: <code>generative_models.inference.optimization.distillation</code></p> <p>Source: <code>generative_models/inference/optimization/distillation.py</code></p>"},{"location":"inference/distillation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/dynamic/","title":"Dynamic","text":"<p>Module: <code>generative_models.inference.batching.dynamic</code></p> <p>Source: <code>generative_models/inference/batching/dynamic.py</code></p>"},{"location":"inference/dynamic/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/energy_generator/","title":"Energy Generator","text":"<p>Module: <code>generative_models.inference.generators.energy_generator</code></p> <p>Source: <code>generative_models/inference/generators/energy_generator.py</code></p>"},{"location":"inference/energy_generator/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/flow_generator/","title":"Flow Generator","text":"<p>Module: <code>generative_models.inference.generators.flow_generator</code></p> <p>Source: <code>generative_models/inference/generators/flow_generator.py</code></p>"},{"location":"inference/flow_generator/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/gan_generator/","title":"Gan Generator","text":"<p>Module: <code>generative_models.inference.generators.gan_generator</code></p> <p>Source: <code>generative_models/inference/generators/gan_generator.py</code></p>"},{"location":"inference/gan_generator/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/grpc/","title":"Grpc","text":"<p>Module: <code>generative_models.inference.serving.grpc</code></p> <p>Source: <code>generative_models/inference/serving/grpc.py</code></p>"},{"location":"inference/grpc/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/latency/","title":"Latency","text":"<p>Module: <code>generative_models.inference.metrics.latency</code></p> <p>Source: <code>generative_models/inference/metrics/latency.py</code></p>"},{"location":"inference/latency/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/lora/","title":"Lora","text":"<p>Module: <code>generative_models.inference.adaptation.lora</code></p> <p>Source: <code>generative_models/inference/adaptation/lora.py</code></p>"},{"location":"inference/lora/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/memory/","title":"Memory","text":"<p>Module: <code>generative_models.inference.metrics.memory</code></p> <p>Source: <code>generative_models/inference/metrics/memory.py</code></p>"},{"location":"inference/memory/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/middleware/","title":"Middleware","text":"<p>Module: <code>generative_models.inference.serving.middleware</code></p> <p>Source: <code>generative_models/inference/serving/middleware.py</code></p>"},{"location":"inference/middleware/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/nucleus/","title":"Nucleus","text":"<p>Module: <code>generative_models.inference.sampling.nucleus</code></p> <p>Source: <code>generative_models/inference/sampling/nucleus.py</code></p>"},{"location":"inference/nucleus/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/onnx/","title":"Onnx","text":"<p>Module: <code>generative_models.inference.conversion.onnx</code></p> <p>Source: <code>generative_models/inference/conversion/onnx.py</code></p>"},{"location":"inference/onnx/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/padding/","title":"Padding","text":"<p>Module: <code>generative_models.inference.batching.padding</code></p> <p>Source: <code>generative_models/inference/batching/padding.py</code></p>"},{"location":"inference/padding/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/pipeline/","title":"Pipeline","text":"<p>Module: <code>generative_models.inference.pipeline</code></p> <p>Source: <code>generative_models/inference/pipeline.py</code></p>"},{"location":"inference/pipeline/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/prefix_tuning/","title":"Prefix Tuning","text":"<p>Module: <code>generative_models.inference.adaptation.prefix_tuning</code></p> <p>Source: <code>generative_models/inference/adaptation/prefix_tuning.py</code></p>"},{"location":"inference/prefix_tuning/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/production/","title":"Production","text":"<p>Module: <code>generative_models.inference.optimization.production</code></p> <p>Source: <code>generative_models/inference/optimization/production.py</code></p>"},{"location":"inference/production/#overview","title":"Overview","text":"<p>Note: This module is experimental and under active development as part of ongoing work towards production-ready inference capabilities.</p> <p>Inference optimization infrastructure for scaled models.</p> <p>This module provides optimization infrastructure including:</p> <ul> <li>Automatic optimization pipeline selection</li> <li>Inference optimization strategies</li> <li>Model adapter classes for different architectures</li> <li>Monitoring and debugging tools</li> </ul> <p>All implementations follow JAX/Flax NNX best practices and prioritize performance through hardware-aware optimization.</p>"},{"location":"inference/production/#classes","title":"Classes","text":""},{"location":"inference/production/#compiledmodel","title":"CompiledModel","text":"<pre><code>class CompiledModel\n</code></pre>"},{"location":"inference/production/#monitoringmetrics","title":"MonitoringMetrics","text":"<pre><code>class MonitoringMetrics\n</code></pre>"},{"location":"inference/production/#optimizationresult","title":"OptimizationResult","text":"<pre><code>class OptimizationResult\n</code></pre>"},{"location":"inference/production/#optimizationtarget","title":"OptimizationTarget","text":"<pre><code>class OptimizationTarget\n</code></pre>"},{"location":"inference/production/#productionmonitor","title":"ProductionMonitor","text":"<pre><code>class ProductionMonitor\n</code></pre>"},{"location":"inference/production/#productionoptimizer","title":"ProductionOptimizer","text":"<pre><code>class ProductionOptimizer\n</code></pre>"},{"location":"inference/production/#productionpipeline","title":"ProductionPipeline","text":"<pre><code>class ProductionPipeline\n</code></pre>"},{"location":"inference/production/#functions","title":"Functions","text":""},{"location":"inference/production/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"inference/production/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"inference/production/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"inference/production/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"inference/production/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"inference/production/#compiled_forward","title":"compiled_forward","text":"<pre><code>def compiled_forward()\n</code></pre>"},{"location":"inference/production/#create_production_optimizer","title":"create_production_optimizer","text":"<pre><code>def create_production_optimizer()\n</code></pre>"},{"location":"inference/production/#create_production_pipeline","title":"create_production_pipeline","text":"<pre><code>def create_production_pipeline()\n</code></pre>"},{"location":"inference/production/#create_production_pipeline_1","title":"create_production_pipeline","text":"<pre><code>def create_production_pipeline()\n</code></pre>"},{"location":"inference/production/#get_metrics","title":"get_metrics","text":"<pre><code>def get_metrics()\n</code></pre>"},{"location":"inference/production/#get_monitoring_metrics","title":"get_monitoring_metrics","text":"<pre><code>def get_monitoring_metrics()\n</code></pre>"},{"location":"inference/production/#optimize_for_production","title":"optimize_for_production","text":"<pre><code>def optimize_for_production()\n</code></pre>"},{"location":"inference/production/#predict","title":"predict","text":"<pre><code>def predict()\n</code></pre>"},{"location":"inference/production/#predict_batch","title":"predict_batch","text":"<pre><code>def predict_batch()\n</code></pre>"},{"location":"inference/production/#record_request","title":"record_request","text":"<pre><code>def record_request()\n</code></pre>"},{"location":"inference/production/#reset","title":"reset","text":"<pre><code>def reset()\n</code></pre>"},{"location":"inference/production/#reset_monitoring","title":"reset_monitoring","text":"<pre><code>def reset_monitoring()\n</code></pre>"},{"location":"inference/production/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 7</li> <li>Functions: 17</li> <li>Imports: 7</li> </ul>"},{"location":"inference/prompt_tuning/","title":"Prompt Tuning","text":"<p>Module: <code>generative_models.inference.adaptation.prompt_tuning</code></p> <p>Source: <code>generative_models/inference/adaptation/prompt_tuning.py</code></p>"},{"location":"inference/prompt_tuning/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/pruning/","title":"Pruning","text":"<p>Module: <code>generative_models.inference.optimization.pruning</code></p> <p>Source: <code>generative_models/inference/optimization/pruning.py</code></p>"},{"location":"inference/pruning/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/quantization/","title":"Quantization","text":"<p>Module: <code>generative_models.inference.optimization.quantization</code></p> <p>Source: <code>generative_models/inference/optimization/quantization.py</code></p>"},{"location":"inference/quantization/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/rest/","title":"Rest","text":"<p>Module: <code>generative_models.inference.serving.rest</code></p> <p>Source: <code>generative_models/inference/serving/rest.py</code></p>"},{"location":"inference/rest/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/stateless/","title":"Stateless","text":"<p>Module: <code>generative_models.inference.serving.stateless</code></p> <p>Source: <code>generative_models/inference/serving/stateless.py</code></p>"},{"location":"inference/stateless/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/streaming/","title":"Streaming","text":"<p>Module: <code>generative_models.inference.batching.streaming</code></p> <p>Source: <code>generative_models/inference/batching/streaming.py</code></p>"},{"location":"inference/streaming/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/temperature/","title":"Temperature","text":"<p>Module: <code>generative_models.inference.sampling.temperature</code></p> <p>Source: <code>generative_models/inference/sampling/temperature.py</code></p>"},{"location":"inference/temperature/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/tensorrt/","title":"Tensorrt","text":"<p>Module: <code>generative_models.inference.conversion.tensorrt</code></p> <p>Source: <code>generative_models/inference/conversion/tensorrt.py</code></p>"},{"location":"inference/tensorrt/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/tfjs/","title":"Tfjs","text":"<p>Module: <code>generative_models.inference.conversion.tfjs</code></p> <p>Source: <code>generative_models/inference/conversion/tfjs.py</code></p>"},{"location":"inference/tfjs/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/throughput/","title":"Throughput","text":"<p>Module: <code>generative_models.inference.metrics.throughput</code></p> <p>Source: <code>generative_models/inference/metrics/throughput.py</code></p>"},{"location":"inference/throughput/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/top_k/","title":"Top K","text":"<p>Module: <code>generative_models.inference.sampling.top_k</code></p> <p>Source: <code>generative_models/inference/sampling/top_k.py</code></p>"},{"location":"inference/top_k/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"inference/vae_generator/","title":"Vae Generator","text":"<p>Module: <code>generative_models.inference.generators.vae_generator</code></p> <p>Source: <code>generative_models/inference/generators/vae_generator.py</code></p>"},{"location":"inference/vae_generator/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"modalities/","title":"Data Modalities","text":"<p>Modality-specific implementations for handling different data types in generative models, including adapters, evaluation metrics, and representation learning.</p>"},{"location":"modalities/#overview","title":"Overview","text":"<ul> <li> <p> Image</p> <p>Convolutional architectures, FID/IS metrics, perceptual losses</p> </li> <li> <p> Text</p> <p>Tokenization, transformers, perplexity metrics</p> </li> <li> <p> Audio</p> <p>Spectrograms, MFCCs, audio quality metrics</p> </li> <li> <p> Protein</p> <p>Sequence encoding, structure prediction, SE(3) equivariance</p> </li> </ul>"},{"location":"modalities/#quick-start","title":"Quick Start","text":""},{"location":"modalities/#using-modalities","title":"Using Modalities","text":"<pre><code>from artifex.generative_models.modalities import get_modality\n\n# Get image modality handler\nimage_modality = get_modality(\"image\", rngs=rngs)\n\n# Get evaluation metrics\nevaluator = image_modality.get_evaluator()\nmetrics = evaluator.evaluate(generated_images, real_images)\n\n# Get model adapter\nadapter = image_modality.get_adapter(\"vae\")\nadapted_model = adapter.adapt(model, config)\n</code></pre>"},{"location":"modalities/#image-modality","title":"Image Modality","text":"<p>Full support for image generation with specialized components.</p>"},{"location":"modalities/#features","title":"Features","text":"<ul> <li>Convolutional encoder/decoder architectures</li> <li>FID, Inception Score, LPIPS metrics</li> <li>Image-specific augmentations</li> <li>Perceptual losses with VGG features</li> </ul>"},{"location":"modalities/#usage","title":"Usage","text":"<pre><code>from artifex.generative_models.modalities.image import (\n    ImageModality,\n    ImageEvaluator,\n    ImageRepresentation,\n)\n\n# Create image modality\nmodality = ImageModality(image_size=(256, 256), channels=3)\n\n# Evaluate generated images\nevaluator = ImageEvaluator()\nmetrics = evaluator.evaluate(\n    generated=fake_images,\n    real=real_images,\n    metrics=[\"fid\", \"inception_score\", \"lpips\"],\n)\n\n# Extract image representations\nrepr_model = ImageRepresentation(pretrained=\"inception_v3\")\nfeatures = repr_model.extract(images)\n</code></pre>"},{"location":"modalities/#modules","title":"Modules","text":"Module Description base Image modality base class adapters Model adapters for images datasets Image dataset utilities evaluation Image quality metrics representations Feature extraction <p> Image Modality Guide</p>"},{"location":"modalities/#text-modality","title":"Text Modality","text":"<p>Support for text generation with language-specific components.</p>"},{"location":"modalities/#features_1","title":"Features","text":"<ul> <li>Multiple tokenization strategies (BPE, SentencePiece)</li> <li>Transformer architectures</li> <li>Perplexity and BLEU metrics</li> <li>Language-specific preprocessing</li> </ul>"},{"location":"modalities/#usage_1","title":"Usage","text":"<pre><code>from artifex.generative_models.modalities.text import (\n    TextModality,\n    TextEvaluator,\n    TextRepresentation,\n)\n\n# Create text modality\nmodality = TextModality(vocab_size=32000, max_length=512)\n\n# Evaluate generated text\nevaluator = TextEvaluator()\nmetrics = evaluator.evaluate(\n    generated=generated_texts,\n    references=reference_texts,\n    metrics=[\"perplexity\", \"bleu\", \"rouge\"],\n)\n\n# Extract text representations\nrepr_model = TextRepresentation(model=\"bert-base\")\nembeddings = repr_model.extract(texts)\n</code></pre>"},{"location":"modalities/#modules_1","title":"Modules","text":"Module Description base Text modality base class datasets Text dataset utilities evaluation Text quality metrics representations Text embeddings <p> Text Modality Guide</p>"},{"location":"modalities/#audio-modality","title":"Audio Modality","text":"<p>Support for audio generation with signal processing components.</p>"},{"location":"modalities/#features_2","title":"Features","text":"<ul> <li>Spectrogram representations (Mel, STFT)</li> <li>Audio quality metrics (FAD, MOS prediction)</li> <li>Time-frequency transformations</li> <li>Audio-specific augmentations</li> </ul>"},{"location":"modalities/#usage_2","title":"Usage","text":"<pre><code>from artifex.generative_models.modalities.audio import (\n    AudioModality,\n    AudioEvaluator,\n    AudioRepresentation,\n)\n\n# Create audio modality\nmodality = AudioModality(sample_rate=16000, n_mels=80)\n\n# Evaluate generated audio\nevaluator = AudioEvaluator()\nmetrics = evaluator.evaluate(\n    generated=fake_audio,\n    real=real_audio,\n    metrics=[\"fad\", \"inception_score\"],\n)\n\n# Extract audio representations\nrepr_model = AudioRepresentation(model=\"vggish\")\nfeatures = repr_model.extract(audio)\n</code></pre>"},{"location":"modalities/#modules_2","title":"Modules","text":"Module Description base Audio modality base class datasets Audio dataset utilities evaluation Audio quality metrics representations Audio features <p> Audio Modality Guide</p>"},{"location":"modalities/#protein-modality","title":"Protein Modality","text":"<p>Support for protein structure generation with biological constraints.</p>"},{"location":"modalities/#features_3","title":"Features","text":"<ul> <li>Amino acid sequence encoding</li> <li>3D structure representation</li> <li>SE(3) equivariant operations</li> <li>Structure quality metrics (RMSD, TM-score)</li> </ul>"},{"location":"modalities/#usage_3","title":"Usage","text":"<pre><code>from artifex.generative_models.modalities.protein import (\n    ProteinModality,\n    ProteinEvaluator,\n    ProteinRepresentation,\n)\n\n# Create protein modality\nmodality = ProteinModality(max_length=256)\n\n# Evaluate generated structures\nevaluator = ProteinEvaluator()\nmetrics = evaluator.evaluate(\n    generated=predicted_structures,\n    reference=native_structures,\n    metrics=[\"rmsd\", \"tm_score\", \"gdt_ts\"],\n)\n\n# Extract protein representations\nrepr_model = ProteinRepresentation(model=\"esm2\")\nembeddings = repr_model.extract(sequences)\n</code></pre>"},{"location":"modalities/#modules_3","title":"Modules","text":"Module Description adapters Model adapters for proteins config Protein configuration losses Structure losses modality Protein modality class utils Protein utilities <p> Protein Modeling Guide</p>"},{"location":"modalities/#multimodal","title":"Multimodal","text":"<p>Support for combining multiple modalities.</p>"},{"location":"modalities/#features_4","title":"Features","text":"<ul> <li>Cross-modal attention</li> <li>Joint embedding spaces</li> <li>Multi-task learning</li> <li>Modality alignment</li> </ul>"},{"location":"modalities/#usage_4","title":"Usage","text":"<pre><code>from artifex.generative_models.modalities.multimodal import (\n    MultiModalModality,\n    MultiModalEvaluator,\n)\n\n# Create multimodal handler\nmodality = MultiModalModality(\n    modalities=[\"image\", \"text\"],\n)\n\n# Evaluate cross-modal generation\nevaluator = MultiModalEvaluator()\nmetrics = evaluator.evaluate(\n    generated={\"image\": images, \"text\": captions},\n    real=real_data,\n)\n</code></pre>"},{"location":"modalities/#modules_4","title":"Modules","text":"Module Description adapters Cross-modal adapters base Multimodal base class datasets Multimodal datasets evaluation Cross-modal metrics representations Joint embeddings <p> Multimodal Guide</p>"},{"location":"modalities/#tabular-modality","title":"Tabular Modality","text":"<p>Support for structured/tabular data generation.</p>"},{"location":"modalities/#usage_5","title":"Usage","text":"<pre><code>from artifex.generative_models.modalities.tabular import (\n    TabularModality,\n    TabularEvaluator,\n)\n\nmodality = TabularModality(\n    categorical_columns=[\"category\", \"type\"],\n    continuous_columns=[\"value\", \"amount\"],\n)\n</code></pre>"},{"location":"modalities/#time-series-modality","title":"Time Series Modality","text":"<p>Support for temporal data generation.</p>"},{"location":"modalities/#usage_6","title":"Usage","text":"<pre><code>from artifex.generative_models.modalities.timeseries import (\n    TimeSeriesModality,\n    TimeSeriesEvaluator,\n)\n\nmodality = TimeSeriesModality(\n    seq_length=100,\n    num_features=5,\n)\n</code></pre>"},{"location":"modalities/#molecular-modality","title":"Molecular Modality","text":"<p>Support for molecular generation.</p>"},{"location":"modalities/#usage_7","title":"Usage","text":"<pre><code>from artifex.generative_models.modalities.molecular import (\n    MolecularModality,\n    MolecularAdapter,\n)\n\nmodality = MolecularModality()\nadapter = modality.get_adapter(\"flow\")\n</code></pre>"},{"location":"modalities/#modality-registry","title":"Modality Registry","text":"<p>Register and retrieve modalities:</p> <pre><code>from artifex.generative_models.modalities import (\n    get_modality,\n    register_modality,\n    list_modalities,\n)\n\n# List available modalities\navailable = list_modalities()\n# ['image', 'text', 'audio', 'protein', 'multimodal', 'tabular', 'timeseries', 'molecular']\n\n# Get modality by name\nmodality = get_modality(\"image\", rngs=rngs)\n\n# Register custom modality\nregister_modality(\"custom\", CustomModality)\n</code></pre> <p> Registry</p>"},{"location":"modalities/#module-reference","title":"Module Reference","text":"Modality Modules Base base, registry Image adapters, base, datasets, evaluation, representations Text base, datasets, evaluation, representations Audio base, datasets, evaluation, representations Protein adapters, config, losses, modality, utils Multimodal adapters, base, datasets, evaluation, representations"},{"location":"modalities/#related-documentation","title":"Related Documentation","text":"<ul> <li>Image Modality Guide</li> <li>Text Modality Guide</li> <li>Audio Modality Guide</li> <li>Multimodal Guide</li> </ul>"},{"location":"modalities/adapters/","title":"Adapters","text":"<p>Module: <code>generative_models.modalities.timeseries.adapters</code></p> <p>Source: <code>generative_models/modalities/timeseries/adapters.py</code></p>"},{"location":"modalities/adapters/#overview","title":"Overview","text":"<p>Model adapters for timeseries modality.</p>"},{"location":"modalities/adapters/#classes","title":"Classes","text":""},{"location":"modalities/adapters/#timeseriesdiffusionadapter","title":"TimeseriesDiffusionAdapter","text":"<pre><code>class TimeseriesDiffusionAdapter\n</code></pre>"},{"location":"modalities/adapters/#timeseriesrnnadapter","title":"TimeseriesRNNAdapter","text":"<pre><code>class TimeseriesRNNAdapter\n</code></pre>"},{"location":"modalities/adapters/#timeseriestransformeradapter","title":"TimeseriesTransformerAdapter","text":"<pre><code>class TimeseriesTransformerAdapter\n</code></pre>"},{"location":"modalities/adapters/#timeseriesvaeadapter","title":"TimeseriesVAEAdapter","text":"<pre><code>class TimeseriesVAEAdapter\n</code></pre>"},{"location":"modalities/adapters/#functions","title":"Functions","text":""},{"location":"modalities/adapters/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/adapters/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/adapters/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/adapters/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/adapters/#create","title":"create","text":"<pre><code>def create()\n</code></pre>"},{"location":"modalities/adapters/#create_1","title":"create","text":"<pre><code>def create()\n</code></pre>"},{"location":"modalities/adapters/#create_2","title":"create","text":"<pre><code>def create()\n</code></pre>"},{"location":"modalities/adapters/#create_3","title":"create","text":"<pre><code>def create()\n</code></pre>"},{"location":"modalities/adapters/#get_timeseries_adapter","title":"get_timeseries_adapter","text":"<pre><code>def get_timeseries_adapter()\n</code></pre>"},{"location":"modalities/adapters/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 9</li> <li>Imports: 17</li> </ul>"},{"location":"modalities/base/","title":"Base","text":"<p>Module: <code>generative_models.modalities.timeseries.base</code></p> <p>Source: <code>generative_models/modalities/timeseries/base.py</code></p>"},{"location":"modalities/base/#overview","title":"Overview","text":"<p>Base timeseries modality implementation.</p>"},{"location":"modalities/base/#classes","title":"Classes","text":""},{"location":"modalities/base/#decompositionmethod","title":"DecompositionMethod","text":"<pre><code>class DecompositionMethod\n</code></pre>"},{"location":"modalities/base/#timeseriesmodality","title":"TimeseriesModality","text":"<pre><code>class TimeseriesModality\n</code></pre>"},{"location":"modalities/base/#timeseriesmodalityconfig","title":"TimeseriesModalityConfig","text":"<pre><code>class TimeseriesModalityConfig\n</code></pre>"},{"location":"modalities/base/#timeseriesrepresentation","title":"TimeseriesRepresentation","text":"<pre><code>class TimeseriesRepresentation\n</code></pre>"},{"location":"modalities/base/#functions","title":"Functions","text":""},{"location":"modalities/base/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/base/#expected_shape","title":"expected_shape","text":"<pre><code>def expected_shape()\n</code></pre>"},{"location":"modalities/base/#get_adapter","title":"get_adapter","text":"<pre><code>def get_adapter()\n</code></pre>"},{"location":"modalities/base/#get_extensions","title":"get_extensions","text":"<pre><code>def get_extensions()\n</code></pre>"},{"location":"modalities/base/#get_feature_info","title":"get_feature_info","text":"<pre><code>def get_feature_info()\n</code></pre>"},{"location":"modalities/base/#is_multivariate","title":"is_multivariate","text":"<pre><code>def is_multivariate()\n</code></pre>"},{"location":"modalities/base/#postprocess","title":"postprocess","text":"<pre><code>def postprocess()\n</code></pre>"},{"location":"modalities/base/#preprocess","title":"preprocess","text":"<pre><code>def preprocess()\n</code></pre>"},{"location":"modalities/base/#validate_data","title":"validate_data","text":"<pre><code>def validate_data()\n</code></pre>"},{"location":"modalities/base/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 9</li> <li>Imports: 8</li> </ul>"},{"location":"modalities/config/","title":"Config","text":"<p>Module: <code>generative_models.modalities.protein.config</code></p> <p>Source: <code>generative_models/modalities/protein/config.py</code></p>"},{"location":"modalities/config/#overview","title":"Overview","text":"<p>Configuration for protein modality.</p> <p>This module provides configuration utilities for the protein modality.</p>"},{"location":"modalities/config/#functions","title":"Functions","text":""},{"location":"modalities/config/#create_default_protein_config","title":"create_default_protein_config","text":"<pre><code>def create_default_protein_config()\n</code></pre>"},{"location":"modalities/config/#register_protein_modality","title":"register_protein_modality","text":"<pre><code>def register_protein_modality()\n</code></pre>"},{"location":"modalities/config/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 2</li> <li>Imports: 4</li> </ul>"},{"location":"modalities/datasets/","title":"Datasets","text":"<p>Module: <code>generative_models.modalities.timeseries.datasets</code></p> <p>Source: <code>generative_models/modalities/timeseries/datasets.py</code></p>"},{"location":"modalities/datasets/#overview","title":"Overview","text":"<p>Synthetic datasets for timeseries modality.</p>"},{"location":"modalities/datasets/#classes","title":"Classes","text":""},{"location":"modalities/datasets/#synthetictimeseriesdataset","title":"SyntheticTimeseriesDataset","text":"<pre><code>class SyntheticTimeseriesDataset\n</code></pre>"},{"location":"modalities/datasets/#functions","title":"Functions","text":""},{"location":"modalities/datasets/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/datasets/#batch_iterator","title":"batch_iterator","text":"<pre><code>def batch_iterator()\n</code></pre>"},{"location":"modalities/datasets/#create_simple_timeseries_dataset","title":"create_simple_timeseries_dataset","text":"<pre><code>def create_simple_timeseries_dataset()\n</code></pre>"},{"location":"modalities/datasets/#create_synthetic_timeseries_dataset","title":"create_synthetic_timeseries_dataset","text":"<pre><code>def create_synthetic_timeseries_dataset()\n</code></pre>"},{"location":"modalities/datasets/#generate_single_series","title":"generate_single_series","text":"<pre><code>def generate_single_series()\n</code></pre>"},{"location":"modalities/datasets/#get_batch","title":"get_batch","text":"<pre><code>def get_batch()\n</code></pre>"},{"location":"modalities/datasets/#get_sample","title":"get_sample","text":"<pre><code>def get_sample()\n</code></pre>"},{"location":"modalities/datasets/#get_statistics","title":"get_statistics","text":"<pre><code>def get_statistics()\n</code></pre>"},{"location":"modalities/datasets/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 8</li> <li>Imports: 5</li> </ul>"},{"location":"modalities/evaluation/","title":"Evaluation","text":"<p>Module: <code>generative_models.modalities.timeseries.evaluation</code></p> <p>Source: <code>generative_models/modalities/timeseries/evaluation.py</code></p>"},{"location":"modalities/evaluation/#overview","title":"Overview","text":"<p>Evaluation metrics for timeseries modality.</p>"},{"location":"modalities/evaluation/#classes","title":"Classes","text":""},{"location":"modalities/evaluation/#timeseriesevaluationsuite","title":"TimeseriesEvaluationSuite","text":"<pre><code>class TimeseriesEvaluationSuite\n</code></pre>"},{"location":"modalities/evaluation/#functions","title":"Functions","text":""},{"location":"modalities/evaluation/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/evaluation/#compute_metrics","title":"compute_metrics","text":"<pre><code>def compute_metrics()\n</code></pre>"},{"location":"modalities/evaluation/#compute_timeseries_metrics","title":"compute_timeseries_metrics","text":"<pre><code>def compute_timeseries_metrics()\n</code></pre>"},{"location":"modalities/evaluation/#evaluate_batch","title":"evaluate_batch","text":"<pre><code>def evaluate_batch()\n</code></pre>"},{"location":"modalities/evaluation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 4</li> <li>Imports: 7</li> </ul>"},{"location":"modalities/losses/","title":"Losses","text":"<p>Module: <code>generative_models.modalities.protein.losses</code></p> <p>Source: <code>generative_models/modalities/protein/losses.py</code></p>"},{"location":"modalities/losses/#overview","title":"Overview","text":"<p>Protein-specific loss functions for generative models.</p> <p>This module provides composable loss functions for protein structure generation and diffusion models.</p>"},{"location":"modalities/losses/#classes","title":"Classes","text":""},{"location":"modalities/losses/#compositeloss","title":"CompositeLoss","text":"<pre><code>class CompositeLoss\n</code></pre>"},{"location":"modalities/losses/#lossregistry","title":"LossRegistry","text":"<pre><code>class LossRegistry\n</code></pre>"},{"location":"modalities/losses/#proteinlossfunction","title":"ProteinLossFunction","text":"<pre><code>class ProteinLossFunction\n</code></pre>"},{"location":"modalities/losses/#functions","title":"Functions","text":""},{"location":"modalities/losses/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"modalities/losses/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"modalities/losses/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/losses/#backbone_loss","title":"backbone_loss","text":"<pre><code>def backbone_loss()\n</code></pre>"},{"location":"modalities/losses/#create_backbone_loss","title":"create_backbone_loss","text":"<pre><code>def create_backbone_loss()\n</code></pre>"},{"location":"modalities/losses/#create_dihedral_loss","title":"create_dihedral_loss","text":"<pre><code>def create_dihedral_loss()\n</code></pre>"},{"location":"modalities/losses/#create_protein_structure_loss","title":"create_protein_structure_loss","text":"<pre><code>def create_protein_structure_loss()\n</code></pre>"},{"location":"modalities/losses/#create_rmsd_loss","title":"create_rmsd_loss","text":"<pre><code>def create_rmsd_loss()\n</code></pre>"},{"location":"modalities/losses/#dihedral_loss","title":"dihedral_loss","text":"<pre><code>def dihedral_loss()\n</code></pre>"},{"location":"modalities/losses/#get_loss","title":"get_loss","text":"<pre><code>def get_loss()\n</code></pre>"},{"location":"modalities/losses/#register_composite_loss","title":"register_composite_loss","text":"<pre><code>def register_composite_loss()\n</code></pre>"},{"location":"modalities/losses/#register_loss","title":"register_loss","text":"<pre><code>def register_loss()\n</code></pre>"},{"location":"modalities/losses/#rmsd_loss","title":"rmsd_loss","text":"<pre><code>def rmsd_loss()\n</code></pre>"},{"location":"modalities/losses/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 13</li> <li>Imports: 3</li> </ul>"},{"location":"modalities/modality/","title":"Modality","text":"<p>Module: <code>generative_models.modalities.protein.modality</code></p> <p>Source: <code>generative_models/modalities/protein/modality.py</code></p>"},{"location":"modalities/modality/#overview","title":"Overview","text":"<p>Protein modality implementation.</p> <p>This module provides the protein modality implementation that adapts generative models to work with protein structure data.</p>"},{"location":"modalities/modality/#classes","title":"Classes","text":""},{"location":"modalities/modality/#proteinmodality","title":"ProteinModality","text":"<pre><code>class ProteinModality\n</code></pre>"},{"location":"modalities/modality/#functions","title":"Functions","text":""},{"location":"modalities/modality/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/modality/#get_adapter","title":"get_adapter","text":"<pre><code>def get_adapter()\n</code></pre>"},{"location":"modalities/modality/#get_extensions","title":"get_extensions","text":"<pre><code>def get_extensions()\n</code></pre>"},{"location":"modalities/modality/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 3</li> <li>Imports: 8</li> </ul>"},{"location":"modalities/registry/","title":"Registry","text":"<p>Module: <code>generative_models.modalities.registry</code></p> <p>Source: <code>generative_models/modalities/registry.py</code></p>"},{"location":"modalities/registry/#overview","title":"Overview","text":"<p>Modality registry for the generative models framework.</p>"},{"location":"modalities/registry/#functions","title":"Functions","text":""},{"location":"modalities/registry/#clear_modalities","title":"clear_modalities","text":"<pre><code>def clear_modalities()\n</code></pre>"},{"location":"modalities/registry/#get_modality","title":"get_modality","text":"<pre><code>def get_modality()\n</code></pre>"},{"location":"modalities/registry/#list_modalities","title":"list_modalities","text":"<pre><code>def list_modalities()\n</code></pre>"},{"location":"modalities/registry/#register_modality","title":"register_modality","text":"<pre><code>def register_modality()\n</code></pre>"},{"location":"modalities/registry/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 4</li> <li>Imports: 8</li> </ul>"},{"location":"modalities/representations/","title":"Representations","text":"<p>Module: <code>generative_models.modalities.timeseries.representations</code></p> <p>Source: <code>generative_models/modalities/timeseries/representations.py</code></p>"},{"location":"modalities/representations/#overview","title":"Overview","text":"<p>Temporal representations and processors for timeseries modality.</p>"},{"location":"modalities/representations/#classes","title":"Classes","text":""},{"location":"modalities/representations/#fourierprocessor","title":"FourierProcessor","text":"<pre><code>class FourierProcessor\n</code></pre>"},{"location":"modalities/representations/#multiscaleprocessor","title":"MultiScaleProcessor","text":"<pre><code>class MultiScaleProcessor\n</code></pre>"},{"location":"modalities/representations/#timeseriesprocessor","title":"TimeseriesProcessor","text":"<pre><code>class TimeseriesProcessor\n</code></pre>"},{"location":"modalities/representations/#trenddecompositionprocessor","title":"TrendDecompositionProcessor","text":"<pre><code>class TrendDecompositionProcessor\n</code></pre>"},{"location":"modalities/representations/#functions","title":"Functions","text":""},{"location":"modalities/representations/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"modalities/representations/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"modalities/representations/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"modalities/representations/#call_3","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"modalities/representations/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/representations/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/representations/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/representations/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"modalities/representations/#process","title":"process","text":"<pre><code>def process()\n</code></pre>"},{"location":"modalities/representations/#reconstruct","title":"reconstruct","text":"<pre><code>def reconstruct()\n</code></pre>"},{"location":"modalities/representations/#reconstruct_1","title":"reconstruct","text":"<pre><code>def reconstruct()\n</code></pre>"},{"location":"modalities/representations/#reverse","title":"reverse","text":"<pre><code>def reverse()\n</code></pre>"},{"location":"modalities/representations/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 12</li> <li>Imports: 4</li> </ul>"},{"location":"modalities/utils/","title":"Utils","text":"<p>Module: <code>generative_models.modalities.protein.utils</code></p> <p>Source: <code>generative_models/modalities/protein/utils.py</code></p>"},{"location":"modalities/utils/#overview","title":"Overview","text":"<p>Utilities for protein modality.</p> <p>This module provides utility functions for creating and working with protein-specific modality components.</p>"},{"location":"modalities/utils/#functions","title":"Functions","text":""},{"location":"modalities/utils/#get_protein_adapter","title":"get_protein_adapter","text":"<pre><code>def get_protein_adapter()\n</code></pre>"},{"location":"modalities/utils/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 1</li> <li>Imports: 4</li> </ul>"},{"location":"models/","title":"Model Gallery","text":"<p>Artifex provides implementations of state-of-the-art generative models with 2025 research compliance, including Diffusion Transformers (DiT), SE(3) equivariant flows for molecular generation, and advanced MCMC sampling with BlackJAX integration.</p> <ul> <li> <p> 7 Model Families</p> <p>VAE, GAN, Diffusion, Flow, EBM, Autoregressive, and Geometric models with 67+ implementations</p> </li> <li> <p> 2025 Research Compliance</p> <p>Latest architectures including DiT, StyleGAN3, SE(3) molecular flows, and score-based diffusion</p> </li> <li> <p> Production Ready</p> <p>Hardware-optimized, fully tested, type-safe implementations built on JAX/Flax NNX</p> </li> <li> <p> Multi-Modal</p> <p>Native support for images, text, audio, proteins, molecules, and 3D geometric data</p> </li> </ul>"},{"location":"models/#overview","title":"Overview","text":"<p>All models in Artifex follow a unified interface and are built on JAX/Flax NNX for:</p> <ul> <li>Hardware acceleration: Automatic GPU/TPU support with XLA optimization</li> <li>Type safety: Full type annotations and protocol-based interfaces</li> <li>Composability: Mix and match components across model types</li> <li>Reproducibility: Deterministic RNG handling and comprehensive testing</li> <li>Scalability: Distributed training with data, model, and pipeline parallelism</li> </ul>"},{"location":"models/#model-families","title":"Model Families","text":""},{"location":"models/#vae-variational-autoencoders","title":"VAE Variational Autoencoders","text":"<p>Latent variable models with probabilistic encoding for representation learning and generation.</p> <p>Available Models:</p> Model Description Key Features Use Cases VAE Standard Variational Autoencoder Gaussian latents, KL regularization Representation learning, compression \u03b2-VAE Disentangled VAE Controllable \u03b2 parameter, beta annealing Disentangled representations \u03b2-VAE with Capacity \u03b2-VAE with capacity control Gradual capacity increase, controlled disentanglement Balance reconstruction and disentanglement Conditional VAE Class-conditional VAE Label conditioning, controlled generation Supervised generation VQ-VAE Vector Quantized VAE Discrete latent codes, codebook learning Discrete representations, compression <p>Quick Start:</p> <pre><code>from artifex.generative_models.models.vae import VAE\nfrom artifex.generative_models.core.configuration import VAEConfig, EncoderConfig, DecoderConfig\nfrom flax import nnx\n\nencoder = EncoderConfig(name=\"encoder\", input_shape=(32, 32, 3), latent_dim=128, hidden_dims=(64, 128, 256), activation=\"relu\")\ndecoder = DecoderConfig(name=\"decoder\", latent_dim=128, output_shape=(32, 32, 3), hidden_dims=(256, 128, 64), activation=\"relu\")\nconfig = VAEConfig(name=\"vae\", encoder=encoder, decoder=decoder, encoder_type=\"cnn\", kl_weight=1.0)\n\nmodel = VAE(config, rngs=nnx.Rngs(0))\n</code></pre> <p>Documentation:</p> <ul> <li>VAE User Guide - Complete guide with examples</li> <li>VAE API Reference - Detailed API documentation</li> </ul>"},{"location":"models/#gan-generative-adversarial-networks","title":"GAN Generative Adversarial Networks","text":"<p>Adversarial training for high-quality image generation and image-to-image translation.</p> <p>Available Models:</p> Model Description Key Features Use Cases DCGAN Deep Convolutional GAN Convolutional architecture, stable training Image generation baseline WGAN Wasserstein GAN Wasserstein distance, critic training Stable training, mode coverage LSGAN Least Squares GAN Least squares loss, improved stability Image generation with stable training StyleGAN Style-based GAN Style mixing, AdaIN layers High-quality face generation StyleGAN3 Alias-free StyleGAN Translation/rotation equivariance Alias-free high-quality generation CycleGAN Cycle-consistent GAN Unpaired translation, cycle loss Image-to-image translation PatchGAN Patch-based discriminator Local image patches, texture detail Image-to-image tasks, super-resolution Conditional GAN Class-conditional GAN Label conditioning Controlled generation <p>Quick Start:</p> <pre><code>from artifex.generative_models.models.gan import DCGAN\nfrom artifex.generative_models.core.configuration import DCGANConfig, GeneratorConfig, DiscriminatorConfig\nfrom flax import nnx\n\ngenerator = GeneratorConfig(name=\"generator\", latent_dim=100, features=(512, 256, 128, 64))\ndiscriminator = DiscriminatorConfig(name=\"discriminator\", features=(64, 128, 256, 512))\nconfig = DCGANConfig(name=\"dcgan\", image_shape=(3, 64, 64), generator=generator, discriminator=discriminator)\n\nmodel = DCGAN(config, rngs=nnx.Rngs(0))\n</code></pre> <p>Documentation:</p> <ul> <li>GAN User Guide - Complete guide with training tips</li> <li>GAN API Reference - Detailed API documentation</li> </ul>"},{"location":"models/#diffusion-diffusion-models","title":"Diffusion Diffusion Models","text":"<p>State-of-the-art denoising diffusion models for high-quality generation.</p> <p>Available Models:</p> Model Description Key Features Use Cases DDPM Denoising Diffusion Probabilistic Models Gaussian diffusion, noise prediction Image generation, baseline DDIM Denoising Diffusion Implicit Models Deterministic sampling, faster inference Fast high-quality generation Score-based Score-based generative models Score matching, SDE/ODE solvers Flexible sampling strategies Latent Diffusion Latent space diffusion VAE encoder/decoder, efficient training High-resolution generation DiT Diffusion Transformer Transformer backbone, scalable Large-scale image generation Stable Diffusion Text-to-image diffusion CLIP conditioning, latent diffusion Text-to-image generation <p>Quick Start:</p> <pre><code>from artifex.generative_models.models.diffusion import DDPMModel\nfrom artifex.generative_models.core.configuration import DDPMConfig, UNetBackboneConfig, NoiseScheduleConfig\nfrom flax import nnx\n\nbackbone = UNetBackboneConfig(name=\"backbone\", in_channels=3, out_channels=3, base_channels=128, channel_mults=(1, 2, 4))\nnoise_schedule = NoiseScheduleConfig(name=\"schedule\", schedule_type=\"linear\", num_timesteps=1000, beta_start=1e-4, beta_end=2e-2)\nconfig = DDPMConfig(name=\"ddpm\", input_shape=(3, 32, 32), backbone=backbone, noise_schedule=noise_schedule)\n\nmodel = DDPMModel(config, rngs=nnx.Rngs(0))\n</code></pre> <p>Documentation:</p> <ul> <li>Diffusion User Guide - Complete guide with sampling methods</li> <li>Diffusion API Reference - Detailed API documentation</li> <li>DiT Architecture - Diffusion Transformer details</li> </ul>"},{"location":"models/#flow-normalizing-flows","title":"Flow Normalizing Flows","text":"<p>Invertible transformations with tractable likelihoods for exact density estimation.</p> <p>Available Models:</p> Model Description Key Features Use Cases RealNVP Real-valued Non-Volume Preserving Affine coupling layers, multi-scale Density estimation baseline Glow Generative Flow Invertible 1x1 convolutions, ActNorm High-quality image generation MAF Masked Autoregressive Flow Autoregressive coupling, parallel training Flexible density estimation IAF Inverse Autoregressive Flow Fast sampling, parallel generation Variational inference Neural Spline Flow Spline-based coupling Smooth transformations, expressive High-quality density estimation SE(3) Molecular Flow Equivariant molecular flows SE(3) symmetry, molecular generation Drug design, molecular modeling Conditional Flow Class-conditional flows Label conditioning Controlled generation <p>Quick Start:</p> <pre><code>from artifex.generative_models.models.flow import NormalizingFlow\nfrom artifex.generative_models.core.configuration import FlowConfig\nfrom flax import nnx\n\nconfig = FlowConfig(name=\"realnvp\", input_shape=(32, 32, 3), num_flows=8, coupling_type=\"affine\")\n\nmodel = NormalizingFlow(config, rngs=nnx.Rngs(0))\n</code></pre> <p>Documentation:</p> <ul> <li>Flow User Guide - Complete guide with coupling layers</li> <li>Flow API Reference - Detailed API documentation</li> <li>SE(3) Molecular Flows - Equivariant flows for molecules</li> </ul>"},{"location":"models/#ebm-energy-based-models","title":"EBM Energy-Based Models","text":"<p>Energy function learning with MCMC sampling for compositional generation.</p> <p>Available Models:</p> Model Description Key Features Use Cases EBM Energy-based model Energy function learning, flexible Compositional generation EBM with MCMC EBM with MCMC sampling Langevin dynamics, HMC, NUTS High-quality sampling Persistent CD Persistent Contrastive Divergence Persistent chains, efficient training Stable EBM training <p>MCMC Samplers (via BlackJAX integration):</p> <ul> <li>HMC: Hamiltonian Monte Carlo</li> <li>NUTS: No-U-Turn Sampler (adaptive HMC)</li> <li>MALA: Metropolis-Adjusted Langevin Algorithm</li> <li>Langevin Dynamics: First-order gradient-based sampling</li> </ul> <p>Quick Start:</p> <pre><code>from artifex.generative_models.models.energy import EBM\nfrom artifex.generative_models.core.configuration import EBMConfig, EnergyNetworkConfig, MCMCConfig\nfrom flax import nnx\n\nenergy_network = EnergyNetworkConfig(name=\"energy_net\", hidden_dims=(512, 256, 128), activation=\"swish\")\nmcmc = MCMCConfig(name=\"mcmc\", n_steps=100, step_size=0.01)\nconfig = EBMConfig(name=\"ebm\", input_dim=784, energy_network=energy_network, mcmc=mcmc)\n\nmodel = EBM(config, rngs=nnx.Rngs(0))\n</code></pre> <p>Documentation:</p> <ul> <li>EBM Guide - Energy-based model details</li> <li>MCMC Sampling - Sampling algorithms</li> </ul>"},{"location":"models/#ar-autoregressive-models","title":"AR Autoregressive Models","text":"<p>Sequential generation with explicit likelihood for ordered data.</p> <p>Available Models:</p> Model Description Key Features Use Cases PixelCNN Autoregressive image model Masked convolutions, pixel-by-pixel Image generation with likelihood WaveNet Autoregressive audio model Dilated convolutions, long context Audio generation, TTS Transformer Transformer-based AR Self-attention, parallel training Text, structured sequences <p>Quick Start:</p> <pre><code>from artifex.generative_models.models.autoregressive import PixelCNN\nfrom artifex.generative_models.core.configuration import PixelCNNConfig\nfrom flax import nnx\n\nconfig = PixelCNNConfig(\n    name=\"pixelcnn\",\n    image_shape=(32, 32, 3),\n    num_layers=8,\n    hidden_channels=128,\n    kernel_size=3,\n    num_classes=256\n)\n\nmodel = PixelCNN(config, rngs=nnx.Rngs(0))\n</code></pre> <p>Documentation:</p> <ul> <li>PixelCNN API - Image autoregressive models</li> <li>WaveNet API - Audio autoregressive models</li> <li>Transformer API - Transformer-based models</li> </ul>"},{"location":"models/#geometric-geometric-models","title":"Geometric Geometric Models","text":"<p>3D structure generation with physical constraints and equivariance.</p> <p>Available Models:</p> Model Description Key Features Use Cases Point Cloud Generator 3D point cloud generation Permutation invariance 3D object generation Mesh Generator 3D mesh generation Vertex/face generation, deformation 3D modeling Protein Graph Protein structure generation Residue graphs, amino acid features Protein design Protein Point Cloud Protein backbone generation C\u03b1 coordinates, backbone geometry Protein structure prediction Voxel Generator Voxel-based 3D generation Regular 3D grid 3D shape generation Graph Generator Graph-based generation Node/edge features, message passing Molecular graphs <p>Quick Start:</p> <pre><code>from artifex.generative_models.models.geometric import PointCloudModel\nfrom artifex.generative_models.core.configuration import PointCloudConfig, PointCloudNetworkConfig\nfrom flax import nnx\n\nnetwork = PointCloudNetworkConfig(name=\"network\", embed_dim=256, num_heads=8, num_layers=6)\nconfig = PointCloudConfig(name=\"point_cloud\", num_points=128, point_dim=3, network=network)\n\nmodel = PointCloudModel(config, rngs=nnx.Rngs(0))\n</code></pre> <p>Documentation:</p> <ul> <li>Protein Models - Protein structure generation</li> <li>Point Cloud Models - 3D point cloud generation</li> <li>Graph Models - Graph-based generation</li> </ul>"},{"location":"models/#model-comparison","title":"Model Comparison","text":"<p>Choose the right model for your task:</p> Model Type Sample Quality Training Stability Speed Exact Likelihood Best For VAE \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u274c (Lower bound) Representation learning, fast sampling GAN \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u274c High-quality images, style transfer Diffusion \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u274c State-of-the-art generation, controllability Flow \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2705 Density estimation, exact inference EBM \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u274c (unnormalized) Compositional generation, flexibility AR \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2705 Sequences, explicit probabilities Geometric \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Varies 3D structures, physical constraints"},{"location":"models/#common-backbones","title":"Common Backbones","text":"<p>Artifex provides reusable backbone architectures used across model types:</p>"},{"location":"models/#u-net","title":"U-Net","text":"<p>Widely used in diffusion models and image-to-image tasks:</p> <pre><code>from artifex.generative_models.models.common.unet import UNet\n\nunet = UNet(\n    in_channels=3,\n    out_channels=3,\n    channels=[128, 256, 512, 1024],\n    num_res_blocks=2,\n    attention_resolutions=[16, 8],\n    rngs=nnx.Rngs(0)\n)\n</code></pre> <p>Documentation: U-Net API</p>"},{"location":"models/#diffusion-transformer-dit","title":"Diffusion Transformer (DiT)","text":"<p>Transformer-based backbone for diffusion models:</p> <pre><code>from artifex.generative_models.models.diffusion.dit import DiT\n\ndit = DiT(\n    input_size=32,\n    patch_size=2,\n    in_channels=3,\n    hidden_size=768,\n    depth=12,\n    num_heads=12,\n    rngs=nnx.Rngs(0)\n)\n</code></pre> <p>Documentation: DiT API</p>"},{"location":"models/#encoders-decoders","title":"Encoders &amp; Decoders","text":"<p>Modular encoder/decoder architectures:</p> <ul> <li>MLP Encoder/Decoder: Fully-connected networks</li> <li>CNN Encoder/Decoder: Convolutional networks for images</li> <li>Conditional Encoder/Decoder: Class-conditional variants</li> <li>ResNet Encoder/Decoder: Residual connections</li> </ul> <p>Documentation: Encoders | Decoders</p>"},{"location":"models/#conditioning-methods","title":"Conditioning Methods","text":"<p>Artifex supports multiple conditioning strategies across models:</p> Method Description Supported Models Use Cases Class Conditioning One-hot encoded labels VAE, GAN, Diffusion, Flow Supervised generation Text Conditioning CLIP embeddings Diffusion, GAN Text-to-image Image Conditioning Concatenation, cross-attention GAN, Diffusion Image-to-image, inpainting Embedding Conditioning Learned embeddings All models Flexible conditioning <p>Documentation: Conditioning Guide</p>"},{"location":"models/#model-registry","title":"Model Registry","text":"<p>All models are registered in a global registry for easy instantiation:</p> <pre><code>from artifex.generative_models.models.registry import (\n    list_models,\n    get_model_class,\n    register_model\n)\n\n# List all available models\navailable = list_models()\nprint(f\"Available models: {len(available)}\")\n\n# Get model class by name\nvae_class = get_model_class(\"vae\")\n\n# Register custom model\nfrom my_models import CustomVAE\nregister_model(\"custom_vae\", CustomVAE)\n</code></pre> <p>Documentation: Model Registry</p>"},{"location":"models/#training","title":"Training","text":"<p>All models follow a unified training interface:</p> <pre><code>from artifex.generative_models.training import Trainer\nfrom artifex.generative_models.core.configuration import TrainingConfig\n\ntraining_config = TrainingConfig(\n    batch_size=128,\n    num_epochs=100,\n    optimizer={\"type\": \"adam\", \"learning_rate\": 1e-3},\n    scheduler={\"type\": \"cosine\", \"warmup_steps\": 1000}\n)\n\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    train_dataset=train_data,\n    val_dataset=val_data\n)\n\ntrainer.train()\n</code></pre> <p>Documentation:</p> <ul> <li>Training Guide - Complete training guide</li> <li>Distributed Training - Multi-GPU/TPU training</li> </ul>"},{"location":"models/#evaluation","title":"Evaluation","text":"<p>Evaluate models with modality-specific metrics:</p> <pre><code>from artifex.benchmarks import EvaluationFramework\n\nframework = EvaluationFramework(\n    model=model,\n    modality=\"image\",\n    metrics=[\"fid\", \"inception_score\", \"lpips\"]\n)\n\nresults = framework.evaluate(test_dataset)\nprint(results)\n</code></pre> <p>Documentation: Benchmarks</p>"},{"location":"models/#examples","title":"Examples","text":"<p>Hands-on examples for each model family:</p> <ul> <li>VAE on MNIST - Basic VAE training</li> <li>GAN on MNIST - Image generation</li> <li>Diffusion on MNIST - Image generation</li> <li>Flow on MNIST - Density estimation</li> <li>Protein generation - Geometric models</li> </ul>"},{"location":"models/#contributing","title":"Contributing","text":"<p>Add new models to Artifex:</p> <ol> <li>Implement model following protocols in <code>core/protocols.py</code></li> <li>Add to appropriate directory (vae/, gan/, diffusion/, etc.)</li> <li>Register in model registry</li> <li>Add comprehensive tests</li> <li>Document API and usage</li> </ol> <p>Documentation: Contributing Guide</p>"},{"location":"models/#api-statistics","title":"API Statistics","text":"<p>Current model coverage:</p> <ul> <li>Total modules: 67</li> <li>Total classes: 135</li> <li>Total functions: 482</li> <li>Model families: 7</li> <li>Conditioning methods: 4</li> <li>Sampling methods: 15+</li> </ul> <ul> <li> <p> User Guides</p> <p>Deep dive into each model family with examples and best practices</p> <p> Browse guides</p> </li> <li> <p> API Reference</p> <p>Complete API documentation for all models and components</p> <p> API docs</p> </li> <li> <p> Tutorials</p> <p>Step-by-step tutorials for common tasks and workflows</p> <p> Start learning</p> </li> <li> <p> Examples</p> <p>Working code examples for all model types</p> <p> See examples</p> </li> </ul>"},{"location":"models/base/","title":"Base","text":"<p>Module: <code>generative_models.models.vae.base</code></p> <p>Source: <code>generative_models/models/vae/base.py</code></p>"},{"location":"models/base/#overview","title":"Overview","text":"<p>VAE base class definition.</p>"},{"location":"models/base/#classes","title":"Classes","text":""},{"location":"models/base/#vae","title":"VAE","text":"<pre><code>class VAE\n</code></pre>"},{"location":"models/base/#functions","title":"Functions","text":""},{"location":"models/base/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/base/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/base/#decode","title":"decode","text":"<pre><code>def decode()\n</code></pre>"},{"location":"models/base/#encode","title":"encode","text":"<pre><code>def encode()\n</code></pre>"},{"location":"models/base/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/base/#interpolate","title":"interpolate","text":"<pre><code>def interpolate()\n</code></pre>"},{"location":"models/base/#latent_traversal","title":"latent_traversal","text":"<pre><code>def latent_traversal()\n</code></pre>"},{"location":"models/base/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/base/#reconstruct","title":"reconstruct","text":"<pre><code>def reconstruct()\n</code></pre>"},{"location":"models/base/#reparameterize","title":"reparameterize","text":"<pre><code>def reparameterize()\n</code></pre>"},{"location":"models/base/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/base/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 11</li> <li>Imports: 5</li> </ul>"},{"location":"models/beta_vae/","title":"Beta Vae","text":"<p>Module: <code>generative_models.models.vae.beta_vae</code></p> <p>Source: <code>generative_models/models/vae/beta_vae.py</code></p>"},{"location":"models/beta_vae/#overview","title":"Overview","text":"<p>Beta Variational Autoencoder implementation.</p>"},{"location":"models/beta_vae/#classes","title":"Classes","text":""},{"location":"models/beta_vae/#betavae","title":"BetaVAE","text":"<pre><code>class BetaVAE\n</code></pre>"},{"location":"models/beta_vae/#betavaewithcapacity","title":"BetaVAEWithCapacity","text":"<pre><code>class BetaVAEWithCapacity\n</code></pre>"},{"location":"models/beta_vae/#functions","title":"Functions","text":""},{"location":"models/beta_vae/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/beta_vae/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/beta_vae/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/beta_vae/#loss_fn_1","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/beta_vae/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 4</li> <li>Imports: 4</li> </ul>"},{"location":"models/conditional/","title":"Conditional","text":"<p>Module: <code>generative_models.models.vae.conditional</code></p> <p>Source: <code>generative_models/models/vae/conditional.py</code></p>"},{"location":"models/conditional/#overview","title":"Overview","text":"<p>Conditional VAE Implementation.</p>"},{"location":"models/conditional/#classes","title":"Classes","text":""},{"location":"models/conditional/#conditionalvae","title":"ConditionalVAE","text":"<pre><code>class ConditionalVAE\n</code></pre>"},{"location":"models/conditional/#functions","title":"Functions","text":""},{"location":"models/conditional/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/conditional/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/conditional/#decode","title":"decode","text":"<pre><code>def decode()\n</code></pre>"},{"location":"models/conditional/#encode","title":"encode","text":"<pre><code>def encode()\n</code></pre>"},{"location":"models/conditional/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/conditional/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/conditional/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 6</li> <li>Imports: 5</li> </ul>"},{"location":"models/conditioning/","title":"Conditioning","text":"<p>Module: <code>generative_models.models.common.conditioning</code></p> <p>Source: <code>generative_models/models/common/conditioning.py</code></p>"},{"location":"models/conditioning/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"models/cyclegan/","title":"Cyclegan","text":"<p>Module: <code>generative_models.models.gan.cyclegan</code></p> <p>Source: <code>generative_models/models/gan/cyclegan.py</code></p>"},{"location":"models/cyclegan/#overview","title":"Overview","text":"<p>CycleGAN implementation for unpaired image-to-image translation.</p> <p>Based on the paper \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\" by Zhu et al. (2017).</p> <p>CycleGAN learns mappings between two image domains X and Y without requiring paired training examples. It uses cycle consistency loss to enforce that translated images can be mapped back to the original domain.</p>"},{"location":"models/cyclegan/#classes","title":"Classes","text":""},{"location":"models/cyclegan/#cyclegan_1","title":"CycleGAN","text":"<pre><code>class CycleGAN\n</code></pre>"},{"location":"models/cyclegan/#cyclegandiscriminator","title":"CycleGANDiscriminator","text":"<pre><code>class CycleGANDiscriminator\n</code></pre>"},{"location":"models/cyclegan/#cyclegangenerator","title":"CycleGANGenerator","text":"<pre><code>class CycleGANGenerator\n</code></pre>"},{"location":"models/cyclegan/#functions","title":"Functions","text":""},{"location":"models/cyclegan/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/cyclegan/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/cyclegan/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/cyclegan/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/cyclegan/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/cyclegan/#compute_cycle_loss","title":"compute_cycle_loss","text":"<pre><code>def compute_cycle_loss()\n</code></pre>"},{"location":"models/cyclegan/#compute_identity_loss","title":"compute_identity_loss","text":"<pre><code>def compute_identity_loss()\n</code></pre>"},{"location":"models/cyclegan/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/cyclegan/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/cyclegan/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 9</li> <li>Imports: 9</li> </ul>"},{"location":"models/dcgan/","title":"Dcgan","text":"<p>Module: <code>generative_models.models.gan.dcgan</code></p> <p>Source: <code>generative_models/models/gan/dcgan.py</code></p>"},{"location":"models/dcgan/#overview","title":"Overview","text":"<p>Deep Convolutional GAN (DCGAN) implementation.</p>"},{"location":"models/dcgan/#classes","title":"Classes","text":""},{"location":"models/dcgan/#dcgan_1","title":"DCGAN","text":"<pre><code>class DCGAN\n</code></pre>"},{"location":"models/dcgan/#dcgandiscriminator","title":"DCGANDiscriminator","text":"<pre><code>class DCGANDiscriminator\n</code></pre>"},{"location":"models/dcgan/#dcgangenerator","title":"DCGANGenerator","text":"<pre><code>class DCGANGenerator\n</code></pre>"},{"location":"models/dcgan/#functions","title":"Functions","text":""},{"location":"models/dcgan/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/dcgan/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/dcgan/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/dcgan/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/dcgan/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/dcgan/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 5</li> <li>Imports: 7</li> </ul>"},{"location":"models/ddim/","title":"Ddim","text":"<p>Module: <code>generative_models.models.diffusion.ddim</code></p> <p>Source: <code>generative_models/models/diffusion/ddim.py</code></p>"},{"location":"models/ddim/#overview","title":"Overview","text":"<p>DDIM (Denoising Diffusion Implicit Models) implementation.</p>"},{"location":"models/ddim/#classes","title":"Classes","text":""},{"location":"models/ddim/#ddimmodel","title":"DDIMModel","text":"<pre><code>class DDIMModel\n</code></pre>"},{"location":"models/ddim/#functions","title":"Functions","text":""},{"location":"models/ddim/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/ddim/#ddim_reverse","title":"ddim_reverse","text":"<pre><code>def ddim_reverse()\n</code></pre>"},{"location":"models/ddim/#ddim_sample","title":"ddim_sample","text":"<pre><code>def ddim_sample()\n</code></pre>"},{"location":"models/ddim/#ddim_step","title":"ddim_step","text":"<pre><code>def ddim_step()\n</code></pre>"},{"location":"models/ddim/#get_ddim_timesteps","title":"get_ddim_timesteps","text":"<pre><code>def get_ddim_timesteps()\n</code></pre>"},{"location":"models/ddim/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/ddim/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 6</li> <li>Imports: 4</li> </ul>"},{"location":"models/ddpm/","title":"Ddpm","text":"<p>Module: <code>generative_models.models.diffusion.ddpm</code></p> <p>Source: <code>generative_models/models/diffusion/ddpm.py</code></p>"},{"location":"models/ddpm/#overview","title":"Overview","text":"<p>DDPM (Denoising Diffusion Probabilistic Models) implementation.</p>"},{"location":"models/ddpm/#classes","title":"Classes","text":""},{"location":"models/ddpm/#ddpmmodel","title":"DDPMModel","text":"<pre><code>class DDPMModel\n</code></pre>"},{"location":"models/ddpm/#functions","title":"Functions","text":""},{"location":"models/ddpm/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/ddpm/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/ddpm/#denoise_step","title":"denoise_step","text":"<pre><code>def denoise_step()\n</code></pre>"},{"location":"models/ddpm/#forward_diffusion","title":"forward_diffusion","text":"<pre><code>def forward_diffusion()\n</code></pre>"},{"location":"models/ddpm/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/ddpm/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 5</li> <li>Imports: 7</li> </ul>"},{"location":"models/decoders/","title":"Decoders","text":"<p>Module: <code>generative_models.models.vae.decoders</code></p> <p>Source: <code>generative_models/models/vae/decoders.py</code></p>"},{"location":"models/decoders/#overview","title":"Overview","text":"<p>Decoder implementations for VAE models.</p>"},{"location":"models/decoders/#classes","title":"Classes","text":""},{"location":"models/decoders/#cnndecoder","title":"CNNDecoder","text":"<pre><code>class CNNDecoder\n</code></pre>"},{"location":"models/decoders/#conditionaldecoder","title":"ConditionalDecoder","text":"<pre><code>class ConditionalDecoder\n</code></pre>"},{"location":"models/decoders/#mlpdecoder","title":"MLPDecoder","text":"<pre><code>class MLPDecoder\n</code></pre>"},{"location":"models/decoders/#resnetdecoder","title":"ResNetDecoder","text":"<pre><code>class ResNetDecoder\n</code></pre>"},{"location":"models/decoders/#functions","title":"Functions","text":""},{"location":"models/decoders/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/decoders/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/decoders/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/decoders/#call_3","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/decoders/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/decoders/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/decoders/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/decoders/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/decoders/#create_decoder_unified","title":"create_decoder_unified","text":"<pre><code>def create_decoder_unified()\n</code></pre>"},{"location":"models/decoders/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 9</li> <li>Imports: 5</li> </ul>"},{"location":"models/diffusion/","title":"Diffusion","text":"<p>Module: <code>generative_models.models.audio.diffusion</code></p> <p>Source: <code>generative_models/models/audio/diffusion.py</code></p>"},{"location":"models/diffusion/#overview","title":"Overview","text":"<p>Audio diffusion model for parallel audio generation.</p>"},{"location":"models/diffusion/#classes","title":"Classes","text":""},{"location":"models/diffusion/#audiodiffusionconfig","title":"AudioDiffusionConfig","text":"<pre><code>class AudioDiffusionConfig\n</code></pre>"},{"location":"models/diffusion/#audiodiffusionmodel","title":"AudioDiffusionModel","text":"<pre><code>class AudioDiffusionModel\n</code></pre>"},{"location":"models/diffusion/#audiounet1d","title":"AudioUNet1D","text":"<pre><code>class AudioUNet1D\n</code></pre>"},{"location":"models/diffusion/#convblock1d","title":"ConvBlock1D","text":"<pre><code>class ConvBlock1D\n</code></pre>"},{"location":"models/diffusion/#downblock1d","title":"DownBlock1D","text":"<pre><code>class DownBlock1D\n</code></pre>"},{"location":"models/diffusion/#timeembedding1d","title":"TimeEmbedding1D","text":"<pre><code>class TimeEmbedding1D\n</code></pre>"},{"location":"models/diffusion/#upblock1d","title":"UpBlock1D","text":"<pre><code>class UpBlock1D\n</code></pre>"},{"location":"models/diffusion/#functions","title":"Functions","text":""},{"location":"models/diffusion/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/diffusion/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/diffusion/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/diffusion/#call_3","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/diffusion/#call_4","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/diffusion/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/diffusion/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/diffusion/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/diffusion/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/diffusion/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/diffusion/#init_5","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/diffusion/#create_audio_unet_backbone","title":"create_audio_unet_backbone","text":"<pre><code>def create_audio_unet_backbone()\n</code></pre>"},{"location":"models/diffusion/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/diffusion/#get_num_groups","title":"get_num_groups","text":"<pre><code>def get_num_groups()\n</code></pre>"},{"location":"models/diffusion/#get_num_groups_1","title":"get_num_groups","text":"<pre><code>def get_num_groups()\n</code></pre>"},{"location":"models/diffusion/#postprocess_audio","title":"postprocess_audio","text":"<pre><code>def postprocess_audio()\n</code></pre>"},{"location":"models/diffusion/#preprocess_audio","title":"preprocess_audio","text":"<pre><code>def preprocess_audio()\n</code></pre>"},{"location":"models/diffusion/#setup_noise_schedule","title":"setup_noise_schedule","text":"<pre><code>def setup_noise_schedule()\n</code></pre>"},{"location":"models/diffusion/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 7</li> <li>Functions: 18</li> <li>Imports: 5</li> </ul>"},{"location":"models/dit/","title":"Dit","text":"<p>Module: <code>generative_models.models.diffusion.dit</code></p> <p>Source: <code>generative_models/models/diffusion/dit.py</code></p>"},{"location":"models/dit/#overview","title":"Overview","text":"<p>DiT (Diffusion Transformer) Model implementation.</p> <p>Integrates the Diffusion Transformer backbone with the diffusion framework.</p>"},{"location":"models/dit/#classes","title":"Classes","text":""},{"location":"models/dit/#ditmodel","title":"DiTModel","text":"<pre><code>class DiTModel\n</code></pre>"},{"location":"models/dit/#functions","title":"Functions","text":""},{"location":"models/dit/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/dit/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/dit/#create_dit_backbone","title":"create_dit_backbone","text":"<pre><code>def create_dit_backbone()\n</code></pre>"},{"location":"models/dit/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/dit/#sample_step","title":"sample_step","text":"<pre><code>def sample_step()\n</code></pre>"},{"location":"models/dit/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 5</li> <li>Imports: 7</li> </ul>"},{"location":"models/ebm/","title":"Ebm","text":"<p>Module: <code>generative_models.models.energy.ebm</code></p> <p>Source: <code>generative_models/models/energy/ebm.py</code></p>"},{"location":"models/ebm/#overview","title":"Overview","text":"<p>Energy-Based Model implementations.</p> <p>This module provides concrete implementations of energy-based models for different data types and use cases.</p>"},{"location":"models/ebm/#classes","title":"Classes","text":""},{"location":"models/ebm/#deepcnnenergyfunction","title":"DeepCNNEnergyFunction","text":"<pre><code>class DeepCNNEnergyFunction\n</code></pre>"},{"location":"models/ebm/#deepebm","title":"DeepEBM","text":"<pre><code>class DeepEBM\n</code></pre>"},{"location":"models/ebm/#ebm_1","title":"EBM","text":"<pre><code>class EBM\n</code></pre>"},{"location":"models/ebm/#energyblock","title":"EnergyBlock","text":"<pre><code>class EnergyBlock\n</code></pre>"},{"location":"models/ebm/#functions","title":"Functions","text":""},{"location":"models/ebm/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/ebm/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/ebm/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/ebm/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/ebm/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/ebm/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/ebm/#create_cifar_ebm","title":"create_cifar_ebm","text":"<pre><code>def create_cifar_ebm()\n</code></pre>"},{"location":"models/ebm/#create_mnist_ebm","title":"create_mnist_ebm","text":"<pre><code>def create_mnist_ebm()\n</code></pre>"},{"location":"models/ebm/#create_simple_ebm","title":"create_simple_ebm","text":"<pre><code>def create_simple_ebm()\n</code></pre>"},{"location":"models/ebm/#get_config","title":"get_config","text":"<pre><code>def get_config()\n</code></pre>"},{"location":"models/ebm/#sample_from_buffer","title":"sample_from_buffer","text":"<pre><code>def sample_from_buffer()\n</code></pre>"},{"location":"models/ebm/#train_step","title":"train_step","text":"<pre><code>def train_step()\n</code></pre>"},{"location":"models/ebm/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 12</li> <li>Imports: 8</li> </ul>"},{"location":"models/encoders/","title":"Encoders","text":"<p>Module: <code>generative_models.models.vae.encoders</code></p> <p>Source: <code>generative_models/models/vae/encoders.py</code></p>"},{"location":"models/encoders/#overview","title":"Overview","text":"<p>Encoder implementations for VAE models.</p>"},{"location":"models/encoders/#classes","title":"Classes","text":""},{"location":"models/encoders/#cnnencoder","title":"CNNEncoder","text":"<pre><code>class CNNEncoder\n</code></pre>"},{"location":"models/encoders/#conditionalencoder","title":"ConditionalEncoder","text":"<pre><code>class ConditionalEncoder\n</code></pre>"},{"location":"models/encoders/#flatten","title":"Flatten","text":"<pre><code>class Flatten\n</code></pre>"},{"location":"models/encoders/#mlpencoder","title":"MLPEncoder","text":"<pre><code>class MLPEncoder\n</code></pre>"},{"location":"models/encoders/#resnetencoder","title":"ResNetEncoder","text":"<pre><code>class ResNetEncoder\n</code></pre>"},{"location":"models/encoders/#functions","title":"Functions","text":""},{"location":"models/encoders/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/encoders/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/encoders/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/encoders/#call_3","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/encoders/#call_4","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/encoders/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/encoders/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/encoders/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/encoders/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/encoders/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/encoders/#create_encoder_unified","title":"create_encoder_unified","text":"<pre><code>def create_encoder_unified()\n</code></pre>"},{"location":"models/encoders/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 5</li> <li>Functions: 11</li> <li>Imports: 4</li> </ul>"},{"location":"models/factories/","title":"Factories","text":"<p>Module: <code>generative_models.models.factories</code></p> <p>Source: <code>generative_models/models/factories.py</code></p>"},{"location":"models/factories/#overview","title":"Overview","text":"<p>Model factories - DEPRECATED.</p> <p>All factory functions have been moved to the centralized factory system.</p> <p>Use:</p> <pre><code>from artifex.generative_models.factory import create_model\nfrom artifex.generative_models.zoo import zoo\n</code></pre> <p>This module is kept for backward compatibility only.</p>"},{"location":"models/factories/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 2</li> </ul>"},{"location":"models/glow/","title":"Glow","text":"<p>Module: <code>generative_models.models.flow.glow</code></p> <p>Source: <code>generative_models/models/flow/glow.py</code></p>"},{"location":"models/glow/#overview","title":"Overview","text":"<p>Glow normalizing flow implementation.</p>"},{"location":"models/glow/#classes","title":"Classes","text":""},{"location":"models/glow/#actnormlayer","title":"ActNormLayer","text":"<pre><code>class ActNormLayer\n</code></pre>"},{"location":"models/glow/#affinecouplinglayer","title":"AffineCouplingLayer","text":"<pre><code>class AffineCouplingLayer\n</code></pre>"},{"location":"models/glow/#glow_1","title":"Glow","text":"<pre><code>class Glow\n</code></pre>"},{"location":"models/glow/#glowblock","title":"GlowBlock","text":"<pre><code>class GlowBlock\n</code></pre>"},{"location":"models/glow/#invertibleconv1x1","title":"InvertibleConv1x1","text":"<pre><code>class InvertibleConv1x1\n</code></pre>"},{"location":"models/glow/#functions","title":"Functions","text":""},{"location":"models/glow/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/glow/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/glow/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/glow/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/glow/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/glow/#forward","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/glow/#forward_1","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/glow/#forward_2","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/glow/#forward_3","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/glow/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/glow/#inverse","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/glow/#inverse_1","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/glow/#inverse_2","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/glow/#inverse_3","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/glow/#log_likelihood","title":"log_likelihood","text":"<pre><code>def log_likelihood()\n</code></pre>"},{"location":"models/glow/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/glow/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 5</li> <li>Functions: 16</li> <li>Imports: 4</li> </ul>"},{"location":"models/graph/","title":"Graph","text":"<p>Module: <code>generative_models.models.geometric.graph</code></p> <p>Source: <code>generative_models/models/geometric/graph.py</code></p>"},{"location":"models/graph/#overview","title":"Overview","text":"<p>Graph model implementation with E(n) Equivariant Graph Neural Networks (EGNN).</p> <p>This module implements a generic graph model with E(n) equivariance support, which is useful for generating and processing molecular structures, including proteins.</p>"},{"location":"models/graph/#classes","title":"Classes","text":""},{"location":"models/graph/#egnnlayer","title":"EGNNLayer","text":"<pre><code>class EGNNLayer\n</code></pre>"},{"location":"models/graph/#graphmodel","title":"GraphModel","text":"<pre><code>class GraphModel\n</code></pre>"},{"location":"models/graph/#functions","title":"Functions","text":""},{"location":"models/graph/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/graph/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/graph/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/graph/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/graph/#get_loss_fn","title":"get_loss_fn","text":"<pre><code>def get_loss_fn()\n</code></pre>"},{"location":"models/graph/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/graph/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/graph/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 7</li> <li>Imports: 6</li> </ul>"},{"location":"models/guidance/","title":"Guidance","text":"<p>Module: <code>generative_models.models.diffusion.guidance</code></p> <p>Source: <code>generative_models/models/diffusion/guidance.py</code></p>"},{"location":"models/guidance/#overview","title":"Overview","text":"<p>Guidance techniques for diffusion models.</p> <p>This module implements various guidance methods including classifier-free guidance, classifier guidance, and other conditional generation techniques.</p>"},{"location":"models/guidance/#classes","title":"Classes","text":""},{"location":"models/guidance/#classifierfreeguidance","title":"ClassifierFreeGuidance","text":"<pre><code>class ClassifierFreeGuidance\n</code></pre>"},{"location":"models/guidance/#classifierguidance","title":"ClassifierGuidance","text":"<pre><code>class ClassifierGuidance\n</code></pre>"},{"location":"models/guidance/#conditionaldiffusionmixin","title":"ConditionalDiffusionMixin","text":"<pre><code>class ConditionalDiffusionMixin\n</code></pre>"},{"location":"models/guidance/#guideddiffusionmodel","title":"GuidedDiffusionModel","text":"<pre><code>class GuidedDiffusionModel\n</code></pre>"},{"location":"models/guidance/#functions","title":"Functions","text":""},{"location":"models/guidance/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/guidance/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/guidance/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/guidance/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/guidance/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/guidance/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/guidance/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/guidance/#apply_guidance","title":"apply_guidance","text":"<pre><code>def apply_guidance()\n</code></pre>"},{"location":"models/guidance/#classifier_fn","title":"classifier_fn","text":"<pre><code>def classifier_fn()\n</code></pre>"},{"location":"models/guidance/#cosine_guidance_schedule","title":"cosine_guidance_schedule","text":"<pre><code>def cosine_guidance_schedule()\n</code></pre>"},{"location":"models/guidance/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/guidance/#guided_sample_step","title":"guided_sample_step","text":"<pre><code>def guided_sample_step()\n</code></pre>"},{"location":"models/guidance/#linear_guidance_schedule","title":"linear_guidance_schedule","text":"<pre><code>def linear_guidance_schedule()\n</code></pre>"},{"location":"models/guidance/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 13</li> <li>Imports: 5</li> </ul>"},{"location":"models/iaf/","title":"Iaf","text":"<p>Module: <code>generative_models.models.flow.iaf</code></p> <p>Source: <code>generative_models/models/flow/iaf.py</code></p>"},{"location":"models/iaf/#overview","title":"Overview","text":"<p>Inverse Autoregressive Flow (IAF) implementation.</p> <p>Based on the paper \"Improved Variational Inference with Inverse Autoregressive Flow\" by Kingma et al. Reference implementations from benchmark_VAE and other sources are used.</p>"},{"location":"models/iaf/#classes","title":"Classes","text":""},{"location":"models/iaf/#iaf_1","title":"IAF","text":"<pre><code>class IAF\n</code></pre>"},{"location":"models/iaf/#iaflayer","title":"IAFLayer","text":"<pre><code>class IAFLayer\n</code></pre>"},{"location":"models/iaf/#functions","title":"Functions","text":""},{"location":"models/iaf/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/iaf/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/iaf/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/iaf/#forward","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/iaf/#forward_1","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/iaf/#inverse","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/iaf/#inverse_1","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/iaf/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"models/iaf/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/iaf/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 9</li> <li>Imports: 7</li> </ul>"},{"location":"models/latent/","title":"Latent","text":"<p>Module: <code>generative_models.models.diffusion.latent</code></p> <p>Source: <code>generative_models/models/diffusion/latent.py</code></p>"},{"location":"models/latent/#overview","title":"Overview","text":"<p>Latent Diffusion Model implementation.</p>"},{"location":"models/latent/#classes","title":"Classes","text":""},{"location":"models/latent/#ldmmodel","title":"LDMModel","text":"<pre><code>class LDMModel\n</code></pre>"},{"location":"models/latent/#simpledecoder","title":"SimpleDecoder","text":"<pre><code>class SimpleDecoder\n</code></pre>"},{"location":"models/latent/#simpleencoder","title":"SimpleEncoder","text":"<pre><code>class SimpleEncoder\n</code></pre>"},{"location":"models/latent/#functions","title":"Functions","text":""},{"location":"models/latent/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/latent/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/latent/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/latent/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/latent/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/latent/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/latent/#decode","title":"decode","text":"<pre><code>def decode()\n</code></pre>"},{"location":"models/latent/#encode","title":"encode","text":"<pre><code>def encode()\n</code></pre>"},{"location":"models/latent/#loss","title":"loss","text":"<pre><code>def loss()\n</code></pre>"},{"location":"models/latent/#output_dim","title":"output_dim","text":"<pre><code>def output_dim()\n</code></pre>"},{"location":"models/latent/#reparameterize","title":"reparameterize","text":"<pre><code>def reparameterize()\n</code></pre>"},{"location":"models/latent/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/latent/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 12</li> <li>Imports: 5</li> </ul>"},{"location":"models/lsgan/","title":"Lsgan","text":"<p>Module: <code>generative_models.models.gan.lsgan</code></p> <p>Source: <code>generative_models/models/gan/lsgan.py</code></p>"},{"location":"models/lsgan/#overview","title":"Overview","text":"<p>Least Squares GAN (LSGAN) implementation.</p>"},{"location":"models/lsgan/#classes","title":"Classes","text":""},{"location":"models/lsgan/#lsgan_1","title":"LSGAN","text":"<pre><code>class LSGAN\n</code></pre>"},{"location":"models/lsgan/#lsgandiscriminator","title":"LSGANDiscriminator","text":"<pre><code>class LSGANDiscriminator\n</code></pre>"},{"location":"models/lsgan/#lsgangenerator","title":"LSGANGenerator","text":"<pre><code>class LSGANGenerator\n</code></pre>"},{"location":"models/lsgan/#functions","title":"Functions","text":""},{"location":"models/lsgan/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/lsgan/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/lsgan/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/lsgan/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/lsgan/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/lsgan/#discriminator_loss","title":"discriminator_loss","text":"<pre><code>def discriminator_loss()\n</code></pre>"},{"location":"models/lsgan/#generator_loss","title":"generator_loss","text":"<pre><code>def generator_loss()\n</code></pre>"},{"location":"models/lsgan/#training_step","title":"training_step","text":"<pre><code>def training_step()\n</code></pre>"},{"location":"models/lsgan/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 8</li> <li>Imports: 8</li> </ul>"},{"location":"models/made/","title":"Made","text":"<p>Module: <code>generative_models.models.flow.made</code></p> <p>Source: <code>generative_models/models/flow/made.py</code></p>"},{"location":"models/made/#overview","title":"Overview","text":"<p>Masked Autoencoder for Distribution Estimation (MADE).</p> <p>This implementation is shared between MAF and IAF models and is based on the reference implementations from benchmark_VAE and other sources.</p>"},{"location":"models/made/#classes","title":"Classes","text":""},{"location":"models/made/#made_1","title":"MADE","text":"<pre><code>class MADE\n</code></pre>"},{"location":"models/made/#functions","title":"Functions","text":""},{"location":"models/made/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/made/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/made/#create_made","title":"create_made","text":"<pre><code>def create_made()\n</code></pre>"},{"location":"models/made/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 3</li> <li>Imports: 4</li> </ul>"},{"location":"models/maf/","title":"Maf","text":"<p>Module: <code>generative_models.models.flow.maf</code></p> <p>Source: <code>generative_models/models/flow/maf.py</code></p>"},{"location":"models/maf/#overview","title":"Overview","text":"<p>Masked Autoregressive Flow (MAF) implementation.</p> <p>Based on the paper \"Masked Autoregressive Flow for Density Estimation\" by Papamakarios et al. Reference implementations from benchmark_VAE and other sources are used as guidance.</p>"},{"location":"models/maf/#classes","title":"Classes","text":""},{"location":"models/maf/#maf_1","title":"MAF","text":"<pre><code>class MAF\n</code></pre>"},{"location":"models/maf/#maflayer","title":"MAFLayer","text":"<pre><code>class MAFLayer\n</code></pre>"},{"location":"models/maf/#functions","title":"Functions","text":""},{"location":"models/maf/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/maf/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/maf/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/maf/#forward","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/maf/#inverse","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/maf/#inverse_1","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/maf/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"models/maf/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/maf/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 8</li> <li>Imports: 7</li> </ul>"},{"location":"models/mcmc/","title":"Mcmc","text":"<p>Module: <code>generative_models.models.energy.mcmc</code></p> <p>Source: <code>generative_models/models/energy/mcmc.py</code></p>"},{"location":"models/mcmc/#overview","title":"Overview","text":"<p>MCMC sampling utilities for energy-based models.</p> <p>This module provides sampling algorithms for energy-based models, including Langevin dynamics and buffer-based sampling strategies.</p>"},{"location":"models/mcmc/#classes","title":"Classes","text":""},{"location":"models/mcmc/#samplebuffer","title":"SampleBuffer","text":"<pre><code>class SampleBuffer\n</code></pre>"},{"location":"models/mcmc/#functions","title":"Functions","text":""},{"location":"models/mcmc/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/mcmc/#energy_single","title":"energy_single","text":"<pre><code>def energy_single()\n</code></pre>"},{"location":"models/mcmc/#energy_single_1","title":"energy_single","text":"<pre><code>def energy_single()\n</code></pre>"},{"location":"models/mcmc/#energy_single_2","title":"energy_single","text":"<pre><code>def energy_single()\n</code></pre>"},{"location":"models/mcmc/#improved_langevin_dynamics","title":"improved_langevin_dynamics","text":"<pre><code>def improved_langevin_dynamics()\n</code></pre>"},{"location":"models/mcmc/#langevin_dynamics","title":"langevin_dynamics","text":"<pre><code>def langevin_dynamics()\n</code></pre>"},{"location":"models/mcmc/#langevin_dynamics_with_trajectory","title":"langevin_dynamics_with_trajectory","text":"<pre><code>def langevin_dynamics_with_trajectory()\n</code></pre>"},{"location":"models/mcmc/#persistent_contrastive_divergence","title":"persistent_contrastive_divergence","text":"<pre><code>def persistent_contrastive_divergence()\n</code></pre>"},{"location":"models/mcmc/#sample_initial","title":"sample_initial","text":"<pre><code>def sample_initial()\n</code></pre>"},{"location":"models/mcmc/#update_buffer","title":"update_buffer","text":"<pre><code>def update_buffer()\n</code></pre>"},{"location":"models/mcmc/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 10</li> <li>Imports: 4</li> </ul>"},{"location":"models/mesh/","title":"Mesh","text":"<p>Module: <code>generative_models.models.geometric.mesh</code></p> <p>Source: <code>generative_models/models/geometric/mesh.py</code></p>"},{"location":"models/mesh/#overview","title":"Overview","text":"<p>Mesh generative model.</p>"},{"location":"models/mesh/#classes","title":"Classes","text":""},{"location":"models/mesh/#meshmodel","title":"MeshModel","text":"<pre><code>class MeshModel\n</code></pre>"},{"location":"models/mesh/#silu","title":"SiLU","text":"<pre><code>class SiLU\n</code></pre>"},{"location":"models/mesh/#functions","title":"Functions","text":""},{"location":"models/mesh/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/mesh/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/mesh/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/mesh/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/mesh/#get_loss_fn","title":"get_loss_fn","text":"<pre><code>def get_loss_fn()\n</code></pre>"},{"location":"models/mesh/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/mesh/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 6</li> <li>Imports: 7</li> </ul>"},{"location":"models/neural_spline/","title":"Neural Spline","text":"<p>Module: <code>generative_models.models.flow.neural_spline</code></p> <p>Source: <code>generative_models/models/flow/neural_spline.py</code></p>"},{"location":"models/neural_spline/#overview","title":"Overview","text":"<p>Neural Spline Flows implementation.</p> <p>Based on the paper: \"Neural Spline Flows\" by Durkan et al. (2019) https://arxiv.org/abs/1906.04032</p> <p>This implementation provides rational quadratic spline transformations for normalizing flows, following the existing architecture patterns.</p>"},{"location":"models/neural_spline/#classes","title":"Classes","text":""},{"location":"models/neural_spline/#neuralsplineflow","title":"NeuralSplineFlow","text":"<pre><code>class NeuralSplineFlow\n</code></pre>"},{"location":"models/neural_spline/#rationalquadraticsplinetransform","title":"RationalQuadraticSplineTransform","text":"<pre><code>class RationalQuadraticSplineTransform\n</code></pre>"},{"location":"models/neural_spline/#splinecouplinglayer","title":"SplineCouplingLayer","text":"<pre><code>class SplineCouplingLayer\n</code></pre>"},{"location":"models/neural_spline/#functions","title":"Functions","text":""},{"location":"models/neural_spline/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/neural_spline/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/neural_spline/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/neural_spline/#apply_spline","title":"apply_spline","text":"<pre><code>def apply_spline()\n</code></pre>"},{"location":"models/neural_spline/#forward","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/neural_spline/#forward_1","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/neural_spline/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/neural_spline/#inverse","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/neural_spline/#inverse_1","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/neural_spline/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"models/neural_spline/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/neural_spline/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/neural_spline/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 12</li> <li>Imports: 5</li> </ul>"},{"location":"models/patchgan/","title":"Patchgan","text":"<p>Module: <code>generative_models.models.gan.patchgan</code></p> <p>Source: <code>generative_models/models/gan/patchgan.py</code></p>"},{"location":"models/patchgan/#overview","title":"Overview","text":"<p>PatchGAN Discriminator implementation.</p> <p>Based on:</p> <ul> <li>Pix2Pix: Image-to-Image Translation with Conditional Adversarial Networks</li> <li>Pix2PixHD: High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</li> </ul>"},{"location":"models/patchgan/#classes","title":"Classes","text":""},{"location":"models/patchgan/#multiscalepatchgandiscriminator","title":"MultiScalePatchGANDiscriminator","text":"<pre><code>class MultiScalePatchGANDiscriminator\n</code></pre>"},{"location":"models/patchgan/#patchgandiscriminator","title":"PatchGANDiscriminator","text":"<pre><code>class PatchGANDiscriminator\n</code></pre>"},{"location":"models/patchgan/#functions","title":"Functions","text":""},{"location":"models/patchgan/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/patchgan/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/patchgan/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/patchgan/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/patchgan/#downsample_image","title":"downsample_image","text":"<pre><code>def downsample_image()\n</code></pre>"},{"location":"models/patchgan/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 5</li> <li>Imports: 5</li> </ul>"},{"location":"models/pixel_cnn/","title":"Pixel Cnn","text":"<p>Module: <code>generative_models.models.autoregressive.pixel_cnn</code></p> <p>Source: <code>generative_models/models/autoregressive/pixel_cnn.py</code></p>"},{"location":"models/pixel_cnn/#overview","title":"Overview","text":"<p>PixelCNN implementation for autoregressive image generation.</p>"},{"location":"models/pixel_cnn/#classes","title":"Classes","text":""},{"location":"models/pixel_cnn/#maskedconv2d","title":"MaskedConv2D","text":"<pre><code>class MaskedConv2D\n</code></pre>"},{"location":"models/pixel_cnn/#pixelcnn","title":"PixelCNN","text":"<pre><code>class PixelCNN\n</code></pre>"},{"location":"models/pixel_cnn/#functions","title":"Functions","text":""},{"location":"models/pixel_cnn/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/pixel_cnn/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/pixel_cnn/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/pixel_cnn/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/pixel_cnn/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/pixel_cnn/#inpaint","title":"inpaint","text":"<pre><code>def inpaint()\n</code></pre>"},{"location":"models/pixel_cnn/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"models/pixel_cnn/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/pixel_cnn/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 8</li> <li>Imports: 6</li> </ul>"},{"location":"models/point_cloud/","title":"Point Cloud","text":"<p>Module: <code>generative_models.models.geometric.point_cloud</code></p> <p>Source: <code>generative_models/models/geometric/point_cloud.py</code></p>"},{"location":"models/point_cloud/#overview","title":"Overview","text":"<p>Point cloud model implementation.</p>"},{"location":"models/point_cloud/#classes","title":"Classes","text":""},{"location":"models/point_cloud/#pointcloudmodel","title":"PointCloudModel","text":"<pre><code>class PointCloudModel\n</code></pre>"},{"location":"models/point_cloud/#transformerblock","title":"TransformerBlock","text":"<pre><code>class TransformerBlock\n</code></pre>"},{"location":"models/point_cloud/#functions","title":"Functions","text":""},{"location":"models/point_cloud/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/point_cloud/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/point_cloud/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/point_cloud/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/point_cloud/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/point_cloud/#get_loss_fn","title":"get_loss_fn","text":"<pre><code>def get_loss_fn()\n</code></pre>"},{"location":"models/point_cloud/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/point_cloud/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/point_cloud/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 8</li> <li>Imports: 7</li> </ul>"},{"location":"models/protein_graph/","title":"Protein Graph","text":"<p>Module: <code>generative_models.models.geometric.protein_graph</code></p> <p>Source: <code>generative_models/models/geometric/protein_graph.py</code></p>"},{"location":"models/protein_graph/#overview","title":"Overview","text":"<p>Protein-specific graph model implementation.</p>"},{"location":"models/protein_graph/#classes","title":"Classes","text":""},{"location":"models/protein_graph/#proteingraphmodel","title":"ProteinGraphModel","text":"<pre><code>class ProteinGraphModel\n</code></pre>"},{"location":"models/protein_graph/#functions","title":"Functions","text":""},{"location":"models/protein_graph/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/protein_graph/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/protein_graph/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/protein_graph/#get_loss_fn","title":"get_loss_fn","text":"<pre><code>def get_loss_fn()\n</code></pre>"},{"location":"models/protein_graph/#protein_loss_fn","title":"protein_loss_fn","text":"<pre><code>def protein_loss_fn()\n</code></pre>"},{"location":"models/protein_graph/#protein_sample","title":"protein_sample","text":"<pre><code>def protein_sample()\n</code></pre>"},{"location":"models/protein_graph/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/protein_graph/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 7</li> <li>Imports: 8</li> </ul>"},{"location":"models/protein_point_cloud/","title":"Protein Point Cloud","text":"<p>Module: <code>generative_models.models.geometric.protein_point_cloud</code></p> <p>Source: <code>generative_models/models/geometric/protein_point_cloud.py</code></p>"},{"location":"models/protein_point_cloud/#overview","title":"Overview","text":"<p>Protein-specific point cloud model implementation.</p>"},{"location":"models/protein_point_cloud/#classes","title":"Classes","text":""},{"location":"models/protein_point_cloud/#proteinpointcloudmodel","title":"ProteinPointCloudModel","text":"<pre><code>class ProteinPointCloudModel\n</code></pre>"},{"location":"models/protein_point_cloud/#functions","title":"Functions","text":""},{"location":"models/protein_point_cloud/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/protein_point_cloud/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/protein_point_cloud/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/protein_point_cloud/#get_loss_fn","title":"get_loss_fn","text":"<pre><code>def get_loss_fn()\n</code></pre>"},{"location":"models/protein_point_cloud/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/protein_point_cloud/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/protein_point_cloud/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 6</li> <li>Imports: 8</li> </ul>"},{"location":"models/real_nvp/","title":"Real Nvp","text":"<p>Module: <code>generative_models.models.flow.real_nvp</code></p> <p>Source: <code>generative_models/models/flow/real_nvp.py</code></p>"},{"location":"models/real_nvp/#overview","title":"Overview","text":"<p>RealNVP (Real-valued Non-Volume Preserving) normalizing flow implementation.</p>"},{"location":"models/real_nvp/#classes","title":"Classes","text":""},{"location":"models/real_nvp/#couplinglayer","title":"CouplingLayer","text":"<pre><code>class CouplingLayer\n</code></pre>"},{"location":"models/real_nvp/#realnvp","title":"RealNVP","text":"<pre><code>class RealNVP\n</code></pre>"},{"location":"models/real_nvp/#functions","title":"Functions","text":""},{"location":"models/real_nvp/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/real_nvp/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/real_nvp/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/real_nvp/#forward","title":"forward","text":"<pre><code>def forward()\n</code></pre>"},{"location":"models/real_nvp/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/real_nvp/#inverse","title":"inverse","text":"<pre><code>def inverse()\n</code></pre>"},{"location":"models/real_nvp/#params","title":"params","text":"<pre><code>def params()\n</code></pre>"},{"location":"models/real_nvp/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 7</li> <li>Imports: 6</li> </ul>"},{"location":"models/registry/","title":"Registry","text":"<p>Module: <code>generative_models.models.registry</code></p> <p>Source: <code>generative_models/models/registry.py</code></p>"},{"location":"models/registry/#overview","title":"Overview","text":"<p>Registry for mapping configuration types to model implementations.</p>"},{"location":"models/registry/#classes","title":"Classes","text":""},{"location":"models/registry/#modelregistry","title":"ModelRegistry","text":"<pre><code>class ModelRegistry\n</code></pre>"},{"location":"models/registry/#functions","title":"Functions","text":""},{"location":"models/registry/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/registry/#create_model","title":"create_model","text":"<pre><code>def create_model()\n</code></pre>"},{"location":"models/registry/#create_model_1","title":"create_model","text":"<pre><code>def create_model()\n</code></pre>"},{"location":"models/registry/#decorator","title":"decorator","text":"<pre><code>def decorator()\n</code></pre>"},{"location":"models/registry/#get_factory","title":"get_factory","text":"<pre><code>def get_factory()\n</code></pre>"},{"location":"models/registry/#get_model_factory","title":"get_model_factory","text":"<pre><code>def get_model_factory()\n</code></pre>"},{"location":"models/registry/#list_models","title":"list_models","text":"<pre><code>def list_models()\n</code></pre>"},{"location":"models/registry/#list_registered_models","title":"list_registered_models","text":"<pre><code>def list_registered_models()\n</code></pre>"},{"location":"models/registry/#register","title":"register","text":"<pre><code>def register()\n</code></pre>"},{"location":"models/registry/#register_model","title":"register_model","text":"<pre><code>def register_model()\n</code></pre>"},{"location":"models/registry/#register_model_legacy","title":"register_model_legacy","text":"<pre><code>def register_model_legacy()\n</code></pre>"},{"location":"models/registry/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 11</li> <li>Imports: 2</li> </ul>"},{"location":"models/score/","title":"Score","text":"<p>Module: <code>generative_models.models.diffusion.score</code></p> <p>Source: <code>generative_models/models/diffusion/score.py</code></p>"},{"location":"models/score/#overview","title":"Overview","text":"<p>Score-based diffusion models.</p> <p>Implementation of score-based diffusion model for generating samples from the data distribution by solving the reverse SDE.</p>"},{"location":"models/score/#classes","title":"Classes","text":""},{"location":"models/score/#scorediffusionmodel","title":"ScoreDiffusionModel","text":"<pre><code>class ScoreDiffusionModel\n</code></pre>"},{"location":"models/score/#functions","title":"Functions","text":""},{"location":"models/score/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/score/#denoise","title":"denoise","text":"<pre><code>def denoise()\n</code></pre>"},{"location":"models/score/#loss","title":"loss","text":"<pre><code>def loss()\n</code></pre>"},{"location":"models/score/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/score/#score_1","title":"score","text":"<pre><code>def score()\n</code></pre>"},{"location":"models/score/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 5</li> <li>Imports: 6</li> </ul>"},{"location":"models/se3_molecular/","title":"Se3 Molecular","text":"<p>Module: <code>generative_models.models.flow.se3_molecular</code></p> <p>Source: <code>generative_models/models/flow/se3_molecular.py</code></p>"},{"location":"models/se3_molecular/#overview","title":"Overview","text":"<p>SE(3)-Equivariant Molecular Flow for conformation generation.</p>"},{"location":"models/se3_molecular/#classes","title":"Classes","text":""},{"location":"models/se3_molecular/#se3couplinglayer","title":"SE3CouplingLayer","text":"<pre><code>class SE3CouplingLayer\n</code></pre>"},{"location":"models/se3_molecular/#se3equivariantlayer","title":"SE3EquivariantLayer","text":"<pre><code>class SE3EquivariantLayer\n</code></pre>"},{"location":"models/se3_molecular/#se3molecularflow","title":"SE3MolecularFlow","text":"<pre><code>class SE3MolecularFlow\n</code></pre>"},{"location":"models/se3_molecular/#functions","title":"Functions","text":""},{"location":"models/se3_molecular/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/se3_molecular/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/se3_molecular/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/se3_molecular/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/se3_molecular/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/se3_molecular/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/se3_molecular/#log_prob","title":"log_prob","text":"<pre><code>def log_prob()\n</code></pre>"},{"location":"models/se3_molecular/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/se3_molecular/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/se3_molecular/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 9</li> <li>Imports: 5</li> </ul>"},{"location":"models/stable_diffusion/","title":"Stable Diffusion","text":"<p>Module: <code>generative_models.models.diffusion.stable_diffusion</code></p> <p>Source: <code>generative_models/models/diffusion/stable_diffusion.py</code></p>"},{"location":"models/stable_diffusion/#overview","title":"Overview","text":"<p>Stable Diffusion implementation.</p> <p>This module implements a simplified version of Stable Diffusion, which is essentially a latent diffusion model with text conditioning and specific architectural choices.</p>"},{"location":"models/stable_diffusion/#classes","title":"Classes","text":""},{"location":"models/stable_diffusion/#stablediffusionmodel","title":"StableDiffusionModel","text":"<pre><code>class StableDiffusionModel\n</code></pre>"},{"location":"models/stable_diffusion/#textencoder","title":"TextEncoder","text":"<pre><code>class TextEncoder\n</code></pre>"},{"location":"models/stable_diffusion/#functions","title":"Functions","text":""},{"location":"models/stable_diffusion/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/stable_diffusion/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/stable_diffusion/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/stable_diffusion/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/stable_diffusion/#compute_text_similarity","title":"compute_text_similarity","text":"<pre><code>def compute_text_similarity()\n</code></pre>"},{"location":"models/stable_diffusion/#encode_text","title":"encode_text","text":"<pre><code>def encode_text()\n</code></pre>"},{"location":"models/stable_diffusion/#generate_with_text","title":"generate_with_text","text":"<pre><code>def generate_with_text()\n</code></pre>"},{"location":"models/stable_diffusion/#interpolate_text","title":"interpolate_text","text":"<pre><code>def interpolate_text()\n</code></pre>"},{"location":"models/stable_diffusion/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 8</li> <li>Imports: 6</li> </ul>"},{"location":"models/stylegan/","title":"Stylegan","text":"<p>Module: <code>generative_models.models.gan.stylegan</code></p> <p>Source: <code>generative_models/models/gan/stylegan.py</code></p>"},{"location":"models/stylegan/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"models/stylegan3/","title":"Stylegan3","text":"<p>Module: <code>generative_models.models.gan.stylegan3</code></p> <p>Source: <code>generative_models/models/gan/stylegan3.py</code></p>"},{"location":"models/stylegan3/#overview","title":"Overview","text":"<p>StyleGAN3 Generator with Translation and Rotation Equivariance.</p> <p>This module implements StyleGAN3 architecture based on the official NVIDIA implementation but simplified for JAX/Flax NNX patterns while maintaining mathematical correctness.</p> <p>Key Features:</p> <ul> <li>Style-based generation with mapping and synthesis networks</li> <li>Simplified but effective modulated convolutions</li> <li>Progressive upsampling architecture</li> <li>JAX/Flax NNX compatibility</li> </ul>"},{"location":"models/stylegan3/#classes","title":"Classes","text":""},{"location":"models/stylegan3/#mappingnetwork","title":"MappingNetwork","text":"<pre><code>class MappingNetwork\n</code></pre>"},{"location":"models/stylegan3/#stylegan3discriminator","title":"StyleGAN3Discriminator","text":"<pre><code>class StyleGAN3Discriminator\n</code></pre>"},{"location":"models/stylegan3/#stylegan3generator","title":"StyleGAN3Generator","text":"<pre><code>class StyleGAN3Generator\n</code></pre>"},{"location":"models/stylegan3/#stylemodulatedconv","title":"StyleModulatedConv","text":"<pre><code>class StyleModulatedConv\n</code></pre>"},{"location":"models/stylegan3/#synthesisblock","title":"SynthesisBlock","text":"<pre><code>class SynthesisBlock\n</code></pre>"},{"location":"models/stylegan3/#synthesisnetwork","title":"SynthesisNetwork","text":"<pre><code>class SynthesisNetwork\n</code></pre>"},{"location":"models/stylegan3/#functions","title":"Functions","text":""},{"location":"models/stylegan3/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/stylegan3/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/stylegan3/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/stylegan3/#call_3","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/stylegan3/#call_4","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/stylegan3/#call_5","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/stylegan3/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/stylegan3/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/stylegan3/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/stylegan3/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/stylegan3/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/stylegan3/#init_5","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/stylegan3/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/stylegan3/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 6</li> <li>Functions: 13</li> <li>Imports: 4</li> </ul>"},{"location":"models/transformer/","title":"Transformer","text":"<p>Module: <code>generative_models.models.autoregressive.transformer</code></p> <p>Source: <code>generative_models/models/autoregressive/transformer.py</code></p>"},{"location":"models/transformer/#overview","title":"Overview","text":"<p>Transformer-based autoregressive model for sequence generation.</p>"},{"location":"models/transformer/#classes","title":"Classes","text":""},{"location":"models/transformer/#transformerautoregressivemodel","title":"TransformerAutoregressiveModel","text":"<pre><code>class TransformerAutoregressiveModel\n</code></pre>"},{"location":"models/transformer/#functions","title":"Functions","text":""},{"location":"models/transformer/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/transformer/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/transformer/#beam_search","title":"beam_search","text":"<pre><code>def beam_search()\n</code></pre>"},{"location":"models/transformer/#compute_perplexity","title":"compute_perplexity","text":"<pre><code>def compute_perplexity()\n</code></pre>"},{"location":"models/transformer/#encode","title":"encode","text":"<pre><code>def encode()\n</code></pre>"},{"location":"models/transformer/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/transformer/#generate_with_cache","title":"generate_with_cache","text":"<pre><code>def generate_with_cache()\n</code></pre>"},{"location":"models/transformer/#get_attention_weights","title":"get_attention_weights","text":"<pre><code>def get_attention_weights()\n</code></pre>"},{"location":"models/transformer/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 8</li> <li>Imports: 6</li> </ul>"},{"location":"models/unet/","title":"Unet","text":"<p>Module: <code>generative_models.models.common.unet</code></p> <p>Source: <code>generative_models/models/common/unet.py</code></p>"},{"location":"models/unet/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"models/voxel/","title":"Voxel","text":"<p>Module: <code>generative_models.models.geometric.voxel</code></p> <p>Source: <code>generative_models/models/geometric/voxel.py</code></p>"},{"location":"models/voxel/#overview","title":"Overview","text":"<p>Voxel generative model.</p>"},{"location":"models/voxel/#classes","title":"Classes","text":""},{"location":"models/voxel/#reshape","title":"Reshape","text":"<pre><code>class Reshape\n</code></pre>"},{"location":"models/voxel/#voxelmodel","title":"VoxelModel","text":"<pre><code>class VoxelModel\n</code></pre>"},{"location":"models/voxel/#functions","title":"Functions","text":""},{"location":"models/voxel/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/voxel/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/voxel/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/voxel/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/voxel/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/voxel/#get_loss_fn","title":"get_loss_fn","text":"<pre><code>def get_loss_fn()\n</code></pre>"},{"location":"models/voxel/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/voxel/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 7</li> <li>Imports: 7</li> </ul>"},{"location":"models/vq_vae/","title":"Vq Vae","text":"<p>Module: <code>generative_models.models.vae.vq_vae</code></p> <p>Source: <code>generative_models/models/vae/vq_vae.py</code></p>"},{"location":"models/vq_vae/#overview","title":"Overview","text":"<p>Vector Quantized VAE Implementation.</p>"},{"location":"models/vq_vae/#classes","title":"Classes","text":""},{"location":"models/vq_vae/#vqvae","title":"VQVAE","text":"<pre><code>class VQVAE\n</code></pre>"},{"location":"models/vq_vae/#functions","title":"Functions","text":""},{"location":"models/vq_vae/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/vq_vae/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/vq_vae/#decode","title":"decode","text":"<pre><code>def decode()\n</code></pre>"},{"location":"models/vq_vae/#encode","title":"encode","text":"<pre><code>def encode()\n</code></pre>"},{"location":"models/vq_vae/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/vq_vae/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/vq_vae/#quantize","title":"quantize","text":"<pre><code>def quantize()\n</code></pre>"},{"location":"models/vq_vae/#sample","title":"sample","text":"<pre><code>def sample()\n</code></pre>"},{"location":"models/vq_vae/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 8</li> <li>Imports: 5</li> </ul>"},{"location":"models/wavenet/","title":"Wavenet","text":"<p>Module: <code>generative_models.models.autoregressive.wavenet</code></p> <p>Source: <code>generative_models/models/autoregressive/wavenet.py</code></p>"},{"location":"models/wavenet/#overview","title":"Overview","text":"<p>WaveNet implementation for autoregressive sequence generation.</p>"},{"location":"models/wavenet/#classes","title":"Classes","text":""},{"location":"models/wavenet/#causalconv1d","title":"CausalConv1D","text":"<pre><code>class CausalConv1D\n</code></pre>"},{"location":"models/wavenet/#gatedactivationunit","title":"GatedActivationUnit","text":"<pre><code>class GatedActivationUnit\n</code></pre>"},{"location":"models/wavenet/#wavenet_1","title":"WaveNet","text":"<pre><code>class WaveNet\n</code></pre>"},{"location":"models/wavenet/#functions","title":"Functions","text":""},{"location":"models/wavenet/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/wavenet/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/wavenet/#call_2","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/wavenet/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/wavenet/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/wavenet/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/wavenet/#compute_receptive_field","title":"compute_receptive_field","text":"<pre><code>def compute_receptive_field()\n</code></pre>"},{"location":"models/wavenet/#conditional_generate","title":"conditional_generate","text":"<pre><code>def conditional_generate()\n</code></pre>"},{"location":"models/wavenet/#generate_fast","title":"generate_fast","text":"<pre><code>def generate_fast()\n</code></pre>"},{"location":"models/wavenet/#get_intermediate_outputs","title":"get_intermediate_outputs","text":"<pre><code>def get_intermediate_outputs()\n</code></pre>"},{"location":"models/wavenet/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"models/wavenet/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 11</li> <li>Imports: 6</li> </ul>"},{"location":"models/wgan/","title":"Wgan","text":"<p>Module: <code>generative_models.models.gan.wgan</code></p> <p>Source: <code>generative_models/models/gan/wgan.py</code></p>"},{"location":"models/wgan/#overview","title":"Overview","text":"<p>Wasserstein GAN with Gradient Penalty (WGAN-GP) implementation.</p>"},{"location":"models/wgan/#classes","title":"Classes","text":""},{"location":"models/wgan/#wgan_1","title":"WGAN","text":"<pre><code>class WGAN\n</code></pre>"},{"location":"models/wgan/#wgandiscriminator","title":"WGANDiscriminator","text":"<pre><code>class WGANDiscriminator\n</code></pre>"},{"location":"models/wgan/#wgangenerator","title":"WGANGenerator","text":"<pre><code>class WGANGenerator\n</code></pre>"},{"location":"models/wgan/#functions","title":"Functions","text":""},{"location":"models/wgan/#call","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/wgan/#call_1","title":"call","text":"<pre><code>def __call__()\n</code></pre>"},{"location":"models/wgan/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/wgan/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/wgan/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"models/wgan/#compute_gradient_penalty","title":"compute_gradient_penalty","text":"<pre><code>def compute_gradient_penalty()\n</code></pre>"},{"location":"models/wgan/#discriminator_fn","title":"discriminator_fn","text":"<pre><code>def discriminator_fn()\n</code></pre>"},{"location":"models/wgan/#discriminator_loss","title":"discriminator_loss","text":"<pre><code>def discriminator_loss()\n</code></pre>"},{"location":"models/wgan/#generate","title":"generate","text":"<pre><code>def generate()\n</code></pre>"},{"location":"models/wgan/#generator_loss","title":"generator_loss","text":"<pre><code>def generator_loss()\n</code></pre>"},{"location":"models/wgan/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 10</li> <li>Imports: 6</li> </ul>"},{"location":"notebooks/","title":"Notebooks","text":"<p>This directory contains Jupyter notebooks with examples, tutorials, and demonstrations of the Artifex library features.</p>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":""},{"location":"notebooks/#protein-structure-generation","title":"Protein Structure Generation","text":"<ul> <li><code>protein_diffusion.ipynb</code>: Implementation of a diffusion model for protein structure generation using JAX and Flax.</li> <li>Demonstrates how to generate 3D protein structures</li> <li>Shows integration with protein-specific constraints</li> <li>Includes visualization of generated structures</li> </ul>"},{"location":"notebooks/#new-model-examples","title":"New Model Examples","text":"<p>Artifex now includes comprehensive examples for new model types:</p> <ul> <li>Energy-Based Models: See <code>examples/</code> directory for EBM training and sampling examples</li> <li>Advanced MCMC Sampling: Langevin dynamics and persistent contrastive divergence</li> <li>GPU Optimization: Matrix multiplication fixes and CUDA setup examples</li> </ul>"},{"location":"notebooks/#running-the-notebooks","title":"Running the Notebooks","text":"<p>To run these notebooks locally:</p> <ol> <li>Ensure you have installed Artifex with development dependencies:</li> </ol> <pre><code># Recommended: Use the cuda-dev environment for full GPU support\nuv sync --extra cuda-dev\n\n# Or just development dependencies\npip install -e \".[dev]\"\n</code></pre> <ol> <li>Set up your CUDA environment (for GPU examples):</li> </ol> <pre><code>./scripts/fresh_cuda_setup.sh\n</code></pre> <ol> <li>Launch Jupyter:</li> </ol> <pre><code>jupyter lab\n</code></pre> <p>or</p> <pre><code>jupyter notebook\n</code></pre> <ol> <li>Navigate to the notebook you want to run.</li> </ol>"},{"location":"notebooks/#notebook-structure","title":"Notebook Structure","text":"<p>Each notebook generally follows this structure:</p> <ol> <li>Introduction and Setup: Overview of the example and necessary imports</li> <li>Data Preparation: Loading and preprocessing data</li> <li>Model Configuration: Setting up the model architecture</li> <li>Training: Training the model on the data</li> <li>Evaluation: Evaluating model performance</li> <li>Generation: Using the model to generate new samples</li> <li>Visualization: Visualizing the results</li> </ol>"},{"location":"notebooks/#adding-new-notebooks","title":"Adding New Notebooks","text":"<p>When adding new notebooks, please:</p> <ol> <li>Ensure all dependencies are documented</li> <li>Include comprehensive explanations with markdown cells</li> <li>Add the notebook to this README</li> <li>Run <code>pre-commit</code> to ensure the notebook is properly formatted and stripped of outputs</li> </ol>"},{"location":"papers/artifex_arxiv_preprint/","title":"Artifex: A Comprehensive Generative Modeling Framework for Research and Production","text":"<p>Authors: Mahdi Shafiei Affiliation: Avitai Bio Email: mahdi@avitai.bio Date: January 26, 2026 Status: Work in Progress - Active Development arXiv ID: [To be assigned] Categories: cs.LG, cs.AI, stat.ML</p>"},{"location":"papers/artifex_arxiv_preprint/#abstract","title":"Abstract","text":"<p>We present Artifex, a comprehensive generative modeling library built on JAX/Flax that provides unified implementations of state-of-the-art generative models across multiple modalities. Artifex aims to address the gap between research prototypes and deployable generative modeling systems by offering a modular, type-safe, and scalable framework. The library implements seven major generative modeling paradigms\u2014Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, Flow-based Models, Autoregressive Models, Energy-Based Models (EBMs), and Geometric Models\u2014with support for eight data modalities: images, text, audio, tabular data, time series, protein structures, molecular data, and multi-modal representations. Artifex introduces architectural patterns including a protocol-based design system, hardware-aware optimization infrastructure, multi-dimensional parallelism strategies, and a unified configuration management system using frozen dataclasses. The framework is currently undergoing active development and refactoring, with APIs subject to change. Artifex provides researchers with a foundation for generative modeling research on JAX.</p> <p>Note: This preprint describes ongoing development. The framework is not yet production-ready and specific performance claims require validation as the codebase stabilizes.</p> <p>Keywords: Generative Models, JAX, Flax, Machine Learning Framework, Scalable AI, Multi-Modal Learning</p>"},{"location":"papers/artifex_arxiv_preprint/#1-introduction","title":"1. Introduction","text":"<p>Generative modeling has emerged as one of the most transformative areas in artificial intelligence, enabling the creation of realistic content across diverse modalities including images, text, audio, and structured data. However, the field faces significant challenges in transitioning from research prototypes to deployable systems. Existing frameworks often lack the scalability, type safety, and comprehensive evaluation capabilities required for serious deployment, while research-focused libraries typically sacrifice robustness for experimental flexibility.</p> <p>The Artifex library aims to address these challenges by providing a unified framework for generative modeling built on the JAX ecosystem with Flax NNX. Artifex combines the performance benefits of JAX's automatic differentiation and XLA compilation with modern neural network design patterns. The framework is currently in active development, with ongoing refactoring to establish strong architectural foundations.</p> <p>Development Status: Artifex is undergoing major refactoring. APIs are subject to change, and users should expect breaking changes between versions. The framework prioritizes establishing correct foundations over backward compatibility.</p>"},{"location":"papers/artifex_arxiv_preprint/#11-key-contributions","title":"1.1 Key Contributions","text":"<p>This work makes the following key contributions to the generative modeling ecosystem:</p> <ol> <li> <p>Unified Generative Modeling Framework: A library implementing seven major generative modeling paradigms with consistent interfaces and type-safe protocols.</p> </li> <li> <p>Multi-Modal Architecture: Support for eight distinct data modalities with specialized adapters, evaluation metrics, and domain-specific constraints.</p> </li> <li> <p>Scaling Infrastructure (Experimental): Hardware-aware optimization and multi-dimensional parallelism strategies. Large-scale distributed training capabilities are under development.</p> </li> <li> <p>Protocol-Based Design System: Type-safe interfaces using Python's Protocol system, enabling static and runtime type checking while maintaining implementation flexibility.</p> </li> <li> <p>Evaluation Framework (In Progress): Standardized metrics and benchmarking infrastructure being developed across supported modalities and model types.</p> </li> <li> <p>Modern JAX/Flax Integration: Full compatibility with Flax NNX patterns, automatic differentiation, and JAX transformations.</p> </li> </ol>"},{"location":"papers/artifex_arxiv_preprint/#12-related-work","title":"1.2 Related Work","text":"<p>Several frameworks have attempted to address the challenges in generative modeling infrastructure. PyTorch-based libraries such as PyTorch Lightning and Hugging Face Transformers provide excellent research capabilities but often lack production scaling features. TensorFlow-based solutions offer better production support but suffer from API complexity and slower research iteration cycles. JAX-based frameworks like Flax and Haiku provide excellent performance but typically focus on specific model types or lack comprehensive evaluation capabilities.</p> <p>Artifex aims to provide a unified framework that addresses the complete generative modeling pipeline while maintaining research flexibility. The library's protocol-based design enables seamless integration with existing JAX ecosystem tools while providing the type safety needed for reliable experimentation.</p>"},{"location":"papers/artifex_arxiv_preprint/#2-framework-architecture","title":"2. Framework Architecture","text":""},{"location":"papers/artifex_arxiv_preprint/#21-core-design-principles","title":"2.1 Core Design Principles","text":"<p>Artifex is built on four fundamental design principles that guide its architecture and implementation:</p> <p>Modularity: The framework employs a modular design where components can be used independently or composed to create complex systems. This enables researchers to experiment with specific components while allowing practitioners to build production systems using proven patterns.</p> <p>Type Safety: All interfaces are defined using Python's Protocol system, providing static type checking during development and runtime verification during execution. This ensures reliability and reduces debugging overhead in production environments.</p> <p>Performance-First Design: Every component is designed with performance as a primary consideration, leveraging JAX's automatic differentiation, XLA compilation, and hardware-aware optimization to achieve optimal execution efficiency.</p> <p>Scalability: The framework provides built-in support for scaling from single-device experimentation to multi-node distributed training, with automatic hardware detection and optimization strategies.</p>"},{"location":"papers/artifex_arxiv_preprint/#22-generative-model-protocol","title":"2.2 Generative Model Protocol","text":"<p>At the core of Artifex's architecture is the <code>GenerativeModelProtocol</code>, which defines a consistent interface for all generative models:</p> <pre><code>@runtime_checkable\nclass GenerativeModelProtocol(Protocol):\n    def __call__(self, x: Any, *, rngs: nnx.Rngs | None = None, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Forward pass through the model.\"\"\"\n        ...\n\n    def generate(self, n_samples: int = 1, *, rngs: nnx.Rngs | None = None, **kwargs) -&gt; jax.Array:\n        \"\"\"Generate samples from the model.\"\"\"\n        ...\n\n    def loss_fn(self, batch: Dict[str, Any], outputs: Dict[str, Any], **kwargs) -&gt; Dict[str, Array]:\n        \"\"\"Compute loss function.\"\"\"\n        ...\n</code></pre> <p>This protocol-based approach enables:</p> <ul> <li>Static and runtime type checking for model implementations</li> <li>Common interfaces across diverse model architectures</li> <li>Generic training and evaluation code that works with any model</li> <li>Type-safe interaction with JAX transformations and optimizers</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#23-model-registry-and-factory-system","title":"2.3 Model Registry and Factory System","text":"<p>Artifex implements a centralized model registry and factory system that enables dynamic model creation and configuration:</p> <pre><code>@register_model(\"vae\")\nclass VAEModel(GenerativeModel):\n    \"\"\"Variational Autoencoder implementation.\"\"\"\n    ...\n\n# Dynamic model creation\nmodel = create_model(config, rngs=rngs)\n</code></pre> <p>The factory system provides:</p> <ul> <li>Centralized model registration and discovery</li> <li>Type-safe model creation with configuration validation</li> <li>Support for model variants and parameter inheritance</li> <li>Integration with the unified configuration system</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#24-configuration-management","title":"2.4 Configuration Management","text":"<p>Artifex employs a hierarchical configuration system built on frozen dataclasses that provides:</p> <ul> <li>Type Safety: All configuration parameters are type-annotated and immutable after creation</li> <li>Immutability: Frozen dataclasses prevent accidental configuration modification</li> <li>Inheritance: Configuration classes use inheritance for shared parameters</li> <li>Validation: Custom <code>__post_init__</code> methods validate configuration consistency</li> </ul> <pre><code>from dataclasses import dataclass, field\n\n@dataclass(frozen=True)\nclass BaseConfig:\n    \"\"\"Base configuration class for all configs.\"\"\"\n    dtype: str = \"float32\"\n\n@dataclass(frozen=True)\nclass VAEConfig(BaseConfig):\n    \"\"\"Configuration for VAE models.\"\"\"\n    latent_dim: int = 64\n    encoder: EncoderConfig = field(default_factory=EncoderConfig)\n    decoder: DecoderConfig = field(default_factory=DecoderConfig)\n    beta: float = 1.0\n</code></pre>"},{"location":"papers/artifex_arxiv_preprint/#3-generative-model-implementations","title":"3. Generative Model Implementations","text":""},{"location":"papers/artifex_arxiv_preprint/#31-model-categories","title":"3.1 Model Categories","text":"<p>Artifex implements seven major categories of generative models, each with multiple variants and specialized implementations:</p>"},{"location":"papers/artifex_arxiv_preprint/#311-variational-autoencoders-vaes","title":"3.1.1 Variational Autoencoders (VAEs)","text":"<ul> <li>Basic VAE: Standard variational autoencoder with KL divergence regularization</li> <li>Beta-VAE: Enhanced disentanglement through \u03b2-parameter control</li> <li>Conditional VAE: Conditional generation with auxiliary information</li> <li>Hierarchical VAE: Multi-level latent representations</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#312-generative-adversarial-networks-gans","title":"3.1.2 Generative Adversarial Networks (GANs)","text":"<ul> <li>DCGAN: Deep Convolutional GAN for image generation</li> <li>WGAN: Wasserstein GAN with gradient penalty</li> <li>Conditional GAN: Conditional generation capabilities</li> <li>LSGAN: Least Squares GAN for improved stability</li> <li>CycleGAN: Unpaired image-to-image translation</li> <li>PatchGAN: Multi-scale discriminator for high-resolution generation</li> <li>StyleGAN3: Alias-free generative adversarial network</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#313-diffusion-models","title":"3.1.3 Diffusion Models","text":"<ul> <li>DDPM: Denoising Diffusion Probabilistic Models</li> <li>DDIM: Denoising Diffusion Implicit Models for faster sampling (in development)</li> <li>Score-based Models: Score-based generative modeling</li> <li>Latent Diffusion: Diffusion in latent space for efficiency</li> <li>DiT: Diffusion Transformers for scalable generation</li> <li>Guided Diffusion: Classifier and classifier-free guidance</li> <li>Stable Diffusion: Text-to-image generation (planned)</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#314-flow-based-models","title":"3.1.4 Flow-based Models","text":"<ul> <li>Glow: Generative Flow with invertible 1\u00d71 convolutions</li> <li>MAF: Masked Autoregressive Flow</li> <li>IAF: Inverse Autoregressive Flow</li> <li>Neural Spline Flows: Continuous normalizing flows with splines</li> <li>Conditional Flows: Conditional normalizing flows</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#315-autoregressive-models","title":"3.1.5 Autoregressive Models","text":"<ul> <li>PixelCNN: Autoregressive image generation with causal convolutions</li> <li>WaveNet: Autoregressive audio generation with dilated convolutions</li> <li>Transformer: Autoregressive text generation with attention mechanisms</li> <li>PixelCNN++: Enhanced PixelCNN with mixture of logistics</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#316-energy-based-models-ebms","title":"3.1.6 Energy-Based Models (EBMs)","text":"<ul> <li>Basic EBM: Energy-based models with Langevin dynamics</li> <li>Contrastive Divergence: Efficient training with persistent contrastive divergence</li> <li>MCMC Sampling: Integration with BlackJAX for advanced sampling</li> <li>Sample Buffers: Efficient sample management for training</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#317-geometric-models","title":"3.1.7 Geometric Models","text":"<ul> <li>Point Cloud Models: 3D point cloud generation with transformers</li> <li>Mesh Models: 3D mesh generation with template deformation</li> <li>Voxel Models: 3D voxel grid generation with 3D convolutions</li> <li>Protein Models: Specialized models for protein structure generation</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#32-implementation-quality","title":"3.2 Implementation Quality","text":"<p>Model implementations in Artifex aim to adhere to the following quality standards:</p> <ul> <li>Test Coverage: Test suite under active development; coverage expanding with refactoring</li> <li>Type Safety: Full type annotations with runtime validation via Protocol system</li> <li>Documentation: Docstrings and usage examples being developed</li> <li>Performance: Implementations optimized with JAX transformations</li> <li>Modularity: Clean separation of concerns with composable components</li> </ul> <p>Note: The codebase is undergoing major refactoring. Some tests are temporarily skipped while APIs stabilize.</p>"},{"location":"papers/artifex_arxiv_preprint/#4-multi-modal-architecture","title":"4. Multi-Modal Architecture","text":""},{"location":"papers/artifex_arxiv_preprint/#41-modality-framework","title":"4.1 Modality Framework","text":"<p>Artifex implements a modality framework that separates model architectures from domain-specific data types. This design enables researchers to experiment with different architectures while maintaining domain-specific optimizations and constraints.</p> <p>The modality framework supports eight distinct data types:</p>"},{"location":"papers/artifex_arxiv_preprint/#411-image-modality","title":"4.1.1 Image Modality","text":"<ul> <li>Representations: RGB, RGBA, grayscale, and multi-channel formats</li> <li>Preprocessing: Standardization, normalization, and augmentation pipelines</li> <li>Evaluation Metrics: FID, Inception Score, LPIPS, and perceptual metrics</li> <li>Constraints: Spatial consistency and visual quality preservation</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#412-text-modality","title":"4.1.2 Text Modality","text":"<ul> <li>Representations: Token-level, subword, and byte-level encodings</li> <li>Tokenization: Support for multiple tokenization strategies</li> <li>Evaluation Metrics: BLEU, ROUGE, perplexity, and diversity metrics</li> <li>Constraints: Semantic coherence and grammatical correctness</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#413-audio-modality","title":"4.1.3 Audio Modality","text":"<ul> <li>Representations: Waveform, spectrogram, and mel-spectrogram formats</li> <li>Preprocessing: Audio normalization, filtering, and feature extraction</li> <li>Evaluation Metrics: Spectral distance, perceptual metrics, and quality scores</li> <li>Constraints: Temporal consistency and audio quality preservation</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#414-tabular-modality","title":"4.1.4 Tabular Modality","text":"<ul> <li>Data Types: Numerical, categorical, ordinal, and binary features</li> <li>Preprocessing: Feature scaling, encoding, and normalization</li> <li>Evaluation Metrics: Statistical distance, privacy scores, and utility metrics</li> <li>Constraints: Data type preservation and privacy protection</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#415-time-series-modality","title":"4.1.5 Time Series Modality","text":"<ul> <li>Representations: Univariate and multivariate time series</li> <li>Preprocessing: Temporal alignment, missing value handling, and normalization</li> <li>Evaluation Metrics: DTW distance, autocorrelation, and spectral metrics</li> <li>Constraints: Temporal consistency and trend preservation</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#416-protein-modality","title":"4.1.6 Protein Modality","text":"<ul> <li>Representations: 3D coordinates, sequence, and structural features</li> <li>Preprocessing: Structural alignment, feature extraction, and normalization</li> <li>Evaluation Metrics: Structural similarity, energy scores, and biological metrics</li> <li>Constraints: Physical constraints, bond lengths, and angles</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#417-molecular-modality","title":"4.1.7 Molecular Modality","text":"<ul> <li>Representations: SMILES strings, molecular graphs, and 3D conformers</li> <li>Preprocessing: Molecule tokenization, graph construction, and conformer generation</li> <li>Evaluation Metrics: Validity scores, drug-likeness metrics, and molecular property prediction</li> <li>Constraints: Chemical validity, valence rules, and stereochemistry</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#418-multi-modal-modality","title":"4.1.8 Multi-Modal Modality","text":"<ul> <li>Representations: Combined embeddings from multiple modalities</li> <li>Preprocessing: Modality-specific encoders with shared latent spaces</li> <li>Evaluation Metrics: Cross-modal consistency and retrieval metrics</li> <li>Constraints: Alignment constraints across modalities</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#42-modality-adapters","title":"4.2 Modality Adapters","text":"<p>Each modality implements specialized adapters that customize generic models for specific data types:</p> <pre><code>class ModelAdapter(Protocol):\n    def adapt_model(self, model: GenerativeModel) -&gt; GenerativeModel:\n        \"\"\"Adapt a generic model for specific modality requirements.\"\"\"\n        ...\n\n    def preprocess(self, data: Any) -&gt; Any:\n        \"\"\"Preprocess data for the specific modality.\"\"\"\n        ...\n\n    def postprocess(self, outputs: Any) -&gt; Any:\n        \"\"\"Postprocess model outputs for the specific modality.\"\"\"\n        ...\n</code></pre>"},{"location":"papers/artifex_arxiv_preprint/#43-multi-modal-integration","title":"4.3 Multi-Modal Integration","text":"<p>Artifex supports multi-modal generation through specialized fusion strategies:</p> <ul> <li>Concatenation: Simple concatenation of modality representations</li> <li>Attention-based Fusion: Cross-modal attention mechanisms</li> <li>Gated Fusion: Learnable gating for modality combination</li> <li>Hierarchical Fusion: Multi-level fusion with different granularities</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#5-scaling-and-performance-infrastructure-experimental","title":"5. Scaling and Performance Infrastructure (Experimental)","text":"<p>Note: The scaling infrastructure is under active development. Distributed training features are experimental and excluded from CI testing.</p>"},{"location":"papers/artifex_arxiv_preprint/#51-hardware-aware-optimization","title":"5.1 Hardware-Aware Optimization","text":"<p>Artifex implements hardware detection and optimization capabilities:</p> <pre><code>class HardwareDetector:\n    def detect_hardware(self) -&gt; HardwareSpecs:\n        \"\"\"Detect available hardware and capabilities.\"\"\"\n        ...\n\n    def estimate_memory_usage(self, batch_size: int, model_size: int) -&gt; float:\n        \"\"\"Estimate memory usage for given configuration.\"\"\"\n        ...\n\n    def get_optimal_configuration(self, model: GenerativeModel) -&gt; OptimizationConfig:\n        \"\"\"Get optimal configuration for detected hardware.\"\"\"\n        ...\n</code></pre> <p>The hardware detection system provides:</p> <ul> <li>Automatic Platform Detection: GPU, TPU, and CPU platform identification</li> <li>Memory Estimation: Accurate memory usage prediction for different model sizes</li> <li>Performance Analysis: Roofline analysis and bottleneck identification</li> <li>Optimization Recommendations: Hardware-specific optimization strategies</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#52-multi-dimensional-parallelism","title":"5.2 Multi-Dimensional Parallelism","text":"<p>Artifex implements sophisticated parallelism strategies for large-scale training:</p>"},{"location":"papers/artifex_arxiv_preprint/#521-data-parallelism","title":"5.2.1 Data Parallelism","text":"<ul> <li>Standard Data Parallel: Replicated model across devices with gradient synchronization</li> <li>Fully Sharded Data Parallel (FSDP): Memory-efficient data parallelism with parameter sharding</li> <li>Gradient Accumulation: Support for large effective batch sizes with limited memory</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#522-tensor-parallelism","title":"5.2.2 Tensor Parallelism","text":"<ul> <li>Model Parallelism: Splitting model layers across devices</li> <li>Pipeline Parallelism: Sequential execution across device pipelines</li> <li>Expert Parallelism: Specialized parallelism for mixture-of-experts models</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#523-multi-dimensional-strategies","title":"5.2.3 Multi-Dimensional Strategies","text":"<pre><code>@dataclass\nclass ShardingConfig:\n    data_parallel_size: int = 1\n    tensor_parallel_size: int = 1\n    pipeline_parallel_size: int = 1\n    fsdp_enabled: bool = False\n    fsdp_min_weight_size: int = 1024\n</code></pre> <p>The multi-dimensional parallelism system enables:</p> <ul> <li>Automatic Configuration: Optimal parallelism configuration based on model size and hardware</li> <li>Dynamic Scaling: Runtime adjustment of parallelism strategies</li> <li>Memory Optimization: Efficient memory usage across different parallelism dimensions</li> <li>Performance Monitoring: Real-time performance tracking and optimization</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#53-device-mesh-management","title":"5.3 Device Mesh Management","text":"<p>Artifex provides sophisticated device mesh management for distributed training:</p> <pre><code>class DeviceMeshManager:\n    def create_mesh(self, mesh_shape: tuple[int, ...], axis_names: tuple[str, ...]) -&gt; Mesh:\n        \"\"\"Create optimized device mesh for given configuration.\"\"\"\n        ...\n\n    def optimize_for_transformer(self, device_count: int, model_size: str) -&gt; tuple[int, ...]:\n        \"\"\"Optimize mesh shape for transformer workloads.\"\"\"\n        ...\n\n    def validate_mesh_config(self, mesh_shape: tuple[int, ...], device_count: int) -&gt; bool:\n        \"\"\"Validate mesh configuration for given hardware.\"\"\"\n        ...\n</code></pre> <p>The device mesh management system provides:</p> <ul> <li>Topology Optimization: Optimal device mesh shapes for different workloads</li> <li>Hardware Validation: Configuration validation against available hardware</li> <li>Performance Prediction: Expected performance for different mesh configurations</li> <li>Automatic Optimization: Hardware-aware mesh shape calculation</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#54-production-optimization-pipeline-planned","title":"5.4 Production Optimization Pipeline (Planned)","text":"<p>Artifex includes a production optimization pipeline (under development):</p> <pre><code>class ProductionOptimizer:\n    def optimize_for_production(self, model: GenerativeModel,\n                              target: OptimizationTarget,\n                              sample_inputs: tuple) -&gt; OptimizationResult:\n        \"\"\"Optimize model for production deployment.\"\"\"\n        ...\n</code></pre> <p>The production optimization pipeline provides:</p> <ul> <li>JIT Compilation: Just-in-time compilation for optimal performance</li> <li>Quantization: Model quantization for reduced memory usage</li> <li>Pruning: Structured and unstructured pruning for efficiency</li> <li>Caching: Intelligent caching strategies for repeated computations</li> <li>Monitoring: Real-time performance monitoring and optimization</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#6-evaluation-and-benchmarking-framework-in-progress","title":"6. Evaluation and Benchmarking Framework (In Progress)","text":"<p>Note: The evaluation framework is being developed. Benchmark results should be considered preliminary and are being validated during the refactoring process.</p>"},{"location":"papers/artifex_arxiv_preprint/#61-metrics-system","title":"6.1 Metrics System","text":"<p>Artifex implements an evaluation framework with standardized metrics across modalities:</p>"},{"location":"papers/artifex_arxiv_preprint/#611-image-metrics","title":"6.1.1 Image Metrics","text":"<ul> <li>Fr\u00e9chet Inception Distance (FID): Quality and diversity assessment</li> <li>Inception Score: Quality assessment using pre-trained classifiers</li> <li>LPIPS: Perceptual similarity using learned features</li> <li>SSIM: Structural similarity index</li> <li>PSNR: Peak signal-to-noise ratio</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#612-text-metrics","title":"6.1.2 Text Metrics","text":"<ul> <li>BLEU: Bilingual evaluation understudy for translation quality</li> <li>ROUGE: Recall-oriented understudy for gisting evaluation</li> <li>Perplexity: Language model perplexity for coherence assessment</li> <li>Diversity Metrics: Lexical and semantic diversity measures</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#613-audio-metrics","title":"6.1.3 Audio Metrics","text":"<ul> <li>Spectral Distance: Frequency domain similarity</li> <li>Perceptual Metrics: Psychoacoustic quality assessment</li> <li>Signal-to-Noise Ratio: Audio quality measurement</li> <li>Temporal Consistency: Time-domain coherence assessment</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#614-tabular-metrics","title":"6.1.4 Tabular Metrics","text":"<ul> <li>Statistical Distance: Distribution similarity measures</li> <li>Privacy Scores: Differential privacy and membership inference</li> <li>Utility Metrics: Downstream task performance</li> <li>Data Quality: Completeness and consistency assessment</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#615-time-series-metrics","title":"6.1.5 Time Series Metrics","text":"<ul> <li>Dynamic Time Warping (DTW): Temporal alignment similarity</li> <li>Autocorrelation: Temporal dependency preservation</li> <li>Spectral Metrics: Frequency domain characteristics</li> <li>Trend Preservation: Long-term pattern consistency</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#616-protein-metrics","title":"6.1.6 Protein Metrics","text":"<ul> <li>Structural Similarity: 3D structure comparison</li> <li>Energy Scores: Physical plausibility assessment</li> <li>Biological Metrics: Functional relevance measures</li> <li>Constraint Satisfaction: Physical constraint compliance</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#62-benchmarking-infrastructure","title":"6.2 Benchmarking Infrastructure","text":"<p>Artifex provides a comprehensive benchmarking system:</p> <pre><code>class BenchmarkSuite:\n    def run_benchmark(self, model: GenerativeModel,\n                     dataset: Dataset,\n                     metrics: list[Metric]) -&gt; BenchmarkResult:\n        \"\"\"Run comprehensive benchmark evaluation.\"\"\"\n        ...\n\n    def compare_models(self, models: list[GenerativeModel]) -&gt; ComparisonResult:\n        \"\"\"Compare multiple models across standardized benchmarks.\"\"\"\n        ...\n</code></pre> <p>The benchmarking infrastructure provides:</p> <ul> <li>Standardized Datasets: Pre-processed datasets for fair comparison</li> <li>Automated Evaluation: Automated metric computation and reporting</li> <li>Performance Tracking: Historical performance tracking and regression detection</li> <li>Result Storage: Persistent storage and analysis of benchmark results</li> <li>CI/CD Integration: Integration with continuous integration pipelines</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#63-evaluation-pipeline","title":"6.3 Evaluation Pipeline","text":"<p>Artifex implements a flexible evaluation pipeline that automatically selects appropriate metrics:</p> <pre><code>class EvaluationPipeline:\n    def evaluate(self, model: GenerativeModel,\n                data: ModalityData,\n                modality: Modality) -&gt; EvaluationResult:\n        \"\"\"Evaluate model with modality-specific metrics.\"\"\"\n        ...\n</code></pre> <p>The evaluation pipeline provides:</p> <ul> <li>Automatic Metric Selection: Appropriate metrics based on modality and model type</li> <li>Batch Processing: Efficient evaluation of large datasets</li> <li>Parallel Evaluation: Parallel metric computation for faster evaluation</li> <li>Result Aggregation: Statistical aggregation of evaluation results</li> <li>Visualization: Automatic generation of evaluation visualizations</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#7-extension-and-customization-framework","title":"7. Extension and Customization Framework","text":""},{"location":"papers/artifex_arxiv_preprint/#71-extension-mechanism","title":"7.1 Extension Mechanism","text":"<p>Artifex provides a flexible extension mechanism for adding domain-specific functionality:</p> <pre><code>class ModelExtension(Protocol):\n    def apply(self, model: GenerativeModel,\n              inputs: Any,\n              outputs: Any) -&gt; dict[str, Any]:\n        \"\"\"Apply extension to model outputs.\"\"\"\n        ...\n\n    def compute_loss(self, batch: dict[str, Any],\n                   outputs: dict[str, Any]) -&gt; dict[str, jax.Array]:\n        \"\"\"Compute extension-specific loss terms.\"\"\"\n        ...\n</code></pre> <p>The extension system enables:</p> <ul> <li>Domain-Specific Constraints: Physical, biological, or domain-specific constraints</li> <li>Custom Loss Functions: Specialized loss functions for specific applications</li> <li>Model Modifications: Non-invasive model modifications and enhancements</li> <li>Evaluation Extensions: Custom evaluation metrics and procedures</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#72-protein-extensions","title":"7.2 Protein Extensions","text":"<p>Artifex includes specialized extensions for protein modeling:</p> <pre><code>class BondLengthExtension(ModelExtension):\n    \"\"\"Extension for enforcing protein bond length constraints.\"\"\"\n\n    def compute_loss(self, batch: dict[str, Any],\n                   outputs: dict[str, Any]) -&gt; dict[str, jax.Array]:\n        \"\"\"Compute bond length constraint loss.\"\"\"\n        ...\n\nclass BondAngleExtension(ModelExtension):\n    \"\"\"Extension for enforcing protein bond angle constraints.\"\"\"\n\n    def compute_loss(self, batch: dict[str, Any],\n                   outputs: dict[str, Any]) -&gt; dict[str, jax.Array]:\n        \"\"\"Compute bond angle constraint loss.\"\"\"\n        ...\n</code></pre>"},{"location":"papers/artifex_arxiv_preprint/#73-custom-extension-development","title":"7.3 Custom Extension Development","text":"<p>The extension framework provides comprehensive support for custom extension development:</p> <ul> <li>Base Classes: Abstract base classes for different extension types</li> <li>Type Safety: Full type checking and validation for extensions</li> <li>Documentation: Comprehensive documentation and examples</li> <li>Testing: Testing utilities for extension validation</li> <li>Integration: Seamless integration with existing models and training pipelines</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#8-implementation-and-quality-assurance","title":"8. Implementation and Quality Assurance","text":"<p>Note: Quality assurance practices are being established during the refactoring phase. Test coverage and documentation are expanding as APIs stabilize.</p>"},{"location":"papers/artifex_arxiv_preprint/#81-code-quality-standards","title":"8.1 Code Quality Standards","text":"<p>Artifex aims for the following code quality standards:</p> <ul> <li>Test-Driven Development: Test suite under active development; some tests temporarily skipped during refactoring</li> <li>Type Safety: Full type annotations with runtime validation via Protocol system</li> <li>Documentation: Docstrings and API documentation being developed</li> <li>Code Style: Consistent code formatting enforced via Ruff and pre-commit hooks</li> <li>Performance: Performance benchmarks being validated</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#82-testing-infrastructure","title":"8.2 Testing Infrastructure","text":"<p>The testing infrastructure includes:</p> <ul> <li>Unit Tests: Unit tests for components; coverage expanding</li> <li>Integration Tests: End-to-end integration testing being developed</li> <li>Performance Tests: Performance regression testing (experimental)</li> <li>GPU Tests: GPU-specific tests with automatic CPU fallback</li> <li>Modality Tests: Modality-specific functionality testing</li> </ul> <p>Current status: Some tests are marked as <code>skip</code> or <code>xfail</code> while APIs are being stabilized. Distributed training tests are excluded from CI.</p>"},{"location":"papers/artifex_arxiv_preprint/#83-continuous-integration","title":"8.3 Continuous Integration","text":"<p>Artifex employs continuous integration:</p> <ul> <li>Automated Testing: Test execution on Ubuntu (CUDA) and macOS platforms</li> <li>Code Quality: Automated linting (Ruff) and type checking (Pyright)</li> <li>Security Scanning: Dependency vulnerability scanning via pip-audit</li> <li>Platform Support: Linux (CUDA), macOS (CPU/Metal) build verification</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#9-experimental-results-and-performance-analysis-preliminary","title":"9. Experimental Results and Performance Analysis (Preliminary)","text":"<p>Disclaimer: The results in this section are preliminary and collected during development. All benchmark numbers require validation as the codebase stabilizes. Production performance claims have not been independently verified.</p>"},{"location":"papers/artifex_arxiv_preprint/#91-model-performance-benchmarks-to-be-validated","title":"9.1 Model Performance Benchmarks (To Be Validated)","text":"<p>The following results are from development testing and should be considered preliminary:</p>"},{"location":"papers/artifex_arxiv_preprint/#911-image-generation-performance","title":"9.1.1 Image Generation Performance","text":"<ul> <li>DDPM on CIFAR-10: FID score ~3-5 range (validation in progress)</li> <li>VAE on MNIST: Basic functionality verified; quantitative metrics pending</li> <li>DCGAN on CelebA: Training verified; FID/IS metrics being validated</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#912-text-generation-performance","title":"9.1.2 Text Generation Performance","text":"<ul> <li>Transformer models: Basic training loop functional</li> <li>Performance benchmarks pending validation</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#913-audio-generation-performance","title":"9.1.3 Audio Generation Performance","text":"<ul> <li>WaveNet: Architecture implemented; benchmarks pending</li> <li>Audio quality metrics to be validated</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#914-protein-structure-generation","title":"9.1.4 Protein Structure Generation","text":"<ul> <li>Point Cloud Models: Physical constraint enforcement implemented</li> <li>Quantitative structure metrics pending validation</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#92-scaling-performance-experimental","title":"9.2 Scaling Performance (Experimental)","text":"<p>Scaling capabilities are under development:</p> <ul> <li>Single GPU: Basic training verified</li> <li>Multi-GPU: Data parallel training implemented; scaling tests pending</li> <li>Distributed Training: Experimental; excluded from CI testing</li> <li>Memory Efficiency: Optimizations in progress</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#93-production-readiness","title":"9.3 Production Readiness","text":"<p>Production deployment capabilities are not yet validated. The framework is in active development and not recommended for production use.</p>"},{"location":"papers/artifex_arxiv_preprint/#10-intended-use-cases-and-applications","title":"10. Intended Use Cases and Applications","text":""},{"location":"papers/artifex_arxiv_preprint/#101-research-applications","title":"10.1 Research Applications","text":"<p>Artifex is designed for research applications including:</p> <ul> <li>Novel Architecture Development: Rapid prototyping of new generative model architectures</li> <li>Multi-Modal Research: Cross-modal generation and representation learning</li> <li>Domain-Specific Applications: Protein design, drug discovery, and materials science</li> <li>Evaluation Studies: Evaluation of generative model performance</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#102-intended-production-applications","title":"10.2 Intended Production Applications","text":"<p>Once stabilized, the framework aims to support:</p> <ul> <li>Data Augmentation: Synthetic data generation for machine learning training</li> <li>Scientific Computing: Molecular and protein structure generation</li> <li>Content Generation: Automated content generation applications</li> </ul> <p>Note: Production deployment is not currently recommended while the framework is under active development.</p>"},{"location":"papers/artifex_arxiv_preprint/#103-educational-applications","title":"10.3 Educational Applications","text":"<p>Artifex can serve as an educational resource:</p> <ul> <li>Graduate Courses: Teaching generative modeling concepts with JAX/Flax</li> <li>Workshops and Tutorials: Hands-on exploration of generative models</li> <li>Research Training: Platform for learning modern generative modeling techniques</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#11-future-directions-and-development-roadmap","title":"11. Future Directions and Development Roadmap","text":""},{"location":"papers/artifex_arxiv_preprint/#111-current-priorities-refactoring-phase","title":"11.1 Current Priorities (Refactoring Phase)","text":"<p>The immediate development focus includes:</p> <ul> <li>API Stabilization: Establishing consistent interfaces across all model types</li> <li>Test Coverage: Expanding test suite and removing skipped tests</li> <li>Documentation: Completing API documentation and usage examples</li> <li>CI/CD: Strengthening continuous integration with comprehensive validation</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#112-planned-enhancements","title":"11.2 Planned Enhancements","text":"<p>After stabilization, the roadmap includes:</p> <ul> <li>DDIM Implementation: Completing faster sampling for diffusion models</li> <li>Stable Diffusion: Text-to-image generation capabilities</li> <li>Enhanced Scaling: Validated distributed training support</li> <li>macOS Metal Support: Full GPU acceleration on Apple Silicon</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#113-research-directions","title":"11.3 Research Directions","text":"<p>The framework aims to enable research in:</p> <ul> <li>Multi-Modal Fusion: Advanced techniques for cross-modal generation</li> <li>Efficient Sampling: Improved sampling techniques for faster generation</li> <li>Uncertainty Quantification: Better uncertainty estimation in generative models</li> <li>Interpretability: Enhanced interpretability and explainability features</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#114-community-contributions","title":"11.4 Community Contributions","text":"<p>Artifex welcomes community contributions:</p> <ul> <li>Extension Development: Domain-specific extensions and modalities</li> <li>Model Implementations: New model variants and architectures</li> <li>Bug Fixes: Issue reports and fixes welcome during refactoring phase</li> <li>Documentation: Improvements to documentation and examples</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#12-conclusion","title":"12. Conclusion","text":"<p>Artifex aims to provide a unified framework for generative modeling research built on JAX/Flax NNX. The framework implements seven major generative modeling paradigms with support for eight data modalities, offering researchers a platform for experimentation and development.</p> <p>Current status of the Artifex framework:</p> <ol> <li>Unified Architecture: Type-safe interfaces using Python Protocols across model types</li> <li>Multi-Modal Support: Eight modalities including image, text, audio, tabular, time series, protein, molecular, and multi-modal</li> <li>JAX/Flax Foundation: Built on modern Flax NNX patterns with JAX transformations</li> <li>Extensibility: Extension mechanism for domain-specific constraints (e.g., protein modeling)</li> <li>Active Development: Major refactoring in progress with APIs subject to change</li> </ol> <p>The framework is currently in active development and not yet recommended for production use. APIs are being stabilized, test coverage is expanding, and performance benchmarks are being validated.</p> <p>Future work focuses on completing the refactoring phase, expanding test coverage, validating performance claims, and building a community of contributors. The open-source nature of Artifex invites collaboration from the machine learning community.</p>"},{"location":"papers/artifex_arxiv_preprint/#acknowledgments","title":"Acknowledgments","text":"<p>The authors thank the JAX and Flax development teams for their excellent frameworks. Special thanks to the broader machine learning community for inspiring the design patterns used in Artifex.</p>"},{"location":"papers/artifex_arxiv_preprint/#references","title":"References","text":"<p>[1] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., ... &amp; Wanderman-Milne, S. (2018). JAX: composable transformations of Python+NumPy programs. arXiv preprint arXiv:1806.01572.</p> <p>[2] Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A., &amp; van Zee, M. (2023). Flax: A neural network library and ecosystem for JAX. GitHub repository.</p> <p>[3] Kingma, D. P., &amp; Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.</p> <p>[4] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... &amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27.</p> <p>[5] Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33, 6840-6851.</p> <p>[6] Rezende, D., &amp; Mohamed, S. (2015). Variational inference with normalizing flows. International conference on machine learning (pp. 1530-1538).</p> <p>[7] Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... &amp; Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.</p> <p>[8] LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., &amp; Huang, F. (2006). A tutorial on energy-based learning. Predicting structured data, 1(0).</p> <p>[9] Qi, C. R., Su, H., Mo, K., &amp; Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 652-660).</p> <p>[10] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</p>"},{"location":"papers/artifex_arxiv_preprint/#appendix-a-installation-and-quick-start","title":"Appendix A: Installation and Quick Start","text":""},{"location":"papers/artifex_arxiv_preprint/#a1-installation","title":"A.1 Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/avitai/artifex.git\ncd artifex\n\n# Run setup script (recommended)\n# For Linux with CUDA:\n./setup.sh\n\n# For macOS:\n./setup.sh --cpu\n# Or with Metal acceleration (Apple Silicon):\n./setup.sh --metal\n\n# Activate the environment\nsource activate.sh\n\n# Alternative: Manual installation with uv\nuv sync --extra cuda-dev  # Linux with CUDA\nuv sync --extra all-cpu   # CPU only\nuv sync --extra all-macos # macOS with Metal\n</code></pre>"},{"location":"papers/artifex_arxiv_preprint/#a2-quick-start-example","title":"A.2 Quick Start Example","text":"<p>Note: APIs are subject to change during development. Check the latest documentation for current usage.</p> <pre><code>import jax\nfrom flax import nnx\nfrom artifex.generative_models.models.vae import VAE\nfrom artifex.generative_models.core.configuration.vae_config import (\n    VAEConfig, EncoderConfig, DecoderConfig\n)\n\n# Create random number generator\nkey = jax.random.key(0)\nrngs = nnx.Rngs(params=key)\n\n# Create VAE configuration\nencoder_config = EncoderConfig(\n    hidden_dims=[32, 64],\n    latent_dim=16,\n)\ndecoder_config = DecoderConfig(\n    hidden_dims=[64, 32],\n    output_dim=(28, 28, 1),\n)\nconfig = VAEConfig(\n    encoder=encoder_config,\n    decoder=decoder_config,\n    latent_dim=16,\n)\n\n# Initialize model (rngs passed only during initialization)\nmodel = VAE(config, rngs=rngs)\n\n# Forward pass (no rngs argument)\nbatch = jax.random.normal(key, (4, 28, 28, 1))\noutputs = model(batch)\n\n# Generate samples (no rngs argument)\nsamples = model.sample(n_samples=4)\nprint(f\"Generated samples shape: {samples.shape}\")\n</code></pre>"},{"location":"papers/artifex_arxiv_preprint/#appendix-b-api-reference","title":"Appendix B: API Reference","text":""},{"location":"papers/artifex_arxiv_preprint/#b1-core-classes","title":"B.1 Core Classes","text":""},{"location":"papers/artifex_arxiv_preprint/#generativemodelprotocol","title":"GenerativeModelProtocol","text":"<p>The base protocol for all generative models in Artifex.</p>"},{"location":"papers/artifex_arxiv_preprint/#modelconfiguration","title":"ModelConfiguration","text":"<p>Configuration class for model instantiation and parameter management.</p>"},{"location":"papers/artifex_arxiv_preprint/#modelregistry","title":"ModelRegistry","text":"<p>Registry system for model discovery and creation.</p>"},{"location":"papers/artifex_arxiv_preprint/#b2-modality-classes","title":"B.2 Modality Classes","text":""},{"location":"papers/artifex_arxiv_preprint/#modality","title":"Modality","text":"<p>Base protocol for data modality implementations.</p>"},{"location":"papers/artifex_arxiv_preprint/#modeladapter","title":"ModelAdapter","text":"<p>Protocol for adapting generic models to specific modalities.</p>"},{"location":"papers/artifex_arxiv_preprint/#b3-evaluation-classes","title":"B.3 Evaluation Classes","text":""},{"location":"papers/artifex_arxiv_preprint/#evaluationpipeline","title":"EvaluationPipeline","text":"<p>Main evaluation pipeline for model assessment.</p>"},{"location":"papers/artifex_arxiv_preprint/#metricsregistry","title":"MetricsRegistry","text":"<p>Registry for evaluation metrics.</p>"},{"location":"papers/artifex_arxiv_preprint/#benchmarksuite","title":"BenchmarkSuite","text":"<p>Comprehensive benchmarking system.</p>"},{"location":"papers/artifex_arxiv_preprint/#appendix-c-hardware-requirements","title":"Appendix C: Hardware Requirements","text":""},{"location":"papers/artifex_arxiv_preprint/#c1-supported-platforms","title":"C.1 Supported Platforms","text":"<ul> <li>Linux: Ubuntu 20.04+, CUDA 11.8+ for GPU acceleration</li> <li>macOS: macOS 12+ (Intel or Apple Silicon), optional Metal acceleration</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#c2-hardware-requirements","title":"C.2 Hardware Requirements","text":"<ul> <li>Development: 16GB RAM, CUDA-compatible GPU recommended</li> <li>Training: 32GB+ RAM, RTX 3080 or better for reasonable training times</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#c3-benchmark-results","title":"C.3 Benchmark Results","text":"<p>Benchmark validation is in progress. Results will be published as they are validated.</p>"},{"location":"papers/artifex_arxiv_preprint/#appendix-d-contributing-guidelines","title":"Appendix D: Contributing Guidelines","text":""},{"location":"papers/artifex_arxiv_preprint/#d1-development-setup","title":"D.1 Development Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/avitai/artifex.git\ncd artifex\n\n# Run setup script\n./setup.sh\n\n# Activate environment\nsource activate.sh\n\n# Install pre-commit hooks\nuv run pre-commit install\n\n# Run tests\nuv run pytest tests/ -v\n</code></pre>"},{"location":"papers/artifex_arxiv_preprint/#d2-code-style","title":"D.2 Code Style","text":"<p>Artifex follows these code style guidelines:</p> <ul> <li>Ruff formatting (replaces Black)</li> <li>Type annotations required (Pyright for type checking)</li> <li>Docstrings for public APIs</li> <li>Test coverage expanding with refactoring</li> </ul>"},{"location":"papers/artifex_arxiv_preprint/#d3-pull-request-process","title":"D.3 Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make changes with tests</li> <li>Ensure all tests pass</li> <li>Submit pull request</li> </ol> <p>This preprint describes Artifex, a generative modeling framework under active development. The source code is available at https://github.com/avitai/artifex. Note that APIs are subject to change and the framework is not yet recommended for production use.</p>"},{"location":"scaling/","title":"Scaling &amp; Distributed Training","text":"<p>Comprehensive tools for scaling generative model training across multiple devices and accelerators.</p>"},{"location":"scaling/#overview","title":"Overview","text":"<p>Artifex provides robust infrastructure for scaling model training from single-GPU experiments to multi-node distributed setups. The scaling module offers:</p> <ul> <li> <p> Device Mesh Management</p> <p>Create and optimize device meshes for different workloads</p> <p> Mesh Utilities</p> </li> <li> <p> Sharding Strategies</p> <p>Data, tensor, FSDP, and pipeline parallelism</p> <p> Sharding Strategies</p> </li> <li> <p> Multi-Dimensional Parallelism</p> <p>Combine strategies for optimal performance</p> <p> Multi-Dimensional</p> </li> <li> <p> Configuration</p> <p>Flexible configuration for complex setups</p> <p> Configuration</p> </li> </ul>"},{"location":"scaling/#quick-start","title":"Quick Start","text":""},{"location":"scaling/#basic-data-parallel-training","title":"Basic Data Parallel Training","text":"<pre><code>import jax\nfrom artifex.generative_models.scaling import mesh_utils, sharding\n\n# Get available devices\ndevices = jax.devices()\nprint(f\"Available devices: {len(devices)}\")\n\n# Create sharding config for data parallelism\nconfig = sharding.ShardingConfig.from_device_count(len(devices))\n\n# Create parallelism config with mesh topology\nparallel_config = sharding.ParallelismConfig.from_sharding_config(config)\n\n# Create device mesh\nmesh_manager = mesh_utils.DeviceMeshManager(\n    mesh_shape=parallel_config.mesh_shape,\n    axis_names=parallel_config.mesh_axis_names,\n)\nmesh = mesh_manager.create_mesh_from_config(parallel_config)\n\nprint(f\"Mesh shape: {parallel_config.mesh_shape}\")\nprint(f\"Axis names: {parallel_config.mesh_axis_names}\")\n</code></pre>"},{"location":"scaling/#tensor-parallel-setup","title":"Tensor Parallel Setup","text":"<pre><code>from artifex.generative_models.scaling.sharding import (\n    ShardingConfig,\n    ParallelismConfig,\n    TensorParallelStrategy,\n)\n\n# Configure tensor parallelism for large models\nconfig = ShardingConfig(\n    data_parallel_size=2,\n    tensor_parallel_size=4,  # 8 GPUs total\n)\n\n# Create tensor parallel strategy\ntensor_strategy = TensorParallelStrategy(\n    axis_name=\"model\",\n    mesh_axis=1,\n    shard_dimension=\"out_features\",\n)\n\n# Get partition specs for attention layers\nqkv_spec = tensor_strategy.get_attention_qkv_spec()\noutput_spec = tensor_strategy.get_attention_output_spec()\n</code></pre>"},{"location":"scaling/#device-mesh-management","title":"Device Mesh Management","text":"<p>The <code>DeviceMeshManager</code> provides utilities for creating and optimizing device meshes.</p>"},{"location":"scaling/#creating-a-device-mesh","title":"Creating a Device Mesh","text":"<pre><code>from artifex.generative_models.scaling.mesh_utils import (\n    DeviceMeshManager,\n    create_device_mesh,\n)\n\n# Simple mesh creation\nmesh = create_device_mesh(\n    mesh_shape=(4, 2),        # 4 data parallel x 2 tensor parallel\n    axis_names=(\"data\", \"model\"),\n)\n\n# Using DeviceMeshManager for more control\nmanager = DeviceMeshManager(\n    mesh_shape=(4, 2),\n    axis_names=(\"data\", \"model\"),\n)\n\n# Get optimal mesh shape for device count\noptimal_shape = manager.get_optimal_mesh_shape(\n    device_count=8,\n    dimensions=2,\n)\nprint(f\"Optimal shape for 8 devices: {optimal_shape}\")\n</code></pre>"},{"location":"scaling/#optimizing-for-transformers","title":"Optimizing for Transformers","text":"<pre><code># Optimize mesh for transformer workloads\noptimal_shape = manager.optimize_for_transformer(\n    device_count=8,\n    model_size=\"7B\",\n    sequence_length=2048,\n)\nprint(f\"Optimal shape for 7B model: {optimal_shape}\")\n\n# For larger models, more tensor parallelism\nlarge_shape = manager.optimize_for_transformer(\n    device_count=32,\n    model_size=\"70B\",\n    sequence_length=4096,\n)\nprint(f\"Optimal shape for 70B model: {large_shape}\")\n</code></pre>"},{"location":"scaling/#validation","title":"Validation","text":"<pre><code># Validate mesh configuration before use\nis_valid = manager.validate_mesh_config(\n    mesh_shape=(4, 2),\n    device_count=8,\n)\nprint(f\"Configuration valid: {is_valid}\")\n</code></pre>"},{"location":"scaling/#sharding-strategies","title":"Sharding Strategies","text":"<p>Artifex provides multiple sharding strategies for different parallelism approaches.</p>"},{"location":"scaling/#data-parallel-strategy","title":"Data Parallel Strategy","text":"<p>Shards data across devices while replicating model parameters.</p> <pre><code>from artifex.generative_models.scaling.sharding import DataParallelStrategy\n\nstrategy = DataParallelStrategy(axis_name=\"data\", mesh_axis=0)\n\n# Get partition spec for a batch of data\n# Shape: (batch, sequence, hidden)\nspec = strategy.get_partition_spec((\"batch\", \"sequence\", \"hidden\"))\n# Result: PartitionSpec(\"data\", None, None)\n\n# Apply sharding to data\nsharded_data = strategy.apply_sharding(batch_data, mesh)\n</code></pre>"},{"location":"scaling/#fsdp-strategy","title":"FSDP Strategy","text":"<p>Fully Sharded Data Parallel for memory-efficient training.</p> <pre><code>from artifex.generative_models.scaling.sharding import FSDPStrategy\n\nstrategy = FSDPStrategy(\n    axis_name=\"fsdp\",\n    mesh_axis=0,\n    min_weight_size=1024,  # Only shard weights &gt;= 1024 in first dim\n)\n\n# Check if a weight should be sharded\nshould_shard = strategy.should_shard_weight(large_weight_matrix)\n\n# Apply FSDP sharding\nsharded_weights = strategy.apply_sharding(weights, mesh)\n</code></pre>"},{"location":"scaling/#tensor-parallel-strategy","title":"Tensor Parallel Strategy","text":"<p>Shards model computation across devices.</p> <pre><code>from artifex.generative_models.scaling.sharding import TensorParallelStrategy\n\nstrategy = TensorParallelStrategy(\n    axis_name=\"model\",\n    mesh_axis=1,\n    shard_dimension=\"out_features\",\n)\n\n# Get specs for attention layers\nqkv_spec = strategy.get_attention_qkv_spec()      # Shard output\noutput_spec = strategy.get_attention_output_spec() # Shard input\n\n# Get specs for linear layers\nlinear_spec = strategy.get_linear_weight_spec()\n</code></pre>"},{"location":"scaling/#pipeline-parallel-strategy","title":"Pipeline Parallel Strategy","text":"<p>Distributes model layers across devices.</p> <pre><code>from artifex.generative_models.scaling.sharding import PipelineParallelStrategy\n\nstrategy = PipelineParallelStrategy(\n    axis_name=\"pipeline\",\n    mesh_axis=2,\n    num_stages=4,\n)\n\n# Assign 24 transformer layers to 4 pipeline stages\nlayer_assignments = strategy.assign_layers_to_stages(num_layers=24)\n# Result: [6, 6, 6, 6] - 6 layers per stage\n\n# Get communication patterns\nforward_pattern = strategy.get_forward_communication_pattern()\nbackward_pattern = strategy.get_backward_communication_pattern()\n</code></pre>"},{"location":"scaling/#multi-dimensional-parallelism","title":"Multi-Dimensional Parallelism","text":"<p>Combine multiple strategies for optimal large-scale training.</p> <pre><code>from artifex.generative_models.scaling.sharding import (\n    MultiDimensionalStrategy,\n    DataParallelStrategy,\n    TensorParallelStrategy,\n    FSDPStrategy,\n    ParallelismConfig,\n    ShardingConfig,\n)\n\n# Create individual strategies\ndata_strategy = DataParallelStrategy(axis_name=\"data\", mesh_axis=0)\ntensor_strategy = TensorParallelStrategy(\n    axis_name=\"model\",\n    mesh_axis=1,\n    shard_dimension=\"out_features\",\n)\nfsdp_strategy = FSDPStrategy(axis_name=\"data\", mesh_axis=0)\n\n# Combine into multi-dimensional strategy\nconfig = ShardingConfig(\n    data_parallel_size=4,\n    tensor_parallel_size=2,\n    fsdp_enabled=True,\n)\nparallel_config = ParallelismConfig.from_sharding_config(config)\n\nmulti_strategy = MultiDimensionalStrategy(\n    strategies={\n        \"data\": data_strategy,\n        \"tensor\": tensor_strategy,\n        \"fsdp\": fsdp_strategy,\n    },\n    config=parallel_config,\n)\n\n# Get combined partition spec for a tensor\ncombined_spec = multi_strategy.get_combined_partition_spec(\n    tensor_name=\"attention.query\",\n    tensor_shape=(\"batch\", \"sequence\", \"hidden\"),\n)\n</code></pre>"},{"location":"scaling/#configuration","title":"Configuration","text":""},{"location":"scaling/#shardingconfig","title":"ShardingConfig","text":"<p>Defines parallelism dimensions.</p> <pre><code>from artifex.generative_models.scaling.sharding import ShardingConfig\n\n# Manual configuration\nconfig = ShardingConfig(\n    data_parallel_size=4,\n    tensor_parallel_size=2,\n    pipeline_parallel_size=1,\n    fsdp_enabled=True,\n    fsdp_min_weight_size=1024,\n)\n\n# Auto-configure from device count\nauto_config = ShardingConfig.from_device_count(device_count=8)\n\n# Get total device requirement\ntotal_devices = config.get_total_device_count()  # 4 * 2 * 1 = 8\n</code></pre>"},{"location":"scaling/#parallelismconfig","title":"ParallelismConfig","text":"<p>Complete parallelism configuration with mesh topology.</p> <pre><code>from artifex.generative_models.scaling.sharding import ParallelismConfig\n\n# Create from sharding config\nparallel_config = ParallelismConfig.from_sharding_config(config)\n\n# Access mesh configuration\nprint(f\"Mesh shape: {parallel_config.mesh_shape}\")\nprint(f\"Axis names: {parallel_config.mesh_axis_names}\")\n\n# Validate configuration\nis_valid = parallel_config.is_valid()\n</code></pre>"},{"location":"scaling/#best-practices","title":"Best Practices","text":""},{"location":"scaling/#choosing-parallelism-strategy","title":"Choosing Parallelism Strategy","text":"Model Size Devices Recommended Strategy &lt; 1B params 1-8 Data Parallel 1B - 10B 8-32 Data + Tensor Parallel 10B - 100B 32-128 Data + Tensor + FSDP &gt; 100B 128+ All strategies + Pipeline"},{"location":"scaling/#memory-optimization","title":"Memory Optimization","text":"<pre><code># For memory-constrained setups, enable FSDP\nconfig = ShardingConfig(\n    data_parallel_size=4,\n    fsdp_enabled=True,\n    fsdp_min_weight_size=512,  # Shard smaller weights\n)\n\n# For very large models, combine with tensor parallel\nconfig = ShardingConfig(\n    data_parallel_size=2,\n    tensor_parallel_size=4,\n    fsdp_enabled=True,\n)\n</code></pre>"},{"location":"scaling/#performance-tips","title":"Performance Tips","text":"<ol> <li>Balance dimensions: Avoid extreme ratios in mesh shape</li> <li>Match workload: Use transformer-optimized shapes for transformers</li> <li>Validate configs: Always validate before creating meshes</li> <li>Monitor memory: Enable FSDP for memory-constrained scenarios</li> </ol>"},{"location":"scaling/#api-reference","title":"API Reference","text":""},{"location":"scaling/#mesh-utilities","title":"Mesh Utilities","text":""},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils","title":"artifex.generative_models.scaling.mesh_utils","text":"<p>Device mesh management utilities for scalable distributed training.</p> <p>This module provides comprehensive device mesh management including: - Device mesh creation and optimization - Topology optimization for different workloads - Validation and configuration utilities - Hardware-aware mesh shape calculation</p> <p>All implementations prioritize performance and follow JAX/Flax NNX patterns.</p>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.DeviceMeshManager","title":"DeviceMeshManager","text":"<pre><code>DeviceMeshManager(\n    mesh_shape: tuple[int, ...], axis_names: tuple[str, ...]\n)\n</code></pre> <p>Device mesh management for distributed training optimization.</p> <p>Provides utilities for creating and optimizing device meshes based on available hardware and workload characteristics.</p>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.DeviceMeshManager.mesh_shape","title":"mesh_shape  <code>instance-attribute</code>","text":"<pre><code>mesh_shape = mesh_shape\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.DeviceMeshManager.axis_names","title":"axis_names  <code>instance-attribute</code>","text":"<pre><code>axis_names = axis_names\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.DeviceMeshManager.create_mesh","title":"create_mesh","text":"<pre><code>create_mesh(\n    mesh_shape: tuple[int, ...], axis_names: tuple[str, ...]\n) -&gt; Mesh\n</code></pre> <p>Create device mesh with specified shape and axis names.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_shape</code> <code>tuple[int, ...]</code> <p>Shape of the device mesh</p> required <code>axis_names</code> <code>tuple[str, ...]</code> <p>Names for each mesh axis</p> required <p>Returns:</p> Type Description <code>Mesh</code> <p>JAX device mesh</p>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.DeviceMeshManager.create_mesh_from_config","title":"create_mesh_from_config","text":"<pre><code>create_mesh_from_config(config: ParallelismConfig) -&gt; Mesh\n</code></pre> <p>Create mesh from parallelism configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ParallelismConfig</code> <p>Parallelism configuration</p> required <p>Returns:</p> Type Description <code>Mesh</code> <p>JAX device mesh</p>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.DeviceMeshManager.get_optimal_mesh_shape","title":"get_optimal_mesh_shape","text":"<pre><code>get_optimal_mesh_shape(\n    device_count: int, dimensions: int = 2\n) -&gt; tuple[int, ...]\n</code></pre> <p>Calculate optimal mesh shape for given device count.</p> <p>Parameters:</p> Name Type Description Default <code>device_count</code> <code>int</code> <p>Number of available devices</p> required <code>dimensions</code> <code>int</code> <p>Number of mesh dimensions</p> <code>2</code> <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Optimal mesh shape tuple</p>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.DeviceMeshManager.optimize_for_transformer","title":"optimize_for_transformer","text":"<pre><code>optimize_for_transformer(\n    device_count: int, model_size: str, sequence_length: int\n) -&gt; tuple[int, ...]\n</code></pre> <p>Optimize mesh shape for transformer workloads.</p> <p>Parameters:</p> Name Type Description Default <code>device_count</code> <code>int</code> <p>Number of available devices</p> required <code>model_size</code> <code>str</code> <p>Model size (e.g., '7B', '13B', '70B')</p> required <code>sequence_length</code> <code>int</code> <p>Input sequence length</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Optimized mesh shape for transformer workloads</p>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.DeviceMeshManager.validate_mesh_config","title":"validate_mesh_config","text":"<pre><code>validate_mesh_config(\n    mesh_shape: tuple[int, ...], device_count: int\n) -&gt; bool\n</code></pre> <p>Validate mesh configuration.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_shape</code> <code>tuple[int, ...]</code> <p>Proposed mesh shape</p> required <code>device_count</code> <code>int</code> <p>Available device count</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if configuration is valid</p>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.create_device_mesh","title":"create_device_mesh","text":"<pre><code>create_device_mesh(\n    mesh_shape: tuple[int, ...], axis_names: tuple[str, ...]\n) -&gt; Mesh\n</code></pre> <p>Create device mesh with specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_shape</code> <code>tuple[int, ...]</code> <p>Shape of the device mesh</p> required <code>axis_names</code> <code>tuple[str, ...]</code> <p>Names for each mesh axis</p> required <p>Returns:</p> Type Description <code>Mesh</code> <p>JAX device mesh</p>"},{"location":"scaling/#artifex.generative_models.scaling.mesh_utils.get_optimal_mesh_shape","title":"get_optimal_mesh_shape","text":"<pre><code>get_optimal_mesh_shape(\n    device_count: int, parallelism_config: ParallelismConfig\n) -&gt; tuple[int, ...]\n</code></pre> <p>Get optimal mesh shape for given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>device_count</code> <code>int</code> <p>Number of available devices</p> required <code>parallelism_config</code> <code>ParallelismConfig</code> <p>Parallelism configuration</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Optimal mesh shape</p>"},{"location":"scaling/#sharding","title":"Sharding","text":""},{"location":"scaling/#artifex.generative_models.scaling.sharding","title":"artifex.generative_models.scaling.sharding","text":"<p>Sharding strategies and parallelism configuration for scalable training.</p> <p>This module provides comprehensive sharding infrastructure including: - Abstract base class for sharding strategies - Concrete implementations for different parallelism types - Multi-dimensional parallelism support - Configuration management for complex sharding setups</p> <p>All implementations prioritize performance and follow JAX/Flax NNX patterns.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingConfig","title":"ShardingConfig  <code>dataclass</code>","text":"<pre><code>ShardingConfig(\n    data_parallel_size: int = 1,\n    tensor_parallel_size: int = 1,\n    pipeline_parallel_size: int = 1,\n    fsdp_enabled: bool = False,\n    fsdp_min_weight_size: int = 1024,\n)\n</code></pre> <p>Configuration for multi-dimensional parallelism setup.</p> <p>Defines the parallelism dimensions and FSDP settings for a model.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingConfig.data_parallel_size","title":"data_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_parallel_size: int = 1\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingConfig.tensor_parallel_size","title":"tensor_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tensor_parallel_size: int = 1\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingConfig.pipeline_parallel_size","title":"pipeline_parallel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pipeline_parallel_size: int = 1\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingConfig.fsdp_enabled","title":"fsdp_enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fsdp_enabled: bool = False\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingConfig.fsdp_min_weight_size","title":"fsdp_min_weight_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fsdp_min_weight_size: int = 1024\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingConfig.get_total_device_count","title":"get_total_device_count","text":"<pre><code>get_total_device_count() -&gt; int\n</code></pre> <p>Calculate total devices needed for this configuration.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingConfig.from_device_count","title":"from_device_count  <code>classmethod</code>","text":"<pre><code>from_device_count(device_count: int) -&gt; ShardingConfig\n</code></pre> <p>Create optimal sharding config for given device count.</p> <p>Uses heuristics to balance different parallelism dimensions.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ParallelismConfig","title":"ParallelismConfig  <code>dataclass</code>","text":"<pre><code>ParallelismConfig(\n    mesh_shape: tuple[int, ...],\n    mesh_axis_names: tuple[str, ...],\n    sharding_config: ShardingConfig,\n)\n</code></pre> <p>Complete parallelism configuration including mesh topology.</p> <p>Combines sharding configuration with device mesh setup.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ParallelismConfig.mesh_shape","title":"mesh_shape  <code>instance-attribute</code>","text":"<pre><code>mesh_shape: tuple[int, ...]\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ParallelismConfig.mesh_axis_names","title":"mesh_axis_names  <code>instance-attribute</code>","text":"<pre><code>mesh_axis_names: tuple[str, ...]\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ParallelismConfig.sharding_config","title":"sharding_config  <code>instance-attribute</code>","text":"<pre><code>sharding_config: ShardingConfig\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ParallelismConfig.is_valid","title":"is_valid","text":"<pre><code>is_valid() -&gt; bool\n</code></pre> <p>Validate that mesh shape matches sharding configuration.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ParallelismConfig.from_sharding_config","title":"from_sharding_config  <code>classmethod</code>","text":"<pre><code>from_sharding_config(\n    config: ShardingConfig,\n) -&gt; ParallelismConfig\n</code></pre> <p>Create parallelism config from sharding configuration.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingStrategy","title":"ShardingStrategy","text":"<pre><code>ShardingStrategy(axis_name: str, mesh_axis: int)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for sharding strategies.</p> <p>Defines the interface that all sharding strategies must implement for consistent handling of different parallelism types.</p> <p>Parameters:</p> Name Type Description Default <code>axis_name</code> <code>str</code> <p>Name of the mesh axis for this strategy</p> required <code>mesh_axis</code> <code>int</code> <p>Index of the mesh axis</p> required"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingStrategy.axis_name","title":"axis_name  <code>instance-attribute</code>","text":"<pre><code>axis_name = axis_name\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingStrategy.mesh_axis","title":"mesh_axis  <code>instance-attribute</code>","text":"<pre><code>mesh_axis = mesh_axis\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingStrategy.get_partition_spec","title":"get_partition_spec  <code>abstractmethod</code>","text":"<pre><code>get_partition_spec(\n    tensor_shape: tuple[str, ...],\n) -&gt; PartitionSpec\n</code></pre> <p>Get partition specification for a tensor with given shape names.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_shape</code> <code>tuple[str, ...]</code> <p>Tuple of dimension names for the tensor</p> required <p>Returns:</p> Type Description <code>PartitionSpec</code> <p>PartitionSpec defining how to shard the tensor</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingStrategy.apply_sharding","title":"apply_sharding  <code>abstractmethod</code>","text":"<pre><code>apply_sharding(array: Array, mesh: Mesh) -&gt; Array\n</code></pre> <p>Apply sharding to an array using the given mesh.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>Array</code> <p>JAX array to shard</p> required <code>mesh</code> <code>Mesh</code> <p>Device mesh for sharding</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Sharded array</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.ShardingStrategy.get_sharding_constraints","title":"get_sharding_constraints","text":"<pre><code>get_sharding_constraints() -&gt; dict[str, Any]\n</code></pre> <p>Get sharding constraints for this strategy.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of sharding constraints</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.DataParallelStrategy","title":"DataParallelStrategy","text":"<pre><code>DataParallelStrategy(axis_name: str, mesh_axis: int)\n</code></pre> <p>               Bases: <code>ShardingStrategy</code></p> <p>Data parallel sharding strategy.</p> <p>Shards the batch dimension across devices while replicating model parameters and computation.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.DataParallelStrategy.get_partition_spec","title":"get_partition_spec","text":"<pre><code>get_partition_spec(\n    tensor_shape: tuple[str, ...],\n) -&gt; PartitionSpec\n</code></pre> <p>Get partition spec for data parallel sharding.</p> <p>Only shards the batch dimension, leaves others replicated.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.DataParallelStrategy.apply_sharding","title":"apply_sharding","text":"<pre><code>apply_sharding(array: Array, mesh: Mesh) -&gt; Array\n</code></pre> <p>Apply data parallel sharding to array.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.FSDPStrategy","title":"FSDPStrategy","text":"<pre><code>FSDPStrategy(\n    axis_name: str,\n    mesh_axis: int,\n    min_weight_size: int = 1024,\n)\n</code></pre> <p>               Bases: <code>ShardingStrategy</code></p> <p>Fully Sharded Data Parallel strategy.</p> <p>Shards model parameters across devices to reduce memory usage while maintaining training efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>axis_name</code> <code>str</code> <p>Name of the mesh axis</p> required <code>mesh_axis</code> <code>int</code> <p>Index of the mesh axis</p> required <code>min_weight_size</code> <code>int</code> <p>Minimum first dimension size to enable sharding</p> <code>1024</code>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.FSDPStrategy.min_weight_size","title":"min_weight_size  <code>instance-attribute</code>","text":"<pre><code>min_weight_size = min_weight_size\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.FSDPStrategy.should_shard_weight","title":"should_shard_weight","text":"<pre><code>should_shard_weight(weight: Array) -&gt; bool\n</code></pre> <p>Determine if a weight should be sharded based on its size.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Array</code> <p>Weight array to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if weight should be sharded, False otherwise</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.FSDPStrategy.get_partition_spec","title":"get_partition_spec","text":"<pre><code>get_partition_spec(\n    tensor_shape: tuple[str, ...],\n) -&gt; PartitionSpec\n</code></pre> <p>Get partition spec for FSDP sharding.</p> <p>Shards along the first dimension of weight tensors.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.FSDPStrategy.get_gradient_partition_spec","title":"get_gradient_partition_spec","text":"<pre><code>get_gradient_partition_spec(\n    tensor_shape: tuple[str, ...],\n) -&gt; PartitionSpec\n</code></pre> <p>Get partition spec for gradient sharding (same as weights).</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.FSDPStrategy.apply_sharding","title":"apply_sharding","text":"<pre><code>apply_sharding(array: Array, mesh: Mesh) -&gt; Array\n</code></pre> <p>Apply FSDP sharding to array.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.TensorParallelStrategy","title":"TensorParallelStrategy","text":"<pre><code>TensorParallelStrategy(\n    axis_name: str,\n    mesh_axis: int,\n    shard_dimension: str | None = None,\n)\n</code></pre> <p>               Bases: <code>ShardingStrategy</code></p> <p>Tensor parallel sharding strategy.</p> <p>Shards model computation across devices by splitting tensors along specific dimensions (typically features).</p> <p>Parameters:</p> Name Type Description Default <code>axis_name</code> <code>str</code> <p>Name of the mesh axis</p> required <code>mesh_axis</code> <code>int</code> <p>Index of the mesh axis</p> required <code>shard_dimension</code> <code>str | None</code> <p>Preferred dimension to shard ('in_features' or 'out_features')</p> <code>None</code>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.TensorParallelStrategy.shard_dimension","title":"shard_dimension  <code>instance-attribute</code>","text":"<pre><code>shard_dimension = shard_dimension\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.TensorParallelStrategy.get_partition_spec","title":"get_partition_spec","text":"<pre><code>get_partition_spec(\n    tensor_shape: tuple[str, ...],\n) -&gt; PartitionSpec\n</code></pre> <p>Get partition spec for tensor parallel sharding.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.TensorParallelStrategy.get_linear_weight_spec","title":"get_linear_weight_spec","text":"<pre><code>get_linear_weight_spec() -&gt; PartitionSpec\n</code></pre> <p>Get partition spec for linear layer weights.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.TensorParallelStrategy.get_attention_qkv_spec","title":"get_attention_qkv_spec","text":"<pre><code>get_attention_qkv_spec() -&gt; PartitionSpec\n</code></pre> <p>Get partition spec for attention QKV projections.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.TensorParallelStrategy.get_attention_output_spec","title":"get_attention_output_spec","text":"<pre><code>get_attention_output_spec() -&gt; PartitionSpec\n</code></pre> <p>Get partition spec for attention output projection.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.TensorParallelStrategy.apply_sharding","title":"apply_sharding","text":"<pre><code>apply_sharding(array: Array, mesh: Mesh) -&gt; Array\n</code></pre> <p>Apply tensor parallel sharding to array.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.PipelineParallelStrategy","title":"PipelineParallelStrategy","text":"<pre><code>PipelineParallelStrategy(\n    axis_name: str, mesh_axis: int, num_stages: int\n)\n</code></pre> <p>               Bases: <code>ShardingStrategy</code></p> <p>Pipeline parallel sharding strategy.</p> <p>Distributes model layers across devices to enable pipeline parallelism for very large models that don't fit on single devices.</p> <p>Parameters:</p> Name Type Description Default <code>axis_name</code> <code>str</code> <p>Name of the mesh axis</p> required <code>mesh_axis</code> <code>int</code> <p>Index of the mesh axis</p> required <code>num_stages</code> <code>int</code> <p>Number of pipeline stages</p> required"},{"location":"scaling/#artifex.generative_models.scaling.sharding.PipelineParallelStrategy.num_stages","title":"num_stages  <code>instance-attribute</code>","text":"<pre><code>num_stages = num_stages\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.PipelineParallelStrategy.assign_layers_to_stages","title":"assign_layers_to_stages","text":"<pre><code>assign_layers_to_stages(num_layers: int) -&gt; list[int]\n</code></pre> <p>Assign layers to pipeline stages.</p> <p>Parameters:</p> Name Type Description Default <code>num_layers</code> <code>int</code> <p>Total number of layers in the model</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>list of layer counts per stage</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.PipelineParallelStrategy.get_partition_spec","title":"get_partition_spec","text":"<pre><code>get_partition_spec(\n    tensor_shape: tuple[str, ...],\n) -&gt; PartitionSpec\n</code></pre> <p>Get partition spec for pipeline parallel sharding.</p> <p>Pipeline parallelism doesn't shard individual tensors, but rather assigns entire layers to different devices.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.PipelineParallelStrategy.get_forward_communication_pattern","title":"get_forward_communication_pattern","text":"<pre><code>get_forward_communication_pattern() -&gt; list[\n    tuple[int, int]\n]\n</code></pre> <p>Get communication pattern for forward pass.</p> <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>list of (source_stage, dest_stage) pairs</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.PipelineParallelStrategy.get_backward_communication_pattern","title":"get_backward_communication_pattern","text":"<pre><code>get_backward_communication_pattern() -&gt; list[\n    tuple[int, int]\n]\n</code></pre> <p>Get communication pattern for backward pass.</p> <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>list of (source_stage, dest_stage) pairs</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.PipelineParallelStrategy.apply_sharding","title":"apply_sharding","text":"<pre><code>apply_sharding(array: Array, mesh: Mesh) -&gt; Array\n</code></pre> <p>Apply pipeline parallel sharding to array.</p> <p>Pipeline parallelism handles layer assignment rather than tensor sharding.</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.MultiDimensionalStrategy","title":"MultiDimensionalStrategy","text":"<pre><code>MultiDimensionalStrategy(\n    strategies: dict[str, ShardingStrategy],\n    config: ParallelismConfig,\n)\n</code></pre> <p>Multi-dimensional parallelism strategy combining multiple approaches.</p> <p>Combines different sharding strategies (data, tensor, FSDP, pipeline) to achieve optimal performance for large-scale training.</p> <p>Parameters:</p> Name Type Description Default <code>strategies</code> <code>dict[str, ShardingStrategy]</code> <p>Dictionary mapping strategy names to strategy instances</p> required <code>config</code> <code>ParallelismConfig</code> <p>Sharding configuration for the multi-dimensional strategy</p> required"},{"location":"scaling/#artifex.generative_models.scaling.sharding.MultiDimensionalStrategy.strategies","title":"strategies  <code>instance-attribute</code>","text":"<pre><code>strategies = strategies\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.MultiDimensionalStrategy.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config\n</code></pre>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.MultiDimensionalStrategy.get_combined_partition_spec","title":"get_combined_partition_spec","text":"<pre><code>get_combined_partition_spec(\n    tensor_name: str, tensor_shape: tuple[str, ...]\n) -&gt; PartitionSpec\n</code></pre> <p>Get combined partition spec from all strategies.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_name</code> <code>str</code> <p>Name/type of the tensor</p> required <code>tensor_shape</code> <code>tuple[str, ...]</code> <p>Shape dimension names of the tensor</p> required <p>Returns:</p> Type Description <code>PartitionSpec</code> <p>Combined PartitionSpec</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.MultiDimensionalStrategy.resolve_sharding_conflicts","title":"resolve_sharding_conflicts","text":"<pre><code>resolve_sharding_conflicts(\n    tensor_name: str,\n    proposed_specs: dict[str, PartitionSpec],\n) -&gt; PartitionSpec\n</code></pre> <p>Resolve conflicts between multiple proposed partition specs.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_name</code> <code>str</code> <p>Name of the tensor</p> required <code>proposed_specs</code> <code>dict[str, PartitionSpec]</code> <p>Dictionary of strategy names to proposed specs</p> required <p>Returns:</p> Type Description <code>PartitionSpec</code> <p>Resolved PartitionSpec</p>"},{"location":"scaling/#artifex.generative_models.scaling.sharding.MultiDimensionalStrategy.create_partition_spec","title":"create_partition_spec","text":"<pre><code>create_partition_spec(\n    param_shape: tuple[int, ...], param_name: str\n) -&gt; PartitionSpec\n</code></pre> <p>Create PartitionSpec for pipeline parallel sharding.</p>"},{"location":"scaling/#related-documentation","title":"Related Documentation","text":"<ul> <li>Distributed Training Guide - User guide for distributed training</li> <li>Model Parallelism - Model parallelism techniques</li> <li>Training Guide - Core training concepts</li> <li>Device Management - Device manager API</li> </ul>"},{"location":"scaling/mesh_utils/","title":"Mesh Utils","text":"<p>Module: <code>generative_models.scaling.mesh_utils</code></p> <p>Source: <code>generative_models/scaling/mesh_utils.py</code></p>"},{"location":"scaling/mesh_utils/#overview","title":"Overview","text":"<p>Device mesh management utilities for scalable distributed training.</p> <p>This module provides comprehensive device mesh management including:</p> <ul> <li>Device mesh creation and optimization</li> <li>Topology optimization for different workloads</li> <li>Validation and configuration utilities</li> <li>Hardware-aware mesh shape calculation</li> </ul> <p>All implementations prioritize performance and follow JAX/Flax NNX patterns.</p>"},{"location":"scaling/mesh_utils/#classes","title":"Classes","text":""},{"location":"scaling/mesh_utils/#devicemeshmanager","title":"DeviceMeshManager","text":"<pre><code>class DeviceMeshManager\n</code></pre>"},{"location":"scaling/mesh_utils/#functions","title":"Functions","text":""},{"location":"scaling/mesh_utils/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"scaling/mesh_utils/#create_device_mesh","title":"create_device_mesh","text":"<pre><code>def create_device_mesh()\n</code></pre>"},{"location":"scaling/mesh_utils/#create_mesh","title":"create_mesh","text":"<pre><code>def create_mesh()\n</code></pre>"},{"location":"scaling/mesh_utils/#create_mesh_from_config","title":"create_mesh_from_config","text":"<pre><code>def create_mesh_from_config()\n</code></pre>"},{"location":"scaling/mesh_utils/#get_optimal_mesh_shape","title":"get_optimal_mesh_shape","text":"<pre><code>def get_optimal_mesh_shape()\n</code></pre>"},{"location":"scaling/mesh_utils/#get_optimal_mesh_shape_1","title":"get_optimal_mesh_shape","text":"<pre><code>def get_optimal_mesh_shape()\n</code></pre>"},{"location":"scaling/mesh_utils/#optimize_for_transformer","title":"optimize_for_transformer","text":"<pre><code>def optimize_for_transformer()\n</code></pre>"},{"location":"scaling/mesh_utils/#validate_mesh_config","title":"validate_mesh_config","text":"<pre><code>def validate_mesh_config()\n</code></pre>"},{"location":"scaling/mesh_utils/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 8</li> <li>Imports: 4</li> </ul>"},{"location":"scaling/sharding/","title":"Sharding","text":"<p>Module: <code>generative_models.scaling.sharding</code></p> <p>Source: <code>generative_models/scaling/sharding.py</code></p>"},{"location":"scaling/sharding/#overview","title":"Overview","text":"<p>Sharding strategies and parallelism configuration for scalable training.</p> <p>This module provides comprehensive sharding infrastructure including:</p> <ul> <li>Abstract base class for sharding strategies</li> <li>Concrete implementations for different parallelism types</li> <li>Multi-dimensional parallelism support</li> <li>Configuration management for complex sharding setups</li> </ul> <p>All implementations prioritize performance and follow JAX/Flax NNX patterns.</p>"},{"location":"scaling/sharding/#classes","title":"Classes","text":""},{"location":"scaling/sharding/#dataparallelstrategy","title":"DataParallelStrategy","text":"<pre><code>class DataParallelStrategy\n</code></pre>"},{"location":"scaling/sharding/#fsdpstrategy","title":"FSDPStrategy","text":"<pre><code>class FSDPStrategy\n</code></pre>"},{"location":"scaling/sharding/#multidimensionalstrategy","title":"MultiDimensionalStrategy","text":"<pre><code>class MultiDimensionalStrategy\n</code></pre>"},{"location":"scaling/sharding/#parallelismconfig","title":"ParallelismConfig","text":"<pre><code>class ParallelismConfig\n</code></pre>"},{"location":"scaling/sharding/#pipelineparallelstrategy","title":"PipelineParallelStrategy","text":"<pre><code>class PipelineParallelStrategy\n</code></pre>"},{"location":"scaling/sharding/#shardingconfig","title":"ShardingConfig","text":"<pre><code>class ShardingConfig\n</code></pre>"},{"location":"scaling/sharding/#shardingstrategy","title":"ShardingStrategy","text":"<pre><code>class ShardingStrategy\n</code></pre>"},{"location":"scaling/sharding/#tensorparallelstrategy","title":"TensorParallelStrategy","text":"<pre><code>class TensorParallelStrategy\n</code></pre>"},{"location":"scaling/sharding/#functions","title":"Functions","text":""},{"location":"scaling/sharding/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"scaling/sharding/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"scaling/sharding/#init_2","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"scaling/sharding/#init_3","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"scaling/sharding/#init_4","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"scaling/sharding/#apply_sharding","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"scaling/sharding/#apply_sharding_1","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"scaling/sharding/#apply_sharding_2","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"scaling/sharding/#apply_sharding_3","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"scaling/sharding/#apply_sharding_4","title":"apply_sharding","text":"<pre><code>def apply_sharding()\n</code></pre>"},{"location":"scaling/sharding/#assign_layers_to_stages","title":"assign_layers_to_stages","text":"<pre><code>def assign_layers_to_stages()\n</code></pre>"},{"location":"scaling/sharding/#create_partition_spec","title":"create_partition_spec","text":"<pre><code>def create_partition_spec()\n</code></pre>"},{"location":"scaling/sharding/#from_device_count","title":"from_device_count","text":"<pre><code>def from_device_count()\n</code></pre>"},{"location":"scaling/sharding/#from_sharding_config","title":"from_sharding_config","text":"<pre><code>def from_sharding_config()\n</code></pre>"},{"location":"scaling/sharding/#get_attention_output_spec","title":"get_attention_output_spec","text":"<pre><code>def get_attention_output_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_attention_qkv_spec","title":"get_attention_qkv_spec","text":"<pre><code>def get_attention_qkv_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_backward_communication_pattern","title":"get_backward_communication_pattern","text":"<pre><code>def get_backward_communication_pattern()\n</code></pre>"},{"location":"scaling/sharding/#get_combined_partition_spec","title":"get_combined_partition_spec","text":"<pre><code>def get_combined_partition_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_forward_communication_pattern","title":"get_forward_communication_pattern","text":"<pre><code>def get_forward_communication_pattern()\n</code></pre>"},{"location":"scaling/sharding/#get_gradient_partition_spec","title":"get_gradient_partition_spec","text":"<pre><code>def get_gradient_partition_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_linear_weight_spec","title":"get_linear_weight_spec","text":"<pre><code>def get_linear_weight_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_partition_spec","title":"get_partition_spec","text":"<pre><code>def get_partition_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_partition_spec_1","title":"get_partition_spec","text":"<pre><code>def get_partition_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_partition_spec_2","title":"get_partition_spec","text":"<pre><code>def get_partition_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_partition_spec_3","title":"get_partition_spec","text":"<pre><code>def get_partition_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_partition_spec_4","title":"get_partition_spec","text":"<pre><code>def get_partition_spec()\n</code></pre>"},{"location":"scaling/sharding/#get_sharding_constraints","title":"get_sharding_constraints","text":"<pre><code>def get_sharding_constraints()\n</code></pre>"},{"location":"scaling/sharding/#get_total_device_count","title":"get_total_device_count","text":"<pre><code>def get_total_device_count()\n</code></pre>"},{"location":"scaling/sharding/#is_valid","title":"is_valid","text":"<pre><code>def is_valid()\n</code></pre>"},{"location":"scaling/sharding/#resolve_sharding_conflicts","title":"resolve_sharding_conflicts","text":"<pre><code>def resolve_sharding_conflicts()\n</code></pre>"},{"location":"scaling/sharding/#should_shard_weight","title":"should_shard_weight","text":"<pre><code>def should_shard_weight()\n</code></pre>"},{"location":"scaling/sharding/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 8</li> <li>Functions: 31</li> <li>Imports: 8</li> </ul>"},{"location":"theory/molecular-constraints/","title":"Molecular Constraints Theory","text":"<p>Coming Soon</p> <p>This page is under development. Check back for comprehensive molecular constraints documentation.</p>"},{"location":"theory/molecular-constraints/#overview","title":"Overview","text":"<p>Molecular constraints in generative models ensure physical validity of generated structures.</p>"},{"location":"theory/molecular-constraints/#topics","title":"Topics","text":"<ul> <li>Bond length constraints</li> <li>Bond angle constraints</li> <li>Torsional constraints</li> <li>Collision avoidance</li> <li>Energy minimization</li> </ul>"},{"location":"theory/molecular-constraints/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling Guide</li> <li>Geometric Models</li> </ul>"},{"location":"training/","title":"Training Systems","text":"<p>Comprehensive training infrastructure for generative models, including model-specific trainers, distributed training, callbacks, and optimization utilities.</p>"},{"location":"training/#overview","title":"Overview","text":"<ul> <li> <p> Model Trainers</p> <p>Specialized trainers for VAE, GAN, Diffusion, Flow, EBM, and Autoregressive models</p> </li> <li> <p> Distributed Training</p> <p>Data parallel and model parallel training across multiple GPUs/TPUs</p> </li> <li> <p> Callbacks</p> <p>Checkpointing, early stopping, logging, and custom callbacks</p> </li> <li> <p> Optimizers</p> <p>AdamW, Lion, Adafactor with learning rate schedulers</p> </li> </ul>"},{"location":"training/#quick-start","title":"Quick Start","text":""},{"location":"training/#basic-training","title":"Basic Training","text":"<pre><code>from artifex.generative_models.training import VAETrainer\nfrom artifex.generative_models.core.configuration import TrainingConfig\n\n# Create training configuration\ntraining_config = TrainingConfig(\n    batch_size=128,\n    num_epochs=100,\n    optimizer={\"type\": \"adam\", \"learning_rate\": 1e-3},\n    scheduler={\"type\": \"cosine\", \"warmup_steps\": 1000},\n)\n\n# Create trainer\ntrainer = VAETrainer(\n    model=model,\n    config=training_config,\n    train_dataset=train_data,\n    val_dataset=val_data,\n)\n\n# Train\ntrainer.train()\n</code></pre>"},{"location":"training/#model-specific-trainers","title":"Model-Specific Trainers","text":"<p>Each model family has a specialized trainer that handles its unique training requirements.</p>"},{"location":"training/#vae-trainer","title":"VAE Trainer","text":"<p>Handles ELBO loss, KL annealing, and reconstruction metrics.</p> <pre><code>from artifex.generative_models.training import VAETrainer\n\ntrainer = VAETrainer(\n    model=vae_model,\n    config=training_config,\n    train_dataset=train_data,\n    kl_annealing=True,\n    kl_warmup_epochs=10,\n)\n</code></pre> <p> VAE Trainer Reference</p>"},{"location":"training/#gan-trainer","title":"GAN Trainer","text":"<p>Manages generator/discriminator alternating updates.</p> <pre><code>from artifex.generative_models.training import GANTrainer\n\ntrainer = GANTrainer(\n    model=gan_model,\n    config=training_config,\n    train_dataset=train_data,\n    d_steps=5,  # Discriminator steps per generator step\n    gp_weight=10.0,  # Gradient penalty weight\n)\n</code></pre> <p> GAN Trainer Reference</p>"},{"location":"training/#diffusion-trainer","title":"Diffusion Trainer","text":"<p>Handles noise scheduling and denoising score matching.</p> <pre><code>from artifex.generative_models.training import DiffusionTrainer\n\ntrainer = DiffusionTrainer(\n    model=diffusion_model,\n    config=training_config,\n    train_dataset=train_data,\n    ema_decay=0.9999,  # Exponential moving average\n)\n</code></pre> <p> Diffusion Trainer Reference</p>"},{"location":"training/#flow-trainer","title":"Flow Trainer","text":"<p>Manages exact likelihood training for normalizing flows.</p> <pre><code>from artifex.generative_models.training import FlowTrainer\n\ntrainer = FlowTrainer(\n    model=flow_model,\n    config=training_config,\n    train_dataset=train_data,\n)\n</code></pre> <p> Flow Trainer Reference</p>"},{"location":"training/#energy-trainer","title":"Energy Trainer","text":"<p>Handles contrastive divergence and MCMC sampling.</p> <pre><code>from artifex.generative_models.training import EnergyTrainer\n\ntrainer = EnergyTrainer(\n    model=ebm_model,\n    config=training_config,\n    train_dataset=train_data,\n    mcmc_steps=10,\n)\n</code></pre> <p> Energy Trainer Reference</p>"},{"location":"training/#autoregressive-trainer","title":"Autoregressive Trainer","text":"<p>Manages sequential likelihood training.</p> <pre><code>from artifex.generative_models.training import AutoregressiveTrainer\n\ntrainer = AutoregressiveTrainer(\n    model=ar_model,\n    config=training_config,\n    train_dataset=train_data,\n)\n</code></pre> <p> Autoregressive Trainer Reference</p>"},{"location":"training/#callbacks","title":"Callbacks","text":"<p>Callbacks allow customization of the training loop.</p>"},{"location":"training/#built-in-callbacks","title":"Built-in Callbacks","text":"Callback Description CheckpointCallback Save model checkpoints EarlyStoppingCallback Stop training when validation plateaus LoggingCallback Log metrics to console/file ProfilingCallback Profile training performance VisualizationCallback Generate sample visualizations"},{"location":"training/#using-callbacks","title":"Using Callbacks","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    CheckpointCallback,\n    EarlyStoppingCallback,\n    LoggingCallback,\n)\n\ncallbacks = [\n    CheckpointCallback(\n        save_dir=\"checkpoints/\",\n        save_every_n_epochs=10,\n        save_best=True,\n        metric=\"val_loss\",\n    ),\n    EarlyStoppingCallback(\n        patience=20,\n        metric=\"val_loss\",\n        mode=\"min\",\n    ),\n    LoggingCallback(\n        log_every_n_steps=100,\n        use_wandb=True,\n    ),\n]\n\ntrainer = VAETrainer(\n    model=model,\n    config=config,\n    train_dataset=train_data,\n    callbacks=callbacks,\n)\n</code></pre>"},{"location":"training/#custom-callbacks","title":"Custom Callbacks","text":"<pre><code>from artifex.generative_models.training.callbacks import BaseCallback\n\nclass CustomCallback(BaseCallback):\n    def on_epoch_start(self, trainer, epoch):\n        print(f\"Starting epoch {epoch}\")\n\n    def on_epoch_end(self, trainer, epoch, metrics):\n        print(f\"Epoch {epoch} completed: {metrics}\")\n\n    def on_train_batch_end(self, trainer, batch_idx, loss):\n        if batch_idx % 100 == 0:\n            print(f\"Batch {batch_idx}: loss={loss:.4f}\")\n</code></pre>"},{"location":"training/#distributed-training","title":"Distributed Training","text":""},{"location":"training/#data-parallel","title":"Data Parallel","text":"<pre><code>from artifex.generative_models.training.distributed import DataParallelTrainer\n\ntrainer = DataParallelTrainer(\n    model=model,\n    config=config,\n    train_dataset=train_data,\n    num_devices=4,  # Use 4 GPUs\n)\n</code></pre> <p> Data Parallel Reference</p>"},{"location":"training/#model-parallel","title":"Model Parallel","text":"<pre><code>from artifex.generative_models.training.distributed import ModelParallelTrainer\n\ntrainer = ModelParallelTrainer(\n    model=large_model,\n    config=config,\n    train_dataset=train_data,\n    mesh_shape=(2, 4),  # 2x4 device mesh\n)\n</code></pre> <p> Model Parallel Reference</p>"},{"location":"training/#device-mesh","title":"Device Mesh","text":"<pre><code>from artifex.generative_models.training.distributed import DeviceMesh\n\nmesh = DeviceMesh(\n    shape=(2, 2),  # 2x2 mesh\n    axis_names=(\"data\", \"model\"),\n)\n</code></pre> <p> Device Mesh Reference</p>"},{"location":"training/#optimizers","title":"Optimizers","text":""},{"location":"training/#available-optimizers","title":"Available Optimizers","text":"Optimizer Description Best For AdamW Adam with weight decay General use Lion Memory-efficient optimizer Large models Adafactor Low memory optimizer Very large models"},{"location":"training/#using-optimizers","title":"Using Optimizers","text":"<pre><code>from artifex.generative_models.training.optimizers import create_optimizer\n\noptimizer = create_optimizer(\n    optimizer_type=\"adamw\",\n    learning_rate=1e-3,\n    weight_decay=0.01,\n    beta1=0.9,\n    beta2=0.999,\n)\n</code></pre>"},{"location":"training/#learning-rate-schedulers","title":"Learning Rate Schedulers","text":""},{"location":"training/#available-schedulers","title":"Available Schedulers","text":"Scheduler Description Cosine Cosine annealing with warmup Linear Linear warmup and decay Exponential Exponential decay"},{"location":"training/#using-schedulers","title":"Using Schedulers","text":"<pre><code>from artifex.generative_models.training.schedulers import create_scheduler\n\nscheduler = create_scheduler(\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    total_steps=100000,\n    min_lr=1e-6,\n)\n</code></pre> <p> Scheduler Reference</p>"},{"location":"training/#rl-training","title":"RL Training","text":"<p>Reinforcement learning trainers for fine-tuning generative models.</p> Trainer Description REINFORCE Policy gradient training PPO Proximal Policy Optimization DPO Direct Preference Optimization GRPO Group Relative Policy Optimization <p> RL Training Guide</p>"},{"location":"training/#advanced-features","title":"Advanced Features","text":""},{"location":"training/#gradient-accumulation","title":"Gradient Accumulation","text":"<pre><code>trainer = VAETrainer(\n    model=model,\n    config=config,\n    train_dataset=train_data,\n    gradient_accumulation_steps=4,\n)\n</code></pre> <p> Gradient Accumulation</p>"},{"location":"training/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>trainer = VAETrainer(\n    model=model,\n    config=config,\n    train_dataset=train_data,\n    mixed_precision=True,  # Use bfloat16\n)\n</code></pre> <p> Mixed Precision</p>"},{"location":"training/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code>trainer = VAETrainer(\n    model=model,\n    config=config,\n    train_dataset=train_data,\n    tracking={\n        \"wandb\": {\"project\": \"my-project\"},\n        \"mlflow\": {\"experiment\": \"vae-experiments\"},\n    },\n)\n</code></pre> <p> Experiment Tracking</p>"},{"location":"training/#module-reference","title":"Module Reference","text":"Category Modules Trainers vae_trainer, gan_trainer, diffusion_trainer, flow_trainer, energy_trainer, autoregressive_trainer Callbacks base, checkpoint, early_stopping, logging, profiling, visualization Distributed data_parallel, model_parallel, mesh, device_placement, distributed_metrics Optimizers adamw, lion, adafactor, optax_wrappers Schedulers cosine, linear, exponential, factory, scheduler RL reinforce, ppo, dpo, grpo Utilities base, gradient_accumulation, mixed_precision, tracking, trainer, utils"},{"location":"training/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Guide - Complete training guide</li> <li>Configuration System - Training configuration</li> <li>Distributed Training - Multi-GPU/TPU guide</li> </ul>"},{"location":"training/adafactor/","title":"Adafactor","text":"<p>Module: <code>generative_models.training.optimizers.adafactor</code></p> <p>Source: <code>generative_models/training/optimizers/adafactor.py</code></p>"},{"location":"training/adafactor/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/adamw/","title":"Adamw","text":"<p>Module: <code>generative_models.training.optimizers.adamw</code></p> <p>Source: <code>generative_models/training/optimizers/adamw.py</code></p>"},{"location":"training/adamw/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/autoregressive_trainer/","title":"Autoregressive Trainer","text":"<p>Module: <code>generative_models.training.trainers.autoregressive_trainer</code></p> <p>Source: <code>generative_models/training/trainers/autoregressive_trainer.py</code></p>"},{"location":"training/autoregressive_trainer/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/base/","title":"Callbacks Base","text":"<p>Module: <code>generative_models.training.callbacks.base</code></p> <p>Source: <code>generative_models/training/callbacks/base.py</code></p>"},{"location":"training/base/#overview","title":"Overview","text":"<p>Base classes and protocols for the training callback system. Provides a lightweight, Protocol-based callback system for training loops that avoids circular imports through the <code>TrainerLike</code> protocol.</p>"},{"location":"training/base/#protocols","title":"Protocols","text":""},{"location":"training/base/#trainerlike","title":"TrainerLike","text":"<pre><code>class TrainerLike(Protocol):\n    \"\"\"Protocol for trainer-like objects that callbacks can interact with.\n\n    This protocol defines the minimal interface that a trainer must implement\n    to work with callbacks. Using a protocol avoids circular imports between\n    callbacks and trainer modules.\n    \"\"\"\n\n    @property\n    def model(self) -&gt; nnx.Module:\n        \"\"\"The NNX model being trained.\"\"\"\n        ...\n</code></pre> <p>Defines the minimal interface for trainers to work with callbacks without causing circular imports. The actual Trainer class satisfies this protocol.</p>"},{"location":"training/base/#trainingcallback","title":"TrainingCallback","text":"<pre><code>class TrainingCallback(Protocol):\n    \"\"\"Protocol defining the callback interface.\"\"\"\n\n    def on_train_begin(self, trainer: TrainerLike) -&gt; None: ...\n    def on_train_end(self, trainer: TrainerLike) -&gt; None: ...\n    def on_epoch_begin(self, trainer: TrainerLike, epoch: int) -&gt; None: ...\n    def on_epoch_end(self, trainer: TrainerLike, epoch: int, logs: dict[str, Any]) -&gt; None: ...\n    def on_batch_begin(self, trainer: TrainerLike, batch: int) -&gt; None: ...\n    def on_batch_end(self, trainer: TrainerLike, batch: int, logs: dict[str, Any]) -&gt; None: ...\n    def on_validation_begin(self, trainer: TrainerLike) -&gt; None: ...\n    def on_validation_end(self, trainer: TrainerLike, logs: dict[str, Any]) -&gt; None: ...\n</code></pre> <p>Defines the 8 lifecycle hooks that callbacks can implement.</p>"},{"location":"training/base/#classes","title":"Classes","text":""},{"location":"training/base/#basecallback","title":"BaseCallback","text":"<pre><code>class BaseCallback:\n    \"\"\"Base callback class with no-op implementations of all hooks.\"\"\"\n</code></pre> <p>Base class providing no-op implementations of all callback hooks. Extend this class and override only the methods you need.</p> <p>Example:</p> <pre><code>from artifex.generative_models.training.callbacks import BaseCallback\n\nclass MyCallback(BaseCallback):\n    def on_epoch_end(self, trainer, epoch, metrics):\n        print(f\"Epoch {epoch} completed with loss: {metrics.get('loss', 'N/A')}\")\n</code></pre>"},{"location":"training/base/#callbacklist","title":"CallbackList","text":"<pre><code>class CallbackList:\n    \"\"\"Container for multiple callbacks that dispatches to all.\"\"\"\n\n    def __init__(self, callbacks: list[TrainingCallback] | None = None): ...\n    def append(self, callback: TrainingCallback) -&gt; None: ...\n</code></pre> <p>Container that holds multiple callbacks and dispatches lifecycle events to all of them.</p> <p>Example:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    CallbackList,\n    EarlyStopping,\n    ModelCheckpoint,\n)\n\ncallbacks = CallbackList([\n    EarlyStopping(EarlyStoppingConfig(patience=10)),\n    ModelCheckpoint(CheckpointConfig(dirpath=\"./checkpoints\")),\n])\n\n# All callbacks receive events\ncallbacks.on_epoch_end(trainer, epoch=5, metrics={\"loss\": 0.5})\n</code></pre>"},{"location":"training/base/#usage","title":"Usage","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    TrainerLike,\n    TrainingCallback,\n    BaseCallback,\n    CallbackList,\n)\n\n# Implement custom callback\nclass LoggingCallback(BaseCallback):\n    def on_batch_end(self, trainer: TrainerLike, batch: int, logs: dict):\n        if batch % 100 == 0:\n            print(f\"Batch {batch}: {logs}\")\n\n# Use with trainer\ncallbacks = CallbackList([LoggingCallback()])\n</code></pre>"},{"location":"training/base/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2 (BaseCallback, CallbackList)</li> <li>Protocols: 2 (TrainerLike, TrainingCallback)</li> <li>Imports: 4</li> </ul>"},{"location":"training/checkpoint/","title":"Checkpointing Callbacks","text":"<p>Module: <code>generative_models.training.callbacks.checkpoint</code></p> <p>Source: <code>generative_models/training/callbacks/checkpoint.py</code></p>"},{"location":"training/checkpoint/#overview","title":"Overview","text":"<p>Model checkpointing callback that monitors metrics and saves checkpoints when they improve. Uses Orbax checkpointing under the hood with minimal overhead when not saving.</p>"},{"location":"training/checkpoint/#classes","title":"Classes","text":""},{"location":"training/checkpoint/#checkpointconfig","title":"CheckpointConfig","text":"<pre><code>@dataclass(slots=True)\nclass CheckpointConfig:\n    \"\"\"Configuration for model checkpointing.\"\"\"\n\n    dirpath: str | Path = \"checkpoints\"\n    filename: str = \"model-{epoch:02d}-{val_loss:.4f}\"\n    monitor: str = \"val_loss\"\n    mode: Literal[\"min\", \"max\"] = \"min\"\n    save_top_k: int = 3\n    save_last: bool = True\n    every_n_epochs: int = 1\n    save_weights_only: bool = False\n</code></pre> <p>Attributes:</p> Attribute Type Default Description <code>dirpath</code> <code>str \\| Path</code> <code>\"checkpoints\"</code> Directory to save checkpoints <code>filename</code> <code>str</code> <code>\"model-{epoch:02d}-{val_loss:.4f}\"</code> Filename template with <code>{epoch}</code> and metric placeholders <code>monitor</code> <code>str</code> <code>\"val_loss\"</code> Metric name to monitor <code>mode</code> <code>Literal[\"min\", \"max\"]</code> <code>\"min\"</code> Whether lower or higher is better <code>save_top_k</code> <code>int</code> <code>3</code> Number of best checkpoints to keep (-1 = all, 0 = none) <code>save_last</code> <code>bool</code> <code>True</code> Whether to save checkpoint on every epoch <code>every_n_epochs</code> <code>int</code> <code>1</code> Save checkpoint every n epochs <code>save_weights_only</code> <code>bool</code> <code>False</code> Only save model weights (not optimizer state)"},{"location":"training/checkpoint/#modelcheckpoint","title":"ModelCheckpoint","text":"<pre><code>class ModelCheckpoint(BaseCallback):\n    \"\"\"Save model checkpoints based on monitored metrics.\"\"\"\n\n    def __init__(self, config: CheckpointConfig): ...\n</code></pre> <p>Callback that saves model checkpoints when monitored metrics improve. Uses Orbax checkpointing infrastructure with automatic cleanup of old checkpoints.</p> <p>Key Properties:</p> Property Type Description <code>best_score</code> <code>float \\| None</code> Best metric value seen so far <code>best_checkpoint_path</code> <code>Path \\| None</code> Path to the best checkpoint <code>saved_checkpoints</code> <code>list[tuple[float, Path]]</code> List of (score, path) for saved checkpoints"},{"location":"training/checkpoint/#usage","title":"Usage","text":""},{"location":"training/checkpoint/#basic-checkpointing","title":"Basic Checkpointing","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    ModelCheckpoint,\n    CheckpointConfig,\n)\n\n# Save best 3 checkpoints based on validation loss\ncheckpoint = ModelCheckpoint(CheckpointConfig(\n    dirpath=\"./checkpoints\",\n    monitor=\"val_loss\",\n    mode=\"min\",\n    save_top_k=3,\n))\n\ntrainer.fit(callbacks=[checkpoint])\n\n# Access best checkpoint after training\nprint(f\"Best checkpoint: {checkpoint.best_checkpoint_path}\")\nprint(f\"Best score: {checkpoint.best_score}\")\n</code></pre>"},{"location":"training/checkpoint/#monitor-accuracy-higher-is-better","title":"Monitor Accuracy (Higher is Better)","text":"<pre><code>checkpoint = ModelCheckpoint(CheckpointConfig(\n    dirpath=\"./checkpoints\",\n    monitor=\"val_accuracy\",\n    mode=\"max\",  # Higher accuracy is better\n    save_top_k=1,  # Keep only the best\n    filename=\"best-model-{epoch:02d}-{val_accuracy:.4f}\",\n))\n</code></pre>"},{"location":"training/checkpoint/#save-all-checkpoints","title":"Save All Checkpoints","text":"<pre><code>checkpoint = ModelCheckpoint(CheckpointConfig(\n    dirpath=\"./checkpoints\",\n    save_top_k=-1,  # Keep all checkpoints\n    every_n_epochs=5,  # Save every 5 epochs\n))\n</code></pre>"},{"location":"training/checkpoint/#custom-filename-template","title":"Custom Filename Template","text":"<pre><code># Filename template supports {epoch} and any metric in logs\ncheckpoint = ModelCheckpoint(CheckpointConfig(\n    filename=\"model-epoch{epoch:03d}-loss{val_loss:.6f}-acc{val_accuracy:.4f}\",\n    monitor=\"val_loss\",\n))\n</code></pre>"},{"location":"training/checkpoint/#combined-with-other-callbacks","title":"Combined with Other Callbacks","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    CallbackList,\n    ModelCheckpoint,\n    CheckpointConfig,\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ProgressBarCallback,\n    ProgressBarConfig,\n)\n\ncallbacks = CallbackList([\n    ModelCheckpoint(CheckpointConfig(\n        dirpath=\"./checkpoints\",\n        monitor=\"val_loss\",\n        save_top_k=3,\n    )),\n    EarlyStopping(EarlyStoppingConfig(\n        monitor=\"val_loss\",\n        patience=10,\n    )),\n    ProgressBarCallback(ProgressBarConfig()),\n])\n\ntrainer.fit(callbacks=callbacks)\n</code></pre>"},{"location":"training/checkpoint/#how-it-works","title":"How It Works","text":"<ol> <li>Metric Monitoring: Tracks the specified metric (<code>monitor</code>) at the end of each epoch</li> <li>Improvement Check: Compares current value against best using <code>mode</code> (min/max)</li> <li>Checkpoint Saving: Uses Orbax infrastructure to save model state</li> <li>Automatic Cleanup: Removes old checkpoints beyond <code>save_top_k</code> limit</li> <li>Best Tracking: Maintains reference to the best checkpoint path</li> </ol>"},{"location":"training/checkpoint/#checkpoint-cleanup-strategy","title":"Checkpoint Cleanup Strategy","text":"<p>When <code>save_top_k &gt; 0</code>, checkpoints are sorted by their metric value:</p> <ul> <li>Min mode: Keeps checkpoints with lowest scores, removes highest</li> <li>Max mode: Keeps checkpoints with highest scores, removes lowest</li> </ul>"},{"location":"training/checkpoint/#integration-with-orbax","title":"Integration with Orbax","text":"<p>ModelCheckpoint uses the existing Orbax-based checkpointing infrastructure:</p> <pre><code>from artifex.generative_models.core.checkpointing import (\n    save_checkpoint,\n    load_checkpoint,\n    setup_checkpoint_manager,\n)\n\n# Checkpoints are saved using Orbax under the hood\n# You can load them manually:\ncheckpoint_manager, _ = setup_checkpoint_manager(\"./checkpoints\")\nmodel = load_checkpoint(checkpoint_manager, model, step=10)\n</code></pre> <p>See Checkpointing Guide for advanced checkpointing features including optimizer state and corruption recovery.</p>"},{"location":"training/checkpoint/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2 (CheckpointConfig, ModelCheckpoint)</li> <li>Dependencies: Orbax checkpointing infrastructure</li> <li>Slots: Uses <code>__slots__</code> for memory efficiency</li> </ul>"},{"location":"training/cosine/","title":"Cosine","text":"<p>Module: <code>generative_models.training.schedulers.cosine</code></p> <p>Source: <code>generative_models/training/schedulers/cosine.py</code></p>"},{"location":"training/cosine/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/data_parallel/","title":"Data Parallel Training","text":"<p>Module: <code>artifex.generative_models.training.distributed.data_parallel</code></p> <p>Source: <code>src/artifex/generative_models/training/distributed/data_parallel.py</code></p>"},{"location":"training/data_parallel/#overview","title":"Overview","text":"<p>The <code>DataParallel</code> class provides utilities for data-parallel training across multiple devices, including batch sharding, model state distribution, and gradient aggregation.</p>"},{"location":"training/data_parallel/#classes","title":"Classes","text":""},{"location":"training/data_parallel/#dataparallel","title":"DataParallel","text":"<p>Data parallel training utilities for Artifex, implemented as an NNX Module.</p> <pre><code>class DataParallel(nnx.Module):\n    \"\"\"Data parallel training utilities for Artifex.\n\n    This class provides methods for creating data-parallel shardings,\n    distributing batches across devices, and aggregating gradients.\n\n    Supports both static method usage (stateless) and instance method\n    usage (stateful).\n    \"\"\"\n</code></pre>"},{"location":"training/data_parallel/#constructor","title":"Constructor","text":"<pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize DataParallel module.\"\"\"\n</code></pre>"},{"location":"training/data_parallel/#methods","title":"Methods","text":""},{"location":"training/data_parallel/#create_data_parallel_sharding","title":"create_data_parallel_sharding","text":"<p>Create a NamedSharding for data parallelism.</p> <pre><code>def create_data_parallel_sharding(\n    self,\n    mesh: Mesh,\n    data_axis: str = \"data\",\n) -&gt; NamedSharding:\n    \"\"\"Create a NamedSharding for data parallelism.\n\n    Args:\n        mesh: The device mesh to use for sharding.\n        data_axis: Name of the data parallel axis in the mesh.\n\n    Returns:\n        A NamedSharding that distributes the first dimension across\n        the data axis.\n    \"\"\"\n</code></pre>"},{"location":"training/data_parallel/#shard_batch","title":"shard_batch","text":"<p>Shard a batch of data across devices.</p> <pre><code>def shard_batch(\n    self,\n    batch: Any,\n    sharding: NamedSharding,\n) -&gt; Any:\n    \"\"\"Shard a batch of data across devices.\n\n    Args:\n        batch: PyTree of data to shard (dict, array, etc.).\n        sharding: The sharding specification to apply.\n\n    Returns:\n        The sharded batch distributed across devices.\n    \"\"\"\n</code></pre>"},{"location":"training/data_parallel/#shard_model_state","title":"shard_model_state","text":"<p>Shard model state across devices.</p> <pre><code>def shard_model_state(\n    self,\n    state: Any,\n    mesh: Mesh,\n    param_sharding: Literal[\"replicate\", \"shard\"] = \"replicate\",\n) -&gt; Any:\n    \"\"\"Shard model state across devices.\n\n    Args:\n        state: The model state (parameters, optimizer state, etc.).\n        mesh: The device mesh to use.\n        param_sharding: How to shard parameters:\n            - \"replicate\": Copy parameters to all devices (default)\n            - \"shard\": Shard parameters across devices\n\n    Returns:\n        The sharded model state.\n    \"\"\"\n</code></pre>"},{"location":"training/data_parallel/#all_reduce_gradients","title":"all_reduce_gradients","text":"<p>Aggregate gradients across devices.</p> <pre><code>def all_reduce_gradients(\n    self,\n    gradients: Any,\n    reduce_type: Literal[\"mean\", \"sum\"] = \"mean\",\n    axis_name: str = \"batch\",\n) -&gt; Any:\n    \"\"\"Aggregate gradients across devices.\n\n    This should be called inside a pmap/shard_map context.\n\n    Args:\n        gradients: PyTree of gradients to aggregate.\n        reduce_type: Type of reduction (\"mean\" or \"sum\").\n        axis_name: The axis name for the parallel reduction.\n\n    Returns:\n        The aggregated gradients.\n\n    Raises:\n        ValueError: If reduce_type is not \"mean\" or \"sum\".\n    \"\"\"\n</code></pre>"},{"location":"training/data_parallel/#replicate_params","title":"replicate_params","text":"<p>Replicate parameters across all devices.</p> <pre><code>def replicate_params(\n    self,\n    params: Any,\n    mesh: Mesh,\n) -&gt; Any:\n    \"\"\"Replicate parameters across all devices.\n\n    Args:\n        params: PyTree of parameters to replicate.\n        mesh: The device mesh to use.\n\n    Returns:\n        The replicated parameters.\n    \"\"\"\n</code></pre>"},{"location":"training/data_parallel/#static-methods","title":"Static Methods","text":"<p>All instance methods have static equivalents with <code>_static</code> suffix:</p> <ul> <li><code>create_data_parallel_sharding_static(mesh, data_axis=\"data\")</code></li> <li><code>shard_batch_static(batch, sharding)</code></li> <li><code>all_reduce_gradients_static(gradients, reduce_type=\"mean\", axis_name=\"batch\")</code></li> </ul>"},{"location":"training/data_parallel/#usage-examples","title":"Usage Examples","text":""},{"location":"training/data_parallel/#basic-data-parallel-training","title":"Basic Data Parallel Training","text":"<pre><code>from artifex.generative_models.training.distributed import (\n    DeviceMeshManager,\n    DataParallel,\n)\nimport jax.numpy as jnp\n\n# Create mesh and data parallel utilities\nmanager = DeviceMeshManager()\nmesh = manager.create_data_parallel_mesh()\ndp = DataParallel()\n\n# Create sharding for batch data\nsharding = dp.create_data_parallel_sharding(mesh)\n\n# Shard batch data\nbatch = {\"inputs\": jnp.ones((32, 784)), \"targets\": jnp.zeros((32,))}\nsharded_batch = dp.shard_batch(batch, sharding)\n</code></pre>"},{"location":"training/data_parallel/#sharding-model-state","title":"Sharding Model State","text":"<pre><code># Replicate model parameters across all devices (default for data parallelism)\nsharded_state = dp.shard_model_state(model_state, mesh, param_sharding=\"replicate\")\n\n# Or shard parameters across devices (for model parallelism)\nsharded_state = dp.shard_model_state(model_state, mesh, param_sharding=\"shard\")\n</code></pre>"},{"location":"training/data_parallel/#gradient-aggregation","title":"Gradient Aggregation","text":"<pre><code>import jax\n\n# Inside a pmap context, aggregate gradients\n@jax.pmap\ndef train_step(params, batch):\n    def loss_fn(p):\n        return compute_loss(p, batch)\n\n    loss, grads = jax.value_and_grad(loss_fn)(params)\n\n    # Average gradients across devices\n    grads = DataParallel.all_reduce_gradients_static(grads, reduce_type=\"mean\")\n\n    return grads, loss\n</code></pre>"},{"location":"training/data_parallel/#using-static-methods","title":"Using Static Methods","text":"<pre><code># Static methods don't require instantiation\nfrom artifex.generative_models.training.distributed import DataParallel\n\nmesh = manager.create_device_mesh({\"data\": 4})\nsharding = DataParallel.create_data_parallel_sharding_static(mesh)\nsharded_batch = DataParallel.shard_batch_static(batch, sharding)\n</code></pre>"},{"location":"training/data_parallel/#complete-training-example","title":"Complete Training Example","text":"<pre><code>from artifex.generative_models.training.distributed import (\n    DeviceMeshManager,\n    DataParallel,\n)\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\n# Setup\nmanager = DeviceMeshManager()\nmesh = manager.create_data_parallel_mesh()\ndp = DataParallel()\n\n# Create model and optimizer (wrt=nnx.Param required in NNX 0.11.0+)\nmodel = MyModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n\n# Create shardings\ndata_sharding = dp.create_data_parallel_sharding(mesh)\n\n# Replicate model state\nmodel_state = nnx.state(model)\nsharded_model_state = dp.shard_model_state(model_state, mesh)\n\n# Training loop\nfor batch in dataloader:\n    # Shard batch\n    sharded_batch = dp.shard_batch(batch, data_sharding)\n\n    # Compute gradients and update\n    # (In practice, wrap this in @jax.jit for performance)\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n\n    # Update model (NNX 0.11.0+ API)\n    optimizer.update(model, grads)\n</code></pre>"},{"location":"training/data_parallel/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1 (<code>DataParallel</code>)</li> <li>Instance Methods: 5</li> <li>Static Methods: 3</li> </ul>"},{"location":"training/device_placement/","title":"Device Placement","text":"<p>Module: <code>artifex.generative_models.training.distributed.device_placement</code></p> <p>Source: <code>src/artifex/generative_models/training/distributed/device_placement.py</code></p>"},{"location":"training/device_placement/#overview","title":"Overview","text":"<p>The <code>DevicePlacement</code> module provides utilities for explicit device placement of JAX arrays and PyTrees, enabling efficient data distribution across accelerators. It includes hardware-aware batch size recommendations based on JAX performance guidelines.</p>"},{"location":"training/device_placement/#enums","title":"Enums","text":""},{"location":"training/device_placement/#hardwaretype","title":"HardwareType","text":"<p>Enumeration of supported hardware types for batch size recommendations.</p> <pre><code>class HardwareType(Enum):\n    \"\"\"Enumeration of supported hardware types.\"\"\"\n    TPU_V5E = \"tpu_v5e\"\n    TPU_V5P = \"tpu_v5p\"\n    TPU_V4 = \"tpu_v4\"\n    H100 = \"h100\"\n    A100 = \"a100\"\n    V100 = \"v100\"\n    CPU = \"cpu\"\n    UNKNOWN = \"unknown\"\n</code></pre>"},{"location":"training/device_placement/#classes","title":"Classes","text":""},{"location":"training/device_placement/#batchsizerecommendation","title":"BatchSizeRecommendation","text":"<p>Hardware-specific batch size recommendations dataclass.</p> <pre><code>@dataclass(frozen=True)\nclass BatchSizeRecommendation:\n    \"\"\"Hardware-specific batch size recommendations.\n\n    Attributes:\n        min_batch_size: Minimum batch size for reasonable efficiency.\n        optimal_batch_size: Optimal batch size for peak throughput.\n        critical_batch_size: Critical batch size for reaching roofline (per JAX guide).\n        max_memory_batch_size: Maximum batch size before OOM (estimate).\n        notes: Additional notes about the recommendation.\n    \"\"\"\n    min_batch_size: int\n    optimal_batch_size: int\n    critical_batch_size: int\n    max_memory_batch_size: int | None = None\n    notes: str = \"\"\n</code></pre>"},{"location":"training/device_placement/#hardware-specific-values","title":"Hardware-Specific Values","text":"Hardware Min Batch Optimal Critical Notes TPU v5e 64 256 240 Critical batch size for reaching roofline TPU v5p 128 512 480 Higher throughput, needs larger batches TPU v4 64 256 192 Similar to v5e, slightly lower critical H100 64 320 298 Critical batch size for roofline A100 32 256 240 For 80GB variant V100 16 128 96 Memory-limited on 16GB variant CPU 1 32 16 Memory-bandwidth bound"},{"location":"training/device_placement/#deviceplacement","title":"DevicePlacement","text":"<p>Utility class for explicit device placement of JAX arrays.</p> <pre><code>class DevicePlacement:\n    \"\"\"Utility class for explicit device placement of JAX arrays.\n\n    This class provides methods for placing arrays on specific devices,\n    distributing batches across devices using sharding, and providing\n    hardware-aware batch size recommendations.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, default_device: Any | None = None) -&gt; None:\n    \"\"\"Initialize DevicePlacement.\n\n    Args:\n        default_device: Default device to use when none is specified.\n            If None, uses jax.devices()[0].\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#methods","title":"Methods","text":""},{"location":"training/device_placement/#place_on_device","title":"place_on_device","text":"<p>Place data on a specific device.</p> <pre><code>def place_on_device(\n    self,\n    data: Any,\n    device: Any | None = None,\n) -&gt; Any:\n    \"\"\"Place data on a specific device.\n\n    Args:\n        data: PyTree of JAX arrays to place on device.\n        device: Target device. If None, uses the default device.\n\n    Returns:\n        PyTree with arrays placed on the specified device.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#distribute_batch","title":"distribute_batch","text":"<p>Distribute data across devices using sharding.</p> <pre><code>def distribute_batch(\n    self,\n    data: Any,\n    sharding: Sharding,\n) -&gt; Any:\n    \"\"\"Distribute data across devices using the specified sharding.\n\n    Args:\n        data: PyTree of JAX arrays to distribute.\n        sharding: JAX Sharding specification.\n\n    Returns:\n        PyTree with arrays distributed according to the sharding.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#replicate_across_devices","title":"replicate_across_devices","text":"<p>Replicate data across all devices.</p> <pre><code>def replicate_across_devices(\n    self,\n    data: Any,\n    devices: list[Any] | None = None,\n) -&gt; Any:\n    \"\"\"Replicate data across all specified devices.\n\n    Args:\n        data: PyTree of JAX arrays to replicate.\n        devices: List of devices to replicate to. If None, uses all devices.\n\n    Returns:\n        PyTree with arrays replicated across devices.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#shard_batch_dim","title":"shard_batch_dim","text":"<p>Shard data along the batch dimension.</p> <pre><code>def shard_batch_dim(\n    self,\n    data: Any,\n    mesh: Mesh,\n    batch_axis: int = 0,\n    mesh_axis: str = \"data\",\n) -&gt; Any:\n    \"\"\"Shard data along the batch dimension.\n\n    This is the most common sharding pattern for data-parallel training,\n    where each device processes a slice of the batch.\n\n    Args:\n        data: PyTree of JAX arrays to shard.\n        mesh: Device mesh to shard across.\n        batch_axis: The axis index representing the batch dimension.\n        mesh_axis: The mesh axis name to shard along.\n\n    Returns:\n        PyTree with arrays sharded along the batch dimension.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#prefetch_to_device","title":"prefetch_to_device","text":"<p>Create a prefetching wrapper for async data placement.</p> <pre><code>def prefetch_to_device(\n    self,\n    data_iterator: Iterator[Any],\n    device: Any | None = None,\n    buffer_size: int = 2,\n) -&gt; Iterator[Any]:\n    \"\"\"Create a prefetching wrapper that places data on device asynchronously.\n\n    This enables overlapping data transfer with computation for improved\n    throughput in training loops.\n\n    Args:\n        data_iterator: Iterator yielding PyTrees of data.\n        device: Target device for prefetching.\n        buffer_size: Number of batches to prefetch.\n\n    Returns:\n        Iterator that yields device-placed data.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#get_batch_size_recommendation","title":"get_batch_size_recommendation","text":"<p>Get hardware-specific batch size recommendations.</p> <pre><code>def get_batch_size_recommendation(\n    self,\n    hardware_type: HardwareType | None = None,\n) -&gt; BatchSizeRecommendation:\n    \"\"\"Get batch size recommendation for the current hardware.\n\n    Args:\n        hardware_type: Override hardware type. If None, uses detected type.\n\n    Returns:\n        BatchSizeRecommendation with hardware-specific values.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#validate_batch_size","title":"validate_batch_size","text":"<p>Validate batch size against hardware recommendations.</p> <pre><code>def validate_batch_size(\n    self,\n    batch_size: int,\n    warn_suboptimal: bool = True,\n) -&gt; tuple[bool, str]:\n    \"\"\"Validate batch size against hardware recommendations.\n\n    Args:\n        batch_size: The batch size to validate.\n        warn_suboptimal: Whether to warn for suboptimal (but valid) sizes.\n\n    Returns:\n        Tuple of (is_valid, message).\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#get_device_info","title":"get_device_info","text":"<p>Get information about available devices.</p> <pre><code>def get_device_info(self) -&gt; dict[str, Any]:\n    \"\"\"Get information about available devices.\n\n    Returns:\n        Dictionary containing device information including:\n        - num_devices: Number of available devices\n        - hardware_type: Detected hardware type\n        - platforms: List of unique platforms\n        - device_kinds: List of device kinds\n        - devices: Detailed list of device info\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#properties","title":"Properties","text":"<ul> <li><code>hardware_type: HardwareType</code> - The detected hardware type</li> <li><code>num_devices: int</code> - Number of available devices</li> </ul>"},{"location":"training/device_placement/#convenience-functions","title":"Convenience Functions","text":""},{"location":"training/device_placement/#place_on_device_1","title":"place_on_device","text":"<pre><code>def place_on_device(data: Any, device: Any | None = None) -&gt; Any:\n    \"\"\"Convenience function for placing data on a device.\n\n    Args:\n        data: PyTree of JAX arrays.\n        device: Target device. If None, uses first available device.\n\n    Returns:\n        PyTree with arrays on the specified device.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#distribute_batch_1","title":"distribute_batch","text":"<pre><code>def distribute_batch(data: Any, sharding: Sharding) -&gt; Any:\n    \"\"\"Convenience function for distributing data across devices.\n\n    Args:\n        data: PyTree of JAX arrays.\n        sharding: JAX Sharding specification.\n\n    Returns:\n        PyTree with arrays distributed according to sharding.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#get_batch_size_recommendation_1","title":"get_batch_size_recommendation","text":"<pre><code>def get_batch_size_recommendation(\n    hardware_type: HardwareType | None = None,\n) -&gt; BatchSizeRecommendation:\n    \"\"\"Get batch size recommendation for current or specified hardware.\n\n    Args:\n        hardware_type: Hardware type to get recommendation for.\n\n    Returns:\n        BatchSizeRecommendation with hardware-specific values.\n    \"\"\"\n</code></pre>"},{"location":"training/device_placement/#usage-examples","title":"Usage Examples","text":""},{"location":"training/device_placement/#basic-device-placement","title":"Basic Device Placement","text":"<pre><code>from artifex.generative_models.training.distributed import (\n    DevicePlacement,\n    place_on_device,\n)\nimport jax.numpy as jnp\n\n# Create placement utility\nplacement = DevicePlacement()\nprint(f\"Detected hardware: {placement.hardware_type}\")\nprint(f\"Available devices: {placement.num_devices}\")\n\n# Place data on default device\ndata = jnp.ones((32, 784))\nplaced_data = placement.place_on_device(data)\n\n# Or use convenience function\nplaced_data = place_on_device(data)\n</code></pre>"},{"location":"training/device_placement/#batch-size-validation","title":"Batch Size Validation","text":"<pre><code>from artifex.generative_models.training.distributed import (\n    DevicePlacement,\n    HardwareType,\n    get_batch_size_recommendation,\n)\n\n# Get recommendation for current hardware\nplacement = DevicePlacement()\nrec = placement.get_batch_size_recommendation()\nprint(f\"Optimal batch size: {rec.optimal_batch_size}\")\nprint(f\"Critical batch size: {rec.critical_batch_size}\")\n\n# Get recommendation for specific hardware\nh100_rec = get_batch_size_recommendation(HardwareType.H100)\nprint(f\"H100 critical batch: {h100_rec.critical_batch_size}\")  # 298\n\n# Validate batch size\nis_valid, message = placement.validate_batch_size(256)\nprint(f\"Valid: {is_valid}, Message: {message}\")\n</code></pre>"},{"location":"training/device_placement/#distributing-batches-with-sharding","title":"Distributing Batches with Sharding","text":"<pre><code>from artifex.generative_models.training.distributed import (\n    DevicePlacement,\n    distribute_batch,\n)\nfrom jax.sharding import Mesh, NamedSharding, PartitionSpec\nimport jax\nimport numpy as np\n\n# Create device mesh\ndevices = jax.devices()\nmesh = Mesh(np.array(devices), axis_names=(\"data\",))\n\n# Create sharding for batch dimension\ndata_sharding = NamedSharding(mesh, PartitionSpec(\"data\", None))\n\n# Distribute data\nplacement = DevicePlacement()\nbatch = {\"images\": jnp.ones((8, 28, 28, 3)), \"labels\": jnp.zeros((8,))}\ndistributed = placement.distribute_batch(batch, data_sharding)\n\n# Or use convenience function\ndistributed = distribute_batch(batch, data_sharding)\n</code></pre>"},{"location":"training/device_placement/#sharding-along-batch-dimension","title":"Sharding Along Batch Dimension","text":"<pre><code>from artifex.generative_models.training.distributed import DevicePlacement\nfrom jax.sharding import Mesh\nimport jax\nimport numpy as np\n\nplacement = DevicePlacement()\n\n# Create mesh\ndevices = jax.devices()\nmesh = Mesh(np.array(devices), axis_names=(\"data\",))\n\n# Shard batch along first dimension\nbatch = {\n    \"images\": jnp.ones((16, 224, 224, 3)),\n    \"labels\": jnp.zeros((16,), dtype=jnp.int32),\n}\nsharded_batch = placement.shard_batch_dim(batch, mesh)\n</code></pre>"},{"location":"training/device_placement/#prefetching-data-to-device","title":"Prefetching Data to Device","text":"<pre><code>from artifex.generative_models.training.distributed import DevicePlacement\n\nplacement = DevicePlacement()\n\n# Create a data iterator\ndef data_generator():\n    for i in range(100):\n        yield {\"batch\": jnp.ones((32, 784)) * i}\n\n# Prefetch data to GPU with buffer of 2 batches\nprefetched = placement.prefetch_to_device(\n    data_generator(),\n    buffer_size=2,\n)\n\n# Training loop with prefetched data\nfor batch in prefetched:\n    # Data is already on GPU when we receive it\n    process_batch(batch)\n</code></pre>"},{"location":"training/device_placement/#replicating-model-weights","title":"Replicating Model Weights","text":"<pre><code>from artifex.generative_models.training.distributed import DevicePlacement\n\nplacement = DevicePlacement()\n\n# Model weights to replicate\nweights = {\n    \"layer1\": jnp.ones((784, 256)),\n    \"layer2\": jnp.ones((256, 10)),\n}\n\n# Replicate across all devices\nreplicated_weights = placement.replicate_across_devices(weights)\n\n# Or replicate to specific devices\ngpu_devices = jax.devices(\"gpu\")[:2]\nreplicated_weights = placement.replicate_across_devices(weights, devices=gpu_devices)\n</code></pre>"},{"location":"training/device_placement/#getting-device-information","title":"Getting Device Information","text":"<pre><code>from artifex.generative_models.training.distributed import DevicePlacement\n\nplacement = DevicePlacement()\ninfo = placement.get_device_info()\n\nprint(f\"Number of devices: {info['num_devices']}\")\nprint(f\"Hardware type: {info['hardware_type']}\")\nprint(f\"Platforms: {info['platforms']}\")\n\nfor device in info['devices']:\n    print(f\"  Device {device['id']}: {device['platform']} ({device['device_kind']})\")\n</code></pre>"},{"location":"training/device_placement/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2 (<code>DevicePlacement</code>, <code>BatchSizeRecommendation</code>)</li> <li>Enums: 1 (<code>HardwareType</code>)</li> <li>Convenience Functions: 3</li> <li>Instance Methods: 7</li> </ul>"},{"location":"training/diffusion_trainer/","title":"Diffusion Trainer","text":"<p>Module: <code>artifex.generative_models.training.trainers.diffusion_trainer</code></p> <p>The Diffusion Trainer provides state-of-the-art training utilities for diffusion models, including multiple prediction types, advanced timestep sampling strategies, loss weighting schemes, and EMA model updates.</p>"},{"location":"training/diffusion_trainer/#overview","title":"Overview","text":"<p>Modern diffusion model training benefits from several advanced techniques. The Diffusion Trainer provides:</p> <ul> <li>Prediction Types: Epsilon, v-prediction, and x-prediction</li> <li>Timestep Sampling: Uniform, logit-normal, and mode-seeking strategies</li> <li>Loss Weighting: Uniform, SNR, min-SNR, and EDM-style weighting</li> <li>EMA Updates: Exponential moving average for stable inference</li> </ul>"},{"location":"training/diffusion_trainer/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training.trainers import (\n    DiffusionTrainer,\n    DiffusionTrainingConfig,\n)\nfrom artifex.generative_models.core.noise_schedules import CosineNoiseSchedule\nfrom flax import nnx\nimport optax\nimport jax\n\n# Create model and optimizer\nmodel = create_diffusion_model(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\nnoise_schedule = CosineNoiseSchedule(num_timesteps=1000)\n\n# Configure diffusion training with SOTA techniques\nconfig = DiffusionTrainingConfig(\n    prediction_type=\"v_prediction\",\n    timestep_sampling=\"logit_normal\",\n    loss_weighting=\"min_snr\",\n    snr_gamma=5.0,\n)\n\ntrainer = DiffusionTrainer(model, optimizer, noise_schedule, config)\n\n# Training loop\nkey = jax.random.key(0)\n\nfor step, batch in enumerate(train_loader):\n    key, subkey = jax.random.split(key)\n    loss, metrics = trainer.train_step(batch, subkey)\n\n    if step % 100 == 0:\n        print(f\"Step {step}: loss={metrics['loss']:.4f}\")\n</code></pre>"},{"location":"training/diffusion_trainer/#configuration","title":"Configuration","text":""},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig","title":"artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig  <code>dataclass</code>","text":"<pre><code>DiffusionTrainingConfig(\n    prediction_type: Literal[\n        \"epsilon\", \"v_prediction\", \"x_start\"\n    ] = \"epsilon\",\n    timestep_sampling: Literal[\n        \"uniform\", \"logit_normal\", \"mode\"\n    ] = \"uniform\",\n    loss_weighting: Literal[\n        \"uniform\", \"snr\", \"min_snr\", \"edm\"\n    ] = \"uniform\",\n    snr_gamma: float = 5.0,\n    logit_normal_loc: float = -0.5,\n    logit_normal_scale: float = 1.0,\n    ema_decay: float = 0.9999,\n    ema_update_every: int = 10,\n)\n</code></pre> <p>Configuration for diffusion model training.</p> <p>Attributes:</p> Name Type Description <code>prediction_type</code> <code>Literal['epsilon', 'v_prediction', 'x_start']</code> <p>What the model predicts. - \"epsilon\": Predicts the added noise - \"v_prediction\": Predicts v = sqrt(alpha)*noise - sqrt(1-alpha)*x0 - \"x_start\": Predicts the original clean data</p> <code>timestep_sampling</code> <code>Literal['uniform', 'logit_normal', 'mode']</code> <p>How to sample timesteps during training. - \"uniform\": Uniform random sampling - \"logit_normal\": Logit-normal distribution (favors middle timesteps) - \"mode\": Mode-seeking (favors high-noise timesteps)</p> <code>loss_weighting</code> <code>Literal['uniform', 'snr', 'min_snr', 'edm']</code> <p>How to weight the loss across timesteps. - \"uniform\": Equal weighting - \"snr\": Weight by signal-to-noise ratio - \"min_snr\": Min-SNR-gamma weighting (3.4x faster convergence) - \"edm\": EDM-style weighting</p> <code>snr_gamma</code> <code>float</code> <p>Gamma parameter for min-SNR weighting (5.0 typical).</p> <code>logit_normal_loc</code> <code>float</code> <p>Location parameter for logit-normal sampling.</p> <code>logit_normal_scale</code> <code>float</code> <p>Scale parameter for logit-normal sampling.</p> <code>ema_decay</code> <code>float</code> <p>EMA decay rate for model weights.</p> <code>ema_update_every</code> <code>int</code> <p>Update EMA every N training steps.</p>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig.prediction_type","title":"prediction_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prediction_type: Literal[\n    \"epsilon\", \"v_prediction\", \"x_start\"\n] = \"epsilon\"\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig.timestep_sampling","title":"timestep_sampling  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timestep_sampling: Literal[\n    \"uniform\", \"logit_normal\", \"mode\"\n] = \"uniform\"\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig.loss_weighting","title":"loss_weighting  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>loss_weighting: Literal[\n    \"uniform\", \"snr\", \"min_snr\", \"edm\"\n] = \"uniform\"\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig.snr_gamma","title":"snr_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>snr_gamma: float = 5.0\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig.logit_normal_loc","title":"logit_normal_loc  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logit_normal_loc: float = -0.5\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig.logit_normal_scale","title":"logit_normal_scale  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logit_normal_scale: float = 1.0\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig.ema_decay","title":"ema_decay  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ema_decay: float = 0.9999\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainingConfig.ema_update_every","title":"ema_update_every  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ema_update_every: int = 10\n</code></pre>"},{"location":"training/diffusion_trainer/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>prediction_type</code> <code>str</code> <code>\"epsilon\"</code> Model prediction: <code>\"epsilon\"</code>, <code>\"v_prediction\"</code>, <code>\"x_start\"</code> <code>timestep_sampling</code> <code>str</code> <code>\"uniform\"</code> Timestep distribution: <code>\"uniform\"</code>, <code>\"logit_normal\"</code>, <code>\"mode\"</code> <code>loss_weighting</code> <code>str</code> <code>\"uniform\"</code> Loss weighting: <code>\"uniform\"</code>, <code>\"snr\"</code>, <code>\"min_snr\"</code>, <code>\"edm\"</code> <code>snr_gamma</code> <code>float</code> <code>5.0</code> Gamma for min-SNR weighting <code>logit_normal_loc</code> <code>float</code> <code>-0.5</code> Logit-normal location parameter <code>logit_normal_scale</code> <code>float</code> <code>1.0</code> Logit-normal scale parameter <code>ema_decay</code> <code>float</code> <code>0.9999</code> EMA decay rate <code>ema_update_every</code> <code>int</code> <code>10</code> Steps between EMA updates"},{"location":"training/diffusion_trainer/#prediction-types","title":"Prediction Types","text":""},{"location":"training/diffusion_trainer/#epsilon-prediction-ddpm","title":"Epsilon Prediction (DDPM)","text":"<p>The classic approach - model predicts the noise added:</p> <pre><code>config = DiffusionTrainingConfig(prediction_type=\"epsilon\")\n# Target: noise that was added to x_0\n</code></pre>"},{"location":"training/diffusion_trainer/#v-prediction","title":"V-Prediction","text":"<p>Model predicts v = sqrt(alpha) noise - sqrt(1-alpha) x_0:</p> <pre><code>config = DiffusionTrainingConfig(prediction_type=\"v_prediction\")\n# Provides more consistent gradients across timesteps\n# Used in Stable Diffusion 3 and Imagen Video\n</code></pre> <p>V-prediction often leads to faster convergence and better sample quality.</p>"},{"location":"training/diffusion_trainer/#x-start-prediction","title":"X-Start Prediction","text":"<p>Model directly predicts the clean data:</p> <pre><code>config = DiffusionTrainingConfig(prediction_type=\"x_start\")\n# Target: original clean data x_0\n</code></pre>"},{"location":"training/diffusion_trainer/#timestep-sampling-strategies","title":"Timestep Sampling Strategies","text":""},{"location":"training/diffusion_trainer/#uniform-sampling","title":"Uniform Sampling","text":"<p>Standard uniform sampling over all timesteps:</p> <pre><code>config = DiffusionTrainingConfig(timestep_sampling=\"uniform\")\n# Equal probability for all timesteps\n</code></pre>"},{"location":"training/diffusion_trainer/#logit-normal-sampling","title":"Logit-Normal Sampling","text":"<p>Favors middle timesteps where learning is most effective:</p> <pre><code>config = DiffusionTrainingConfig(\n    timestep_sampling=\"logit_normal\",\n    logit_normal_loc=-0.5,\n    logit_normal_scale=1.0,\n)\n# Used in Stable Diffusion 3 for improved convergence\n</code></pre>"},{"location":"training/diffusion_trainer/#mode-seeking-sampling","title":"Mode-Seeking Sampling","text":"<p>Favors high-noise timesteps for improved generation quality:</p> <pre><code>config = DiffusionTrainingConfig(timestep_sampling=\"mode\")\n# Quadratic bias toward larger timesteps\n</code></pre>"},{"location":"training/diffusion_trainer/#loss-weighting-schemes","title":"Loss Weighting Schemes","text":""},{"location":"training/diffusion_trainer/#uniform-weighting","title":"Uniform Weighting","text":"<p>No weighting - all timesteps contribute equally:</p> <pre><code>config = DiffusionTrainingConfig(loss_weighting=\"uniform\")\n</code></pre>"},{"location":"training/diffusion_trainer/#snr-weighting","title":"SNR Weighting","text":"<p>Weight by signal-to-noise ratio:</p> <pre><code>config = DiffusionTrainingConfig(loss_weighting=\"snr\")\n# weight = alpha / (1 - alpha)\n</code></pre>"},{"location":"training/diffusion_trainer/#min-snr-gamma-weighting","title":"Min-SNR-Gamma Weighting","text":"<p>Clips high SNR weights for 3.4x faster convergence:</p> <pre><code>config = DiffusionTrainingConfig(\n    loss_weighting=\"min_snr\",\n    snr_gamma=5.0,\n)\n# weight = min(SNR, gamma) / SNR\n# Down-weights low-noise timesteps where SNR is high\n</code></pre> <p>Min-SNR-gamma is the recommended weighting scheme for most use cases.</p>"},{"location":"training/diffusion_trainer/#edm-weighting","title":"EDM Weighting","text":"<p>EDM-style weighting based on sigma:</p> <pre><code>config = DiffusionTrainingConfig(loss_weighting=\"edm\")\n# weight = 1 / (sigma^2 + 1)\n</code></pre>"},{"location":"training/diffusion_trainer/#ema-exponential-moving-average","title":"EMA (Exponential Moving Average)","text":"<p>Maintain an EMA of model parameters for stable inference:</p> <pre><code>config = DiffusionTrainingConfig(\n    ema_decay=0.9999,\n    ema_update_every=10,\n)\n\n# After training, get EMA parameters\nema_params = trainer.get_ema_params()\n\n# Apply EMA params to model for inference\nfrom flax import nnx\nnnx.update(model, ema_params)\n</code></pre>"},{"location":"training/diffusion_trainer/#api-reference","title":"API Reference","text":""},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer","title":"artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer","text":"<pre><code>DiffusionTrainer(\n    noise_schedule: NoiseScheduleProtocol,\n    config: DiffusionTrainingConfig | None = None,\n)\n</code></pre> <p>Diffusion model trainer with modern training techniques.</p> <p>This trainer provides a JIT-compatible interface for training diffusion models with state-of-the-art techniques. The train_step method takes model and optimizer as explicit arguments, allowing it to be wrapped with nnx.jit for performance.</p> Features <ul> <li>Multiple prediction types (epsilon, v, x0)</li> <li>Non-uniform timestep sampling (logit-normal, mode-seeking)</li> <li>Loss weighting (SNR, min-SNR, EDM)</li> <li>EMA model updates (call update_ema separately, outside JIT)</li> </ul> <p>Example (non-JIT):     <pre><code>from artifex.generative_models.training.trainers import (\n    DiffusionTrainer,\n    DiffusionTrainingConfig,\n)\n\n# Create trainer with noise schedule and config\nconfig = DiffusionTrainingConfig(\n    prediction_type=\"v_prediction\",\n    timestep_sampling=\"logit_normal\",\n    loss_weighting=\"min_snr\",\n)\ntrainer = DiffusionTrainer(noise_schedule, config)\n\n# Create model and optimizer separately\nmodel = DDPMModel(model_config, rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adamw(1e-4))\n\n# Training loop\nfor batch in data:\n    rng, step_rng = jax.random.split(rng)\n    loss, metrics = trainer.train_step(model, optimizer, batch, step_rng)\n    trainer.update_ema(model)  # EMA updates outside train_step\n</code></pre></p> <p>Example (JIT-compiled):     <pre><code>trainer = DiffusionTrainer(noise_schedule, config)\njit_step = nnx.jit(trainer.train_step)\n\nfor batch in data:\n    rng, step_rng = jax.random.split(rng)\n    loss, metrics = jit_step(model, optimizer, batch, step_rng)\n    trainer.update_ema(model)  # Outside JIT\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>noise_schedule</code> <code>NoiseScheduleProtocol</code> <p>Noise schedule with alphas_cumprod and add_noise.</p> required <code>config</code> <code>DiffusionTrainingConfig | None</code> <p>Diffusion training configuration.</p> <code>None</code>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.noise_schedule","title":"noise_schedule  <code>instance-attribute</code>","text":"<pre><code>noise_schedule = noise_schedule\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or DiffusionTrainingConfig()\n</code></pre>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.sample_timesteps","title":"sample_timesteps","text":"<pre><code>sample_timesteps(batch_size: int, key: Array) -&gt; Array\n</code></pre> <p>Sample timesteps according to configured strategy.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of timesteps to sample.</p> required <code>key</code> <code>Array</code> <p>PRNG key for random sampling.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Integer timesteps array of shape (batch_size,).</p>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.get_loss_weight","title":"get_loss_weight","text":"<pre><code>get_loss_weight(t: Array) -&gt; Array\n</code></pre> <p>Compute loss weight for given timesteps.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>Array</code> <p>Integer timesteps array.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Loss weights for each timestep.</p>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.compute_target","title":"compute_target","text":"<pre><code>compute_target(x0: Array, noise: Array, t: Array) -&gt; Array\n</code></pre> <p>Compute prediction target based on prediction type.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Array</code> <p>Original clean data.</p> required <code>noise</code> <code>Array</code> <p>Added noise.</p> required <code>t</code> <code>Array</code> <p>Timesteps.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Target for the model prediction.</p>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    model: Module, batch: dict[str, Any], key: Array\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Compute diffusion training loss.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Diffusion model to evaluate.</p> required <code>batch</code> <code>dict[str, Any]</code> <p>Batch dictionary with \"image\" or \"data\" key.</p> required <code>key</code> <code>Array</code> <p>PRNG key for sampling noise and timesteps.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (total_loss, metrics_dict).</p>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.train_step","title":"train_step","text":"<pre><code>train_step(\n    model: Module,\n    optimizer: Optimizer,\n    batch: dict[str, Any],\n    key: Array,\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Execute a single training step.</p> <p>This method can be wrapped with nnx.jit for performance:     jit_step = nnx.jit(trainer.train_step)     loss, metrics = jit_step(model, optimizer, batch, key)</p> <p>Note: Call update_ema() separately after train_step for EMA updates.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Diffusion model to train.</p> required <code>optimizer</code> <code>Optimizer</code> <p>NNX optimizer for parameter updates.</p> required <code>batch</code> <code>dict[str, Any]</code> <p>Batch dictionary with \"image\" or \"data\" key.</p> required <code>key</code> <code>Array</code> <p>PRNG key for sampling.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.update_ema","title":"update_ema","text":"<pre><code>update_ema(model: Module) -&gt; None\n</code></pre> <p>Update EMA parameters.</p> <p>Call this method separately after train_step, outside of JIT.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model whose parameters to use for EMA update.</p> required"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.get_ema_params","title":"get_ema_params","text":"<pre><code>get_ema_params(model: Module) -&gt; Any\n</code></pre> <p>Get EMA parameters for inference.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to get fallback state from if EMA not initialized.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>EMA parameters, or current model state if EMA not initialized.</p>"},{"location":"training/diffusion_trainer/#artifex.generative_models.training.trainers.diffusion_trainer.DiffusionTrainer.create_loss_fn","title":"create_loss_fn","text":"<pre><code>create_loss_fn() -&gt; Callable[\n    [Module, dict[str, Any], Array],\n    tuple[Array, dict[str, Any]],\n]\n</code></pre> <p>Create loss function compatible with base Trainer.</p> <p>This enables integration with the base Trainer for callbacks, checkpointing, logging, and other training infrastructure.</p> <p>Returns:</p> Type Description <code>Callable[[Module, dict[str, Any], Array], tuple[Array, dict[str, Any]]]</code> <p>Function with signature: (model, batch, rng) -&gt; (loss, metrics)</p>"},{"location":"training/diffusion_trainer/#noise-schedule-protocol","title":"Noise Schedule Protocol","text":"<p>The trainer works with any noise schedule implementing the <code>NoiseScheduleProtocol</code>:</p> <pre><code>from typing import Protocol\nimport jax\n\nclass NoiseScheduleProtocol(Protocol):\n    \"\"\"Protocol for noise schedules used by diffusion trainers.\"\"\"\n\n    num_timesteps: int\n    alphas_cumprod: jax.Array\n\n    def add_noise(\n        self,\n        x: jax.Array,\n        noise: jax.Array,\n        t: jax.Array,\n    ) -&gt; jax.Array:\n        \"\"\"Add noise to data at given timesteps.\"\"\"\n        ...\n</code></pre> <p>Artifex provides several noise schedule implementations:</p> <pre><code>from artifex.generative_models.core.noise_schedules import (\n    LinearNoiseSchedule,\n    CosineNoiseSchedule,\n    SquaredCosineNoiseSchedule,\n)\n\n# Linear schedule (DDPM default)\nschedule = LinearNoiseSchedule(num_timesteps=1000)\n\n# Cosine schedule (improved for images)\nschedule = CosineNoiseSchedule(num_timesteps=1000)\n</code></pre>"},{"location":"training/diffusion_trainer/#integration-with-base-trainer","title":"Integration with Base Trainer","text":"<p>Use <code>create_loss_fn()</code> for integration with callbacks and checkpointing:</p> <pre><code>from artifex.generative_models.training import Trainer\nfrom artifex.generative_models.training.trainers import (\n    DiffusionTrainer,\n    DiffusionTrainingConfig,\n)\nfrom artifex.generative_models.training.callbacks import (\n    ModelCheckpoint,\n    CheckpointConfig,\n)\n\n# Create diffusion trainer\ndiff_config = DiffusionTrainingConfig(\n    prediction_type=\"v_prediction\",\n    loss_weighting=\"min_snr\",\n)\ndiff_trainer = DiffusionTrainer(model, optimizer, noise_schedule, diff_config)\n\n# Get loss function for base Trainer\nloss_fn = diff_trainer.create_loss_fn()\n\n# Use with base Trainer for callbacks\ncallbacks = [\n    ModelCheckpoint(CheckpointConfig(dirpath=\"checkpoints\", monitor=\"loss\")),\n]\n</code></pre>"},{"location":"training/diffusion_trainer/#model-requirements","title":"Model Requirements","text":"<p>The Diffusion Trainer expects models with the following interface:</p> <pre><code>class DiffusionModel(nnx.Module):\n    def __call__(\n        self,\n        x_noisy: jax.Array,\n        t: jax.Array,\n    ) -&gt; jax.Array:\n        \"\"\"Predict noise/v/x_0 from noisy input.\n\n        Args:\n            x_noisy: Noisy data, shape (batch, ...).\n            t: Integer timesteps, shape (batch,).\n\n        Returns:\n            Prediction matching prediction_type, shape (batch, ...).\n        \"\"\"\n        ...\n</code></pre>"},{"location":"training/diffusion_trainer/#training-metrics","title":"Training Metrics","text":"Metric Description <code>loss</code> Weighted loss (affected by loss_weighting) <code>loss_unweighted</code> Raw MSE loss without weighting"},{"location":"training/diffusion_trainer/#recommended-configurations","title":"Recommended Configurations","text":""},{"location":"training/diffusion_trainer/#high-quality-image-generation","title":"High-Quality Image Generation","text":"<pre><code>config = DiffusionTrainingConfig(\n    prediction_type=\"v_prediction\",\n    timestep_sampling=\"logit_normal\",\n    loss_weighting=\"min_snr\",\n    snr_gamma=5.0,\n    ema_decay=0.9999,\n)\n</code></pre>"},{"location":"training/diffusion_trainer/#fast-training","title":"Fast Training","text":"<pre><code>config = DiffusionTrainingConfig(\n    prediction_type=\"epsilon\",\n    timestep_sampling=\"uniform\",\n    loss_weighting=\"min_snr\",\n    snr_gamma=5.0,\n)\n</code></pre>"},{"location":"training/diffusion_trainer/#large-models-edm-style","title":"Large Models (EDM-style)","text":"<pre><code>config = DiffusionTrainingConfig(\n    prediction_type=\"epsilon\",\n    timestep_sampling=\"logit_normal\",\n    loss_weighting=\"edm\",\n    ema_decay=0.9999,\n    ema_update_every=1,\n)\n</code></pre>"},{"location":"training/diffusion_trainer/#references","title":"References","text":"<ul> <li>DDPM: Denoising Diffusion Probabilistic Models</li> <li>V-Prediction: Progressive Distillation</li> <li>Min-SNR Weighting</li> <li>EDM: Elucidating the Design Space of Diffusion-Based Generative Models</li> </ul>"},{"location":"training/distributed_metrics/","title":"Distributed Metrics","text":"<p>Module: <code>artifex.generative_models.training.distributed.metrics</code></p> <p>Source: <code>src/artifex/generative_models/training/distributed/metrics.py</code></p>"},{"location":"training/distributed_metrics/#overview","title":"Overview","text":"<p>The <code>DistributedMetrics</code> class provides unified utilities for collecting and aggregating metrics across multiple devices in distributed training settings. It supports both static method usage (stateless) and NNX module instantiation (stateful).</p>"},{"location":"training/distributed_metrics/#classes","title":"Classes","text":""},{"location":"training/distributed_metrics/#distributedmetrics","title":"DistributedMetrics","text":"<p>Unified distributed metrics collection utilities for Artifex, implemented as an NNX Module.</p> <pre><code>class DistributedMetrics(nnx.Module):\n    \"\"\"Unified distributed metrics collection utilities for Artifex.\n\n    This class provides methods for aggregating metrics across multiple devices\n    in a distributed training setting, including mean, sum, and custom reduction\n    operations.\n\n    Supports both static method usage (stateless) and instance method\n    usage (stateful).\n    \"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#constructor","title":"Constructor","text":"<pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize DistributedMetrics module.\"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#methods","title":"Methods","text":""},{"location":"training/distributed_metrics/#all_gather","title":"all_gather","text":"<p>Gather metrics from all devices.</p> <pre><code>def all_gather(\n    self,\n    metrics: dict[str, Any],\n    axis_name: str = \"batch\",\n) -&gt; dict[str, Any]:\n    \"\"\"Gather metrics from all devices.\n\n    Args:\n        metrics: The metrics to gather.\n        axis_name: The name of the axis to gather across.\n\n    Returns:\n        A dictionary of gathered metrics.\n    \"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#reduce_mean","title":"reduce_mean","text":"<p>Compute the mean of metrics across devices.</p> <pre><code>def reduce_mean(\n    self,\n    metrics: dict[str, Any],\n    axis_name: str = \"batch\",\n) -&gt; dict[str, Any]:\n    \"\"\"Compute the mean of metrics across devices.\n\n    Args:\n        metrics: The metrics to reduce.\n        axis_name: The name of the axis to reduce across.\n\n    Returns:\n        A dictionary of mean metrics.\n    \"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#reduce_sum","title":"reduce_sum","text":"<p>Compute the sum of metrics across devices.</p> <pre><code>def reduce_sum(\n    self,\n    metrics: dict[str, Any],\n    axis_name: str = \"batch\",\n) -&gt; dict[str, Any]:\n    \"\"\"Compute the sum of metrics across devices.\n\n    Args:\n        metrics: The metrics to reduce.\n        axis_name: The name of the axis to reduce across.\n\n    Returns:\n        A dictionary of summed metrics.\n    \"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#reduce_max","title":"reduce_max","text":"<p>Compute the maximum of metrics across devices.</p> <pre><code>def reduce_max(\n    self,\n    metrics: dict[str, Any],\n    axis_name: str = \"batch\",\n) -&gt; dict[str, Any]:\n    \"\"\"Compute the maximum of metrics across devices.\n\n    Args:\n        metrics: The metrics to reduce.\n        axis_name: The name of the axis to reduce across.\n\n    Returns:\n        A dictionary of maximum metrics.\n    \"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#reduce_min","title":"reduce_min","text":"<p>Compute the minimum of metrics across devices.</p> <pre><code>def reduce_min(\n    self,\n    metrics: dict[str, Any],\n    axis_name: str = \"batch\",\n) -&gt; dict[str, Any]:\n    \"\"\"Compute the minimum of metrics across devices.\n\n    Args:\n        metrics: The metrics to reduce.\n        axis_name: The name of the axis to reduce across.\n\n    Returns:\n        A dictionary of minimum metrics.\n    \"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#reduce_custom","title":"reduce_custom","text":"<p>Apply custom reduction operations to metrics.</p> <pre><code>def reduce_custom(\n    self,\n    metrics: dict[str, Any],\n    reduce_fn: dict[str, str | None] | None = None,\n    axis_name: str = \"batch\",\n) -&gt; dict[str, Any]:\n    \"\"\"Apply custom reduction operations to metrics.\n\n    Args:\n        metrics: The metrics to reduce.\n        reduce_fn: A dictionary mapping metric names to reduction operations.\n            Each operation should be one of {\"mean\", \"sum\", \"max\", \"min\"}.\n            If None, defaults to \"mean\" for all metrics.\n        axis_name: The name of the axis to reduce across.\n\n    Returns:\n        A dictionary of reduced metrics.\n    \"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#collect_from_devices","title":"collect_from_devices","text":"<p>Collect metrics from all devices (outside pmap context).</p> <pre><code>def collect_from_devices(\n    self,\n    metrics: dict[str, Any],\n) -&gt; dict[str, list[Any] | Any]:\n    \"\"\"Collect metrics from all devices.\n\n    This function should be called outside of a pmapped function to collect\n    metrics from all devices.\n\n    Args:\n        metrics: The metrics from all devices, with the first dimension\n            corresponding to the device axis.\n\n    Returns:\n        A dictionary of metrics, with each value being a list of the values\n        from each device.\n    \"\"\"\n</code></pre>"},{"location":"training/distributed_metrics/#static-methods","title":"Static Methods","text":"<p>All instance methods have static equivalents with <code>_static</code> suffix:</p> <ul> <li><code>all_gather_static(metrics, axis_name=\"batch\")</code></li> <li><code>reduce_mean_static(metrics, axis_name=\"batch\")</code></li> <li><code>reduce_sum_static(metrics, axis_name=\"batch\")</code></li> <li><code>reduce_max_static(metrics, axis_name=\"batch\")</code></li> <li><code>reduce_min_static(metrics, axis_name=\"batch\")</code></li> <li><code>reduce_custom_static(metrics, reduce_fn=None, axis_name=\"batch\")</code></li> <li><code>collect_from_devices_static(metrics)</code></li> </ul>"},{"location":"training/distributed_metrics/#usage-examples","title":"Usage Examples","text":""},{"location":"training/distributed_metrics/#basic-metrics-reduction","title":"Basic Metrics Reduction","text":"<pre><code>from artifex.generative_models.training.distributed import DistributedMetrics\nimport jax\nimport jax.numpy as jnp\n\n# Create instance\ndm = DistributedMetrics()\n\n# Inside a pmap context, reduce metrics across devices\n@jax.pmap\ndef train_step(params, batch):\n    # ... compute loss and gradients ...\n    loss = compute_loss(params, batch)\n\n    # Reduce metrics across devices\n    metrics = {\"loss\": loss, \"accuracy\": acc}\n    metrics = dm.reduce_mean(metrics)\n\n    return params, metrics\n</code></pre>"},{"location":"training/distributed_metrics/#static-method-usage","title":"Static Method Usage","text":"<pre><code>from artifex.generative_models.training.distributed import DistributedMetrics\nimport jax\n\n# Static methods don't require instantiation\n@jax.pmap\ndef train_step(params, batch):\n    loss = compute_loss(params, batch)\n\n    # Use static method\n    metrics = {\"loss\": loss}\n    metrics = DistributedMetrics.reduce_mean_static(metrics)\n\n    return params, metrics\n</code></pre>"},{"location":"training/distributed_metrics/#custom-reduction-operations","title":"Custom Reduction Operations","text":"<pre><code>from artifex.generative_models.training.distributed import DistributedMetrics\n\ndm = DistributedMetrics()\n\n# Define custom reduction per metric\n@jax.pmap\ndef train_step(params, batch):\n    loss = compute_loss(params, batch)\n    batch_size = batch[\"data\"].shape[0]\n    max_grad_norm = compute_grad_norm(grads)\n\n    metrics = {\n        \"loss\": loss,\n        \"total_samples\": batch_size,\n        \"max_grad_norm\": max_grad_norm,\n    }\n\n    # Custom reductions: mean for loss, sum for samples, max for grad norm\n    reduce_ops = {\n        \"loss\": \"mean\",\n        \"total_samples\": \"sum\",\n        \"max_grad_norm\": \"max\",\n    }\n    metrics = dm.reduce_custom(metrics, reduce_fn=reduce_ops)\n\n    return params, metrics\n</code></pre>"},{"location":"training/distributed_metrics/#gathering-metrics-from-all-devices","title":"Gathering Metrics from All Devices","text":"<pre><code>from artifex.generative_models.training.distributed import DistributedMetrics\n\ndm = DistributedMetrics()\n\n@jax.pmap\ndef evaluate_step(params, batch):\n    predictions = model(params, batch[\"data\"])\n\n    # Gather predictions from all devices (not reduce)\n    results = {\"predictions\": predictions}\n    results = dm.all_gather(results)\n\n    return results\n</code></pre>"},{"location":"training/distributed_metrics/#collecting-per-device-metrics","title":"Collecting Per-Device Metrics","text":"<pre><code>from artifex.generative_models.training.distributed import DistributedMetrics\n\ndm = DistributedMetrics()\n\n# After pmap returns metrics with device dimension\nmetrics_per_device = train_step(params, batch)\n# Shape of metrics_per_device[\"loss\"]: (num_devices,)\n\n# Collect into list of per-device values\ncollected = dm.collect_from_devices(metrics_per_device)\n# collected[\"loss\"] is now a list of values, one per device\n\n# Useful for debugging or logging per-device metrics\nfor i, loss in enumerate(collected[\"loss\"]):\n    print(f\"Device {i} loss: {loss}\")\n</code></pre>"},{"location":"training/distributed_metrics/#integration-with-training-loop","title":"Integration with Training Loop","text":"<pre><code>from artifex.generative_models.training.distributed import (\n    DeviceMeshManager,\n    DataParallel,\n    DistributedMetrics,\n)\nimport jax\nimport jax.numpy as jnp\n\n# Setup distributed training\nmanager = DeviceMeshManager()\nmesh = manager.create_data_parallel_mesh()\ndp = DataParallel()\ndm = DistributedMetrics()\n\n# Create shardings\ndata_sharding = dp.create_data_parallel_sharding(mesh)\n\n@jax.jit\ndef train_step(params, optimizer_state, batch):\n    def loss_fn(p):\n        logits = model.apply(p, batch[\"data\"])\n        loss = jnp.mean((logits - batch[\"targets\"]) ** 2)\n        return loss, {\"loss\": loss}\n\n    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n\n    # Aggregate gradients and metrics\n    grads = dp.all_reduce_gradients(grads, reduce_type=\"mean\")\n\n    # Update parameters\n    updates, optimizer_state = optimizer.update(grads, optimizer_state)\n    params = optax.apply_updates(params, updates)\n\n    return params, optimizer_state, metrics\n\n# Training loop\nfor batch in dataloader:\n    # Shard batch\n    sharded_batch = dp.shard_batch(batch, data_sharding)\n\n    # Training step\n    params, optimizer_state, metrics = train_step(\n        params, optimizer_state, sharded_batch\n    )\n\n    print(f\"Loss: {metrics['loss']}\")\n</code></pre>"},{"location":"training/distributed_metrics/#complete-multi-gpu-example","title":"Complete Multi-GPU Example","text":"<pre><code>from artifex.generative_models.training.distributed import (\n    DeviceMeshManager,\n    DataParallel,\n    DistributedMetrics,\n)\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\nimport optax\n\n# Model definition\nclass SimpleModel(nnx.Module):\n    def __init__(self, *, rngs: nnx.Rngs):\n        super().__init__()\n        self.dense1 = nnx.Linear(784, 256, rngs=rngs)\n        self.dense2 = nnx.Linear(256, 10, rngs=rngs)\n\n    def __call__(self, x):\n        x = nnx.relu(self.dense1(x))\n        return self.dense2(x)\n\n# Setup\nmanager = DeviceMeshManager()\nmesh = manager.create_data_parallel_mesh()\ndp = DataParallel()\ndm = DistributedMetrics()\n\n# Create model and optimizer (wrt=nnx.Param required in NNX 0.11.0+)\nmodel = SimpleModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n\n# Training step with metrics aggregation\ndef train_step(model, optimizer, batch):\n    def loss_fn(m):\n        logits = m(batch[\"images\"])\n        loss = jnp.mean(\n            optax.softmax_cross_entropy_with_integer_labels(\n                logits, batch[\"labels\"]\n            )\n        )\n        accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == batch[\"labels\"])\n        return loss, {\"loss\": loss, \"accuracy\": accuracy}\n\n    (loss, metrics), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n\n    return metrics\n\n# Training loop\ndata_sharding = dp.create_data_parallel_sharding(mesh)\n\nfor batch in dataloader:\n    sharded_batch = dp.shard_batch(batch, data_sharding)\n    metrics = train_step(model, optimizer, sharded_batch)\n    print(f\"Loss: {metrics['loss']:.4f}, Accuracy: {metrics['accuracy']:.4f}\")\n</code></pre>"},{"location":"training/distributed_metrics/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1 (<code>DistributedMetrics</code>)</li> <li>Instance Methods: 7</li> <li>Static Methods: 7</li> </ul>"},{"location":"training/dpo/","title":"DPO Trainer","text":"<p>Module: <code>artifex.generative_models.training.rl.dpo</code></p> <p>The DPO (Direct Preference Optimization) Trainer enables learning from preference pairs without requiring an explicit reward model or RL optimization loop.</p>"},{"location":"training/dpo/#overview","title":"Overview","text":"<p>DPO directly optimizes the policy to prefer chosen responses over rejected ones:</p> <ul> <li>No Reward Model: Learns directly from preferences</li> <li>Stable Training: Uses supervised-learning-style updates</li> <li>SimPO Support: Reference-free variant for simpler setup</li> <li>Label Smoothing: Robustness to noisy preferences</li> </ul>"},{"location":"training/dpo/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training import DPOConfig, DPOTrainer\nfrom flax import nnx\nimport optax\n\n# Create policy model\nmodel = PolicyModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-5), wrt=nnx.Param)\n\n# Configure DPO\nconfig = DPOConfig(\n    beta=0.1,\n    label_smoothing=0.0,\n    reference_free=False,\n)\n\ntrainer = DPOTrainer(model, optimizer, config)\n\n# Training with preference pairs\nbatch = {\n    \"chosen_log_probs\": chosen_log_probs,\n    \"rejected_log_probs\": rejected_log_probs,\n    \"ref_chosen_log_probs\": ref_chosen,\n    \"ref_rejected_log_probs\": ref_rejected,\n}\nmetrics = trainer.train_step(batch)\n</code></pre>"},{"location":"training/dpo/#configuration","title":"Configuration","text":""},{"location":"training/dpo/#artifex.generative_models.training.rl.configs.DPOConfig","title":"artifex.generative_models.training.rl.configs.DPOConfig  <code>dataclass</code>","text":"<pre><code>DPOConfig(\n    beta: float = 0.1,\n    label_smoothing: float = 0.0,\n    reference_free: bool = False,\n)\n</code></pre> <p>Configuration for Direct Preference Optimization.</p> <p>DPO enables preference learning without an explicit reward model. SimPO mode (reference_free=True) eliminates the need for a reference model.</p> <p>Attributes:</p> Name Type Description <code>beta</code> <code>float</code> <p>Reward scaling parameter. Higher values = stronger preference. Default 0.1.</p> <code>label_smoothing</code> <code>float</code> <p>Label smoothing for preference loss. Default 0.0.</p> <code>reference_free</code> <code>bool</code> <p>Whether to use SimPO-style reference-free training. When True, no reference model is needed. Default False.</p>"},{"location":"training/dpo/#artifex.generative_models.training.rl.configs.DPOConfig.beta","title":"beta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>beta: float = 0.1\n</code></pre>"},{"location":"training/dpo/#artifex.generative_models.training.rl.configs.DPOConfig.label_smoothing","title":"label_smoothing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>label_smoothing: float = 0.0\n</code></pre>"},{"location":"training/dpo/#artifex.generative_models.training.rl.configs.DPOConfig.reference_free","title":"reference_free  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reference_free: bool = False\n</code></pre>"},{"location":"training/dpo/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>beta</code> <code>float</code> <code>0.1</code> Temperature parameter for reward scaling <code>label_smoothing</code> <code>float</code> <code>0.0</code> Label smoothing for robustness <code>reference_free</code> <code>bool</code> <code>False</code> Use SimPO (reference-free) mode"},{"location":"training/dpo/#algorithm","title":"Algorithm","text":""},{"location":"training/dpo/#standard-dpo","title":"Standard DPO","text":"<p>DPO optimizes the Bradley-Terry preference model:</p> \\[\\mathcal{L}_{DPO} = -\\log \\sigma\\left(\\beta \\left[ \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right]\\right)\\] <p>Where:</p> <ul> <li>\\(y_w\\) is the preferred (chosen) response</li> <li>\\(y_l\\) is the rejected response</li> <li>\\(\\pi_{ref}\\) is the reference policy (frozen)</li> <li>\\(\\beta\\) controls the implicit reward scaling</li> </ul>"},{"location":"training/dpo/#simpo-reference-free","title":"SimPO (Reference-Free)","text":"<p>SimPO eliminates the reference model by using length-normalized log probabilities:</p> \\[\\mathcal{L}_{SimPO} = -\\log \\sigma\\left(\\beta \\left[ \\frac{\\log \\pi_\\theta(y_w|x)}{|y_w|} - \\frac{\\log \\pi_\\theta(y_l|x)}{|y_l|} \\right]\\right)\\] <p>Enable with <code>reference_free=True</code>:</p> <pre><code>config = DPOConfig(\n    beta=0.1,\n    reference_free=True,  # SimPO mode\n)\n</code></pre>"},{"location":"training/dpo/#label-smoothing","title":"Label Smoothing","text":"<p>For robustness to noisy preference labels:</p> <pre><code>config = DPOConfig(\n    beta=0.1,\n    label_smoothing=0.1,  # 10% label smoothing\n)\n</code></pre>"},{"location":"training/dpo/#api-reference","title":"API Reference","text":""},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer","title":"artifex.generative_models.training.rl.dpo.DPOTrainer","text":"<pre><code>DPOTrainer(\n    model: Module,\n    reference_model: Module | None,\n    optimizer: Optimizer,\n    config: DPOConfig | None = None,\n)\n</code></pre> <p>Direct Preference Optimization trainer.</p> <p>Implements DPO for preference learning: - Learns from preference pairs (chosen, rejected) - Uses log-ratio between policy and reference model - SimPO mode eliminates need for reference model</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>Policy model to train.</p> <code>reference_model</code> <p>Frozen reference model (None in SimPO mode).</p> <code>optimizer</code> <p>Flax NNX optimizer.</p> <code>config</code> <p>DPO configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Policy model to train.</p> required <code>reference_model</code> <code>Module | None</code> <p>Frozen reference model. Can be None if config.reference_free=True (SimPO mode).</p> required <code>optimizer</code> <code>Optimizer</code> <p>Flax NNX optimizer for the model.</p> required <code>config</code> <code>DPOConfig | None</code> <p>DPO configuration. Uses defaults if not provided.</p> <code>None</code>"},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer.reference_model","title":"reference_model  <code>instance-attribute</code>","text":"<pre><code>reference_model = reference_model\n</code></pre>"},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = optimizer\n</code></pre>"},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config if config is not None else DPOConfig()\n</code></pre>"},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer.compute_log_probs","title":"compute_log_probs","text":"<pre><code>compute_log_probs(model: Module, sequences: Array) -&gt; Array\n</code></pre> <p>Compute per-sequence log probabilities (autoregressive).</p> <p>Uses the standard autoregressive formulation: for each position t, gather the log probability of the actual next token, then average over the sequence dimension to get a per-sequence score.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to compute log probs with.</p> required <code>sequences</code> <code>Array</code> <p>Input sequences with shape (batch_size, seq_len).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Log probabilities with shape (batch_size,).</p>"},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer.compute_log_ratios","title":"compute_log_ratios","text":"<pre><code>compute_log_ratios(sequences: Array) -&gt; Array\n</code></pre> <p>Compute log ratios between policy and reference.</p> <p>log_ratio = log(pi(y|x)) - log(ref(y|x))</p> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>Array</code> <p>Input sequences with shape (batch_size, seq_len).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Log ratios with shape (batch_size,).</p>"},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    batch: dict[str, Array],\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Compute DPO loss.</p> <p>DPO loss: -log(sigmoid(beta * (log_ratio_chosen - log_ratio_rejected)))</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Array]</code> <p>Dictionary containing: - \"chosen\": Chosen sequences. - \"rejected\": Rejected sequences.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/dpo/#artifex.generative_models.training.rl.dpo.DPOTrainer.train_step","title":"train_step","text":"<pre><code>train_step(\n    batch: dict[str, Array],\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Perform a single DPO training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Array]</code> <p>Dictionary containing: - \"chosen\": Chosen sequences. - \"rejected\": Rejected sequences.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/dpo/#training-metrics","title":"Training Metrics","text":"Metric Description <code>dpo_loss</code> DPO/SimPO loss value <code>reward_accuracy</code> Fraction where chosen &gt; rejected reward"},{"location":"training/dpo/#data-format","title":"Data Format","text":""},{"location":"training/dpo/#standard-dpo_1","title":"Standard DPO","text":"<p>Requires log probabilities from both policy and reference model:</p> <pre><code>batch = {\n    # Policy log probs\n    \"chosen_log_probs\": policy_chosen,      # shape: (batch,)\n    \"rejected_log_probs\": policy_rejected,  # shape: (batch,)\n    # Reference model log probs (frozen)\n    \"ref_chosen_log_probs\": ref_chosen,     # shape: (batch,)\n    \"ref_rejected_log_probs\": ref_rejected, # shape: (batch,)\n}\n</code></pre>"},{"location":"training/dpo/#simpo-reference-free_1","title":"SimPO (Reference-Free)","text":"<p>Only requires policy log probabilities:</p> <pre><code>batch = {\n    \"chosen_log_probs\": chosen_log_probs,\n    \"rejected_log_probs\": rejected_log_probs,\n    # No reference model log probs needed\n}\n</code></pre>"},{"location":"training/dpo/#beta-parameter","title":"Beta Parameter","text":"<p>The <code>beta</code> parameter controls the sharpness of the preference:</p> <ul> <li>Lower beta (0.01-0.05): Softer preferences, more exploration</li> <li>Standard beta (0.1): Default, good balance</li> <li>Higher beta (0.5-1.0): Sharper preferences, stronger alignment</li> </ul>"},{"location":"training/dpo/#preparing-preference-data","title":"Preparing Preference Data","text":"<pre><code>def prepare_dpo_batch(\n    model,\n    ref_model,\n    prompts,\n    chosen_responses,\n    rejected_responses,\n):\n    \"\"\"Prepare batch for DPO training.\n\n    Args:\n        model: Policy model being trained\n        ref_model: Frozen reference model\n        prompts: Input prompts\n        chosen_responses: Preferred completions\n        rejected_responses: Non-preferred completions\n\n    Returns:\n        Batch dict for DPO trainer\n    \"\"\"\n    # Compute log probs from policy\n    chosen_log_probs = compute_log_probs(model, prompts, chosen_responses)\n    rejected_log_probs = compute_log_probs(model, prompts, rejected_responses)\n\n    # Compute log probs from reference (no gradients)\n    ref_chosen = compute_log_probs(ref_model, prompts, chosen_responses)\n    ref_rejected = compute_log_probs(ref_model, prompts, rejected_responses)\n\n    return {\n        \"chosen_log_probs\": chosen_log_probs,\n        \"rejected_log_probs\": rejected_log_probs,\n        \"ref_chosen_log_probs\": ref_chosen,\n        \"ref_rejected_log_probs\": ref_rejected,\n    }\n</code></pre>"},{"location":"training/dpo/#use-cases","title":"Use Cases","text":"<p>DPO is recommended for:</p> <ul> <li>Alignment: When you have human preference data</li> <li>No reward model: Simpler than RLHF pipeline</li> <li>Fine-tuning LLMs: Preference tuning for language models</li> <li>Image generation: Preference-based image quality tuning</li> </ul>"},{"location":"training/dpo/#comparison-with-rl-methods","title":"Comparison with RL Methods","text":"Aspect DPO PPO/GRPO Requires reward model No Yes Training stability High Medium Sample efficiency High Lower Flexibility Less More Online learning No Yes"},{"location":"training/dpo/#related-documentation","title":"Related Documentation","text":"<ul> <li>RL Training Guide - Comprehensive RL training guide</li> <li>PPO Trainer - Policy gradient with value function</li> <li>GRPO Trainer - Memory-efficient RL</li> <li>REINFORCE Trainer - Basic policy gradient</li> </ul>"},{"location":"training/dpo/#references","title":"References","text":"<ul> <li>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</li> <li>SimPO: Simple Preference Optimization with a Reference-Free Reward</li> </ul>"},{"location":"training/early_stopping/","title":"Early Stopping Callbacks","text":"<p>Module: <code>generative_models.training.callbacks.early_stopping</code></p> <p>Source: <code>generative_models/training/callbacks/early_stopping.py</code></p>"},{"location":"training/early_stopping/#overview","title":"Overview","text":"<p>Early stopping callback that monitors a metric and stops training when it stops improving. Designed for minimal overhead with simple comparisons and no external dependencies.</p>"},{"location":"training/early_stopping/#classes","title":"Classes","text":""},{"location":"training/early_stopping/#earlystoppingconfig","title":"EarlyStoppingConfig","text":"<pre><code>@dataclass(slots=True)\nclass EarlyStoppingConfig:\n    \"\"\"Configuration for early stopping.\"\"\"\n\n    monitor: str = \"val_loss\"\n    min_delta: float = 0.0\n    patience: int = 10\n    mode: Literal[\"min\", \"max\"] = \"min\"\n    check_finite: bool = True\n    stopping_threshold: float | None = None\n    divergence_threshold: float | None = None\n</code></pre> <p>Attributes:</p> Attribute Type Default Description <code>monitor</code> <code>str</code> <code>\"val_loss\"</code> Metric name to monitor <code>min_delta</code> <code>float</code> <code>0.0</code> Minimum change to qualify as improvement <code>patience</code> <code>int</code> <code>10</code> Epochs without improvement before stopping <code>mode</code> <code>Literal[\"min\", \"max\"]</code> <code>\"min\"</code> Whether lower or higher is better <code>check_finite</code> <code>bool</code> <code>True</code> Stop when metric becomes NaN or Inf <code>stopping_threshold</code> <code>float \\| None</code> <code>None</code> Stop immediately when metric reaches this value <code>divergence_threshold</code> <code>float \\| None</code> <code>None</code> Stop if metric exceeds this (min mode only)"},{"location":"training/early_stopping/#earlystopping","title":"EarlyStopping","text":"<pre><code>class EarlyStopping(BaseCallback):\n    \"\"\"Stop training when a monitored metric stops improving.\"\"\"\n\n    def __init__(self, config: EarlyStoppingConfig): ...\n</code></pre> <p>Callback that tracks a metric and signals when training should stop due to lack of improvement or other conditions.</p> <p>Key Properties:</p> Property Type Description <code>should_stop</code> <code>bool</code> Whether training should stop <code>best_score</code> <code>float \\| None</code> Best metric value seen <code>wait_count</code> <code>int</code> Number of epochs without improvement <code>stopped_epoch</code> <code>int \\| None</code> Epoch when stopping was triggered"},{"location":"training/early_stopping/#usage","title":"Usage","text":""},{"location":"training/early_stopping/#basic-early-stopping","title":"Basic Early Stopping","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    EarlyStopping,\n    EarlyStoppingConfig,\n)\n\n# Stop if validation loss doesn't improve for 10 epochs\nearly_stop = EarlyStopping(EarlyStoppingConfig(\n    monitor=\"val_loss\",\n    patience=10,\n    mode=\"min\",\n))\n\ntrainer.fit(callbacks=[early_stop])\n\n# Check if training stopped early\nif early_stop.should_stop:\n    print(f\"Stopped at epoch {early_stop.stopped_epoch}\")\n</code></pre>"},{"location":"training/early_stopping/#monitor-accuracy-higher-is-better","title":"Monitor Accuracy (Higher is Better)","text":"<pre><code>early_stop = EarlyStopping(EarlyStoppingConfig(\n    monitor=\"val_accuracy\",\n    patience=15,\n    mode=\"max\",  # Higher accuracy is better\n    min_delta=0.001,  # Require at least 0.1% improvement\n))\n</code></pre>"},{"location":"training/early_stopping/#stop-at-target-performance","title":"Stop at Target Performance","text":"<pre><code># Stop when validation loss reaches 0.01 (goal achieved)\nearly_stop = EarlyStopping(EarlyStoppingConfig(\n    monitor=\"val_loss\",\n    stopping_threshold=0.01,\n    mode=\"min\",\n))\n</code></pre>"},{"location":"training/early_stopping/#detect-training-divergence","title":"Detect Training Divergence","text":"<pre><code># Stop if loss explodes (divergence detection)\nearly_stop = EarlyStopping(EarlyStoppingConfig(\n    monitor=\"val_loss\",\n    patience=10,\n    mode=\"min\",\n    divergence_threshold=10.0,  # Stop if loss &gt; 10\n    check_finite=True,  # Also stop on NaN/Inf\n))\n</code></pre>"},{"location":"training/early_stopping/#combined-with-checkpointing","title":"Combined with Checkpointing","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    CallbackList,\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n)\n\ncallbacks = CallbackList([\n    ModelCheckpoint(CheckpointConfig(\n        dirpath=\"./checkpoints\",\n        monitor=\"val_loss\",\n        save_top_k=3,\n    )),\n    EarlyStopping(EarlyStoppingConfig(\n        monitor=\"val_loss\",\n        patience=10,\n        min_delta=1e-4,\n    )),\n])\n\ntrainer.fit(callbacks=callbacks)\n</code></pre>"},{"location":"training/early_stopping/#how-it-works","title":"How It Works","text":"<ol> <li>Metric Tracking: At each epoch end, reads the monitored metric from logs</li> <li>Finite Check: Optionally stops immediately if metric is NaN or Inf</li> <li>Threshold Check: Optionally stops if target performance reached</li> <li>Divergence Check: Optionally stops if metric exceeds threshold (min mode)</li> <li>Improvement Check: Compares current value to best with <code>min_delta</code> tolerance</li> <li>Patience Tracking: Counts epochs without improvement, stops when patience exceeded</li> </ol>"},{"location":"training/early_stopping/#improvement-criteria","title":"Improvement Criteria","text":"<p>For a value to be considered an improvement:</p> <ul> <li>Min mode: <code>current &lt; best - min_delta</code></li> <li>Max mode: <code>current &gt; best + min_delta</code></li> </ul>"},{"location":"training/early_stopping/#stopping-conditions","title":"Stopping Conditions","text":"<p>Training stops when ANY of these conditions are met:</p> <ol> <li>Non-finite value: <code>check_finite=True</code> and metric is NaN or Inf</li> <li>Goal reached: <code>stopping_threshold</code> is set and metric reaches target</li> <li>Divergence: <code>divergence_threshold</code> is set and metric exceeds it (min mode)</li> <li>No improvement: <code>patience</code> epochs pass without improvement</li> </ol>"},{"location":"training/early_stopping/#integration-with-training-loop","title":"Integration with Training Loop","text":"<p>The trainer checks <code>callback.should_stop</code> after each epoch:</p> <pre><code># Inside trainer\nfor epoch in range(num_epochs):\n    # Training step...\n    logs = {\"val_loss\": val_loss, \"val_accuracy\": val_acc}\n\n    # Call callbacks\n    for callback in callbacks:\n        callback.on_epoch_end(self, epoch, logs)\n\n        # Check for early stopping\n        if hasattr(callback, \"should_stop\") and callback.should_stop:\n            print(f\"Early stopping triggered at epoch {epoch}\")\n            return\n</code></pre>"},{"location":"training/early_stopping/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2 (EarlyStoppingConfig, EarlyStopping)</li> <li>Dependencies: None (uses only Python standard library)</li> <li>Slots: Uses <code>__slots__</code> for memory efficiency</li> </ul>"},{"location":"training/energy_trainer/","title":"Energy Trainer","text":"<p>Module: <code>generative_models.training.trainers.energy_trainer</code></p> <p>Source: <code>generative_models/training/trainers/energy_trainer.py</code></p>"},{"location":"training/energy_trainer/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/exponential/","title":"Exponential","text":"<p>Module: <code>generative_models.training.schedulers.exponential</code></p> <p>Source: <code>generative_models/training/schedulers/exponential.py</code></p>"},{"location":"training/exponential/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/factory/","title":"Factory","text":"<p>Module: <code>generative_models.training.schedulers.factory</code></p> <p>Source: <code>generative_models/training/schedulers/factory.py</code></p>"},{"location":"training/factory/#overview","title":"Overview","text":"<p>Learning rate scheduler factory for unified configuration.</p>"},{"location":"training/factory/#functions","title":"Functions","text":""},{"location":"training/factory/#create_scheduler","title":"create_scheduler","text":"<pre><code>def create_scheduler()\n</code></pre>"},{"location":"training/factory/#cyclic_schedule","title":"cyclic_schedule","text":"<pre><code>def cyclic_schedule()\n</code></pre>"},{"location":"training/factory/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 2</li> <li>Imports: 2</li> </ul>"},{"location":"training/flow_trainer/","title":"Flow Trainer","text":"<p>Module: <code>artifex.generative_models.training.trainers.flow_trainer</code></p> <p>The Flow Trainer provides specialized training utilities for flow matching models, including Conditional Flow Matching (CFM), Optimal Transport CFM (OT-CFM), and various time sampling strategies.</p>"},{"location":"training/flow_trainer/#overview","title":"Overview","text":"<p>Flow matching enables simulation-free training of continuous normalizing flows. The Flow Trainer provides:</p> <ul> <li>Flow Types: Standard CFM, OT-CFM, and Rectified Flow</li> <li>Time Sampling: Uniform, logit-normal, and U-shaped strategies</li> <li>Linear Interpolation: Straight paths from noise to data</li> <li>Minimal Noise: Configurable sigma_min for path endpoints</li> </ul>"},{"location":"training/flow_trainer/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training.trainers import (\n    FlowTrainer,\n    FlowTrainingConfig,\n)\nfrom flax import nnx\nimport optax\nimport jax\n\n# Create model and optimizer\nmodel = create_flow_model(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n# Configure flow matching training\nconfig = FlowTrainingConfig(\n    flow_type=\"cfm\",\n    time_sampling=\"logit_normal\",\n    sigma_min=0.001,\n)\n\ntrainer = FlowTrainer(model, optimizer, config)\n\n# Training loop\nkey = jax.random.key(0)\n\nfor step, batch in enumerate(train_loader):\n    key, subkey = jax.random.split(key)\n    loss, metrics = trainer.train_step(batch, subkey)\n\n    if step % 100 == 0:\n        print(f\"Step {step}: loss={metrics['loss']:.4f}\")\n</code></pre>"},{"location":"training/flow_trainer/#configuration","title":"Configuration","text":""},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig","title":"artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig  <code>dataclass</code>","text":"<pre><code>FlowTrainingConfig(\n    flow_type: Literal[\n        \"cfm\", \"ot_cfm\", \"rectified_flow\"\n    ] = \"cfm\",\n    time_sampling: Literal[\n        \"uniform\", \"logit_normal\", \"u_shaped\"\n    ] = \"uniform\",\n    sigma_min: float = 0.001,\n    use_ot: bool = False,\n    ot_regularization: float = 0.01,\n    logit_normal_loc: float = 0.0,\n    logit_normal_scale: float = 1.0,\n)\n</code></pre> <p>Configuration for flow matching training.</p> <p>Attributes:</p> Name Type Description <code>flow_type</code> <code>Literal['cfm', 'ot_cfm', 'rectified_flow']</code> <p>Type of flow matching. - \"cfm\": Standard Conditional Flow Matching - \"ot_cfm\": Optimal Transport CFM for straighter paths - \"rectified_flow\": Rectified Flow for straighter paths</p> <code>time_sampling</code> <code>Literal['uniform', 'logit_normal', 'u_shaped']</code> <p>How to sample time values during training. - \"uniform\": Uniform sampling in [0, 1] - \"logit_normal\": Logit-normal (favors middle times) - \"u_shaped\": U-shaped (favors endpoints, good for rectified flows)</p> <code>sigma_min</code> <code>float</code> <p>Minimum noise level for the Gaussian path.</p> <code>use_ot</code> <code>bool</code> <p>Whether to use optimal transport coupling.</p> <code>ot_regularization</code> <code>float</code> <p>Regularization for OT (Sinkhorn epsilon).</p> <code>logit_normal_loc</code> <code>float</code> <p>Location parameter for logit-normal sampling.</p> <code>logit_normal_scale</code> <code>float</code> <p>Scale parameter for logit-normal sampling.</p>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig.flow_type","title":"flow_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>flow_type: Literal[\"cfm\", \"ot_cfm\", \"rectified_flow\"] = (\n    \"cfm\"\n)\n</code></pre>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig.time_sampling","title":"time_sampling  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>time_sampling: Literal[\n    \"uniform\", \"logit_normal\", \"u_shaped\"\n] = \"uniform\"\n</code></pre>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig.sigma_min","title":"sigma_min  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sigma_min: float = 0.001\n</code></pre>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig.use_ot","title":"use_ot  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_ot: bool = False\n</code></pre>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig.ot_regularization","title":"ot_regularization  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ot_regularization: float = 0.01\n</code></pre>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig.logit_normal_loc","title":"logit_normal_loc  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logit_normal_loc: float = 0.0\n</code></pre>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainingConfig.logit_normal_scale","title":"logit_normal_scale  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>logit_normal_scale: float = 1.0\n</code></pre>"},{"location":"training/flow_trainer/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>flow_type</code> <code>str</code> <code>\"cfm\"</code> Flow type: <code>\"cfm\"</code>, <code>\"ot_cfm\"</code>, <code>\"rectified_flow\"</code> <code>time_sampling</code> <code>str</code> <code>\"uniform\"</code> Time distribution: <code>\"uniform\"</code>, <code>\"logit_normal\"</code>, <code>\"u_shaped\"</code> <code>sigma_min</code> <code>float</code> <code>0.001</code> Minimum noise level for paths <code>use_ot</code> <code>bool</code> <code>False</code> Enable optimal transport coupling <code>ot_regularization</code> <code>float</code> <code>0.01</code> Sinkhorn regularization for OT <code>logit_normal_loc</code> <code>float</code> <code>0.0</code> Logit-normal location parameter <code>logit_normal_scale</code> <code>float</code> <code>1.0</code> Logit-normal scale parameter"},{"location":"training/flow_trainer/#flow-types","title":"Flow Types","text":""},{"location":"training/flow_trainer/#conditional-flow-matching-cfm","title":"Conditional Flow Matching (CFM)","text":"<p>Standard CFM with linear interpolation paths:</p> <pre><code>config = FlowTrainingConfig(flow_type=\"cfm\")\n# Learns velocity field: v(x_t, t) = x_1 - x_0\n</code></pre> <p>The interpolation path is defined as:</p> \\[x_t = (1 - t) x_0 + t x_1\\] <p>where \\(x_0\\) is noise and \\(x_1\\) is data.</p>"},{"location":"training/flow_trainer/#optimal-transport-cfm-ot-cfm","title":"Optimal Transport CFM (OT-CFM)","text":"<p>CFM with optimal transport coupling for straighter paths:</p> <pre><code>config = FlowTrainingConfig(\n    flow_type=\"ot_cfm\",\n    use_ot=True,\n    ot_regularization=0.01,\n)\n# Uses minibatch OT to pair noise and data samples\n</code></pre>"},{"location":"training/flow_trainer/#rectified-flow","title":"Rectified Flow","text":"<p>Straighten paths through reflow iterations:</p> <pre><code>config = FlowTrainingConfig(flow_type=\"rectified_flow\")\n# Single reflow iteration typically sufficient\n</code></pre>"},{"location":"training/flow_trainer/#time-sampling-strategies","title":"Time Sampling Strategies","text":""},{"location":"training/flow_trainer/#uniform-sampling","title":"Uniform Sampling","text":"<p>Standard uniform sampling in [0, 1]:</p> <pre><code>config = FlowTrainingConfig(time_sampling=\"uniform\")\n# Equal probability across all time values\n</code></pre>"},{"location":"training/flow_trainer/#logit-normal-sampling","title":"Logit-Normal Sampling","text":"<p>Favors middle time values for improved convergence:</p> <pre><code>config = FlowTrainingConfig(\n    time_sampling=\"logit_normal\",\n    logit_normal_loc=0.0,\n    logit_normal_scale=1.0,\n)\n</code></pre>"},{"location":"training/flow_trainer/#u-shaped-sampling","title":"U-Shaped Sampling","text":"<p>Favors endpoints (t=0 and t=1), useful for rectified flows:</p> <pre><code>config = FlowTrainingConfig(time_sampling=\"u_shaped\")\n# More samples near 0 and 1 where endpoint behavior is critical\n</code></pre> <p>U-shaped sampling is computed as:</p> \\[t = \\sin^2(\\pi u / 2)\\] <p>where \\(u \\sim \\text{Uniform}(0, 1)\\).</p>"},{"location":"training/flow_trainer/#api-reference","title":"API Reference","text":""},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainer","title":"artifex.generative_models.training.trainers.flow_trainer.FlowTrainer","text":"<pre><code>FlowTrainer(config: FlowTrainingConfig | None = None)\n</code></pre> <p>Flow matching trainer with modern training techniques.</p> <p>This trainer provides a JIT-compatible interface for training flow matching models. The train_step method takes model and optimizer as explicit arguments, allowing it to be wrapped with nnx.jit for performance.</p> Features <ul> <li>Multiple flow types (CFM, OT-CFM, Rectified Flow)</li> <li>Non-uniform time sampling (logit-normal, u-shaped)</li> <li>Optimal transport coupling support</li> <li>DRY integration with base Trainer via create_loss_fn()</li> </ul> <p>The flow matching objective learns a velocity field v_theta(x_t, t) that transports samples from noise distribution to data distribution along straight paths in probability space.</p> <p>Example (non-JIT):     <pre><code>from artifex.generative_models.training.trainers import (\n    FlowTrainer,\n    FlowTrainingConfig,\n)\n\nconfig = FlowTrainingConfig(\n    flow_type=\"cfm\",\n    time_sampling=\"logit_normal\",\n)\ntrainer = FlowTrainer(config)\n\n# Create model and optimizer separately\nmodel = FlowModel(config, rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4))\n\n# Training loop\nfor batch in data:\n    rng, step_rng = jax.random.split(rng)\n    loss, metrics = trainer.train_step(model, optimizer, batch, step_rng)\n</code></pre></p> <p>Example (JIT-compiled):     <pre><code>trainer = FlowTrainer(config)\njit_step = nnx.jit(trainer.train_step)\n\nfor batch in data:\n    rng, step_rng = jax.random.split(rng)\n    loss, metrics = jit_step(model, optimizer, batch, step_rng)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>FlowTrainingConfig | None</code> <p>Flow training configuration.</p> <code>None</code>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or FlowTrainingConfig()\n</code></pre>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainer.sample_time","title":"sample_time","text":"<pre><code>sample_time(batch_size: int, key: Array) -&gt; Array\n</code></pre> <p>Sample time values according to configured strategy.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of time values to sample.</p> required <code>key</code> <code>Array</code> <p>PRNG key for random sampling.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Time values array of shape (batch_size, 1) in [0, 1].</p>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainer.compute_conditional_vector_field","title":"compute_conditional_vector_field","text":"<pre><code>compute_conditional_vector_field(\n    x0: Array, x1: Array, t: Array\n) -&gt; tuple[Array, Array]\n</code></pre> <p>Compute interpolated point and target vector field.</p> For linear interpolation path <p>x_t = (1 - t) * x0 + t * x1 u_t = x1 - x0 (constant velocity)</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Array</code> <p>Source samples (noise), shape (batch, ...).</p> required <code>x1</code> <code>Array</code> <p>Target samples (data), shape (batch, ...).</p> required <code>t</code> <code>Array</code> <p>Time values, shape (batch, 1).</p> required <p>Returns:</p> Type Description <code>tuple[Array, Array]</code> <p>Tuple of (x_t, u_t) where: - x_t: Interpolated points, shape (batch, ...) - u_t: Target velocity field, shape (batch, ...)</p>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    model: Module, batch: dict[str, Any], key: Array\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Compute flow matching loss.</p> The loss is the MSE between predicted and target velocity <p>L = E_{t, x0, x1} || v_theta(x_t, t) - u_t ||^2</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Flow model (velocity field) to evaluate.</p> required <code>batch</code> <code>dict[str, Any]</code> <p>Batch dictionary with \"image\" or \"data\" key.</p> required <code>key</code> <code>Array</code> <p>PRNG key for sampling noise and time.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (total_loss, metrics_dict).</p>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainer.train_step","title":"train_step","text":"<pre><code>train_step(\n    model: Module,\n    optimizer: Optimizer,\n    batch: dict[str, Any],\n    key: Array,\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Execute a single training step.</p> <p>This method can be wrapped with nnx.jit for performance:     jit_step = nnx.jit(trainer.train_step)     loss, metrics = jit_step(model, optimizer, batch, key)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Flow model to train.</p> required <code>optimizer</code> <code>Optimizer</code> <p>NNX optimizer for parameter updates.</p> required <code>batch</code> <code>dict[str, Any]</code> <p>Batch dictionary with \"image\" or \"data\" key.</p> required <code>key</code> <code>Array</code> <p>PRNG key for sampling.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/flow_trainer/#artifex.generative_models.training.trainers.flow_trainer.FlowTrainer.create_loss_fn","title":"create_loss_fn","text":"<pre><code>create_loss_fn() -&gt; Callable[\n    [Module, dict[str, Any], Array],\n    tuple[Array, dict[str, Any]],\n]\n</code></pre> <p>Create loss function compatible with base Trainer.</p> <p>This enables integration with the base Trainer for callbacks, checkpointing, logging, and other training infrastructure.</p> <p>Returns:</p> Type Description <code>Callable[[Module, dict[str, Any], Array], tuple[Array, dict[str, Any]]]</code> <p>Function with signature: (model, batch, rng) -&gt; (loss, metrics)</p>"},{"location":"training/flow_trainer/#flow-matching-theory","title":"Flow Matching Theory","text":"<p>Flow matching learns a velocity field \\(v_\\theta(x_t, t)\\) that transports samples from noise distribution to data distribution.</p>"},{"location":"training/flow_trainer/#training-objective","title":"Training Objective","text":"<p>The CFM loss is:</p> \\[\\mathcal{L} = \\mathbb{E}_{t, x_0, x_1} \\|v_\\theta(x_t, t) - u_t\\|^2\\] <p>where:</p> <ul> <li>\\(x_0 \\sim \\mathcal{N}(0, I)\\) (source noise)</li> <li>\\(x_1 \\sim p_{\\text{data}}\\) (target data)</li> <li>\\(x_t = (1-t) x_0 + t x_1\\) (interpolated point)</li> <li>\\(u_t = x_1 - x_0\\) (target velocity)</li> </ul>"},{"location":"training/flow_trainer/#sampling","title":"Sampling","text":"<p>Generate samples by solving the ODE from t=0 to t=1:</p> \\[\\frac{dx}{dt} = v_\\theta(x, t)\\]"},{"location":"training/flow_trainer/#integration-with-base-trainer","title":"Integration with Base Trainer","text":"<p>Use <code>create_loss_fn()</code> for integration with callbacks and checkpointing:</p> <pre><code>from artifex.generative_models.training import Trainer\nfrom artifex.generative_models.training.trainers import FlowTrainer, FlowTrainingConfig\nfrom artifex.generative_models.training.callbacks import (\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n)\n\n# Create flow trainer\nflow_config = FlowTrainingConfig(\n    flow_type=\"cfm\",\n    time_sampling=\"logit_normal\",\n)\nflow_trainer = FlowTrainer(model, optimizer, flow_config)\n\n# Get loss function for base Trainer\nloss_fn = flow_trainer.create_loss_fn()\n\n# Use with callbacks\ncallbacks = [\n    EarlyStopping(EarlyStoppingConfig(monitor=\"loss\", patience=10)),\n    ModelCheckpoint(CheckpointConfig(dirpath=\"checkpoints\", monitor=\"loss\")),\n]\n</code></pre>"},{"location":"training/flow_trainer/#model-requirements","title":"Model Requirements","text":"<p>The Flow Trainer expects models with the following interface:</p> <pre><code>class FlowModel(nnx.Module):\n    def __call__(\n        self,\n        x_t: jax.Array,\n        t: jax.Array,\n    ) -&gt; jax.Array:\n        \"\"\"Predict velocity at (x_t, t).\n\n        Args:\n            x_t: Points along flow path, shape (batch, ...).\n            t: Time values in [0, 1], shape (batch,).\n\n        Returns:\n            Predicted velocity field, shape (batch, ...).\n        \"\"\"\n        ...\n</code></pre>"},{"location":"training/flow_trainer/#training-metrics","title":"Training Metrics","text":"Metric Description <code>loss</code> MSE between predicted and target velocity"},{"location":"training/flow_trainer/#recommended-configurations","title":"Recommended Configurations","text":""},{"location":"training/flow_trainer/#standard-cfm-training","title":"Standard CFM Training","text":"<pre><code>config = FlowTrainingConfig(\n    flow_type=\"cfm\",\n    time_sampling=\"uniform\",\n    sigma_min=0.001,\n)\n</code></pre>"},{"location":"training/flow_trainer/#high-quality-generation","title":"High-Quality Generation","text":"<pre><code>config = FlowTrainingConfig(\n    flow_type=\"cfm\",\n    time_sampling=\"logit_normal\",\n    logit_normal_loc=0.0,\n    logit_normal_scale=1.0,\n)\n</code></pre>"},{"location":"training/flow_trainer/#rectified-flow_1","title":"Rectified Flow","text":"<pre><code>config = FlowTrainingConfig(\n    flow_type=\"rectified_flow\",\n    time_sampling=\"u_shaped\",\n)\n</code></pre>"},{"location":"training/flow_trainer/#sampling-from-trained-models","title":"Sampling from Trained Models","text":"<p>After training, generate samples using ODE integration:</p> <pre><code>from jax.experimental.ode import odeint\nimport jax.numpy as jnp\n\ndef sample(model, shape, key, num_steps=100):\n    \"\"\"Generate samples from trained flow model.\"\"\"\n    # Start from noise\n    x_0 = jax.random.normal(key, shape)\n\n    # Define ODE function\n    def velocity_fn(x, t):\n        t_batch = jnp.full((x.shape[0],), t)\n        return model(x, t_batch)\n\n    # Integrate from t=0 to t=1\n    ts = jnp.linspace(0, 1, num_steps)\n    trajectory = odeint(velocity_fn, x_0, ts)\n\n    # Return final sample at t=1\n    return trajectory[-1]\n\n# Generate samples\nsamples = sample(model, (batch_size, *data_shape), key)\n</code></pre>"},{"location":"training/flow_trainer/#references","title":"References","text":"<ul> <li>Flow Matching for Generative Modeling</li> <li>OT-CFM: Improving and Simplifying Flow Matching</li> <li>Rectified Flow</li> </ul>"},{"location":"training/gan_trainer/","title":"GAN Trainer","text":"<p>Module: <code>artifex.generative_models.training.trainers.gan_trainer</code></p> <p>The GAN Trainer provides specialized training utilities for Generative Adversarial Networks, including multiple loss variants, gradient penalty regularization, and R1 regularization for stable training.</p>"},{"location":"training/gan_trainer/#overview","title":"Overview","text":"<p>GAN training requires careful balancing between generator and discriminator. The GAN Trainer handles this through:</p> <ul> <li>Multiple Loss Types: Vanilla, Wasserstein, Hinge, and Least Squares GAN</li> <li>Gradient Penalty: WGAN-GP regularization for stable Wasserstein training</li> <li>R1 Regularization: Gradient penalty on real data for improved stability</li> <li>Label Smoothing: One-sided smoothing to prevent overconfidence</li> </ul>"},{"location":"training/gan_trainer/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training.trainers import (\n    GANTrainer,\n    GANTrainingConfig,\n)\nfrom flax import nnx\nimport optax\nimport jax\n\n# Create models and optimizers\ngenerator = create_generator(rngs=nnx.Rngs(0))\ndiscriminator = create_discriminator(rngs=nnx.Rngs(1))\n\ng_optimizer = nnx.Optimizer(generator, optax.adam(1e-4, b1=0.5), wrt=nnx.Param)\nd_optimizer = nnx.Optimizer(discriminator, optax.adam(1e-4, b1=0.5), wrt=nnx.Param)\n\n# Configure GAN training\nconfig = GANTrainingConfig(\n    loss_type=\"wasserstein\",\n    n_critic=5,\n    gp_weight=10.0,\n)\n\ntrainer = GANTrainer(\n    generator=generator,\n    discriminator=discriminator,\n    g_optimizer=g_optimizer,\n    d_optimizer=d_optimizer,\n    config=config,\n)\n\n# Training loop\nkey = jax.random.key(0)\nlatent_dim = 128\n\nfor step, batch in enumerate(train_loader):\n    key, d_key, g_key, z_key = jax.random.split(key, 4)\n    real = batch[\"image\"]\n    z = jax.random.normal(z_key, (real.shape[0], latent_dim))\n\n    # Train discriminator\n    d_loss, d_metrics = trainer.discriminator_step(real, z, d_key)\n\n    # Train generator every n_critic steps\n    if step % config.n_critic == 0:\n        z = jax.random.normal(z_key, (real.shape[0], latent_dim))\n        g_loss, g_metrics = trainer.generator_step(z)\n</code></pre>"},{"location":"training/gan_trainer/#configuration","title":"Configuration","text":""},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainingConfig","title":"artifex.generative_models.training.trainers.gan_trainer.GANTrainingConfig  <code>dataclass</code>","text":"<pre><code>GANTrainingConfig(\n    loss_type: Literal[\n        \"vanilla\", \"wasserstein\", \"hinge\", \"lsgan\"\n    ] = \"vanilla\",\n    n_critic: int = 1,\n    gp_weight: float = 10.0,\n    gp_target: float = 1.0,\n    r1_weight: float = 0.0,\n    label_smoothing: float = 0.0,\n)\n</code></pre> <p>Configuration for GAN-specific training.</p> <p>Attributes:</p> Name Type Description <code>loss_type</code> <code>Literal['vanilla', 'wasserstein', 'hinge', 'lsgan']</code> <p>GAN loss variant. - \"vanilla\": Standard GAN loss (BCE) - \"wasserstein\": Wasserstein distance (requires gradient penalty) - \"hinge\": Hinge loss (used in BigGAN, StyleGAN2) - \"lsgan\": Least squares GAN</p> <code>n_critic</code> <code>int</code> <p>Discriminator updates per generator update.</p> <code>gp_weight</code> <code>float</code> <p>Gradient penalty weight (for WGAN-GP).</p> <code>gp_target</code> <code>float</code> <p>Target gradient norm (usually 1.0).</p> <code>r1_weight</code> <code>float</code> <p>R1 regularization weight.</p> <code>label_smoothing</code> <code>float</code> <p>Smooth real labels to [1-smoothing, 1].</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainingConfig.loss_type","title":"loss_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>loss_type: Literal[\n    \"vanilla\", \"wasserstein\", \"hinge\", \"lsgan\"\n] = \"vanilla\"\n</code></pre>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainingConfig.n_critic","title":"n_critic  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_critic: int = 1\n</code></pre>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainingConfig.gp_weight","title":"gp_weight  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gp_weight: float = 10.0\n</code></pre>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainingConfig.gp_target","title":"gp_target  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gp_target: float = 1.0\n</code></pre>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainingConfig.r1_weight","title":"r1_weight  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>r1_weight: float = 0.0\n</code></pre>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainingConfig.label_smoothing","title":"label_smoothing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>label_smoothing: float = 0.0\n</code></pre>"},{"location":"training/gan_trainer/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>loss_type</code> <code>str</code> <code>\"vanilla\"</code> Loss variant: <code>\"vanilla\"</code>, <code>\"wasserstein\"</code>, <code>\"hinge\"</code>, <code>\"lsgan\"</code> <code>n_critic</code> <code>int</code> <code>1</code> Discriminator updates per generator update <code>gp_weight</code> <code>float</code> <code>10.0</code> Gradient penalty weight (WGAN-GP) <code>gp_target</code> <code>float</code> <code>1.0</code> Target gradient norm for GP <code>r1_weight</code> <code>float</code> <code>0.0</code> R1 regularization weight <code>label_smoothing</code> <code>float</code> <code>0.0</code> One-sided label smoothing for real labels"},{"location":"training/gan_trainer/#loss-types","title":"Loss Types","text":""},{"location":"training/gan_trainer/#vanilla-gan-non-saturating","title":"Vanilla GAN (Non-Saturating)","text":"<p>Standard GAN with non-saturating generator loss for numerical stability:</p> <pre><code>config = GANTrainingConfig(loss_type=\"vanilla\")\n# Uses log(1 - sigmoid) form with softplus for stability\n</code></pre>"},{"location":"training/gan_trainer/#wasserstein-gan","title":"Wasserstein GAN","text":"<p>Earth Mover's distance with gradient penalty:</p> <pre><code>config = GANTrainingConfig(\n    loss_type=\"wasserstein\",\n    gp_weight=10.0,  # Required for WGAN-GP\n    n_critic=5,      # More D updates per G update\n)\n</code></pre>"},{"location":"training/gan_trainer/#hinge-loss","title":"Hinge Loss","text":"<p>Hinge loss used in BigGAN and StyleGAN2:</p> <pre><code>config = GANTrainingConfig(loss_type=\"hinge\")\n# D loss: relu(1 - D(real)) + relu(1 + D(fake))\n# G loss: -D(fake)\n</code></pre>"},{"location":"training/gan_trainer/#least-squares-gan","title":"Least Squares GAN","text":"<p>Mean squared error between predictions and targets:</p> <pre><code>config = GANTrainingConfig(loss_type=\"lsgan\")\n# More stable gradients than vanilla GAN\n</code></pre>"},{"location":"training/gan_trainer/#regularization-techniques","title":"Regularization Techniques","text":""},{"location":"training/gan_trainer/#gradient-penalty-wgan-gp","title":"Gradient Penalty (WGAN-GP)","text":"<p>Enforces 1-Lipschitz constraint via gradient penalty on interpolated samples:</p> <pre><code>config = GANTrainingConfig(\n    loss_type=\"wasserstein\",\n    gp_weight=10.0,\n    gp_target=1.0,  # Target gradient norm\n)\n</code></pre> <p>The gradient penalty is computed as:</p> \\[\\lambda \\mathbb{E}_{\\hat{x}}[(||\\nabla_{\\hat{x}} D(\\hat{x})||_2 - 1)^2]\\] <p>where \\(\\hat{x}\\) is interpolated between real and fake samples.</p>"},{"location":"training/gan_trainer/#r1-regularization","title":"R1 Regularization","text":"<p>Gradient penalty on real data only, used in StyleGAN:</p> <pre><code>config = GANTrainingConfig(\n    loss_type=\"hinge\",\n    r1_weight=10.0,  # R1 penalty weight\n)\n</code></pre> <p>R1 penalty is computed as:</p> \\[\\frac{\\gamma}{2} \\mathbb{E}_{x \\sim p_{data}}[||\\nabla_x D(x)||_2^2]\\]"},{"location":"training/gan_trainer/#label-smoothing","title":"Label Smoothing","text":"<p>One-sided label smoothing to prevent discriminator overconfidence:</p> <pre><code>config = GANTrainingConfig(\n    loss_type=\"vanilla\",\n    label_smoothing=0.1,  # Real labels: 0.9 instead of 1.0\n)\n</code></pre>"},{"location":"training/gan_trainer/#api-reference","title":"API Reference","text":""},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer","title":"artifex.generative_models.training.trainers.gan_trainer.GANTrainer","text":"<pre><code>GANTrainer(config: GANTrainingConfig | None = None)\n</code></pre> <p>GAN-specific trainer with multiple loss variants.</p> <p>This trainer provides a JIT-compatible interface for adversarial training with support for multiple loss functions and regularization techniques. The step methods take models and optimizers as explicit arguments, allowing them to be wrapped with nnx.jit for performance.</p> Features <ul> <li>Multiple loss types (vanilla, wasserstein, hinge, lsgan)</li> <li>Configurable discriminator/generator update ratio</li> <li>WGAN-GP gradient penalty</li> <li>R1 regularization for discriminator</li> <li>Label smoothing</li> </ul> <p>Example (non-JIT):     <pre><code>from artifex.generative_models.training.trainers import (\n    GANTrainer,\n    GANTrainingConfig,\n)\n\nconfig = GANTrainingConfig(\n    loss_type=\"wasserstein\",\n    n_critic=5,\n    gp_weight=10.0,\n)\ntrainer = GANTrainer(config)\n\n# Create models and optimizers separately\ngenerator = Generator(rngs=nnx.Rngs(0))\ndiscriminator = Discriminator(rngs=nnx.Rngs(1))\ng_optimizer = nnx.Optimizer(generator, optax.adam(1e-4))\nd_optimizer = nnx.Optimizer(discriminator, optax.adam(1e-4))\n\n# Training loop\nfor step in range(num_steps):\n    rng, d_key, g_key = jax.random.split(rng, 3)\n    d_loss, d_metrics = trainer.discriminator_step(\n        generator, discriminator, d_optimizer, real_batch, z, d_key\n    )\n    if step % config.n_critic == 0:\n        g_loss, g_metrics = trainer.generator_step(\n            generator, discriminator, g_optimizer, z\n        )\n</code></pre></p> <p>Example (JIT-compiled):     <pre><code>trainer = GANTrainer(config)\njit_d_step = nnx.jit(trainer.discriminator_step)\njit_g_step = nnx.jit(trainer.generator_step)\n\nfor step in range(num_steps):\n    d_loss, d_metrics = jit_d_step(\n        generator, discriminator, d_optimizer, real_batch, z, d_key\n    )\n    if step % config.n_critic == 0:\n        g_loss, g_metrics = jit_g_step(\n            generator, discriminator, g_optimizer, z\n        )\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GANTrainingConfig | None</code> <p>GAN training configuration.</p> <code>None</code>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or GANTrainingConfig()\n</code></pre>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_d_loss_vanilla","title":"compute_d_loss_vanilla","text":"<pre><code>compute_d_loss_vanilla(\n    d_real: Array, d_fake: Array\n) -&gt; Array\n</code></pre> <p>Compute vanilla GAN discriminator loss.</p> <p>Uses non-saturating loss from core/losses for numerical stability.</p> <p>Parameters:</p> Name Type Description Default <code>d_real</code> <code>Array</code> <p>Discriminator output for real samples (logits).</p> required <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples (logits).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Discriminator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_d_loss_wasserstein","title":"compute_d_loss_wasserstein","text":"<pre><code>compute_d_loss_wasserstein(\n    d_real: Array, d_fake: Array\n) -&gt; Array\n</code></pre> <p>Compute Wasserstein discriminator loss.</p> <p>Uses wasserstein_discriminator_loss from core/losses.</p> <p>Parameters:</p> Name Type Description Default <code>d_real</code> <code>Array</code> <p>Discriminator output for real samples.</p> required <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Discriminator loss (negative critic loss).</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_d_loss_hinge","title":"compute_d_loss_hinge","text":"<pre><code>compute_d_loss_hinge(d_real: Array, d_fake: Array) -&gt; Array\n</code></pre> <p>Compute hinge discriminator loss.</p> <p>Uses hinge_discriminator_loss from core/losses.</p> <p>Parameters:</p> Name Type Description Default <code>d_real</code> <code>Array</code> <p>Discriminator output for real samples.</p> required <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Discriminator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_d_loss_lsgan","title":"compute_d_loss_lsgan","text":"<pre><code>compute_d_loss_lsgan(d_real: Array, d_fake: Array) -&gt; Array\n</code></pre> <p>Compute least squares GAN discriminator loss.</p> <p>Uses least_squares_discriminator_loss from core/losses.</p> <p>Parameters:</p> Name Type Description Default <code>d_real</code> <code>Array</code> <p>Discriminator output for real samples.</p> required <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Discriminator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_discriminator_loss","title":"compute_discriminator_loss","text":"<pre><code>compute_discriminator_loss(\n    d_real: Array, d_fake: Array\n) -&gt; Array\n</code></pre> <p>Compute discriminator loss based on configured loss type.</p> <p>Parameters:</p> Name Type Description Default <code>d_real</code> <code>Array</code> <p>Discriminator output for real samples.</p> required <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Discriminator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_g_loss_vanilla","title":"compute_g_loss_vanilla","text":"<pre><code>compute_g_loss_vanilla(d_fake: Array) -&gt; Array\n</code></pre> <p>Compute vanilla GAN generator loss.</p> <p>Uses ns_vanilla_generator_loss from core/losses.</p> <p>Parameters:</p> Name Type Description Default <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples (logits).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Generator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_g_loss_wasserstein","title":"compute_g_loss_wasserstein","text":"<pre><code>compute_g_loss_wasserstein(d_fake: Array) -&gt; Array\n</code></pre> <p>Compute Wasserstein generator loss.</p> <p>Uses wasserstein_generator_loss from core/losses.</p> <p>Parameters:</p> Name Type Description Default <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Generator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_g_loss_hinge","title":"compute_g_loss_hinge","text":"<pre><code>compute_g_loss_hinge(d_fake: Array) -&gt; Array\n</code></pre> <p>Compute hinge generator loss.</p> <p>Uses hinge_generator_loss from core/losses.</p> <p>Parameters:</p> Name Type Description Default <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Generator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_g_loss_lsgan","title":"compute_g_loss_lsgan","text":"<pre><code>compute_g_loss_lsgan(d_fake: Array) -&gt; Array\n</code></pre> <p>Compute least squares GAN generator loss.</p> <p>Uses least_squares_generator_loss from core/losses.</p> <p>Parameters:</p> Name Type Description Default <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Generator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_generator_loss","title":"compute_generator_loss","text":"<pre><code>compute_generator_loss(d_fake: Array) -&gt; Array\n</code></pre> <p>Compute generator loss based on configured loss type.</p> <p>Parameters:</p> Name Type Description Default <code>d_fake</code> <code>Array</code> <p>Discriminator output for fake samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Generator loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_gradient_penalty","title":"compute_gradient_penalty","text":"<pre><code>compute_gradient_penalty(\n    discriminator: Module,\n    real: Array,\n    fake: Array,\n    key: Array,\n) -&gt; Array\n</code></pre> <p>Compute WGAN-GP gradient penalty.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator</code> <code>Module</code> <p>Discriminator model.</p> required <code>real</code> <code>Array</code> <p>Real samples.</p> required <code>fake</code> <code>Array</code> <p>Fake samples (must have same shape as real).</p> required <code>key</code> <code>Array</code> <p>PRNG key for interpolation.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Gradient penalty loss.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.compute_r1_penalty","title":"compute_r1_penalty","text":"<pre><code>compute_r1_penalty(\n    discriminator: Module, real: Array\n) -&gt; Array\n</code></pre> <p>Compute R1 regularization penalty.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator</code> <code>Module</code> <p>Discriminator model.</p> required <code>real</code> <code>Array</code> <p>Real samples.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>R1 penalty.</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.discriminator_step","title":"discriminator_step","text":"<pre><code>discriminator_step(\n    generator: Module,\n    discriminator: Module,\n    d_optimizer: Optimizer,\n    real: Array,\n    z: Array,\n    key: Array,\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Execute a discriminator training step.</p> <p>This method can be wrapped with nnx.jit for performance:     jit_step = nnx.jit(trainer.discriminator_step)     loss, metrics = jit_step(generator, discriminator, d_optimizer, real, z, key)</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Module</code> <p>Generator model.</p> required <code>discriminator</code> <code>Module</code> <p>Discriminator model.</p> required <code>d_optimizer</code> <code>Optimizer</code> <p>Optimizer for discriminator.</p> required <code>real</code> <code>Array</code> <p>Real samples.</p> required <code>z</code> <code>Array</code> <p>Latent vectors for generator.</p> required <code>key</code> <code>Array</code> <p>PRNG key for gradient penalty.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/gan_trainer/#artifex.generative_models.training.trainers.gan_trainer.GANTrainer.generator_step","title":"generator_step","text":"<pre><code>generator_step(\n    generator: Module,\n    discriminator: Module,\n    g_optimizer: Optimizer,\n    z: Array,\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Execute a generator training step.</p> <p>This method can be wrapped with nnx.jit for performance:     jit_step = nnx.jit(trainer.generator_step)     loss, metrics = jit_step(generator, discriminator, g_optimizer, z)</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Module</code> <p>Generator model.</p> required <code>discriminator</code> <code>Module</code> <p>Discriminator model.</p> required <code>g_optimizer</code> <code>Optimizer</code> <p>Optimizer for generator.</p> required <code>z</code> <code>Array</code> <p>Latent vectors for generator.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/gan_trainer/#training-patterns","title":"Training Patterns","text":""},{"location":"training/gan_trainer/#standard-gan-training","title":"Standard GAN Training","text":"<pre><code>for step, batch in enumerate(train_loader):\n    key, subkey = jax.random.split(key)\n    real = batch[\"image\"]\n    z = jax.random.normal(subkey, (batch_size, latent_dim))\n\n    # Train discriminator\n    d_loss, d_metrics = trainer.discriminator_step(real, z, subkey)\n\n    # Train generator (every step for vanilla/hinge/lsgan)\n    g_loss, g_metrics = trainer.generator_step(z)\n</code></pre>"},{"location":"training/gan_trainer/#wgan-training-multiple-d-updates","title":"WGAN Training (Multiple D Updates)","text":"<pre><code>for step, batch in enumerate(train_loader):\n    key, d_key, g_key = jax.random.split(key, 3)\n    real = batch[\"image\"]\n    z = jax.random.normal(d_key, (batch_size, latent_dim))\n\n    # Multiple discriminator updates\n    for _ in range(config.n_critic):\n        d_loss, d_metrics = trainer.discriminator_step(real, z, d_key)\n\n    # Single generator update\n    z = jax.random.normal(g_key, (batch_size, latent_dim))\n    g_loss, g_metrics = trainer.generator_step(z)\n</code></pre>"},{"location":"training/gan_trainer/#progressive-training","title":"Progressive Training","text":"<p>For high-resolution generation, progressively grow resolution:</p> <pre><code>resolutions = [4, 8, 16, 32, 64, 128]\n\nfor resolution in resolutions:\n    # Update model for this resolution\n    generator.grow_layer()\n    discriminator.grow_layer()\n\n    # Train at this resolution\n    for step in range(steps_per_resolution):\n        # ... training step ...\n</code></pre>"},{"location":"training/gan_trainer/#model-requirements","title":"Model Requirements","text":""},{"location":"training/gan_trainer/#generator-interface","title":"Generator Interface","text":"<pre><code>class Generator(nnx.Module):\n    def __call__(self, z: jax.Array) -&gt; jax.Array:\n        \"\"\"Generate images from latent vectors.\n\n        Args:\n            z: Latent vectors, shape (batch, latent_dim).\n\n        Returns:\n            Generated images, shape (batch, H, W, C).\n        \"\"\"\n        ...\n</code></pre>"},{"location":"training/gan_trainer/#discriminator-interface","title":"Discriminator Interface","text":"<pre><code>class Discriminator(nnx.Module):\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Classify real/fake images.\n\n        Args:\n            x: Images, shape (batch, H, W, C).\n\n        Returns:\n            Logits (unbounded scores), shape (batch,) or (batch, 1).\n        \"\"\"\n        ...\n</code></pre>"},{"location":"training/gan_trainer/#training-metrics","title":"Training Metrics","text":""},{"location":"training/gan_trainer/#discriminator-metrics","title":"Discriminator Metrics","text":"Metric Description <code>d_loss</code> Base discriminator loss <code>d_loss_total</code> Total loss including regularization <code>d_real</code> Mean discriminator output on real samples <code>d_fake</code> Mean discriminator output on fake samples <code>gp_loss</code> Gradient penalty loss (if enabled) <code>r1_loss</code> R1 regularization loss (if enabled)"},{"location":"training/gan_trainer/#generator-metrics","title":"Generator Metrics","text":"Metric Description <code>g_loss</code> Generator loss <code>d_fake_g</code> Mean discriminator output on generated samples"},{"location":"training/gan_trainer/#loss-functions","title":"Loss Functions","text":"<p>The GAN Trainer uses loss functions from <code>artifex.generative_models.core.losses.adversarial</code>:</p> <pre><code>from artifex.generative_models.core.losses import (\n    # Vanilla GAN (non-saturating)\n    ns_vanilla_generator_loss,\n    ns_vanilla_discriminator_loss,\n    # Wasserstein\n    wasserstein_generator_loss,\n    wasserstein_discriminator_loss,\n    # Hinge\n    hinge_generator_loss,\n    hinge_discriminator_loss,\n    # Least Squares\n    least_squares_generator_loss,\n    least_squares_discriminator_loss,\n)\n</code></pre>"},{"location":"training/gan_trainer/#references","title":"References","text":"<ul> <li>WGAN-GP: Improved Training of Wasserstein GANs</li> <li>Hinge Loss for GANs (SAGAN)</li> <li>R1 Regularization (StyleGAN)</li> <li>LSGAN: Least Squares Generative Adversarial Networks</li> </ul>"},{"location":"training/gradient_accumulation/","title":"Gradient Accumulation","text":"<p>Module: <code>generative_models.training.gradient_accumulation</code></p> <p>Source: <code>generative_models/training/gradient_accumulation.py</code></p>"},{"location":"training/gradient_accumulation/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/grpo/","title":"GRPO Trainer","text":"<p>Module: <code>artifex.generative_models.training.rl.grpo</code></p> <p>The GRPO (Group Relative Policy Optimization) Trainer provides memory-efficient RL training by eliminating the value network through group-normalized advantages.</p>"},{"location":"training/grpo/#overview","title":"Overview","text":"<p>GRPO, pioneered by DeepSeek, achieves approximately 50% memory savings compared to PPO:</p> <ul> <li>No Value Network: Eliminates critic, saving significant memory</li> <li>Group Normalization: Normalizes advantages within groups of generations</li> <li>PPO-Style Clipping: Maintains training stability</li> <li>Optional KL Penalty: Prevents policy drift from reference</li> </ul>"},{"location":"training/grpo/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training import GRPOConfig, GRPOTrainer\nfrom flax import nnx\nimport optax\n\n# Create policy model (no value head needed!)\nmodel = PolicyModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n# Configure GRPO\nconfig = GRPOConfig(\n    num_generations=4,    # Generate 4 samples per prompt\n    clip_param=0.2,\n    beta=0.01,            # KL penalty coefficient\n    entropy_coeff=0.01,\n)\n\ntrainer = GRPOTrainer(model, optimizer, config)\n\n# Training with grouped generations\n# batch_size = num_prompts * num_generations\nbatch = {\n    \"observations\": observations,  # (batch_size, ...)\n    \"actions\": actions,            # (batch_size, ...)\n    \"rewards\": rewards,            # (batch_size,)\n    \"log_probs\": old_log_probs,    # (batch_size,)\n}\nmetrics = trainer.train_step(batch)\n</code></pre>"},{"location":"training/grpo/#configuration","title":"Configuration","text":""},{"location":"training/grpo/#artifex.generative_models.training.rl.configs.GRPOConfig","title":"artifex.generative_models.training.rl.configs.GRPOConfig  <code>dataclass</code>","text":"<pre><code>GRPOConfig(\n    num_generations: int = 4,\n    clip_param: float = 0.2,\n    beta: float = 0.01,\n    entropy_coeff: float = 0.01,\n    gamma: float = 0.99,\n)\n</code></pre> <p>Configuration for Group Relative Policy Optimization.</p> <p>GRPO is a critic-free RL algorithm from DeepSeek-R1 that: - Generates multiple completions per prompt (num_generations) - Normalizes advantages within each group - Uses PPO-style clipping - Saves ~50% memory by eliminating the value network</p> <p>Attributes:</p> Name Type Description <code>num_generations</code> <code>int</code> <p>Number of completions to generate per prompt. Default 4.</p> <code>clip_param</code> <code>float</code> <p>Clipping parameter for surrogate objective. Default 0.2.</p> <code>beta</code> <code>float</code> <p>KL penalty coefficient for regularization. Default 0.01.</p> <code>entropy_coeff</code> <code>float</code> <p>Coefficient for entropy bonus. Default 0.01.</p> <code>gamma</code> <code>float</code> <p>Discount factor (used if computing returns). Default 0.99.</p>"},{"location":"training/grpo/#artifex.generative_models.training.rl.configs.GRPOConfig.num_generations","title":"num_generations  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_generations: int = 4\n</code></pre>"},{"location":"training/grpo/#artifex.generative_models.training.rl.configs.GRPOConfig.clip_param","title":"clip_param  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clip_param: float = 0.2\n</code></pre>"},{"location":"training/grpo/#artifex.generative_models.training.rl.configs.GRPOConfig.beta","title":"beta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>beta: float = 0.01\n</code></pre>"},{"location":"training/grpo/#artifex.generative_models.training.rl.configs.GRPOConfig.entropy_coeff","title":"entropy_coeff  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>entropy_coeff: float = 0.01\n</code></pre>"},{"location":"training/grpo/#artifex.generative_models.training.rl.configs.GRPOConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.99\n</code></pre>"},{"location":"training/grpo/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>num_generations</code> <code>int</code> <code>4</code> Number of generations per prompt (group size G) <code>clip_param</code> <code>float</code> <code>0.2</code> PPO-style clipping parameter <code>beta</code> <code>float</code> <code>0.01</code> KL divergence penalty coefficient <code>entropy_coeff</code> <code>float</code> <code>0.01</code> Entropy bonus coefficient <code>gamma</code> <code>float</code> <code>0.99</code> Discount factor"},{"location":"training/grpo/#algorithm","title":"Algorithm","text":""},{"location":"training/grpo/#group-normalized-advantages","title":"Group-Normalized Advantages","text":"<p>Instead of learning a value function, GRPO normalizes rewards within groups:</p> <ol> <li> <p>Generate G samples per prompt: For each prompt \\(x_i\\), generate \\(G\\) completions \\(\\{y_{i,1}, ..., y_{i,G}\\}\\)</p> </li> <li> <p>Compute rewards: Evaluate each generation \\(r_{i,j} = R(x_i, y_{i,j})\\)</p> </li> <li> <p>Normalize within groups:    $\\(\\hat{A}_{i,j} = \\frac{r_{i,j} - \\mu_i}{\\sigma_i + \\epsilon}\\)$    where \\(\\mu_i = \\frac{1}{G}\\sum_j r_{i,j}\\) and \\(\\sigma_i = \\sqrt{\\frac{1}{G}\\sum_j (r_{i,j} - \\mu_i)^2}\\)</p> </li> <li> <p>Apply PPO clipping: Use normalized advantages with clipped surrogate loss</p> </li> </ol>"},{"location":"training/grpo/#why-it-works","title":"Why It Works","text":"<ul> <li>Relative comparison: By normalizing within groups, GRPO compares generations to each other rather than to an absolute baseline</li> <li>Self-normalization: Each prompt serves as its own baseline through group statistics</li> <li>Memory efficient: No value network parameters or forward passes needed</li> </ul>"},{"location":"training/grpo/#kl-penalty-optional","title":"KL Penalty (Optional)","text":"<p>To prevent the policy from drifting too far from the reference:</p> \\[\\mathcal{L}_{total} = \\mathcal{L}_{policy} + \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{ref})\\] <p>Provide <code>ref_log_probs</code> in the batch to enable KL penalty:</p> <pre><code>batch = {\n    \"observations\": observations,\n    \"actions\": actions,\n    \"rewards\": rewards,\n    \"log_probs\": old_log_probs,\n    \"ref_log_probs\": ref_log_probs,  # From frozen reference model\n}\n</code></pre>"},{"location":"training/grpo/#api-reference","title":"API Reference","text":""},{"location":"training/grpo/#artifex.generative_models.training.rl.grpo.GRPOTrainer","title":"artifex.generative_models.training.rl.grpo.GRPOTrainer","text":"<pre><code>GRPOTrainer(\n    model: Module,\n    optimizer: Optimizer,\n    config: GRPOConfig | None = None,\n    reference_model: Module | None = None,\n)\n</code></pre> <p>Group Relative Policy Optimization trainer.</p> <p>GRPO is a critic-free algorithm that: 1. Generates G completions per prompt 2. Computes rewards for each completion 3. Normalizes rewards within each group: (r - mean) / std 4. Uses normalized rewards as advantages 5. Applies PPO-style clipped objective</p> <p>This eliminates the need for a value network, saving ~50% memory.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>Policy model to train.</p> <code>optimizer</code> <p>Flax NNX optimizer.</p> <code>config</code> <p>GRPO configuration.</p> <code>reference_model</code> <p>Optional frozen reference for KL penalty.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Policy model to train.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Flax NNX optimizer for the model.</p> required <code>config</code> <code>GRPOConfig | None</code> <p>GRPO configuration. Uses defaults if not provided.</p> <code>None</code> <code>reference_model</code> <code>Module | None</code> <p>Optional frozen reference model for KL penalty.</p> <code>None</code>"},{"location":"training/grpo/#artifex.generative_models.training.rl.grpo.GRPOTrainer.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"training/grpo/#artifex.generative_models.training.rl.grpo.GRPOTrainer.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = optimizer\n</code></pre>"},{"location":"training/grpo/#artifex.generative_models.training.rl.grpo.GRPOTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config if config is not None else GRPOConfig()\n</code></pre>"},{"location":"training/grpo/#artifex.generative_models.training.rl.grpo.GRPOTrainer.reference_model","title":"reference_model  <code>instance-attribute</code>","text":"<pre><code>reference_model = reference_model\n</code></pre>"},{"location":"training/grpo/#artifex.generative_models.training.rl.grpo.GRPOTrainer.normalize_group_rewards","title":"normalize_group_rewards","text":"<pre><code>normalize_group_rewards(\n    rewards: Array, group_size: int, eps: float = 1e-08\n) -&gt; Array\n</code></pre> <p>Normalize rewards within each group.</p> <p>For GRPO, we have G generations per prompt. We normalize rewards within each group to zero mean, unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>rewards</code> <code>Array</code> <p>Rewards with shape (batch_size,) where batch_size is num_prompts * group_size.</p> required <code>group_size</code> <code>int</code> <p>Number of generations per prompt (G).</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Array</code> <p>Group-normalized advantages with same shape.</p>"},{"location":"training/grpo/#artifex.generative_models.training.rl.grpo.GRPOTrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    states: Array,\n    actions: Array,\n    old_log_probs: Array,\n    rewards: Array,\n    group_size: int | None = None,\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Compute GRPO loss.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Array</code> <p>Input states with shape (batch_size, ...).</p> required <code>actions</code> <code>Array</code> <p>Actions taken with shape (batch_size,).</p> required <code>old_log_probs</code> <code>Array</code> <p>Log probs from old policy with shape (batch_size,).</p> required <code>rewards</code> <code>Array</code> <p>Rewards with shape (batch_size,).</p> required <code>group_size</code> <code>int | None</code> <p>Number of generations per prompt. If None, uses config.num_generations.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/grpo/#artifex.generative_models.training.rl.grpo.GRPOTrainer.train_step","title":"train_step","text":"<pre><code>train_step(\n    batch: dict[str, Array],\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Perform a single GRPO training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Array]</code> <p>Dictionary containing: - \"states\": Input states. - \"actions\": Actions taken. - \"old_log_probs\": Log probs from old policy. - \"rewards\": Rewards for each completion. - \"group_size\": Optional, number of generations per prompt.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/grpo/#training-metrics","title":"Training Metrics","text":"Metric Description <code>policy_loss</code> Clipped surrogate policy loss <code>approx_kl</code> Approximate KL divergence from old policy <code>kl_penalty</code> KL penalty term (if ref_log_probs provided)"},{"location":"training/grpo/#data-organization","title":"Data Organization","text":"<p>GRPO expects data organized by groups. For <code>num_prompts=N</code> and <code>num_generations=G</code>:</p> <pre><code># Total batch size = N * G\nbatch_size = num_prompts * num_generations\n\n# Data layout: [prompt1_gen1, prompt1_gen2, ..., prompt1_genG,\n#               prompt2_gen1, prompt2_gen2, ..., prompt2_genG, ...]\nobservations = jnp.zeros((batch_size, obs_dim))\nactions = jnp.zeros((batch_size, action_dim))\nrewards = jnp.zeros((batch_size,))\nlog_probs = jnp.zeros((batch_size,))\n</code></pre>"},{"location":"training/grpo/#custom-group-size","title":"Custom Group Size","text":"<p>Override the default group size per batch:</p> <pre><code>batch = {\n    \"observations\": observations,\n    \"actions\": actions,\n    \"rewards\": rewards,\n    \"log_probs\": log_probs,\n    \"group_size\": 8,  # Override config.num_generations\n}\n</code></pre>"},{"location":"training/grpo/#complete-training-example","title":"Complete Training Example","text":"<pre><code>from artifex.generative_models.training import GRPOConfig, GRPOTrainer\nfrom flax import nnx\nimport optax\nimport jax.numpy as jnp\n\ndef train_with_grpo(\n    model,\n    reward_fn,\n    prompts_loader,\n    num_epochs: int = 10,\n    num_generations: int = 4,\n    learning_rate: float = 1e-4,\n):\n    \"\"\"Train a generative model with GRPO.\"\"\"\n    optimizer = nnx.Optimizer(model, optax.adam(learning_rate), wrt=nnx.Param)\n\n    config = GRPOConfig(\n        num_generations=num_generations,\n        clip_param=0.2,\n        beta=0.01,\n    )\n    trainer = GRPOTrainer(model, optimizer, config)\n\n    for epoch in range(num_epochs):\n        for prompts in prompts_loader:\n            num_prompts = len(prompts)\n\n            # Generate multiple samples per prompt\n            all_samples = []\n            all_log_probs = []\n            for prompt in prompts:\n                for _ in range(num_generations):\n                    sample, log_prob = model.generate(prompt, return_log_prob=True)\n                    all_samples.append(sample)\n                    all_log_probs.append(log_prob)\n\n            samples = jnp.stack(all_samples)\n            log_probs = jnp.stack(all_log_probs)\n\n            # Compute rewards\n            rewards = reward_fn(samples, prompts.repeat(num_generations))\n\n            # GRPO training step\n            batch = {\n                \"observations\": prompts.repeat(num_generations, axis=0),\n                \"actions\": samples,\n                \"rewards\": rewards,\n                \"log_probs\": log_probs,\n            }\n            metrics = trainer.train_step(batch)\n\n            print(f\"Epoch {epoch}: loss={metrics['policy_loss']:.4f}\")\n\n    return model\n</code></pre>"},{"location":"training/grpo/#memory-comparison","title":"Memory Comparison","text":"Method Policy Params Value Params Total Memory PPO P ~P ~2P GRPO P 0 P <p>For a 7B parameter model:</p> <ul> <li>PPO: ~14B parameters (policy + value head)</li> <li>GRPO: ~7B parameters (policy only)</li> </ul> <p>This translates to approximately 50% memory savings.</p>"},{"location":"training/grpo/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":""},{"location":"training/grpo/#number-of-generations-g","title":"Number of Generations (G)","text":"<ul> <li>4: Good default, balance of diversity and efficiency</li> <li>2: Minimum useful, less reliable normalization</li> <li>8: Better statistics, higher compute cost</li> <li>16+: Diminishing returns, very high compute</li> </ul>"},{"location":"training/grpo/#beta-kl-coefficient","title":"Beta (KL Coefficient)","text":"<ul> <li>0.001-0.01: More exploration, faster learning</li> <li>0.01-0.1: Standard range</li> <li>0.1+: Conservative updates, slower but safer</li> </ul>"},{"location":"training/grpo/#clip-parameter","title":"Clip Parameter","text":"<ul> <li>0.2: Standard PPO default</li> <li>0.1-0.3: Reasonable range</li> </ul>"},{"location":"training/grpo/#use-cases","title":"Use Cases","text":"<p>GRPO is recommended for:</p> <ul> <li>Large language models: Where memory is constrained</li> <li>Image generation: Diffusion model fine-tuning with CLIP rewards</li> <li>Resource-limited settings: Single GPU training of large models</li> <li>Fast iteration: Simpler setup than PPO (no value network training)</li> </ul>"},{"location":"training/grpo/#comparison-with-ppo","title":"Comparison with PPO","text":"Aspect GRPO PPO Memory ~50% less Higher Value function None Required Advantage estimation Group normalization GAE Sample efficiency Requires more generations More efficient Implementation complexity Simpler More complex"},{"location":"training/grpo/#related-documentation","title":"Related Documentation","text":"<ul> <li>RL Training Guide - Comprehensive RL training guide</li> <li>PPO Trainer - Traditional actor-critic RL</li> <li>DPO Trainer - Preference-based learning</li> <li>REINFORCE Trainer - Basic policy gradient</li> </ul>"},{"location":"training/grpo/#references","title":"References","text":"<ul> <li>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</li> <li>GRPO in Tunix - Production JAX implementation</li> </ul>"},{"location":"training/linear/","title":"Linear","text":"<p>Module: <code>generative_models.training.schedulers.linear</code></p> <p>Source: <code>generative_models/training/schedulers/linear.py</code></p>"},{"location":"training/linear/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/lion/","title":"Lion","text":"<p>Module: <code>generative_models.training.optimizers.lion</code></p> <p>Source: <code>generative_models/training/optimizers/lion.py</code></p>"},{"location":"training/lion/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/logging/","title":"Logging Callbacks","text":"<p>Module: <code>generative_models.training.callbacks.logging</code></p> <p>Source: <code>generative_models/training/callbacks/logging.py</code></p>"},{"location":"training/logging/#overview","title":"Overview","text":"<p>Logging callbacks for integrating experiment tracking and progress monitoring into training loops. These callbacks wrap existing logging infrastructure for seamless integration with the callback system.</p>"},{"location":"training/logging/#classes","title":"Classes","text":""},{"location":"training/logging/#loggercallbackconfig","title":"LoggerCallbackConfig","text":"<pre><code>@dataclass(slots=True)\nclass LoggerCallbackConfig:\n    log_every_n_steps: int = 1\n    log_on_epoch_end: bool = True\n    prefix: str = \"\"\n</code></pre> <p>Configuration for base logger callback.</p> <p>Attributes:</p> <ul> <li><code>log_every_n_steps</code>: Log metrics every N training steps</li> <li><code>log_on_epoch_end</code>: Whether to log metrics at end of each epoch</li> <li><code>prefix</code>: Prefix to add to metric names</li> </ul>"},{"location":"training/logging/#loggercallback","title":"LoggerCallback","text":"<pre><code>class LoggerCallback(BaseCallback):\n    def __init__(\n        self,\n        logger: Logger,\n        config: Optional[LoggerCallbackConfig] = None,\n    )\n</code></pre> <p>Base callback that wraps any <code>Logger</code> instance for training integration.</p> <p>Parameters:</p> <ul> <li><code>logger</code>: Logger instance to use for logging (from <code>utils.logging</code>)</li> <li><code>config</code>: Logging configuration. Uses defaults if not provided</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.utils.logging import ConsoleLogger\nfrom artifex.generative_models.training.callbacks import LoggerCallback\n\nlogger = ConsoleLogger(name=\"training\")\ncallback = LoggerCallback(logger=logger)\ntrainer.fit(callbacks=[callback])\n</code></pre>"},{"location":"training/logging/#wandbloggerconfig","title":"WandbLoggerConfig","text":"<pre><code>@dataclass(slots=True)\nclass WandbLoggerConfig:\n    project: str\n    entity: Optional[str] = None\n    name: Optional[str] = None\n    tags: list[str] = field(default_factory=list)\n    notes: Optional[str] = None\n    config: dict[str, Any] = field(default_factory=dict)\n    mode: Literal[\"online\", \"offline\", \"disabled\"] = \"online\"\n    resume: Literal[\"allow\", \"never\", \"must\", \"auto\"] | bool | None = None\n    log_every_n_steps: int = 1\n    log_on_epoch_end: bool = True\n    log_dir: Optional[str] = None\n</code></pre> <p>Configuration for W&amp;B logging callback.</p> <p>Attributes:</p> <ul> <li><code>project</code>: W&amp;B project name (required)</li> <li><code>entity</code>: W&amp;B entity (username or team name)</li> <li><code>name</code>: Run name. If None, W&amp;B auto-generates</li> <li><code>tags</code>: List of tags for the run</li> <li><code>notes</code>: Notes about the run</li> <li><code>config</code>: Dictionary of hyperparameters to log</li> <li><code>mode</code>: W&amp;B mode: \"online\", \"offline\", or \"disabled\"</li> <li><code>resume</code>: Whether to resume a previous run</li> <li><code>log_every_n_steps</code>: Log metrics every N training steps</li> <li><code>log_on_epoch_end</code>: Whether to log metrics at end of each epoch</li> <li><code>log_dir</code>: Local directory for W&amp;B files</li> </ul>"},{"location":"training/logging/#wandbloggercallback","title":"WandbLoggerCallback","text":"<pre><code>class WandbLoggerCallback(BaseCallback):\n    def __init__(self, config: WandbLoggerConfig)\n</code></pre> <p>Weights &amp; Biases experiment tracking callback.</p> <p>Features:</p> <ul> <li>Automatic metric logging</li> <li>Hyperparameter tracking</li> <li>Run resumption support</li> <li>Multiple run modes (online, offline, disabled)</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n)\n\nconfig = WandbLoggerConfig(\n    project=\"my-project\",\n    name=\"experiment-1\",\n    tags=[\"baseline\", \"vae\"],\n    config={\"learning_rate\": 1e-3, \"batch_size\": 32},\n)\ncallback = WandbLoggerCallback(config=config)\ntrainer.fit(callbacks=[callback])\n</code></pre>"},{"location":"training/logging/#tensorboardloggerconfig","title":"TensorBoardLoggerConfig","text":"<pre><code>@dataclass(slots=True)\nclass TensorBoardLoggerConfig:\n    log_dir: str = \"logs/tensorboard\"\n    flush_secs: int = 120\n    max_queue: int = 10\n    log_every_n_steps: int = 1\n    log_on_epoch_end: bool = True\n    log_graph: bool = False\n</code></pre> <p>Configuration for TensorBoard logging callback.</p> <p>Attributes:</p> <ul> <li><code>log_dir</code>: Directory for TensorBoard logs</li> <li><code>flush_secs</code>: How often to flush to disk (seconds)</li> <li><code>max_queue</code>: Max queue size for pending events</li> <li><code>log_every_n_steps</code>: Log metrics every N training steps</li> <li><code>log_on_epoch_end</code>: Whether to log metrics at end of each epoch</li> <li><code>log_graph</code>: Whether to log model graph</li> </ul>"},{"location":"training/logging/#tensorboardloggercallback","title":"TensorBoardLoggerCallback","text":"<pre><code>class TensorBoardLoggerCallback(BaseCallback):\n    def __init__(self, config: TensorBoardLoggerConfig)\n</code></pre> <p>TensorBoard logging callback.</p> <p>Features:</p> <ul> <li>Scalar metrics logging</li> <li>Configurable flush interval</li> <li>Per-step and per-epoch logging</li> </ul> <p>Requirements: Requires <code>tensorboard</code> package (<code>pip install tensorboard</code>)</p> <p>Example:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    TensorBoardLoggerCallback,\n    TensorBoardLoggerConfig,\n)\n\nconfig = TensorBoardLoggerConfig(\n    log_dir=\"logs/experiment-1\",\n    flush_secs=60,\n)\ncallback = TensorBoardLoggerCallback(config=config)\ntrainer.fit(callbacks=[callback])\n</code></pre>"},{"location":"training/logging/#progressbarconfig","title":"ProgressBarConfig","text":"<pre><code>@dataclass(slots=True)\nclass ProgressBarConfig:\n    refresh_rate: int = 10\n    show_eta: bool = True\n    show_metrics: bool = True\n    leave: bool = True\n    disable: bool = False\n</code></pre> <p>Configuration for progress bar callback.</p> <p>Attributes:</p> <ul> <li><code>refresh_rate</code>: How often to refresh the progress bar (steps)</li> <li><code>show_eta</code>: Whether to show estimated time of arrival</li> <li><code>show_metrics</code>: Whether to display metrics in progress bar</li> <li><code>leave</code>: Whether to leave progress bar after completion</li> <li><code>disable</code>: Whether to disable progress bar entirely</li> </ul>"},{"location":"training/logging/#progressbarcallback","title":"ProgressBarCallback","text":"<pre><code>class ProgressBarCallback(BaseCallback):\n    def __init__(self, config: Optional[ProgressBarConfig] = None)\n</code></pre> <p>Rich console progress bar callback.</p> <p>Features:</p> <ul> <li>Real-time training progress</li> <li>Metric display</li> <li>ETA estimation</li> <li>Nested progress for epochs/steps</li> </ul> <p>Requirements: Requires <code>rich</code> package (<code>pip install rich</code>)</p> <p>Example:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    ProgressBarCallback,\n    ProgressBarConfig,\n)\n\nconfig = ProgressBarConfig(\n    refresh_rate=10,\n    show_eta=True,\n    show_metrics=True,\n)\ncallback = ProgressBarCallback(config=config)\ntrainer.fit(callbacks=[callback])\n</code></pre>"},{"location":"training/logging/#usage-with-multiple-callbacks","title":"Usage with Multiple Callbacks","text":"<p>Logging callbacks can be combined with other callbacks:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    CallbackList,\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n    ProgressBarCallback,\n)\n\n# Configure callbacks\ncallbacks = CallbackList([\n    # Logging\n    WandbLoggerCallback(WandbLoggerConfig(\n        project=\"my-project\",\n        name=\"experiment-1\",\n    )),\n    ProgressBarCallback(),\n\n    # Training control\n    EarlyStopping(EarlyStoppingConfig(\n        monitor=\"val_loss\",\n        patience=10,\n    )),\n\n    # Checkpointing\n    ModelCheckpoint(CheckpointConfig(\n        dirpath=\"checkpoints\",\n        monitor=\"val_loss\",\n    )),\n])\n\ntrainer.fit(callbacks=callbacks)\n</code></pre>"},{"location":"training/logging/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 8</li> <li>Functions: 0</li> <li>Imports: 6</li> </ul>"},{"location":"training/mesh/","title":"Device Mesh Management","text":"<p>Module: <code>artifex.generative_models.training.distributed.mesh</code></p> <p>Source: <code>src/artifex/generative_models/training/distributed/mesh.py</code></p>"},{"location":"training/mesh/#overview","title":"Overview","text":"<p>The <code>DeviceMeshManager</code> class provides utilities for creating and managing JAX device meshes for distributed training. It supports various parallelism strategies including data parallelism, model parallelism, and hybrid parallelism.</p>"},{"location":"training/mesh/#classes","title":"Classes","text":""},{"location":"training/mesh/#devicemeshmanager","title":"DeviceMeshManager","text":"<p>Manager for creating and configuring JAX device meshes.</p> <pre><code>class DeviceMeshManager:\n    \"\"\"Manager for creating and configuring JAX device meshes.\n\n    This class provides methods for creating device meshes with various\n    configurations for data parallelism, model parallelism, and hybrid\n    parallelism strategies.\n    \"\"\"\n</code></pre>"},{"location":"training/mesh/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, devices: Sequence[Any] | None = None) -&gt; None:\n    \"\"\"Initialize DeviceMeshManager.\n\n    Args:\n        devices: Optional list of devices to use. If None, uses all\n            available devices from jax.devices().\n    \"\"\"\n</code></pre>"},{"location":"training/mesh/#methods","title":"Methods","text":""},{"location":"training/mesh/#create_device_mesh","title":"create_device_mesh","text":"<p>Create a device mesh with the specified shape.</p> <pre><code>def create_device_mesh(\n    self,\n    mesh_shape: dict[str, int] | list[tuple[str, int]],\n    devices: Sequence[Any] | None = None,\n) -&gt; Mesh:\n    \"\"\"Create a device mesh with the specified shape.\n\n    Args:\n        mesh_shape: Shape specification as either:\n            - dict mapping axis names to sizes, e.g., {\"data\": 2, \"model\": 1}\n            - list of (axis_name, size) tuples, e.g., [(\"data\", 2), (\"model\", 1)]\n        devices: Optional list of devices to use.\n\n    Returns:\n        A JAX Mesh with the specified configuration.\n\n    Raises:\n        ValueError: If mesh requires more devices than available.\n    \"\"\"\n</code></pre>"},{"location":"training/mesh/#create_data_parallel_mesh","title":"create_data_parallel_mesh","text":"<p>Create a mesh for data parallelism.</p> <pre><code>def create_data_parallel_mesh(\n    self,\n    num_devices: int | None = None,\n    axis_name: str = \"data\",\n) -&gt; Mesh:\n    \"\"\"Create a mesh for data parallelism.\n\n    Args:\n        num_devices: Number of devices to use. If None, uses all available.\n        axis_name: Name of the data parallel axis.\n\n    Returns:\n        A JAX Mesh configured for data parallelism.\n    \"\"\"\n</code></pre>"},{"location":"training/mesh/#create_model_parallel_mesh","title":"create_model_parallel_mesh","text":"<p>Create a mesh for model parallelism.</p> <pre><code>def create_model_parallel_mesh(\n    self,\n    num_devices: int | None = None,\n    axis_name: str = \"model\",\n) -&gt; Mesh:\n    \"\"\"Create a mesh for model parallelism.\n\n    Args:\n        num_devices: Number of devices to use. If None, uses all available.\n        axis_name: Name of the model parallel axis.\n\n    Returns:\n        A JAX Mesh configured for model parallelism.\n    \"\"\"\n</code></pre>"},{"location":"training/mesh/#create_hybrid_mesh","title":"create_hybrid_mesh","text":"<p>Create a mesh for hybrid data and model parallelism.</p> <pre><code>def create_hybrid_mesh(\n    self,\n    data_parallel_size: int = 1,\n    model_parallel_size: int = 1,\n    data_axis: str = \"data\",\n    model_axis: str = \"model\",\n) -&gt; Mesh:\n    \"\"\"Create a mesh for hybrid data and model parallelism.\n\n    Args:\n        data_parallel_size: Number of devices for data parallelism.\n        model_parallel_size: Number of devices for model parallelism.\n        data_axis: Name of the data parallel axis.\n        model_axis: Name of the model parallel axis.\n\n    Returns:\n        A JAX Mesh configured for hybrid parallelism.\n    \"\"\"\n</code></pre>"},{"location":"training/mesh/#get_mesh_info","title":"get_mesh_info","text":"<p>Get information about a device mesh.</p> <pre><code>def get_mesh_info(self, mesh: Mesh) -&gt; dict[str, Any]:\n    \"\"\"Get information about a device mesh.\n\n    Args:\n        mesh: The mesh to get information about.\n\n    Returns:\n        Dictionary containing mesh information with keys:\n            - total_devices: Total number of devices in the mesh\n            - axes: Dict mapping axis names to their sizes\n    \"\"\"\n</code></pre>"},{"location":"training/mesh/#properties","title":"Properties","text":"<ul> <li><code>num_devices: int</code> - Number of available devices</li> <li><code>devices: list[Any]</code> - List of available devices</li> </ul>"},{"location":"training/mesh/#usage-examples","title":"Usage Examples","text":""},{"location":"training/mesh/#basic-usage","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.training.distributed import DeviceMeshManager\n\n# Create manager (uses all available devices)\nmanager = DeviceMeshManager()\nprint(f\"Available devices: {manager.num_devices}\")\n\n# Create a data-parallel mesh using all devices\nmesh = manager.create_data_parallel_mesh()\n</code></pre>"},{"location":"training/mesh/#data-parallelism","title":"Data Parallelism","text":"<pre><code># Create mesh for data parallelism with 4 devices\nmanager = DeviceMeshManager()\nmesh = manager.create_data_parallel_mesh(num_devices=4, axis_name=\"batch\")\n\n# Use the mesh with JAX sharding\nfrom jax.sharding import NamedSharding, PartitionSpec\n\n# Shard data along batch dimension\ndata_sharding = NamedSharding(mesh, PartitionSpec(\"batch\"))\n</code></pre>"},{"location":"training/mesh/#model-parallelism","title":"Model Parallelism","text":"<pre><code># Create mesh for model parallelism\nmanager = DeviceMeshManager()\nmesh = manager.create_model_parallel_mesh(num_devices=2, axis_name=\"model\")\n\n# Shard model parameters\nparam_sharding = NamedSharding(mesh, PartitionSpec(None, \"model\"))\n</code></pre>"},{"location":"training/mesh/#hybrid-parallelism","title":"Hybrid Parallelism","text":"<pre><code># Create 2D mesh: 2 devices for data, 2 for model (total 4 devices)\nmanager = DeviceMeshManager()\nmesh = manager.create_hybrid_mesh(\n    data_parallel_size=2,\n    model_parallel_size=2,\n    data_axis=\"data\",\n    model_axis=\"model\",\n)\n\n# Get mesh info\ninfo = manager.get_mesh_info(mesh)\nprint(f\"Total devices: {info['total_devices']}\")\nprint(f\"Axes: {info['axes']}\")  # {'data': 2, 'model': 2}\n</code></pre>"},{"location":"training/mesh/#using-dict-or-list-specification","title":"Using Dict or List Specification","text":"<pre><code>manager = DeviceMeshManager()\n\n# Using dict specification\nmesh_dict = manager.create_device_mesh({\"data\": 2, \"model\": 1})\n\n# Using list of tuples specification\nmesh_list = manager.create_device_mesh([(\"data\", 2), (\"model\", 1)])\n</code></pre>"},{"location":"training/mesh/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1 (<code>DeviceMeshManager</code>)</li> <li>Methods: 5 public methods</li> <li>Properties: 2 (<code>num_devices</code>, <code>devices</code>)</li> </ul>"},{"location":"training/mixed_precision/","title":"Mixed Precision","text":"<p>Module: <code>generative_models.training.mixed_precision</code></p> <p>Source: <code>generative_models/training/mixed_precision.py</code></p>"},{"location":"training/mixed_precision/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/model_parallel/","title":"Model Parallel","text":"<p>Module: <code>artifex.generative_models.training.distributed.model_parallel</code></p> <p>Source: <code>src/artifex/generative_models/training/distributed/model_parallel.py</code></p>"},{"location":"training/model_parallel/#overview","title":"Overview","text":"<p>Model parallelism (tensor parallelism) splits model layers across devices, useful when models don't fit in single-device memory. This module is planned for future implementation.</p>"},{"location":"training/model_parallel/#current-status","title":"Current Status","text":"<p>This module is part of the planned distributed training infrastructure and is scheduled for implementation in a future release. For current model parallelism needs, you can use JAX's native sharding APIs directly.</p>"},{"location":"training/model_parallel/#planned-features","title":"Planned Features","text":"<p>The model parallel module will provide:</p> <ul> <li>FSDP-style parameter sharding - Full parameter sharding across devices</li> <li>Gradient-only sharding (ZeRO-2) - Shard only optimizer states and gradients</li> <li>Automatic resharding - Manage parameter gathering and scattering</li> <li>Memory-efficient training - Enable training of models larger than single-device memory</li> </ul>"},{"location":"training/model_parallel/#using-jax-native-sharding-current-approach","title":"Using JAX Native Sharding (Current Approach)","text":"<p>Until the model parallel module is implemented, use JAX's native sharding APIs:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec as P, NamedSharding\nfrom flax import nnx\n\n# Create 2D device mesh: (data_parallel, model_parallel)\ndevices = jax.devices()\nmesh = Mesh(\n    devices.reshape(2, 2),  # 2 data parallel, 2 model parallel\n    axis_names=(\"data\", \"model\")\n)\n\n# Define sharding for model parameters\n# Shard weights along model axis, replicate bias\nweight_sharding = NamedSharding(mesh, P(None, \"model\"))  # (in_features, out_features)\nbias_sharding = NamedSharding(mesh, P(\"model\"))  # (out_features,)\n\n# Create model with sharded parameters\nclass ShardedLinear(nnx.Module):\n    \"\"\"Linear layer with sharded weights.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        *,\n        rngs: nnx.Rngs,\n        mesh: Mesh,\n    ):\n        super().__init__()\n\n        # Create weight with sharding\n        self.weight = nnx.Param(\n            nnx.initializers.lecun_normal()(\n                rngs.params(),\n                (in_features, out_features)\n            )\n        )\n\n        # Apply sharding\n        self.weight = jax.device_put(\n            self.weight,\n            NamedSharding(mesh, P(None, \"model\"))\n        )\n\n        # Create bias with sharding\n        self.bias = nnx.Param(\n            jnp.zeros(out_features)\n        )\n        self.bias = jax.device_put(\n            self.bias,\n            NamedSharding(mesh, P(\"model\"))\n        )\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        # Computation automatically distributed\n        return x @ self.weight + self.bias\n</code></pre>"},{"location":"training/model_parallel/#related-documentation","title":"Related Documentation","text":"<p>For current distributed training capabilities, see:</p> <ul> <li>Device Mesh Management - Creating and managing device meshes</li> <li>Data Parallel Training - Data parallelism utilities</li> <li>Device Placement - Explicit device placement</li> <li>Distributed Metrics - Aggregating metrics across devices</li> </ul>"},{"location":"training/model_parallel/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0 (Planned for future release)</li> <li>Functions: 0 (Planned for future release)</li> <li>Imports: 0</li> </ul>"},{"location":"training/optax_wrappers/","title":"Optax Wrappers","text":"<p>Module: <code>generative_models.training.optimizers.optax_wrappers</code></p> <p>Source: <code>generative_models/training/optimizers/optax_wrappers.py</code></p>"},{"location":"training/optax_wrappers/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/ppo/","title":"PPO Trainer","text":"<p>Module: <code>artifex.generative_models.training.rl.ppo</code></p> <p>The PPO (Proximal Policy Optimization) Trainer provides stable policy gradient training through clipped surrogate objectives and Generalized Advantage Estimation (GAE).</p>"},{"location":"training/ppo/#overview","title":"Overview","text":"<p>PPO is a state-of-the-art policy gradient method that maintains training stability through:</p> <ul> <li>Clipped Surrogate Loss: Prevents large policy updates</li> <li>Generalized Advantage Estimation: Balances bias-variance in advantage computation</li> <li>Value Function Learning: Learns state values for advantage estimation</li> <li>Entropy Bonus: Encourages exploration</li> </ul>"},{"location":"training/ppo/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training import PPOConfig, PPOTrainer\nfrom flax import nnx\nimport optax\n\n# Create actor-critic model\nmodel = ActorCriticModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(3e-4), wrt=nnx.Param)\n\n# Configure PPO\nconfig = PPOConfig(\n    gamma=0.99,\n    gae_lambda=0.95,\n    clip_param=0.2,\n    vf_coeff=0.5,\n    entropy_coeff=0.01,\n    max_grad_norm=0.5,\n)\n\ntrainer = PPOTrainer(model, optimizer, config)\n\n# Training step with trajectory\ntrajectory = {\n    \"observations\": observations,\n    \"actions\": actions,\n    \"rewards\": rewards,\n    \"values\": values,\n    \"log_probs\": old_log_probs,\n    \"dones\": dones,\n}\nmetrics = trainer.train_step(trajectory)\n</code></pre>"},{"location":"training/ppo/#configuration","title":"Configuration","text":""},{"location":"training/ppo/#artifex.generative_models.training.rl.configs.PPOConfig","title":"artifex.generative_models.training.rl.configs.PPOConfig  <code>dataclass</code>","text":"<pre><code>PPOConfig(\n    gamma: float = 0.99,\n    gae_lambda: float = 0.95,\n    clip_param: float = 0.2,\n    vf_coeff: float = 0.5,\n    entropy_coeff: float = 0.01,\n    max_grad_norm: float = 0.5,\n)\n</code></pre> <p>Configuration for Proximal Policy Optimization.</p> <p>Implements PPO with clipped surrogate objective and GAE.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <code>float</code> <p>Discount factor for computing returns. Default 0.99.</p> <code>gae_lambda</code> <code>float</code> <p>Lambda for Generalized Advantage Estimation. Default 0.95.</p> <code>clip_param</code> <code>float</code> <p>Clipping parameter for surrogate objective. Default 0.2.</p> <code>vf_coeff</code> <code>float</code> <p>Coefficient for value function loss. Default 0.5.</p> <code>entropy_coeff</code> <code>float</code> <p>Coefficient for entropy bonus. Default 0.01.</p> <code>max_grad_norm</code> <code>float</code> <p>Maximum gradient norm for clipping. Default 0.5.</p>"},{"location":"training/ppo/#artifex.generative_models.training.rl.configs.PPOConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.99\n</code></pre>"},{"location":"training/ppo/#artifex.generative_models.training.rl.configs.PPOConfig.gae_lambda","title":"gae_lambda  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gae_lambda: float = 0.95\n</code></pre>"},{"location":"training/ppo/#artifex.generative_models.training.rl.configs.PPOConfig.clip_param","title":"clip_param  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>clip_param: float = 0.2\n</code></pre>"},{"location":"training/ppo/#artifex.generative_models.training.rl.configs.PPOConfig.vf_coeff","title":"vf_coeff  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vf_coeff: float = 0.5\n</code></pre>"},{"location":"training/ppo/#artifex.generative_models.training.rl.configs.PPOConfig.entropy_coeff","title":"entropy_coeff  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>entropy_coeff: float = 0.01\n</code></pre>"},{"location":"training/ppo/#artifex.generative_models.training.rl.configs.PPOConfig.max_grad_norm","title":"max_grad_norm  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_grad_norm: float = 0.5\n</code></pre>"},{"location":"training/ppo/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>gamma</code> <code>float</code> <code>0.99</code> Discount factor for GAE <code>gae_lambda</code> <code>float</code> <code>0.95</code> Lambda for GAE (bias-variance trade-off) <code>clip_param</code> <code>float</code> <code>0.2</code> Clipping parameter epsilon <code>vf_coeff</code> <code>float</code> <code>0.5</code> Value function loss coefficient <code>entropy_coeff</code> <code>float</code> <code>0.01</code> Entropy bonus coefficient <code>max_grad_norm</code> <code>float</code> <code>0.5</code> Maximum gradient norm for clipping"},{"location":"training/ppo/#algorithm","title":"Algorithm","text":""},{"location":"training/ppo/#clipped-surrogate-objective","title":"Clipped Surrogate Objective","text":"<p>PPO uses a clipped surrogate objective to prevent large policy updates:</p> \\[\\mathcal{L}^{CLIP} = \\mathbb{E}\\left[\\min\\left(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\\right)\\right]\\] <p>Where \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\) is the probability ratio.</p>"},{"location":"training/ppo/#generalized-advantage-estimation","title":"Generalized Advantage Estimation","text":"<p>GAE computes advantages using TD residuals:</p> \\[A_t^{GAE} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}\\] <p>Where \\(\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\) is the TD residual.</p> <p>Lambda parameter:</p> <ul> <li>\\(\\lambda = 0\\): TD(0) - low variance, high bias</li> <li>\\(\\lambda = 1\\): Monte Carlo - high variance, low bias</li> <li>\\(\\lambda = 0.95\\): Good balance (default)</li> </ul>"},{"location":"training/ppo/#value-function-loss","title":"Value Function Loss","text":"\\[\\mathcal{L}^{VF} = (V_\\theta(s_t) - V_t^{target})^2\\]"},{"location":"training/ppo/#full-objective","title":"Full Objective","text":"\\[\\mathcal{L} = -\\mathcal{L}^{CLIP} + c_1 \\mathcal{L}^{VF} - c_2 H(\\pi)\\]"},{"location":"training/ppo/#api-reference","title":"API Reference","text":""},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer","title":"artifex.generative_models.training.rl.ppo.PPOTrainer","text":"<pre><code>PPOTrainer(\n    model: Module,\n    optimizer: Optimizer,\n    config: PPOConfig | None = None,\n)\n</code></pre> <p>Proximal Policy Optimization trainer.</p> <p>Implements PPO with: - Clipped surrogate objective for stable policy updates - GAE for advantage estimation - Value function fitting - Entropy bonus for exploration - Gradient clipping</p> <p>The model must be an Actor-Critic that returns (action_logits, value).</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>Actor-Critic network.</p> <code>optimizer</code> <p>Flax NNX optimizer.</p> <code>config</code> <p>PPO configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Actor-Critic network that returns (logits, value).</p> required <code>optimizer</code> <code>Optimizer</code> <p>Flax NNX optimizer for the model.</p> required <code>config</code> <code>PPOConfig | None</code> <p>PPO configuration. Uses defaults if not provided.</p> <code>None</code>"},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = optimizer\n</code></pre>"},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config if config is not None else PPOConfig()\n</code></pre>"},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer.compute_gae","title":"compute_gae","text":"<pre><code>compute_gae(\n    rewards: Array, values: Array, dones: Array\n) -&gt; Array\n</code></pre> <p>Compute Generalized Advantage Estimation.</p> <p>Parameters:</p> Name Type Description Default <code>rewards</code> <code>Array</code> <p>Rewards with shape (T,).</p> required <code>values</code> <code>Array</code> <p>Values with shape (T+1,), including next state value.</p> required <code>dones</code> <code>Array</code> <p>Done flags with shape (T,).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Advantages with shape (T,).</p>"},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer.compute_clipped_loss","title":"compute_clipped_loss","text":"<pre><code>compute_clipped_loss(\n    log_probs: Array,\n    old_log_probs: Array,\n    advantages: Array,\n) -&gt; Array\n</code></pre> <p>Compute clipped surrogate policy loss.</p> <p>Parameters:</p> Name Type Description Default <code>log_probs</code> <code>Array</code> <p>Current policy log probabilities.</p> required <code>old_log_probs</code> <code>Array</code> <p>Old policy log probabilities.</p> required <code>advantages</code> <code>Array</code> <p>Advantage estimates.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Clipped surrogate loss.</p>"},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer.compute_value_loss","title":"compute_value_loss","text":"<pre><code>compute_value_loss(values: Array, returns: Array) -&gt; Array\n</code></pre> <p>Compute value function loss (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Array</code> <p>Predicted values.</p> required <code>returns</code> <code>Array</code> <p>Target returns.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Value function loss.</p>"},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer.compute_entropy","title":"compute_entropy","text":"<pre><code>compute_entropy(log_probs: Array) -&gt; Array\n</code></pre> <p>Compute policy entropy.</p> <p>Parameters:</p> Name Type Description Default <code>log_probs</code> <code>Array</code> <p>Log probabilities with shape (..., num_actions).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Mean entropy.</p>"},{"location":"training/ppo/#artifex.generative_models.training.rl.ppo.PPOTrainer.train_step","title":"train_step","text":"<pre><code>train_step(\n    batch: dict[str, Array],\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Perform a single PPO training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Array]</code> <p>Dictionary containing: - \"states\": Batch of states. - \"actions\": Actions taken. - \"old_log_probs\": Log probs from old policy. - \"returns\": Target returns. - \"advantages\": Advantage estimates.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/ppo/#training-metrics","title":"Training Metrics","text":"Metric Description <code>policy_loss</code> Clipped surrogate policy loss <code>value_loss</code> Value function MSE loss <code>entropy</code> Policy entropy (exploration measure)"},{"location":"training/ppo/#model-requirements","title":"Model Requirements","text":"<p>PPO requires an actor-critic model that outputs both action probabilities and value estimates:</p> <pre><code>class ActorCriticModel(nnx.Module):\n    def __call__(self, observations) -&gt; tuple[jax.Array, jax.Array]:\n        \"\"\"Forward pass returning (log_probs, values).\n\n        Args:\n            observations: State observations.\n\n        Returns:\n            Tuple of:\n                - log_probs: Action log probabilities, shape (batch, num_actions)\n                - values: State value estimates, shape (batch,)\n        \"\"\"\n        ...\n</code></pre>"},{"location":"training/ppo/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":""},{"location":"training/ppo/#clip-parameter-epsilon","title":"Clip Parameter (epsilon)","text":"<ul> <li>0.1-0.2: Standard range, 0.2 is most common</li> <li>Lower values \u2192 more conservative updates</li> <li>Higher values \u2192 larger policy changes allowed</li> </ul>"},{"location":"training/ppo/#gae-lambda","title":"GAE Lambda","text":"<ul> <li>0.95: Good default for most tasks</li> <li>0.99: Lower bias, higher variance (longer-horizon tasks)</li> <li>0.9: Higher bias, lower variance (shorter-horizon tasks)</li> </ul>"},{"location":"training/ppo/#value-function-coefficient","title":"Value Function Coefficient","text":"<ul> <li>0.5: Standard choice</li> <li>Higher values \u2192 more emphasis on accurate value estimation</li> </ul>"},{"location":"training/ppo/#use-cases","title":"Use Cases","text":"<p>PPO is recommended for:</p> <ul> <li>Complex tasks: When REINFORCE is too unstable</li> <li>Continuous control: Robotics, physics simulations</li> <li>Games: Atari, board games, video games</li> <li>Large models: When you can afford the value network memory</li> </ul> <p>For memory-constrained settings, consider GRPO.</p>"},{"location":"training/ppo/#related-documentation","title":"Related Documentation","text":"<ul> <li>RL Training Guide - Comprehensive RL training guide</li> <li>REINFORCE Trainer - Simpler baseline algorithm</li> <li>GRPO Trainer - Memory-efficient alternative</li> <li>DPO Trainer - Preference-based learning</li> </ul>"},{"location":"training/profiling/","title":"Profiling Callbacks","text":"<p>Module: <code>generative_models.training.callbacks.profiling</code></p> <p>Source: <code>generative_models/training/callbacks/profiling.py</code></p>"},{"location":"training/profiling/#overview","title":"Overview","text":"<p>Profiling callbacks for integrating JAX-native performance analysis into training loops. These callbacks provide trace-based profiling for TensorBoard visualization and memory usage tracking with minimal overhead.</p>"},{"location":"training/profiling/#classes","title":"Classes","text":""},{"location":"training/profiling/#profilingconfig","title":"ProfilingConfig","text":"<pre><code>@dataclass(slots=True)\nclass ProfilingConfig:\n    log_dir: str = \"logs/profiles\"\n    start_step: int = 10\n    end_step: int = 20\n    trace_memory: bool = True\n    trace_python: bool = False\n</code></pre> <p>Configuration for JAX trace profiling.</p> <p>Attributes:</p> <ul> <li><code>log_dir</code>: Directory to save profiling traces</li> <li><code>start_step</code>: Step at which to start profiling (skip warmup)</li> <li><code>end_step</code>: Step at which to stop profiling</li> <li><code>trace_memory</code>: Whether to include memory usage in traces</li> <li><code>trace_python</code>: Whether to trace Python execution (slower but more detail)</li> </ul>"},{"location":"training/profiling/#jaxprofiler","title":"JAXProfiler","text":"<pre><code>class JAXProfiler(BaseCallback):\n    def __init__(self, config: ProfilingConfig)\n</code></pre> <p>JAX profiler callback for performance analysis.</p> <p>Integrates with JAX's built-in profiler to capture traces that can be viewed in TensorBoard or Perfetto. Automatically skips warmup steps to get more representative profiling data.</p> <p>Features:</p> <ul> <li>Integration with JAX's built-in profiler</li> <li>TensorBoard trace visualization</li> <li>Configurable profiling window (start/end steps)</li> <li>Automatic cleanup on training end</li> <li>No interference with JIT compilation</li> <li>Minimal overhead outside profiling window</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    JAXProfiler,\n    ProfilingConfig,\n)\n\nconfig = ProfilingConfig(\n    log_dir=\"logs/profiles\",\n    start_step=10,  # Skip JIT warmup\n    end_step=20,    # Profile 10 steps\n)\nprofiler = JAXProfiler(config)\ntrainer.fit(callbacks=[profiler])\n\n# View in TensorBoard:\n# tensorboard --logdir logs/profiles\n</code></pre> <p>Best Practices:</p> <ul> <li>Set <code>start_step</code> after JIT warmup (typically 5-10 steps)</li> <li>Keep profiling window small (10-20 steps) to minimize impact</li> <li>Use <code>trace_python=True</code> only when debugging Python bottlenecks</li> <li>Traces can be viewed in TensorBoard or Perfetto</li> </ul>"},{"location":"training/profiling/#memoryprofileconfig","title":"MemoryProfileConfig","text":"<pre><code>@dataclass(slots=True)\nclass MemoryProfileConfig:\n    log_dir: str = \"logs/memory\"\n    profile_every_n_steps: int = 100\n    log_device_memory: bool = True\n</code></pre> <p>Configuration for memory profiling.</p> <p>Attributes:</p> <ul> <li><code>log_dir</code>: Directory to save memory profile data</li> <li><code>profile_every_n_steps</code>: Collect memory info every N steps</li> <li><code>log_device_memory</code>: Whether to log device (GPU/TPU) memory stats</li> </ul>"},{"location":"training/profiling/#memoryprofiler","title":"MemoryProfiler","text":"<pre><code>class MemoryProfiler(BaseCallback):\n    def __init__(self, config: MemoryProfileConfig)\n</code></pre> <p>Memory usage profiling callback.</p> <p>Tracks memory usage during training and saves a timeline to JSON. Useful for identifying memory leaks and understanding memory patterns.</p> <p>Features:</p> <ul> <li>Track JAX device memory usage (GPU/TPU)</li> <li>Log peak memory per step</li> <li>Export memory timeline to JSON</li> <li>Configurable profiling interval</li> <li>Minimal overhead between collection intervals</li> </ul> <p>Example:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    MemoryProfiler,\n    MemoryProfileConfig,\n)\n\nconfig = MemoryProfileConfig(\n    log_dir=\"logs/memory\",\n    profile_every_n_steps=50,\n)\nprofiler = MemoryProfiler(config)\ntrainer.fit(callbacks=[profiler])\n\n# Memory profile saved to logs/memory/memory_profile.json\n</code></pre> <p>Output Format:</p> <p>The memory profile is saved as JSON with the following structure:</p> <pre><code>[\n  {\n    \"step\": 0,\n    \"memory\": {\n      \"cuda:0\": {\n        \"bytes_in_use\": 1073741824,\n        \"peak_bytes_in_use\": 2147483648\n      }\n    }\n  },\n  {\n    \"step\": 100,\n    \"memory\": {\n      \"cuda:0\": {\n        \"bytes_in_use\": 1073741824,\n        \"peak_bytes_in_use\": 2147483648\n      }\n    }\n  }\n]\n</code></pre> <p>Note: Not all devices support <code>memory_stats()</code>. CPU devices typically return <code>None</code>, in which case those devices are skipped.</p>"},{"location":"training/profiling/#usage-with-multiple-callbacks","title":"Usage with Multiple Callbacks","text":"<p>Profiling callbacks can be combined with other callbacks:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    CallbackList,\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n    JAXProfiler,\n    ProfilingConfig,\n    MemoryProfiler,\n    MemoryProfileConfig,\n    ProgressBarCallback,\n)\n\n# Configure callbacks\ncallbacks = CallbackList([\n    # Profiling (runs first to capture full training)\n    JAXProfiler(ProfilingConfig(\n        log_dir=\"logs/profiles\",\n        start_step=10,\n        end_step=20,\n    )),\n    MemoryProfiler(MemoryProfileConfig(\n        log_dir=\"logs/memory\",\n        profile_every_n_steps=100,\n    )),\n\n    # Progress display\n    ProgressBarCallback(),\n\n    # Training control\n    EarlyStopping(EarlyStoppingConfig(\n        monitor=\"val_loss\",\n        patience=10,\n    )),\n\n    # Checkpointing\n    ModelCheckpoint(CheckpointConfig(\n        dirpath=\"checkpoints\",\n        monitor=\"val_loss\",\n    )),\n])\n\ntrainer.fit(callbacks=callbacks)\n</code></pre>"},{"location":"training/profiling/#performance-considerations","title":"Performance Considerations","text":"<p>Both profiling callbacks are designed for minimal overhead:</p> <ul> <li>JAXProfiler: Zero overhead outside the profiling window (start_step to end_step)</li> <li>MemoryProfiler: Only collects data at configured intervals; no overhead between intervals</li> </ul> <p>The callbacks do not interfere with JAX's JIT compilation. JIT-compiled functions produce identical results before, during, and after profiling.</p>"},{"location":"training/profiling/#viewing-traces","title":"Viewing Traces","text":""},{"location":"training/profiling/#tensorboard","title":"TensorBoard","text":"<pre><code>tensorboard --logdir logs/profiles\n</code></pre> <p>Navigate to the \"Profile\" tab to view:</p> <ul> <li>XLA compilation times</li> <li>Device execution times</li> <li>Memory allocation patterns</li> <li>Kernel execution traces</li> </ul>"},{"location":"training/profiling/#perfetto","title":"Perfetto","text":"<ol> <li>Go to Perfetto UI</li> <li>Click \"Open trace file\"</li> <li>Select the <code>.trace</code> file from your log directory</li> </ol> <p>Perfetto provides more detailed trace analysis capabilities including:</p> <ul> <li>Timeline visualization</li> <li>Flame graphs</li> <li>Memory analysis</li> </ul>"},{"location":"training/profiling/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 4</li> <li>Functions: 0</li> <li>Imports: 4</li> </ul>"},{"location":"training/reinforce/","title":"REINFORCE Trainer","text":"<p>Module: <code>artifex.generative_models.training.rl.reinforce</code></p> <p>The REINFORCE Trainer implements the basic policy gradient algorithm with variance reduction through return normalization and entropy bonus for exploration.</p>"},{"location":"training/reinforce/#overview","title":"Overview","text":"<p>REINFORCE is the simplest policy gradient algorithm, computing gradient updates based on discounted returns. This implementation includes:</p> <ul> <li>Discounted Returns: Efficient backward pass computation</li> <li>Return Normalization: Variance reduction for stable training</li> <li>Entropy Bonus: Encourages exploration and prevents premature convergence</li> </ul>"},{"location":"training/reinforce/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training import (\n    REINFORCEConfig,\n    REINFORCETrainer,\n)\nfrom flax import nnx\nimport optax\n\n# Create model and optimizer\nmodel = PolicyModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n# Configure REINFORCE\nconfig = REINFORCEConfig(\n    gamma=0.99,\n    normalize_returns=True,\n    entropy_coeff=0.01,\n)\n\ntrainer = REINFORCETrainer(model, optimizer, config)\n\n# Training step\nbatch = {\n    \"observations\": observations,\n    \"actions\": actions,\n    \"rewards\": rewards,\n}\nmetrics = trainer.train_step(batch)\n</code></pre>"},{"location":"training/reinforce/#configuration","title":"Configuration","text":""},{"location":"training/reinforce/#artifex.generative_models.training.rl.configs.REINFORCEConfig","title":"artifex.generative_models.training.rl.configs.REINFORCEConfig  <code>dataclass</code>","text":"<pre><code>REINFORCEConfig(\n    gamma: float = 0.99,\n    normalize_returns: bool = True,\n    entropy_coeff: float = 0.01,\n)\n</code></pre> <p>Configuration for REINFORCE policy gradient algorithm.</p> <p>Attributes:</p> Name Type Description <code>gamma</code> <code>float</code> <p>Discount factor for computing returns. Default 0.99.</p> <code>normalize_returns</code> <code>bool</code> <p>Whether to normalize returns for variance reduction.</p> <code>entropy_coeff</code> <code>float</code> <p>Coefficient for entropy bonus to encourage exploration.</p>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.configs.REINFORCEConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.99\n</code></pre>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.configs.REINFORCEConfig.normalize_returns","title":"normalize_returns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>normalize_returns: bool = True\n</code></pre>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.configs.REINFORCEConfig.entropy_coeff","title":"entropy_coeff  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>entropy_coeff: float = 0.01\n</code></pre>"},{"location":"training/reinforce/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>gamma</code> <code>float</code> <code>0.99</code> Discount factor for computing returns <code>normalize_returns</code> <code>bool</code> <code>True</code> Normalize returns for variance reduction <code>entropy_coeff</code> <code>float</code> <code>0.01</code> Coefficient for entropy bonus"},{"location":"training/reinforce/#algorithm","title":"Algorithm","text":"<p>REINFORCE computes the policy gradient:</p> \\[\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t \\right]\\] <p>Where \\(G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k\\) is the discounted return from time step \\(t\\).</p>"},{"location":"training/reinforce/#variance-reduction","title":"Variance Reduction","text":"<p>With <code>normalize_returns=True</code>, returns are normalized:</p> \\[\\hat{G}_t = \\frac{G_t - \\mu_G}{\\sigma_G + \\epsilon}\\]"},{"location":"training/reinforce/#entropy-bonus","title":"Entropy Bonus","text":"<p>The entropy bonus encourages exploration:</p> \\[\\mathcal{L} = -\\mathbb{E}[\\log \\pi(a|s) \\cdot G] - \\lambda_H H(\\pi)\\] <p>Where \\(H(\\pi) = -\\sum \\pi(a|s) \\log \\pi(a|s)\\) is the policy entropy.</p>"},{"location":"training/reinforce/#api-reference","title":"API Reference","text":""},{"location":"training/reinforce/#artifex.generative_models.training.rl.reinforce.REINFORCETrainer","title":"artifex.generative_models.training.rl.reinforce.REINFORCETrainer","text":"<pre><code>REINFORCETrainer(\n    model: Module,\n    optimizer: Optimizer,\n    config: REINFORCEConfig | None = None,\n)\n</code></pre> <p>REINFORCE policy gradient trainer.</p> <p>Implements the REINFORCE algorithm with variance reduction techniques: - Discounted returns for credit assignment - Return normalization to stabilize gradients - Entropy bonus to encourage exploration</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>Policy network (must output action logits).</p> <code>optimizer</code> <p>Flax NNX optimizer.</p> <code>config</code> <p>REINFORCE configuration.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Policy network that outputs action logits.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Flax NNX optimizer for the model.</p> required <code>config</code> <code>REINFORCEConfig | None</code> <p>REINFORCE configuration. Uses defaults if not provided.</p> <code>None</code>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.reinforce.REINFORCETrainer.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.reinforce.REINFORCETrainer.optimizer","title":"optimizer  <code>instance-attribute</code>","text":"<pre><code>optimizer = optimizer\n</code></pre>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.reinforce.REINFORCETrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config if config is not None else REINFORCEConfig()\n</code></pre>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.reinforce.REINFORCETrainer.compute_returns","title":"compute_returns","text":"<pre><code>compute_returns(rewards: Array) -&gt; Array\n</code></pre> <p>Compute discounted returns from rewards.</p> <p>Parameters:</p> Name Type Description Default <code>rewards</code> <code>Array</code> <p>Array of rewards with shape (T,).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Array of discounted returns with shape (T,).</p>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.reinforce.REINFORCETrainer.normalize_returns","title":"normalize_returns","text":"<pre><code>normalize_returns(returns: Array) -&gt; Array\n</code></pre> <p>Normalize returns to zero mean and unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>returns</code> <code>Array</code> <p>Array of returns to normalize.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Normalized returns.</p>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.reinforce.REINFORCETrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    states: Array, actions: Array, returns: Array\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Compute REINFORCE loss.</p> <p>Loss = -E[log(pi(a|s)) * R] - entropy_coeff * H(pi)</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Array</code> <p>Batch of states with shape (batch_size, ...).</p> required <code>actions</code> <code>Array</code> <p>Batch of actions taken with shape (batch_size,).</p> required <code>returns</code> <code>Array</code> <p>Discounted returns with shape (batch_size,).</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/reinforce/#artifex.generative_models.training.rl.reinforce.REINFORCETrainer.train_step","title":"train_step","text":"<pre><code>train_step(\n    trajectory: dict[str, Array],\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Perform a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>trajectory</code> <code>dict[str, Array]</code> <p>Dictionary containing: - \"states\": Batch of states. - \"actions\": Actions taken. - \"rewards\": Rewards received.</p> required <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/reinforce/#training-metrics","title":"Training Metrics","text":"Metric Description <code>policy_loss</code> Policy gradient loss (negated for minimization)"},{"location":"training/reinforce/#use-cases","title":"Use Cases","text":"<p>REINFORCE is best suited for:</p> <ul> <li>Simple baselines: Quick experiments before more sophisticated methods</li> <li>Low-dimensional action spaces: Works well when action space is small</li> <li>Research: Understanding policy gradient fundamentals</li> </ul> <p>For more stable training, consider PPO or GRPO.</p>"},{"location":"training/reinforce/#related-documentation","title":"Related Documentation","text":"<ul> <li>RL Training Guide - Comprehensive RL training guide</li> <li>PPO Trainer - More stable policy gradient training</li> <li>GRPO Trainer - Memory-efficient critic-free RL</li> </ul>"},{"location":"training/scheduler/","title":"Scheduler","text":"<p>Module: <code>generative_models.training.schedulers.scheduler</code></p> <p>Source: <code>generative_models/training/schedulers/scheduler.py</code></p>"},{"location":"training/scheduler/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/tracking/","title":"Tracking","text":"<p>Module: <code>generative_models.training.tracking</code></p> <p>Source: <code>generative_models/training/tracking.py</code></p>"},{"location":"training/tracking/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/trainer/","title":"Trainer","text":"<p>Module: <code>generative_models.training.trainer</code></p> <p>Source: <code>generative_models/training/trainer.py</code></p>"},{"location":"training/trainer/#overview","title":"Overview","text":"<p>Trainer for generative models.</p>"},{"location":"training/trainer/#classes","title":"Classes","text":""},{"location":"training/trainer/#trainer_1","title":"Trainer","text":"<pre><code>class Trainer\n</code></pre>"},{"location":"training/trainer/#trainingstate","title":"TrainingState","text":"<pre><code>class TrainingState\n</code></pre>"},{"location":"training/trainer/#functions","title":"Functions","text":""},{"location":"training/trainer/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"training/trainer/#create","title":"create","text":"<pre><code>def create()\n</code></pre>"},{"location":"training/trainer/#evaluate","title":"evaluate","text":"<pre><code>def evaluate()\n</code></pre>"},{"location":"training/trainer/#generate_samples","title":"generate_samples","text":"<pre><code>def generate_samples()\n</code></pre>"},{"location":"training/trainer/#load_checkpoint","title":"load_checkpoint","text":"<pre><code>def load_checkpoint()\n</code></pre>"},{"location":"training/trainer/#loss_fn","title":"loss_fn","text":"<pre><code>def loss_fn()\n</code></pre>"},{"location":"training/trainer/#save_checkpoint","title":"save_checkpoint","text":"<pre><code>def save_checkpoint()\n</code></pre>"},{"location":"training/trainer/#train","title":"train","text":"<pre><code>def train()\n</code></pre>"},{"location":"training/trainer/#train_epoch","title":"train_epoch","text":"<pre><code>def train_epoch()\n</code></pre>"},{"location":"training/trainer/#train_step","title":"train_step","text":"<pre><code>def train_step()\n</code></pre>"},{"location":"training/trainer/#validate_step","title":"validate_step","text":"<pre><code>def validate_step()\n</code></pre>"},{"location":"training/trainer/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 11</li> <li>Imports: 10</li> </ul>"},{"location":"training/utils/","title":"Utils","text":"<p>Module: <code>generative_models.training.utils</code></p> <p>Source: <code>generative_models/training/utils.py</code></p>"},{"location":"training/utils/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"training/vae_trainer/","title":"VAE Trainer","text":"<p>Module: <code>artifex.generative_models.training.trainers.vae_trainer</code></p> <p>The VAE Trainer provides specialized training utilities for Variational Autoencoders, including KL divergence annealing schedules, beta-VAE weighting for disentanglement, and free bits constraints to prevent posterior collapse.</p>"},{"location":"training/vae_trainer/#overview","title":"Overview","text":"<p>Training VAEs requires balancing reconstruction quality against latent space regularization. The VAE Trainer handles this balance through:</p> <ul> <li>KL Annealing: Gradual increase of KL weight to prevent posterior collapse</li> <li>Beta-VAE Weighting: Control disentanglement vs reconstruction trade-off</li> <li>Free Bits Constraint: Minimum KL per dimension to ensure information flow</li> </ul>"},{"location":"training/vae_trainer/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training.trainers import (\n    VAETrainer,\n    VAETrainingConfig,\n)\nfrom flax import nnx\nimport optax\n\n# Create model and optimizer\nmodel = create_vae_model(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n# Configure VAE-specific training\nconfig = VAETrainingConfig(\n    kl_annealing=\"cyclical\",\n    kl_warmup_steps=5000,\n    beta=4.0,\n    free_bits=0.5,\n)\n\ntrainer = VAETrainer(model, optimizer, config)\n\n# Training loop\nfor step, batch in enumerate(train_loader):\n    loss, metrics = trainer.train_step(batch, step=step)\n    if step % 100 == 0:\n        print(f\"Step {step}: loss={metrics['loss']:.4f}, \"\n              f\"recon={metrics['recon_loss']:.4f}, kl={metrics['kl_loss']:.4f}\")\n</code></pre>"},{"location":"training/vae_trainer/#configuration","title":"Configuration","text":""},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainingConfig","title":"artifex.generative_models.training.trainers.vae_trainer.VAETrainingConfig  <code>dataclass</code>","text":"<pre><code>VAETrainingConfig(\n    kl_annealing: Literal[\n        \"none\", \"linear\", \"sigmoid\", \"cyclical\"\n    ] = \"linear\",\n    kl_warmup_steps: int = 10000,\n    beta: float = 1.0,\n    free_bits: float = 0.0,\n    cyclical_period: int = 10000,\n)\n</code></pre> <p>Configuration for VAE-specific training.</p> <p>Attributes:</p> Name Type Description <code>kl_annealing</code> <code>Literal['none', 'linear', 'sigmoid', 'cyclical']</code> <p>Type of KL annealing schedule. - \"none\": No annealing, use full beta from start - \"linear\": Linear warmup from 0 to beta - \"sigmoid\": Sigmoid-shaped warmup - \"cyclical\": Cyclical annealing with periodic resets</p> <code>kl_warmup_steps</code> <code>int</code> <p>Number of steps to reach full KL weight.</p> <code>beta</code> <code>float</code> <p>Final beta weight for KL term (beta-VAE). Higher values encourage disentanglement but may hurt reconstruction.</p> <code>free_bits</code> <code>float</code> <p>Minimum KL per latent dimension (0 = disabled). Prevents posterior collapse by ensuring minimum information flow.</p> <code>cyclical_period</code> <code>int</code> <p>Period for cyclical annealing (if used).</p>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainingConfig.kl_annealing","title":"kl_annealing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kl_annealing: Literal[\n    \"none\", \"linear\", \"sigmoid\", \"cyclical\"\n] = \"linear\"\n</code></pre>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainingConfig.kl_warmup_steps","title":"kl_warmup_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kl_warmup_steps: int = 10000\n</code></pre>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainingConfig.beta","title":"beta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>beta: float = 1.0\n</code></pre>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainingConfig.free_bits","title":"free_bits  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>free_bits: float = 0.0\n</code></pre>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainingConfig.cyclical_period","title":"cyclical_period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cyclical_period: int = 10000\n</code></pre>"},{"location":"training/vae_trainer/#configuration-options","title":"Configuration Options","text":"Parameter Type Default Description <code>kl_annealing</code> <code>str</code> <code>\"linear\"</code> KL weight schedule: <code>\"none\"</code>, <code>\"linear\"</code>, <code>\"sigmoid\"</code>, <code>\"cyclical\"</code> <code>kl_warmup_steps</code> <code>int</code> <code>10000</code> Steps to reach full KL weight <code>beta</code> <code>float</code> <code>1.0</code> Final KL weight (beta-VAE parameter) <code>free_bits</code> <code>float</code> <code>0.0</code> Minimum KL per latent dimension <code>cyclical_period</code> <code>int</code> <code>10000</code> Period for cyclical annealing"},{"location":"training/vae_trainer/#kl-annealing-schedules","title":"KL Annealing Schedules","text":""},{"location":"training/vae_trainer/#none-constant","title":"None (Constant)","text":"<p>No annealing - use full beta weight from the start:</p> <pre><code>config = VAETrainingConfig(kl_annealing=\"none\", beta=1.0)\n# KL weight = 1.0 at all steps\n</code></pre>"},{"location":"training/vae_trainer/#linear-warmup","title":"Linear Warmup","text":"<p>Linearly increase KL weight from 0 to beta:</p> <pre><code>config = VAETrainingConfig(\n    kl_annealing=\"linear\",\n    kl_warmup_steps=10000,\n    beta=1.0,\n)\n# KL weight = beta * min(1.0, step / warmup_steps)\n</code></pre>"},{"location":"training/vae_trainer/#sigmoid-warmup","title":"Sigmoid Warmup","text":"<p>S-shaped warmup curve centered at half the warmup steps:</p> <pre><code>config = VAETrainingConfig(\n    kl_annealing=\"sigmoid\",\n    kl_warmup_steps=10000,\n    beta=1.0,\n)\n</code></pre>"},{"location":"training/vae_trainer/#cyclical-annealing","title":"Cyclical Annealing","text":"<p>Periodically reset KL weight to encourage information flow:</p> <pre><code>config = VAETrainingConfig(\n    kl_annealing=\"cyclical\",\n    cyclical_period=5000,\n    beta=4.0,\n)\n# KL weight cycles: 0 -&gt; beta -&gt; 0 -&gt; beta -&gt; ...\n</code></pre> <p>Cyclical annealing helps prevent posterior collapse by periodically \"reopening\" information pathways.</p>"},{"location":"training/vae_trainer/#beta-vae-training","title":"Beta-VAE Training","text":"<p>Higher beta values encourage disentangled representations at the cost of reconstruction quality:</p> <pre><code># Standard VAE (beta=1)\nstandard_config = VAETrainingConfig(beta=1.0)\n\n# Beta-VAE for disentanglement (beta=4)\ndisentangled_config = VAETrainingConfig(beta=4.0)\n\n# Strong regularization (beta=10)\nstrong_reg_config = VAETrainingConfig(beta=10.0)\n</code></pre>"},{"location":"training/vae_trainer/#free-bits-constraint","title":"Free Bits Constraint","text":"<p>Prevent posterior collapse by ensuring minimum KL per latent dimension:</p> <pre><code>config = VAETrainingConfig(\n    free_bits=0.5,  # Minimum 0.5 nats per dimension\n    beta=1.0,\n)\n</code></pre> <p>The free bits constraint ensures each latent dimension carries at least the specified amount of information.</p>"},{"location":"training/vae_trainer/#api-reference","title":"API Reference","text":""},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer","title":"artifex.generative_models.training.trainers.vae_trainer.VAETrainer","text":"<pre><code>VAETrainer(config: VAETrainingConfig | None = None)\n</code></pre> <p>VAE-specific trainer with KL annealing and beta-VAE support.</p> <p>This trainer provides a JIT-compatible interface for training VAEs with: - KL annealing schedules (linear, sigmoid, cyclical) - Beta-VAE weighting for disentanglement - Free bits constraint to prevent posterior collapse</p> <p>The train_step method takes model and optimizer as explicit arguments, allowing it to be wrapped with nnx.jit for performance.</p> The trainer computes the ELBO loss with configurable KL weighting <p>L = reconstruction_loss + beta * kl_weight(step) * kl_loss</p> <p>Example (non-JIT):     <pre><code>from artifex.generative_models.training.trainers import (\n    VAETrainer,\n    VAETrainingConfig,\n)\n\nconfig = VAETrainingConfig(\n    kl_annealing=\"cyclical\",\n    beta=4.0,\n    free_bits=0.5,\n)\ntrainer = VAETrainer(config)\n\n# Create model and optimizer separately\nmodel = VAEModel(config, rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4))\n\n# Training loop\nfor step, batch in enumerate(data):\n    loss, metrics = trainer.train_step(model, optimizer, batch, step=step)\n</code></pre></p> <p>Example (JIT-compiled):     <pre><code>trainer = VAETrainer(config)\njit_step = nnx.jit(trainer.train_step)\n\nfor step, batch in enumerate(data):\n    loss, metrics = jit_step(model, optimizer, batch, step=step)\n</code></pre></p> Note <p>The model is expected to return (reconstruction, mean, logvar) from its forward pass. The trainer handles loss computation and KL annealing.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>VAETrainingConfig | None</code> <p>VAE training configuration. Uses defaults if not provided.</p> <code>None</code>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or VAETrainingConfig()\n</code></pre>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer.get_kl_weight","title":"get_kl_weight","text":"<pre><code>get_kl_weight(step: int | Array) -&gt; Array\n</code></pre> <p>Compute KL weight based on annealing schedule.</p> <p>This method is JIT-compatible - uses JAX operations instead of Python builtins.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int | Array</code> <p>Current training step (can be traced array for JIT).</p> required <p>Returns:</p> Type Description <code>Array</code> <p>KL weight multiplier (0.0 to beta).</p>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer.apply_free_bits","title":"apply_free_bits","text":"<pre><code>apply_free_bits(kl_per_dim: Array) -&gt; Array\n</code></pre> <p>Apply free bits constraint to KL divergence.</p> <p>Ensures minimum KL per latent dimension to prevent posterior collapse.</p> <p>Parameters:</p> Name Type Description Default <code>kl_per_dim</code> <code>Array</code> <p>KL divergence per latent dimension.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>KL divergence with free bits applied.</p>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer.compute_kl_loss","title":"compute_kl_loss","text":"<pre><code>compute_kl_loss(\n    mean: Array, logvar: Array\n) -&gt; tuple[Array, Array]\n</code></pre> <p>Compute KL divergence from standard normal.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Array</code> <p>Latent mean, shape (batch, latent_dim).</p> required <code>logvar</code> <code>Array</code> <p>Latent log-variance, shape (batch, latent_dim).</p> required <p>Returns:</p> Type Description <code>tuple[Array, Array]</code> <p>Tuple of (total_kl_loss, kl_per_sample) where: - total_kl_loss: Scalar mean KL loss - kl_per_sample: KL loss per sample, shape (batch,)</p>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer.compute_reconstruction_loss","title":"compute_reconstruction_loss","text":"<pre><code>compute_reconstruction_loss(\n    x: Array,\n    recon_x: Array,\n    loss_type: Literal[\"mse\", \"bce\"] = \"mse\",\n) -&gt; Array\n</code></pre> <p>Compute reconstruction loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>Original input, shape (batch, ...).</p> required <code>recon_x</code> <code>Array</code> <p>Reconstructed output, shape (batch, ...).</p> required <code>loss_type</code> <code>Literal['mse', 'bce']</code> <p>Type of reconstruction loss (\"mse\" or \"bce\").</p> <code>'mse'</code> <p>Returns:</p> Type Description <code>Array</code> <p>Scalar reconstruction loss.</p>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer.compute_loss","title":"compute_loss","text":"<pre><code>compute_loss(\n    model: Module,\n    batch: dict[str, Any],\n    step: int,\n    loss_type: Literal[\"mse\", \"bce\"] = \"mse\",\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Compute VAE loss with KL annealing.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>VAE model to evaluate.</p> required <code>batch</code> <code>dict[str, Any]</code> <p>Batch dictionary with \"image\" or \"data\" key.</p> required <code>step</code> <code>int</code> <p>Current training step for annealing.</p> required <code>loss_type</code> <code>Literal['mse', 'bce']</code> <p>Type of reconstruction loss.</p> <code>'mse'</code> <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (total_loss, metrics_dict).</p>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer.train_step","title":"train_step","text":"<pre><code>train_step(\n    model: Module,\n    optimizer: Optimizer,\n    batch: dict[str, Any],\n    step: int = 0,\n    loss_type: Literal[\"mse\", \"bce\"] = \"mse\",\n) -&gt; tuple[Array, dict[str, Any]]\n</code></pre> <p>Execute a single training step.</p> <p>This method can be wrapped with nnx.jit for performance:     jit_step = nnx.jit(trainer.train_step)     loss, metrics = jit_step(model, optimizer, batch, step=step)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>VAE model to train.</p> required <code>optimizer</code> <code>Optimizer</code> <p>NNX optimizer for parameter updates.</p> required <code>batch</code> <code>dict[str, Any]</code> <p>Batch dictionary with \"image\" or \"data\" key.</p> required <code>step</code> <code>int</code> <p>Current training step for annealing.</p> <code>0</code> <code>loss_type</code> <code>Literal['mse', 'bce']</code> <p>Type of reconstruction loss.</p> <code>'mse'</code> <p>Returns:</p> Type Description <code>tuple[Array, dict[str, Any]]</code> <p>Tuple of (loss, metrics_dict).</p>"},{"location":"training/vae_trainer/#artifex.generative_models.training.trainers.vae_trainer.VAETrainer.create_loss_fn","title":"create_loss_fn","text":"<pre><code>create_loss_fn(\n    loss_type: Literal[\"mse\", \"bce\"] = \"mse\",\n) -&gt; Callable[\n    [Module, dict[str, Any], Array, Array],\n    tuple[Array, dict[str, Any]],\n]\n</code></pre> <p>Create loss function compatible with train_epoch_staged.</p> <p>This enables DRY integration - VAE-specific training logic can be used with the staged training infrastructure.</p> <p>Parameters:</p> Name Type Description Default <code>loss_type</code> <code>Literal['mse', 'bce']</code> <p>Type of reconstruction loss.</p> <code>'mse'</code> <p>Returns:</p> Type Description <code>Callable[[Module, dict[str, Any], Array, Array], tuple[Array, dict[str, Any]]]</code> <p>Function with signature: (model, batch, rng, step) -&gt; (loss, metrics)</p> <code>Callable[[Module, dict[str, Any], Array, Array], tuple[Array, dict[str, Any]]]</code> <p>The step parameter is passed dynamically by train_epoch_staged,</p> <code>Callable[[Module, dict[str, Any], Array, Array], tuple[Array, dict[str, Any]]]</code> <p>enabling proper KL annealing inside JIT-compiled fori_loop.</p>"},{"location":"training/vae_trainer/#integration-with-base-trainer","title":"Integration with Base Trainer","text":"<p>The VAE Trainer provides a <code>create_loss_fn()</code> method for seamless integration with the base Trainer's callbacks, checkpointing, and logging infrastructure:</p> <pre><code>from artifex.generative_models.training import Trainer\nfrom artifex.generative_models.training.trainers import VAETrainer, VAETrainingConfig\nfrom artifex.generative_models.training.callbacks import (\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n)\n\n# Create VAE-specific trainer\nvae_config = VAETrainingConfig(kl_annealing=\"cyclical\", beta=4.0)\nvae_trainer = VAETrainer(model, optimizer, vae_config)\n\n# Create loss function for a specific training step\n# Note: step is required for KL annealing\ndef make_loss_fn(step: int):\n    return vae_trainer.create_loss_fn(step=step)\n\n# Use with base Trainer for callbacks\ncallbacks = [\n    EarlyStopping(EarlyStoppingConfig(monitor=\"val_loss\", patience=10)),\n    ModelCheckpoint(CheckpointConfig(dirpath=\"checkpoints\", monitor=\"val_loss\")),\n]\n</code></pre>"},{"location":"training/vae_trainer/#model-requirements","title":"Model Requirements","text":"<p>The VAE Trainer expects models with the following interface:</p> <pre><code>class VAEModel(nnx.Module):\n    def __call__(self, x: jax.Array) -&gt; tuple[jax.Array, jax.Array, jax.Array]:\n        \"\"\"Forward pass returning (reconstruction, mean, logvar).\n\n        Args:\n            x: Input data, shape (batch, ...).\n\n        Returns:\n            Tuple of:\n                - recon_x: Reconstructed data, shape (batch, ...)\n                - mean: Latent mean, shape (batch, latent_dim)\n                - logvar: Latent log-variance, shape (batch, latent_dim)\n        \"\"\"\n        ...\n</code></pre>"},{"location":"training/vae_trainer/#reconstruction-loss-types","title":"Reconstruction Loss Types","text":"<p>The trainer supports MSE and BCE reconstruction losses:</p> <pre><code># Mean Squared Error (default, for continuous data)\nloss, metrics = trainer.train_step(batch, step=100, loss_type=\"mse\")\n\n# Binary Cross-Entropy (for images normalized to [0, 1])\nloss, metrics = trainer.train_step(batch, step=100, loss_type=\"bce\")\n</code></pre>"},{"location":"training/vae_trainer/#training-metrics","title":"Training Metrics","text":"<p>The trainer returns detailed metrics for monitoring:</p> Metric Description <code>loss</code> Total ELBO loss <code>recon_loss</code> Reconstruction loss <code>kl_loss</code> KL divergence (unweighted) <code>kl_weight</code> Current KL weight from annealing"},{"location":"training/vae_trainer/#references","title":"References","text":"<ul> <li>Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</li> <li>Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing</li> <li>Free Bits for VAEs</li> </ul>"},{"location":"training/visualization/","title":"Visualization","text":"<p>Module: <code>generative_models.training.callbacks.visualization</code></p> <p>Source: <code>generative_models/training/callbacks/visualization.py</code></p>"},{"location":"training/visualization/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"tutorials/protein-modeling/","title":"Protein Modeling Tutorial","text":"<p>Coming Soon</p> <p>This page is under development. Check back for the protein modeling tutorial.</p>"},{"location":"tutorials/protein-modeling/#overview","title":"Overview","text":"<p>Step-by-step tutorial for protein structure generation.</p>"},{"location":"tutorials/protein-modeling/#related-documentation","title":"Related Documentation","text":"<ul> <li>Protein Modeling Guide</li> <li>Protein Extensions</li> </ul>"},{"location":"user-guide/advanced/architectures/","title":"Custom Architectures","text":"<p>Build custom model architectures using Flax NNX in Artifex. This guide covers advanced architectural patterns, custom layers, and integration with Artifex's training and evaluation systems.</p> <ul> <li> <p> Custom Layers</p> <p>Create custom neural network layers with Flax NNX</p> <p> Learn more</p> </li> <li> <p> Custom Models</p> <p>Build complete custom generative models</p> <p> Learn more</p> </li> <li> <p> Architecture Patterns</p> <p>Common architectural patterns and best practices</p> <p> Learn more</p> </li> <li> <p> Integration</p> <p>Integrate custom models with Artifex's systems</p> <p> Learn more</p> </li> </ul>"},{"location":"user-guide/advanced/architectures/#overview","title":"Overview","text":"<p>Artifex provides flexibility to create custom architectures while maintaining compatibility with the training, evaluation, and deployment infrastructure.</p>"},{"location":"user-guide/advanced/architectures/#why-custom-architectures","title":"Why Custom Architectures?","text":"<p>Build custom architectures when:</p> <ul> <li>Research: Implementing novel architectural ideas</li> <li>Domain-Specific: Specialized requirements (proteins, molecules, etc.)</li> <li>Optimization: Custom operations for performance</li> <li>Experimentation: Rapid prototyping of new ideas</li> </ul>"},{"location":"user-guide/advanced/architectures/#custom-layers","title":"Custom Layers","text":"<p>Create custom neural network layers using Flax NNX.</p>"},{"location":"user-guide/advanced/architectures/#basic-custom-layer","title":"Basic Custom Layer","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass CustomLinear(nnx.Module):\n    \"\"\"Custom linear layer with additional features.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        *,\n        use_bias: bool = True,\n        weight_init: callable = nnx.initializers.lecun_normal(),\n        bias_init: callable = nnx.initializers.zeros_init(),\n        rngs: nnx.Rngs,\n        dtype: jnp.dtype = jnp.float32,\n    ):\n        super().__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.use_bias = use_bias\n\n        # Initialize weight\n        self.weight = nnx.Param(\n            weight_init(rngs.params(), (in_features, out_features), dtype)\n        )\n\n        # Initialize bias if needed\n        if use_bias:\n            self.bias = nnx.Param(\n                bias_init(rngs.params(), (out_features,), dtype)\n            )\n        else:\n            self.bias = None\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass.\n\n        Args:\n            x: Input tensor (..., in_features)\n\n        Returns:\n            Output tensor (..., out_features)\n        \"\"\"\n        # Matrix multiplication\n        output = x @ self.weight.value\n\n        # Add bias\n        if self.use_bias:\n            output = output + self.bias.value\n\n        return output\n\n\n# Usage\nlayer = CustomLinear(\n    in_features=784,\n    out_features=256,\n    rngs=nnx.Rngs(0)\n)\n\nx = jnp.ones((32, 784))\noutput = layer(x)\nprint(f\"Output shape: {output.shape}\")  # (32, 256)\n</code></pre>"},{"location":"user-guide/advanced/architectures/#advanced-custom-layer-with-regularization","title":"Advanced Custom Layer with Regularization","text":"<pre><code>class RegularizedLinear(nnx.Module):\n    \"\"\"Linear layer with built-in regularization.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        *,\n        dropout_rate: float = 0.0,\n        weight_decay: float = 0.0,\n        spectral_norm: bool = False,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.weight_decay = weight_decay\n        self.spectral_norm = spectral_norm\n\n        # Weight initialization\n        self.weight = nnx.Param(\n            nnx.initializers.lecun_normal()(\n                rngs.params(),\n                (in_features, out_features)\n            )\n        )\n\n        self.bias = nnx.Param(jnp.zeros(out_features))\n\n        # Dropout\n        if dropout_rate &gt; 0:\n            self.dropout = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n        else:\n            self.dropout = None\n\n    def _apply_spectral_norm(self, weight: jax.Array) -&gt; jax.Array:\n        \"\"\"Apply spectral normalization to weight.\"\"\"\n        # Compute largest singular value\n        u, s, vh = jnp.linalg.svd(weight, full_matrices=False)\n\n        # Normalize by largest singular value\n        weight_normalized = weight / s[0]\n\n        return weight_normalized\n\n    def __call__(\n        self,\n        x: jax.Array,\n        *,\n        deterministic: bool = False,\n    ) -&gt; jax.Array:\n        \"\"\"Forward pass with regularization.\n\n        Args:\n            x: Input tensor\n            deterministic: If True, disable dropout\n\n        Returns:\n            Output tensor\n        \"\"\"\n        # Get weight\n        weight = self.weight.value\n\n        # Apply spectral normalization\n        if self.spectral_norm:\n            weight = self._apply_spectral_norm(weight)\n\n        # Linear transformation\n        output = x @ weight + self.bias.value\n\n        # Apply dropout\n        if self.dropout is not None and not deterministic:\n            output = self.dropout(output)\n\n        return output\n\n    def get_regularization_loss(self) -&gt; jax.Array:\n        \"\"\"Compute regularization loss for this layer.\"\"\"\n        if self.weight_decay &gt; 0:\n            # L2 regularization\n            return self.weight_decay * jnp.sum(self.weight.value ** 2)\n        return 0.0\n\n\n# Usage\nlayer = RegularizedLinear(\n    in_features=784,\n    out_features=256,\n    dropout_rate=0.1,\n    weight_decay=1e-4,\n    spectral_norm=True,\n    rngs=nnx.Rngs(0)\n)\n\n# Forward pass\nx = jnp.ones((32, 784))\noutput = layer(x, deterministic=False)\n\n# Get regularization loss\nreg_loss = layer.get_regularization_loss()\n</code></pre>"},{"location":"user-guide/advanced/architectures/#attention-layer","title":"Attention Layer","text":"<pre><code>class MultiHeadAttention(nnx.Module):\n    \"\"\"Multi-head attention layer.\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        *,\n        dropout_rate: float = 0.0,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n\n        # Q, K, V projections\n        self.q_proj = nnx.Linear(hidden_size, hidden_size, rngs=rngs)\n        self.k_proj = nnx.Linear(hidden_size, hidden_size, rngs=rngs)\n        self.v_proj = nnx.Linear(hidden_size, hidden_size, rngs=rngs)\n\n        # Output projection\n        self.out_proj = nnx.Linear(hidden_size, hidden_size, rngs=rngs)\n\n        # Dropout\n        if dropout_rate &gt; 0:\n            self.dropout = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n        else:\n            self.dropout = None\n\n    def __call__(\n        self,\n        x: jax.Array,\n        mask: jax.Array | None = None,\n        *,\n        deterministic: bool = False,\n    ) -&gt; jax.Array:\n        \"\"\"Multi-head attention forward pass.\n\n        Args:\n            x: Input tensor (batch, seq_len, hidden_size)\n            mask: Optional attention mask (batch, seq_len, seq_len)\n            deterministic: If True, disable dropout\n\n        Returns:\n            Output tensor (batch, seq_len, hidden_size)\n        \"\"\"\n        batch_size, seq_len, _ = x.shape\n\n        # Project to Q, K, V\n        q = self.q_proj(x)  # (batch, seq_len, hidden_size)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n\n        # Reshape for multi-head attention\n        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # Transpose: (batch, num_heads, seq_len, head_dim)\n        q = jnp.transpose(q, (0, 2, 1, 3))\n        k = jnp.transpose(k, (0, 2, 1, 3))\n        v = jnp.transpose(v, (0, 2, 1, 3))\n\n        # Scaled dot-product attention\n        scale = jnp.sqrt(self.head_dim)\n        scores = jnp.einsum(\"bhqd,bhkd-&gt;bhqk\", q, k) / scale\n\n        # Apply mask if provided\n        if mask is not None:\n            # Expand mask for heads: (batch, 1, seq_len, seq_len)\n            mask = mask[:, None, :, :]\n            scores = jnp.where(mask, scores, -1e9)\n\n        # Softmax\n        attention_weights = nnx.softmax(scores, axis=-1)\n\n        # Apply dropout\n        if self.dropout is not None and not deterministic:\n            attention_weights = self.dropout(attention_weights)\n\n        # Attend to values\n        context = jnp.einsum(\"bhqk,bhkd-&gt;bhqd\", attention_weights, v)\n\n        # Reshape back: (batch, seq_len, hidden_size)\n        context = jnp.transpose(context, (0, 2, 1, 3))\n        context = context.reshape(batch_size, seq_len, self.hidden_size)\n\n        # Output projection\n        output = self.out_proj(context)\n\n        return output\n\n\n# Usage\nattention = MultiHeadAttention(\n    hidden_size=512,\n    num_heads=8,\n    dropout_rate=0.1,\n    rngs=nnx.Rngs(0)\n)\n\nx = jnp.ones((2, 10, 512))  # (batch=2, seq_len=10, hidden=512)\noutput = attention(x, deterministic=False)\nprint(f\"Output shape: {output.shape}\")  # (2, 10, 512)\n</code></pre>"},{"location":"user-guide/advanced/architectures/#residual-block","title":"Residual Block","text":"<pre><code>class ResidualBlock(nnx.Module):\n    \"\"\"Residual block with normalization and activation.\"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        *,\n        stride: int = 1,\n        downsample: bool = False,\n        activation: callable = nnx.relu,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.activation = activation\n\n        # Main path\n        self.conv1 = nnx.Conv(\n            in_features=channels,\n            out_features=channels,\n            kernel_size=(3, 3),\n            strides=(stride, stride),\n            padding=\"SAME\",\n            rngs=rngs,\n        )\n        self.bn1 = nnx.BatchNorm(num_features=channels, rngs=rngs)\n\n        self.conv2 = nnx.Conv(\n            in_features=channels,\n            out_features=channels,\n            kernel_size=(3, 3),\n            strides=(1, 1),\n            padding=\"SAME\",\n            rngs=rngs,\n        )\n        self.bn2 = nnx.BatchNorm(num_features=channels, rngs=rngs)\n\n        # Shortcut path\n        if downsample:\n            self.shortcut = nnx.Conv(\n                in_features=channels,\n                out_features=channels,\n                kernel_size=(1, 1),\n                strides=(stride, stride),\n                padding=\"VALID\",\n                rngs=rngs,\n            )\n            self.shortcut_bn = nnx.BatchNorm(num_features=channels, rngs=rngs)\n        else:\n            self.shortcut = None\n\n    def __call__(\n        self,\n        x: jax.Array,\n        *,\n        use_running_average: bool = False,\n    ) -&gt; jax.Array:\n        \"\"\"Forward pass through residual block.\n\n        Args:\n            x: Input tensor (batch, height, width, channels)\n            use_running_average: Use running stats for batch norm\n\n        Returns:\n            Output tensor (batch, height, width, channels)\n        \"\"\"\n        # Save input for residual connection\n        identity = x\n\n        # Main path\n        out = self.conv1(x)\n        out = self.bn1(out, use_running_average=use_running_average)\n        out = self.activation(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out, use_running_average=use_running_average)\n\n        # Shortcut path\n        if self.shortcut is not None:\n            identity = self.shortcut(identity)\n            identity = self.shortcut_bn(\n                identity,\n                use_running_average=use_running_average\n            )\n\n        # Residual connection\n        out = out + identity\n        out = self.activation(out)\n\n        return out\n\n\n# Usage\nblock = ResidualBlock(\n    channels=64,\n    stride=2,\n    downsample=True,\n    rngs=nnx.Rngs(0)\n)\n\nx = jnp.ones((2, 32, 32, 64))\noutput = block(x, use_running_average=False)\nprint(f\"Output shape: {output.shape}\")  # (2, 16, 16, 64)\n</code></pre>"},{"location":"user-guide/advanced/architectures/#custom-models","title":"Custom Models","text":"<p>Build complete custom generative models.</p>"},{"location":"user-guide/advanced/architectures/#custom-vae-architecture","title":"Custom VAE Architecture","text":"<pre><code>from artifex.generative_models.core.protocols import GenerativeModel\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\nclass CustomVAE(nnx.Module):\n    \"\"\"Custom VAE with flexible architecture.\"\"\"\n\n    def __init__(\n        self,\n        input_shape: tuple,\n        latent_dim: int,\n        encoder_layers: list[int],\n        decoder_layers: list[int],\n        *,\n        activation: callable = nnx.relu,\n        use_batch_norm: bool = True,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.input_shape = input_shape\n        self.latent_dim = latent_dim\n        self.activation = activation\n\n        # Flatten input size\n        self.input_dim = int(jnp.prod(jnp.array(input_shape)))\n\n        # Encoder\n        self.encoder = self._build_encoder(\n            encoder_layers,\n            use_batch_norm,\n            rngs\n        )\n\n        # Latent projections\n        self.mean_layer = nnx.Linear(\n            in_features=encoder_layers[-1],\n            out_features=latent_dim,\n            rngs=rngs,\n        )\n        self.logvar_layer = nnx.Linear(\n            in_features=encoder_layers[-1],\n            out_features=latent_dim,\n            rngs=rngs,\n        )\n\n        # Decoder\n        self.decoder = self._build_decoder(\n            decoder_layers,\n            use_batch_norm,\n            rngs\n        )\n\n        # Output layer\n        self.output_layer = nnx.Linear(\n            in_features=decoder_layers[-1],\n            out_features=self.input_dim,\n            rngs=rngs,\n        )\n\n    def _build_encoder(\n        self,\n        layers: list[int],\n        use_batch_norm: bool,\n        rngs: nnx.Rngs,\n    ) -&gt; list:\n        \"\"\"Build encoder layers.\"\"\"\n        encoder_layers = []\n\n        # Input layer\n        encoder_layers.append(nnx.Linear(self.input_dim, layers[0], rngs=rngs))\n        if use_batch_norm:\n            encoder_layers.append(nnx.BatchNorm(layers[0], rngs=rngs))\n\n        # Hidden layers\n        for i in range(len(layers) - 1):\n            encoder_layers.append(\n                nnx.Linear(layers[i], layers[i + 1], rngs=rngs)\n            )\n            if use_batch_norm:\n                encoder_layers.append(nnx.BatchNorm(layers[i + 1], rngs=rngs))\n\n        return encoder_layers\n\n    def _build_decoder(\n        self,\n        layers: list[int],\n        use_batch_norm: bool,\n        rngs: nnx.Rngs,\n    ) -&gt; list:\n        \"\"\"Build decoder layers.\"\"\"\n        decoder_layers = []\n\n        # Input layer (from latent)\n        decoder_layers.append(nnx.Linear(self.latent_dim, layers[0], rngs=rngs))\n        if use_batch_norm:\n            decoder_layers.append(nnx.BatchNorm(layers[0], rngs=rngs))\n\n        # Hidden layers\n        for i in range(len(layers) - 1):\n            decoder_layers.append(\n                nnx.Linear(layers[i], layers[i + 1], rngs=rngs)\n            )\n            if use_batch_norm:\n                decoder_layers.append(nnx.BatchNorm(layers[i + 1], rngs=rngs))\n\n        return decoder_layers\n\n    def encode(\n        self,\n        x: jax.Array,\n        *,\n        use_running_average: bool = False,\n    ) -&gt; dict[str, jax.Array]:\n        \"\"\"Encode input to latent distribution.\n\n        Args:\n            x: Input tensor (batch, *input_shape)\n            use_running_average: Use running stats for batch norm\n\n        Returns:\n            Dictionary with 'mean' and 'logvar'\n        \"\"\"\n        # Flatten input\n        batch_size = x.shape[0]\n        x = x.reshape(batch_size, -1)\n\n        # Forward through encoder\n        for layer in self.encoder:\n            if isinstance(layer, nnx.BatchNorm):\n                x = layer(x, use_running_average=use_running_average)\n            else:\n                x = layer(x)\n            x = self.activation(x)\n\n        # Latent parameters\n        mean = self.mean_layer(x)\n        logvar = self.logvar_layer(x)\n\n        return {\"mean\": mean, \"logvar\": logvar}\n\n    def reparameterize(\n        self,\n        mean: jax.Array,\n        logvar: jax.Array,\n        *,\n        rngs: nnx.Rngs | None = None,\n    ) -&gt; jax.Array:\n        \"\"\"Reparameterization trick.\n\n        Args:\n            mean: Mean of latent distribution\n            logvar: Log variance of latent distribution\n            rngs: RNG for sampling\n\n        Returns:\n            Sampled latent vector\n        \"\"\"\n        if rngs is not None and \"sample\" in rngs:\n            key = rngs.sample()\n        else:\n            key = jax.random.key(0)\n\n        std = jnp.exp(0.5 * logvar)\n        eps = jax.random.normal(key, mean.shape)\n        z = mean + eps * std\n\n        return z\n\n    def decode(\n        self,\n        z: jax.Array,\n        *,\n        use_running_average: bool = False,\n    ) -&gt; jax.Array:\n        \"\"\"Decode latent vector to reconstruction.\n\n        Args:\n            z: Latent vector (batch, latent_dim)\n            use_running_average: Use running stats for batch norm\n\n        Returns:\n            Reconstruction (batch, *input_shape)\n        \"\"\"\n        x = z\n\n        # Forward through decoder\n        for layer in self.decoder:\n            if isinstance(layer, nnx.BatchNorm):\n                x = layer(x, use_running_average=use_running_average)\n            else:\n                x = layer(x)\n            x = self.activation(x)\n\n        # Output layer\n        x = self.output_layer(x)\n        x = nnx.sigmoid(x)  # Normalize to [0, 1]\n\n        # Reshape to input shape\n        batch_size = z.shape[0]\n        x = x.reshape(batch_size, *self.input_shape)\n\n        return x\n\n    def __call__(\n        self,\n        x: jax.Array,\n        *,\n        rngs: nnx.Rngs | None = None,\n        use_running_average: bool = False,\n    ) -&gt; dict[str, jax.Array]:\n        \"\"\"Full forward pass (encode-reparameterize-decode).\n\n        Args:\n            x: Input tensor (batch, *input_shape)\n            rngs: RNG for sampling\n            use_running_average: Use running stats for batch norm\n\n        Returns:\n            Dictionary with 'reconstruction', 'mean', 'logvar', 'latent'\n        \"\"\"\n        # Encode\n        latent_params = self.encode(x, use_running_average=use_running_average)\n\n        # Reparameterize\n        z = self.reparameterize(\n            latent_params[\"mean\"],\n            latent_params[\"logvar\"],\n            rngs=rngs\n        )\n\n        # Decode\n        reconstruction = self.decode(z, use_running_average=use_running_average)\n\n        return {\n            \"reconstruction\": reconstruction,\n            \"mean\": latent_params[\"mean\"],\n            \"logvar\": latent_params[\"logvar\"],\n            \"latent\": z,\n        }\n\n\n# Create custom VAE\nmodel = CustomVAE(\n    input_shape=(28, 28, 1),  # MNIST-like\n    latent_dim=20,\n    encoder_layers=[512, 256, 128],\n    decoder_layers=[128, 256, 512],\n    use_batch_norm=True,\n    rngs=nnx.Rngs(0),\n)\n\n# Forward pass\nx = jnp.ones((32, 28, 28, 1))\noutput = model(x, rngs=nnx.Rngs(1))\n\nprint(f\"Reconstruction shape: {output['reconstruction'].shape}\")  # (32, 28, 28, 1)\nprint(f\"Latent shape: {output['latent'].shape}\")  # (32, 20)\n</code></pre>"},{"location":"user-guide/advanced/architectures/#custom-gan-with-advanced-techniques","title":"Custom GAN with Advanced Techniques","text":"<pre><code>class CustomGenerator(nnx.Module):\n    \"\"\"Custom generator with self-attention.\"\"\"\n\n    def __init__(\n        self,\n        latent_dim: int,\n        output_shape: tuple,\n        hidden_dims: list[int],\n        *,\n        use_attention: bool = True,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.output_shape = output_shape\n        self.use_attention = use_attention\n\n        # Build generator network\n        self.layers = []\n\n        # Initial projection\n        self.layers.append(nnx.Linear(latent_dim, hidden_dims[0], rngs=rngs))\n        self.layers.append(nnx.BatchNorm(hidden_dims[0], rngs=rngs))\n\n        # Hidden layers\n        for i in range(len(hidden_dims) - 1):\n            self.layers.append(\n                nnx.Linear(hidden_dims[i], hidden_dims[i + 1], rngs=rngs)\n            )\n            self.layers.append(nnx.BatchNorm(hidden_dims[i + 1], rngs=rngs))\n\n            # Add self-attention at middle layer\n            if use_attention and i == len(hidden_dims) // 2:\n                self.attention = MultiHeadAttention(\n                    hidden_size=hidden_dims[i + 1],\n                    num_heads=4,\n                    rngs=rngs,\n                )\n\n        # Output layer\n        output_dim = int(jnp.prod(jnp.array(output_shape)))\n        self.output_layer = nnx.Linear(hidden_dims[-1], output_dim, rngs=rngs)\n\n    def __call__(\n        self,\n        z: jax.Array,\n        *,\n        use_running_average: bool = False,\n    ) -&gt; jax.Array:\n        \"\"\"Generate samples from noise.\n\n        Args:\n            z: Noise vector (batch, latent_dim)\n            use_running_average: Use running stats for batch norm\n\n        Returns:\n            Generated samples (batch, *output_shape)\n        \"\"\"\n        x = z\n\n        # Forward through layers\n        for i, layer in enumerate(self.layers):\n            if isinstance(layer, nnx.BatchNorm):\n                x = layer(x, use_running_average=use_running_average)\n            else:\n                x = layer(x)\n                x = nnx.relu(x)\n\n            # Apply self-attention if available\n            if self.use_attention and hasattr(self, \"attention\"):\n                if i == len(self.layers) // 2:\n                    # Reshape for attention (add sequence dimension)\n                    batch_size = x.shape[0]\n                    x = x.reshape(batch_size, 1, -1)\n                    x = self.attention(x)\n                    x = x.reshape(batch_size, -1)\n\n        # Output layer with tanh activation\n        x = self.output_layer(x)\n        x = nnx.tanh(x)\n\n        # Reshape to output shape\n        batch_size = z.shape[0]\n        x = x.reshape(batch_size, *self.output_shape)\n\n        return x\n\n\nclass CustomDiscriminator(nnx.Module):\n    \"\"\"Custom discriminator with spectral normalization.\"\"\"\n\n    def __init__(\n        self,\n        input_shape: tuple,\n        hidden_dims: list[int],\n        *,\n        spectral_norm: bool = True,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.input_shape = input_shape\n        self.spectral_norm = spectral_norm\n\n        input_dim = int(jnp.prod(jnp.array(input_shape)))\n\n        # Build discriminator network\n        self.layers = []\n\n        # Input layer\n        if spectral_norm:\n            self.layers.append(\n                RegularizedLinear(\n                    input_dim,\n                    hidden_dims[0],\n                    spectral_norm=True,\n                    rngs=rngs,\n                )\n            )\n        else:\n            self.layers.append(nnx.Linear(input_dim, hidden_dims[0], rngs=rngs))\n\n        # Hidden layers\n        for i in range(len(hidden_dims) - 1):\n            if spectral_norm:\n                self.layers.append(\n                    RegularizedLinear(\n                        hidden_dims[i],\n                        hidden_dims[i + 1],\n                        spectral_norm=True,\n                        rngs=rngs,\n                    )\n                )\n            else:\n                self.layers.append(\n                    nnx.Linear(hidden_dims[i], hidden_dims[i + 1], rngs=rngs)\n                )\n\n        # Output layer (binary classification)\n        if spectral_norm:\n            self.output_layer = RegularizedLinear(\n                hidden_dims[-1], 1,\n                spectral_norm=True,\n                rngs=rngs,\n            )\n        else:\n            self.output_layer = nnx.Linear(hidden_dims[-1], 1, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Discriminate real vs fake samples.\n\n        Args:\n            x: Input samples (batch, *input_shape)\n\n        Returns:\n            Logits (batch, 1)\n        \"\"\"\n        # Flatten input\n        batch_size = x.shape[0]\n        x = x.reshape(batch_size, -1)\n\n        # Forward through layers\n        for layer in self.layers:\n            x = layer(x)\n            x = nnx.leaky_relu(x, negative_slope=0.2)\n\n        # Output layer (no activation, return logits)\n        logits = self.output_layer(x)\n\n        return logits\n\n\n# Create custom GAN\ngenerator = CustomGenerator(\n    latent_dim=100,\n    output_shape=(28, 28, 1),\n    hidden_dims=[256, 512, 1024],\n    use_attention=True,\n    rngs=nnx.Rngs(0),\n)\n\ndiscriminator = CustomDiscriminator(\n    input_shape=(28, 28, 1),\n    hidden_dims=[1024, 512, 256],\n    spectral_norm=True,\n    rngs=nnx.Rngs(1),\n)\n\n# Generate samples\nz = jax.random.normal(jax.random.key(0), (32, 100))\nfake_samples = generator(z)\nprint(f\"Generated shape: {fake_samples.shape}\")  # (32, 28, 28, 1)\n\n# Discriminate\nreal_samples = jnp.ones((32, 28, 28, 1))\nreal_logits = discriminator(real_samples)\nfake_logits = discriminator(fake_samples)\nprint(f\"Real logits shape: {real_logits.shape}\")  # (32, 1)\n</code></pre>"},{"location":"user-guide/advanced/architectures/#architecture-patterns","title":"Architecture Patterns","text":"<p>Common architectural patterns and best practices.</p>"},{"location":"user-guide/advanced/architectures/#residual-connections","title":"Residual Connections","text":"<pre><code>class ResidualNetwork(nnx.Module):\n    \"\"\"Network with residual connections.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        num_blocks: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        # Input projection\n        self.input_proj = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n\n        # Residual blocks\n        self.blocks = [\n            self._create_residual_block(hidden_dim, rngs)\n            for _ in range(num_blocks)\n        ]\n\n        # Output projection\n        self.output_proj = nnx.Linear(hidden_dim, input_dim, rngs=rngs)\n\n    def _create_residual_block(\n        self,\n        hidden_dim: int,\n        rngs: nnx.Rngs,\n    ) -&gt; list:\n        \"\"\"Create a single residual block.\"\"\"\n        return [\n            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs),\n            nnx.LayerNorm(hidden_dim, rngs=rngs),\n            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs),\n            nnx.LayerNorm(hidden_dim, rngs=rngs),\n        ]\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with residual connections.\"\"\"\n        # Input projection\n        x = self.input_proj(x)\n\n        # Residual blocks\n        for block_layers in self.blocks:\n            residual = x\n\n            # Forward through block layers\n            x = block_layers[0](x)  # Linear\n            x = nnx.relu(x)\n            x = block_layers[1](x)  # LayerNorm\n\n            x = block_layers[2](x)  # Linear\n            x = block_layers[3](x)  # LayerNorm\n\n            # Residual connection\n            x = x + residual\n            x = nnx.relu(x)\n\n        # Output projection\n        x = self.output_proj(x)\n\n        return x\n</code></pre>"},{"location":"user-guide/advanced/architectures/#skip-connections-u-net-style","title":"Skip Connections (U-Net Style)","text":"<pre><code>class UNetEncoder(nnx.Module):\n    \"\"\"U-Net style encoder with skip connections.\"\"\"\n\n    def __init__(\n        self,\n        channels_list: list[int],\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.down_blocks = []\n\n        for i in range(len(channels_list) - 1):\n            in_channels = channels_list[i]\n            out_channels = channels_list[i + 1]\n\n            # Downsampling block\n            block = [\n                nnx.Conv(\n                    in_features=in_channels,\n                    out_features=out_channels,\n                    kernel_size=(3, 3),\n                    strides=(2, 2),\n                    padding=\"SAME\",\n                    rngs=rngs,\n                ),\n                nnx.BatchNorm(out_channels, rngs=rngs),\n            ]\n            self.down_blocks.append(block)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        *,\n        use_running_average: bool = False,\n    ) -&gt; tuple[jax.Array, list[jax.Array]]:\n        \"\"\"Forward pass with skip connections.\n\n        Args:\n            x: Input tensor\n            use_running_average: Use running stats for batch norm\n\n        Returns:\n            (encoded, skip_connections)\n        \"\"\"\n        skip_connections = []\n\n        for block in self.down_blocks:\n            # Save for skip connection\n            skip_connections.append(x)\n\n            # Downsample\n            x = block[0](x)  # Conv\n            x = block[1](x, use_running_average=use_running_average)  # BatchNorm\n            x = nnx.relu(x)\n\n        return x, skip_connections\n\n\nclass UNetDecoder(nnx.Module):\n    \"\"\"U-Net style decoder with skip connections.\"\"\"\n\n    def __init__(\n        self,\n        channels_list: list[int],\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.up_blocks = []\n\n        for i in range(len(channels_list) - 1):\n            in_channels = channels_list[i]\n            out_channels = channels_list[i + 1]\n\n            # Upsampling block\n            block = [\n                nnx.ConvTranspose(\n                    in_features=in_channels,\n                    out_features=out_channels,\n                    kernel_size=(3, 3),\n                    strides=(2, 2),\n                    padding=\"SAME\",\n                    rngs=rngs,\n                ),\n                nnx.BatchNorm(out_channels, rngs=rngs),\n            ]\n            self.up_blocks.append(block)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        skip_connections: list[jax.Array],\n        *,\n        use_running_average: bool = False,\n    ) -&gt; jax.Array:\n        \"\"\"Forward pass with skip connections.\n\n        Args:\n            x: Encoded tensor\n            skip_connections: Skip connections from encoder\n            use_running_average: Use running stats for batch norm\n\n        Returns:\n            Decoded tensor\n        \"\"\"\n        for i, block in enumerate(self.up_blocks):\n            # Upsample\n            x = block[0](x)  # ConvTranspose\n            x = block[1](x, use_running_average=use_running_average)  # BatchNorm\n            x = nnx.relu(x)\n\n            # Add skip connection\n            if i &lt; len(skip_connections):\n                skip = skip_connections[-(i + 1)]\n                x = jnp.concatenate([x, skip], axis=-1)\n\n        return x\n</code></pre>"},{"location":"user-guide/advanced/architectures/#dense-connections-densenet-style","title":"Dense Connections (DenseNet Style)","text":"<pre><code>class DenseBlock(nnx.Module):\n    \"\"\"Dense block with concatenated connections.\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        growth_rate: int,\n        num_layers: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        self.growth_rate = growth_rate\n\n        self.layers = []\n        for i in range(num_layers):\n            layer_in_channels = in_channels + i * growth_rate\n\n            layer = [\n                nnx.BatchNorm(layer_in_channels, rngs=rngs),\n                nnx.Conv(\n                    in_features=layer_in_channels,\n                    out_features=growth_rate,\n                    kernel_size=(3, 3),\n                    padding=\"SAME\",\n                    rngs=rngs,\n                ),\n            ]\n            self.layers.append(layer)\n\n    def __call__(\n        self,\n        x: jax.Array,\n        *,\n        use_running_average: bool = False,\n    ) -&gt; jax.Array:\n        \"\"\"Forward pass with dense connections.\n\n        Args:\n            x: Input tensor\n            use_running_average: Use running stats for batch norm\n\n        Returns:\n            Output tensor with all features concatenated\n        \"\"\"\n        features = [x]\n\n        for layer in self.layers:\n            # BatchNorm + ReLU + Conv\n            out = layer[0](x, use_running_average=use_running_average)\n            out = nnx.relu(out)\n            out = layer[1](out)\n\n            # Concatenate with previous features\n            features.append(out)\n            x = jnp.concatenate(features, axis=-1)\n\n        return x\n</code></pre>"},{"location":"user-guide/advanced/architectures/#artifex-integration","title":"Artifex Integration","text":"<p>Integrate custom models with Artifex's systems.</p>"},{"location":"user-guide/advanced/architectures/#implementing-the-generativemodel-protocol","title":"Implementing the GenerativeModel Protocol","text":"<pre><code>from artifex.generative_models.core.protocols import GenerativeModel\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\nclass MyCustomGenerativeModel(nnx.Module):\n    \"\"\"Custom model implementing Artifex's GenerativeModel protocol.\"\"\"\n\n    def __init__(\n        self,\n        config: dict,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        # Extract config\n        self.latent_dim = config.get(\"latent_dim\", 20)\n        self.input_shape = config.get(\"input_shape\", (28, 28, 1))\n\n        # Build architecture (your custom design)\n        self.encoder = CustomVAE(\n            input_shape=self.input_shape,\n            latent_dim=self.latent_dim,\n            encoder_layers=[512, 256],\n            decoder_layers=[256, 512],\n            rngs=rngs,\n        )\n\n    def __call__(\n        self,\n        x: jax.Array,\n        *,\n        rngs: nnx.Rngs | None = None,\n        **kwargs\n    ) -&gt; dict[str, jax.Array]:\n        \"\"\"Forward pass returning Artifex-compatible output.\n\n        Must return dictionary with at least:\n        - 'loss': scalar loss for training\n        - Model-specific outputs (e.g., 'reconstruction', 'samples')\n        \"\"\"\n        # Forward pass\n        output = self.encoder(x, rngs=rngs, **kwargs)\n\n        # Compute loss\n        reconstruction_loss = jnp.mean((x - output[\"reconstruction\"]) ** 2)\n        kl_loss = -0.5 * jnp.mean(\n            1 + output[\"logvar\"] - output[\"mean\"] ** 2 - jnp.exp(output[\"logvar\"])\n        )\n        total_loss = reconstruction_loss + kl_loss\n\n        # Return Artifex-compatible output\n        return {\n            \"loss\": total_loss,\n            \"reconstruction\": output[\"reconstruction\"],\n            \"latent\": output[\"latent\"],\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n\n    def sample(\n        self,\n        num_samples: int,\n        *,\n        rngs: nnx.Rngs | None = None,\n        **kwargs\n    ) -&gt; jax.Array:\n        \"\"\"Generate samples (required for GenerativeModel protocol).\"\"\"\n        if rngs is not None and \"sample\" in rngs:\n            key = rngs.sample()\n        else:\n            key = jax.random.key(0)\n\n        # Sample from prior\n        z = jax.random.normal(key, (num_samples, self.latent_dim))\n\n        # Decode\n        samples = self.encoder.decode(z, **kwargs)\n\n        return samples\n\n\n# Usage with Artifex\nfrom artifex.generative_models.training.trainer import Trainer\n\nmodel = MyCustomGenerativeModel(\n    config={\"latent_dim\": 20, \"input_shape\": (28, 28, 1)},\n    rngs=nnx.Rngs(0),\n)\n\n# Integrate with Artifex's trainer\ntrainer = Trainer(\n    model=model,\n    # ... other config\n)\n\n# Training and evaluation work automatically\n# trainer.train(train_dataset, val_dataset)\n</code></pre>"},{"location":"user-guide/advanced/architectures/#custom-loss-functions","title":"Custom Loss Functions","text":"<pre><code>def custom_vae_loss(\n    model: nnx.Module,\n    batch: dict[str, jax.Array],\n    *,\n    beta: float = 1.0,\n    **kwargs\n) -&gt; tuple[jax.Array, dict]:\n    \"\"\"Custom VAE loss with \u03b2-weighting.\n\n    Args:\n        model: The VAE model\n        batch: Batch dictionary with 'data' key\n        beta: Weight for KL divergence term\n\n    Returns:\n        (total_loss, metrics_dict)\n    \"\"\"\n    # Forward pass\n    output = model(batch[\"data\"], **kwargs)\n\n    # Reconstruction loss\n    recon_loss = jnp.mean(\n        (batch[\"data\"] - output[\"reconstruction\"]) ** 2\n    )\n\n    # KL divergence\n    kl_loss = -0.5 * jnp.mean(\n        1 + output[\"logvar\"]\n        - output[\"mean\"] ** 2\n        - jnp.exp(output[\"logvar\"])\n    )\n\n    # Total loss with \u03b2-weighting\n    total_loss = recon_loss + beta * kl_loss\n\n    # Metrics for logging\n    metrics = {\n        \"loss\": total_loss,\n        \"reconstruction_loss\": recon_loss,\n        \"kl_loss\": kl_loss,\n        \"beta\": beta,\n    }\n\n    return total_loss, metrics\n\n\n# Use custom loss in training\n@jax.jit\ndef train_step(model_state, batch, optimizer_state):\n    \"\"\"Training step with custom loss.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n\n    # Compute loss and gradients\n    (loss, metrics), grads = nnx.value_and_grad(\n        lambda m: custom_vae_loss(m, batch, beta=2.0),\n        has_aux=True\n    )(model)\n\n    # Update parameters\n    updates, optimizer_state = optimizer.update(grads, optimizer_state)\n    model_state = optax.apply_updates(model_state, updates)\n\n    return model_state, optimizer_state, metrics\n</code></pre>"},{"location":"user-guide/advanced/architectures/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/advanced/architectures/#do","title":"DO","text":"<ul> <li>\u2705 Follow Flax NNX patterns - use <code>nnx.Module</code>, <code>nnx.Param</code></li> <li>\u2705 Call <code>super().__init__()</code> - always in module constructors</li> <li>\u2705 Use proper RNG handling - check if key exists, provide fallback</li> <li>\u2705 Implement protocols - match Artifex's interface expectations</li> <li>\u2705 Return dictionaries - structured outputs for logging</li> <li>\u2705 Use type hints - document input/output shapes</li> <li>\u2705 Test components separately - unit test layers before integration</li> <li>\u2705 Profile performance - measure speed and memory</li> <li>\u2705 Document architecture - explain design choices</li> <li>\u2705 Version your models - track architectural changes</li> </ul>"},{"location":"user-guide/advanced/architectures/#dont","title":"DON'T","text":"<ul> <li>\u274c Don't use Flax Linen - only use Flax NNX</li> <li>\u274c Don't forget super().**init()** - causes initialization issues</li> <li>\u274c Don't use numpy inside modules - use <code>jax.numpy</code> instead</li> <li>\u274c Don't mix PyTorch/TensorFlow - stay in JAX ecosystem</li> <li>\u274c Don't hardcode shapes - make them configurable</li> <li>\u274c Don't skip validation - verify outputs are correct</li> <li>\u274c Don't ignore memory - monitor GPU usage</li> <li>\u274c Don't over-engineer - start simple, add complexity as needed</li> <li>\u274c Don't skip documentation - explain architecture decisions</li> <li>\u274c Don't forget batch dimensions - always handle batched inputs</li> </ul>"},{"location":"user-guide/advanced/architectures/#summary","title":"Summary","text":"<p>Custom architectures in Artifex:</p> <ol> <li>Custom Layers: Build reusable components with Flax NNX</li> <li>Custom Models: Create complete generative models</li> <li>Architecture Patterns: Residual, skip, dense connections</li> <li>Artifex Integration: Implement protocols for seamless integration</li> </ol> <p>Key principles:</p> <ul> <li>Use Flax NNX exclusively</li> <li>Follow Artifex's protocol interfaces</li> <li>Return structured outputs (dictionaries)</li> <li>Document architecture choices</li> <li>Test and profile before deploying</li> </ul>"},{"location":"user-guide/advanced/architectures/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Advanced Examples</p> <p>See complete examples of custom architectures in action</p> <p> View examples</p> </li> <li> <p> Distributed Training</p> <p>Scale custom models with distributed training</p> <p> Distributed guide</p> </li> <li> <p> Model Parallelism</p> <p>Parallelize large custom models</p> <p> Parallelism guide</p> </li> <li> <p> Training Guide</p> <p>Train custom models with Artifex's trainer</p> <p> Training guide</p> </li> </ul>"},{"location":"user-guide/advanced/checkpointing/","title":"Checkpointing","text":"<p>Checkpointing strategies for saving model state and reducing memory usage during training. Artifex provides both gradient checkpointing (activation recomputation) and model checkpointing (state persistence) using Orbax.</p> <ul> <li> <p> Model Checkpointing</p> <p>Save and restore model state with Orbax</p> <p> Learn more</p> </li> <li> <p> Gradient Checkpointing</p> <p>Trade computation for memory with activation recomputation</p> <p> Learn more</p> </li> <li> <p> Checkpointing Strategies</p> <p>Optimize when and how to checkpoint</p> <p> Learn more</p> </li> <li> <p> Recovery</p> <p>Recover from failures and resume training</p> <p> Learn more</p> </li> </ul>"},{"location":"user-guide/advanced/checkpointing/#overview","title":"Overview","text":"<p>Two types of checkpointing in Artifex:</p> <ol> <li>Model Checkpointing: Save model state to disk for:</li> <li>Training resumption after interruption</li> <li>Model deployment and inference</li> <li> <p>Experiment tracking and reproducibility</p> </li> <li> <p>Gradient Checkpointing: Recompute activations during backward pass to:</p> </li> <li>Reduce memory usage (trade compute for memory)</li> <li>Train larger models or bigger batches</li> <li>Enable training on memory-limited hardware</li> </ol>"},{"location":"user-guide/advanced/checkpointing/#model-checkpointing","title":"Model Checkpointing","text":"<p>Save and restore model state using Orbax checkpoint manager.</p>"},{"location":"user-guide/advanced/checkpointing/#basic-model-checkpointing","title":"Basic Model Checkpointing","text":"<pre><code>import orbax.checkpoint as ocp\nfrom flax import nnx\nfrom artifex.generative_models.core.checkpointing import (\n    setup_checkpoint_manager,\n    save_checkpoint,\n    load_checkpoint,\n)\n\n# Create model\nmodel = create_vae_model(config, rngs=nnx.Rngs(0))\n\n# Setup checkpoint manager\ncheckpoint_manager, checkpoint_dir = setup_checkpoint_manager(\n    base_dir=\"./checkpoints/experiment_1\"\n)\n\n# Training loop\nfor step in range(num_steps):\n    # Training step\n    model_state, loss = train_step(nnx.state(model), batch)\n    nnx.update(model, model_state)\n\n    # Save checkpoint every N steps\n    if (step + 1) % save_every == 0:\n        checkpoint_manager = save_checkpoint(\n            checkpoint_manager,\n            model,\n            step=step + 1\n        )\n        print(f\"Saved checkpoint at step {step + 1}\")\n\nprint(f\"Training complete. Checkpoints saved to {checkpoint_dir}\")\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#loading-checkpoints","title":"Loading Checkpoints","text":"<pre><code>from artifex.generative_models.core.checkpointing import (\n    load_checkpoint,\n    setup_checkpoint_manager,\n)\nfrom flax import nnx\n\n# Setup checkpoint manager (same directory)\ncheckpoint_manager, _ = setup_checkpoint_manager(\n    base_dir=\"./checkpoints/experiment_1\"\n)\n\n# Create model template (same structure as saved model)\nmodel_template = create_vae_model(config, rngs=nnx.Rngs(0))\n\n# Load latest checkpoint\nrestored_model, step = load_checkpoint(\n    checkpoint_manager,\n    target_model_template=model_template,\n    step=None,  # None = load latest\n)\n\nif restored_model is not None:\n    print(f\"Restored model from step {step}\")\n    model = restored_model\nelse:\n    print(\"No checkpoint found, starting from scratch\")\n    model = model_template\n\n# Continue training from restored state\nfor step in range(step + 1, num_steps):\n    model_state, loss = train_step(nnx.state(model), batch)\n    nnx.update(model, model_state)\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#loading-specific-checkpoints","title":"Loading Specific Checkpoints","text":"<pre><code># Load specific checkpoint by step\nrestored_model, step = load_checkpoint(\n    checkpoint_manager,\n    target_model_template=model_template,\n    step=5000,  # Load checkpoint from step 5000\n)\n\n# List available checkpoints\nlatest_step = checkpoint_manager.latest_step()\nall_steps = checkpoint_manager.all_steps()\n\nprint(f\"Latest checkpoint: step {latest_step}\")\nprint(f\"Available checkpoints: {all_steps}\")\n\n# Load best checkpoint (based on external tracking)\n# You would track best step separately\nbest_step = 7500  # From your tracking\nrestored_model, step = load_checkpoint(\n    checkpoint_manager,\n    target_model_template=model_template,\n    step=best_step,\n)\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#checkpointing-with-optimizer-state","title":"Checkpointing with Optimizer State","text":"<p>Artifex provides built-in functions for saving and loading both model and optimizer state:</p> <pre><code>from flax import nnx\nimport optax\nfrom artifex.generative_models.core.checkpointing import (\n    setup_checkpoint_manager,\n    save_checkpoint_with_optimizer,\n    load_checkpoint_with_optimizer,\n)\n\n# Create model and optimizer\nmodel = create_vae_model(config, rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n# Setup checkpoint manager\ncheckpoint_manager, _ = setup_checkpoint_manager(\n    base_dir=\"./checkpoints/with_optimizer\"\n)\n\n# Training with optimizer checkpointing\nfor step in range(num_steps):\n    # Training step\n    grads = nnx.grad(loss_fn)(model)\n    optimizer.update(model, grads)\n\n    # Save checkpoint with optimizer\n    if (step + 1) % save_every == 0:\n        save_checkpoint_with_optimizer(\n            checkpoint_manager, model, optimizer, step + 1\n        )\n\n# Load checkpoint with optimizer\nmodel_template = create_vae_model(config, rngs=nnx.Rngs(0))\noptimizer_template = nnx.Optimizer(model_template, optax.adam(1e-4), wrt=nnx.Param)\n\nmodel, optimizer, step = load_checkpoint_with_optimizer(\n    checkpoint_manager, model_template, optimizer_template\n)\n\nif model is not None:\n    print(f\"Resumed from step {step}\")\nelse:\n    print(\"No checkpoint found, starting from scratch\")\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#asynchronous-checkpointing","title":"Asynchronous Checkpointing","text":"<p>Checkpoint without blocking training:</p> <pre><code>import orbax.checkpoint as ocp\nfrom flax import nnx\n\n# Create checkpoint manager with async options\noptions = ocp.CheckpointManagerOptions(\n    max_to_keep=5,\n    create=True,\n    save_interval_steps=1,  # Allow saving every step\n    # Async saving\n    enable_async_checkpointing=True,\n)\n\ncheckpoint_manager = ocp.CheckpointManager(\n    directory=\"./checkpoints/async\",\n    options=options,\n)\n\n# Training loop with async checkpointing\nfor step in range(num_steps):\n    # Training step\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n\n    # Save checkpoint asynchronously\n    if (step + 1) % save_every == 0:\n        model_state = nnx.state(model)\n\n        save_args = ocp.args.Composite(\n            model=ocp.args.StandardSave(model_state)\n        )\n\n        # Non-blocking save\n        checkpoint_manager.save(step + 1, args=save_args)\n\n        # Continue training immediately\n        # Checkpoint happens in background\n\n    # Optional: Check if previous save finished\n    if checkpoint_manager.check_for_errors():\n        print(\"Checkpoint error detected!\")\n\n# Wait for final checkpoint to finish\ncheckpoint_manager.wait_until_finished()\nprint(\"All checkpoints saved\")\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#checkpoint-retention-policies","title":"Checkpoint Retention Policies","text":"<p>Control which checkpoints to keep:</p> <pre><code>import orbax.checkpoint as ocp\n\n# Keep only last N checkpoints\noptions = ocp.CheckpointManagerOptions(\n    max_to_keep=5,  # Keep last 5 checkpoints\n    create=True,\n)\n\n# Keep all checkpoints (be careful with disk space)\noptions = ocp.CheckpointManagerOptions(\n    max_to_keep=None,  # Keep all\n    create=True,\n)\n\n# Custom retention: Keep specific checkpoints\nclass CustomCheckpointManager:\n    \"\"\"Checkpoint manager with custom retention policy.\"\"\"\n\n    def __init__(self, base_dir: str):\n        self.base_dir = base_dir\n        self.manager = setup_checkpoint_manager(base_dir)[0]\n        self.keep_steps = set()  # Steps to always keep\n\n    def save(self, model, step: int, keep: bool = False):\n        \"\"\"Save checkpoint, optionally marking it to keep.\"\"\"\n        save_checkpoint(self.manager, model, step)\n\n        if keep:\n            self.keep_steps.add(step)\n\n        # Clean up old checkpoints not in keep_steps\n        all_steps = self.manager.all_steps()\n        if len(all_steps) &gt; 10:  # Keep at most 10 checkpoints\n            # Remove oldest checkpoints not marked to keep\n            steps_to_remove = sorted(all_steps)[:-5]  # Keep 5 recent\n            for s in steps_to_remove:\n                if s not in self.keep_steps:\n                    self.manager.delete(s)\n\n\n# Usage\nmanager = CustomCheckpointManager(\"./checkpoints/custom\")\n\nfor step in range(num_steps):\n    # Training\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n\n    # Save checkpoint\n    if (step + 1) % save_every == 0:\n        # Mark checkpoints with best validation loss to keep\n        is_best = (val_loss &lt; best_val_loss)\n        manager.save(model, step + 1, keep=is_best)\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>Reduce memory by recomputing activations during backward pass.</p>"},{"location":"user-guide/advanced/checkpointing/#basic-gradient-checkpointing","title":"Basic Gradient Checkpointing","text":"<pre><code>import jax\nfrom jax.ad_checkpoint import checkpoint as jax_checkpoint\nfrom flax import nnx\n\nclass CheckpointedModel(nnx.Module):\n    \"\"\"Model with gradient checkpointing.\"\"\"\n\n    def __init__(\n        self,\n        num_layers: int,\n        hidden_dim: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        # Create layers\n        self.layers = [\n            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n            for _ in range(num_layers)\n        ]\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with checkpointing.\"\"\"\n        # Checkpoint each layer\n        for layer in self.layers:\n            # Activations not stored in memory\n            # Will be recomputed during backward pass\n            x = jax_checkpoint(lambda x: nnx.relu(layer(x)))(x)\n\n        return x\n\n\n# Create model\nmodel = CheckpointedModel(\n    num_layers=100,  # Can train much deeper models\n    hidden_dim=1024,\n    rngs=nnx.Rngs(0),\n)\n\n# Training step (automatic recomputation)\ndef loss_fn(model, x):\n    output = model(x)\n    return jnp.mean(output ** 2)\n\n# Compute gradients (recomputes activations as needed)\nloss, grads = nnx.value_and_grad(loss_fn)(model, x)\n\n# Memory usage: ~50% reduction\n# Training time: ~30% slower (due to recomputation)\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#selective-checkpointing","title":"Selective Checkpointing","text":"<p>Checkpoint only expensive operations:</p> <pre><code>from jax.ad_checkpoint import checkpoint as jax_checkpoint\nfrom flax import nnx\n\nclass SelectiveCheckpointedTransformer(nnx.Module):\n    \"\"\"Transformer with selective checkpointing.\"\"\"\n\n    def __init__(\n        self,\n        num_layers: int,\n        hidden_size: int,\n        num_heads: int,\n        *,\n        rngs: nnx.Rngs,\n        checkpoint_attention: bool = True,\n        checkpoint_ffn: bool = False,\n        checkpoint_every_n: int = 1,\n    ):\n        super().__init__()\n        self.checkpoint_attention = checkpoint_attention\n        self.checkpoint_ffn = checkpoint_ffn\n        self.checkpoint_every_n = checkpoint_every_n\n\n        # Create layers\n        self.layers = []\n        for i in range(num_layers):\n            layer = TransformerLayer(\n                hidden_size=hidden_size,\n                num_heads=num_heads,\n                rngs=rngs,\n            )\n            self.layers.append(layer)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with selective checkpointing.\"\"\"\n        for i, layer in enumerate(self.layers):\n            # Checkpoint every N layers\n            should_checkpoint = (i % self.checkpoint_every_n == 0)\n\n            if should_checkpoint:\n                # Checkpoint entire layer\n                x = jax_checkpoint(layer)(x)\n            else:\n                # No checkpointing\n                x = layer(x)\n\n        return x\n\n\nclass TransformerLayer(nnx.Module):\n    \"\"\"Single transformer layer with fine-grained checkpointing.\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        *,\n        rngs: nnx.Rngs,\n        checkpoint_attention: bool = True,\n        checkpoint_ffn: bool = False,\n    ):\n        super().__init__()\n        self.checkpoint_attention = checkpoint_attention\n        self.checkpoint_ffn = checkpoint_ffn\n\n        self.attention = MultiHeadAttention(hidden_size, num_heads, rngs=rngs)\n        self.ffn = FeedForward(hidden_size, 4 * hidden_size, rngs=rngs)\n        self.ln1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.ln2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with component-level checkpointing.\"\"\"\n        # Attention block\n        residual = x\n        x = self.ln1(x)\n\n        if self.checkpoint_attention:\n            # Checkpoint attention (quadratic memory in seq_len)\n            x = jax_checkpoint(self.attention)(x)\n        else:\n            x = self.attention(x)\n\n        x = residual + x\n\n        # FFN block\n        residual = x\n        x = self.ln2(x)\n\n        if self.checkpoint_ffn:\n            # Checkpoint FFN (linear memory, fast)\n            x = jax_checkpoint(self.ffn)(x)\n        else:\n            x = self.ffn(x)\n\n        x = residual + x\n\n        return x\n\n\n# Usage: Checkpoint attention only (biggest memory savings)\nmodel = SelectiveCheckpointedTransformer(\n    num_layers=24,\n    hidden_size=1024,\n    num_heads=16,\n    rngs=nnx.Rngs(0),\n    checkpoint_attention=True,  # Checkpoint attention\n    checkpoint_ffn=False,  # Don't checkpoint FFN\n    checkpoint_every_n=2,  # Checkpoint every 2nd layer\n)\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#checkpoint-policy-functions","title":"Checkpoint Policy Functions","text":"<p>Custom policies for what to checkpoint:</p> <pre><code>from jax.ad_checkpoint import checkpoint_policies\n\ndef custom_checkpoint_policy(\n    model: nnx.Module,\n    memory_budget: float = 0.5,\n) -&gt; callable:\n    \"\"\"Create custom checkpoint policy based on memory budget.\n\n    Args:\n        model: The model to checkpoint\n        memory_budget: Fraction of memory to use (0.5 = 50%)\n\n    Returns:\n        Policy function for selective checkpointing\n    \"\"\"\n    # Analyze model to find expensive operations\n    def get_operation_cost(op_name: str) -&gt; float:\n        \"\"\"Estimate memory cost of operation.\"\"\"\n        if \"attention\" in op_name:\n            return 1.0  # High cost (quadratic)\n        elif \"ffn\" in op_name or \"linear\" in op_name:\n            return 0.3  # Medium cost\n        elif \"norm\" in op_name:\n            return 0.1  # Low cost\n        else:\n            return 0.2  # Default\n\n    # Create policy\n    def should_checkpoint(primitive, *args, **kwargs):\n        \"\"\"Decide whether to checkpoint this operation.\"\"\"\n        op_name = str(primitive).lower()\n        cost = get_operation_cost(op_name)\n\n        # Checkpoint if cost exceeds budget threshold\n        return cost &gt; (1.0 - memory_budget)\n\n    return should_checkpoint\n\n\n# Use custom policy\npolicy = custom_checkpoint_policy(model, memory_budget=0.7)\n\n# Apply policy to model\n@jax_checkpoint(policy=policy)\ndef forward_with_policy(model, x):\n    return model(x)\n\noutput = forward_with_policy(model, x)\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#remat-rematerialization","title":"Remat (Rematerialization)","text":"<p>JAX's automatic checkpointing using <code>jax.checkpoint</code> with policies:</p> <pre><code>import jax\nfrom jax.ad_checkpoint import checkpoint as jax_checkpoint\nfrom flax import nnx\n\nclass RematModel(nnx.Module):\n    \"\"\"Model using JAX remat for automatic checkpointing.\"\"\"\n\n    def __init__(\n        self,\n        num_layers: int,\n        hidden_dim: int,\n        *,\n        rngs: nnx.Rngs,\n        checkpoint_policy: str = \"everything_saveable\",\n    ):\n        super().__init__()\n        self.checkpoint_policy = checkpoint_policy\n\n        self.layers = [\n            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n            for _ in range(num_layers)\n        ]\n\n    def _forward_layer(self, layer, x):\n        \"\"\"Forward pass through single layer.\"\"\"\n        return nnx.relu(layer(x))\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with remat policy.\"\"\"\n        # Choose checkpointing policy\n        if self.checkpoint_policy == \"everything_saveable\":\n            # Save everything that doesn't require recomputation\n            policy = jax.checkpoint_policies.everything_saveable\n        elif self.checkpoint_policy == \"nothing_saveable\":\n            # Recompute everything (maximum memory savings)\n            policy = jax.checkpoint_policies.nothing_saveable\n        elif self.checkpoint_policy == \"dots_with_no_batch_dims\":\n            # Only checkpoint matrix multiplications\n            policy = jax.checkpoint_policies.dots_with_no_batch_dims_saveable\n        else:\n            policy = None\n\n        # Apply checkpointing with policy\n        for layer in self.layers:\n            if policy:\n                x = jax_checkpoint(\n                    lambda x: self._forward_layer(layer, x),\n                    policy=policy\n                )(x)\n            else:\n                x = self._forward_layer(layer, x)\n\n        return x\n\n\n# Compare policies\nfor policy in [\"everything_saveable\", \"nothing_saveable\", \"dots_with_no_batch_dims\"]:\n    model = RematModel(\n        num_layers=50,\n        hidden_dim=1024,\n        rngs=nnx.Rngs(0),\n        checkpoint_policy=policy,\n    )\n\n    # Measure memory and time\n    x = jnp.ones((32, 1024))\n\n    def loss_fn(model, x):\n        return jnp.mean(model(x) ** 2)\n\n    # Training step\n    loss, grads = nnx.value_and_grad(loss_fn)(model, x)\n\n    print(f\"Policy: {policy}\")\n    print(f\"  Loss: {loss}\")\n    # Memory and time would vary by policy\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#memory-time-trade-off-analysis","title":"Memory-Time Trade-off Analysis","text":"<pre><code>import time\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\ndef benchmark_checkpointing(\n    num_layers: int,\n    hidden_dim: int,\n    batch_size: int,\n    checkpoint_every_n: int = 1,\n) -&gt; dict:\n    \"\"\"Benchmark different checkpointing strategies.\"\"\"\n    results = {}\n\n    for strategy in [\"none\", \"all\", \"selective\"]:\n        # Create model\n        if strategy == \"none\":\n            # No checkpointing\n            model = create_standard_model(num_layers, hidden_dim)\n        elif strategy == \"all\":\n            # Checkpoint every layer\n            model = create_checkpointed_model(\n                num_layers, hidden_dim, checkpoint_every_n=1\n            )\n        else:\n            # Selective checkpointing\n            model = create_checkpointed_model(\n                num_layers, hidden_dim, checkpoint_every_n=checkpoint_every_n\n            )\n\n        # Measure time and memory\n        x = jnp.ones((batch_size, hidden_dim))\n\n        def loss_fn(model, x):\n            return jnp.mean(model(x) ** 2)\n\n        # Warmup\n        loss, grads = nnx.value_and_grad(loss_fn)(model, x)\n\n        # Benchmark\n        start = time.time()\n        for _ in range(10):\n            loss, grads = nnx.value_and_grad(loss_fn)(model, x)\n        duration = (time.time() - start) / 10\n\n        results[strategy] = {\n            \"time_per_step\": duration,\n            \"loss\": float(loss),\n        }\n\n    return results\n\n\n# Run benchmark\nresults = benchmark_checkpointing(\n    num_layers=50,\n    hidden_dim=1024,\n    batch_size=32,\n    checkpoint_every_n=5,\n)\n\nfor strategy, metrics in results.items():\n    print(f\"\\n{strategy.upper()}:\")\n    print(f\"  Time per step: {metrics['time_per_step']:.3f}s\")\n    print(f\"  Loss: {metrics['loss']:.4f}\")\n\n# Typical results:\n# NONE: Fast (1.0x), high memory (1.0x)\n# ALL: Slow (1.3x), low memory (0.5x)\n# SELECTIVE: Medium (1.15x), medium memory (0.7x)\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#checkpointing-strategies","title":"Checkpointing Strategies","text":"<p>Optimize when and how to checkpoint for best results.</p>"},{"location":"user-guide/advanced/checkpointing/#checkpoint-frequency","title":"Checkpoint Frequency","text":"<pre><code>class AdaptiveCheckpointing:\n    \"\"\"Adaptive checkpoint frequency based on training dynamics.\"\"\"\n\n    def __init__(\n        self,\n        base_interval: int = 1000,\n        min_interval: int = 500,\n        max_interval: int = 5000,\n    ):\n        self.base_interval = base_interval\n        self.min_interval = min_interval\n        self.max_interval = max_interval\n\n        self.loss_history = []\n        self.current_interval = base_interval\n\n    def should_checkpoint(self, step: int, loss: float) -&gt; bool:\n        \"\"\"Decide if we should checkpoint at this step.\"\"\"\n        self.loss_history.append(loss)\n\n        # Always checkpoint at base interval\n        if step % self.current_interval == 0:\n            return True\n\n        # More frequent checkpointing if loss unstable\n        if len(self.loss_history) &gt; 10:\n            recent_losses = self.loss_history[-10:]\n            loss_std = jnp.std(jnp.array(recent_losses))\n\n            if loss_std &gt; 0.1:\n                # Unstable: Checkpoint more frequently\n                self.current_interval = max(\n                    self.min_interval,\n                    self.current_interval // 2\n                )\n            else:\n                # Stable: Checkpoint less frequently\n                self.current_interval = min(\n                    self.max_interval,\n                    self.current_interval * 2\n                )\n\n        return False\n\n    def force_checkpoint(self) -&gt; bool:\n        \"\"\"Force checkpoint (e.g., at end of epoch).\"\"\"\n        return True\n\n\n# Usage\nadaptive = AdaptiveCheckpointing(base_interval=1000)\n\nfor step in range(num_steps):\n    # Training step\n    loss, grads = nnx.value_and_grad(loss_fn)(model, batch)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n\n    # Adaptive checkpointing\n    if adaptive.should_checkpoint(step, float(loss)):\n        save_checkpoint(checkpoint_manager, model, step)\n\n    # Force checkpoint at epoch end\n    if (step + 1) % steps_per_epoch == 0:\n        save_checkpoint(checkpoint_manager, model, step)\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#checkpoint-sharding","title":"Checkpoint Sharding","text":"<p>Shard large checkpoints for faster I/O:</p> <pre><code>import orbax.checkpoint as ocp\nfrom flax import nnx\nimport jax\n\ndef save_sharded_checkpoint(\n    checkpoint_manager,\n    model,\n    step: int,\n    num_shards: int = 4,\n):\n    \"\"\"Save checkpoint sharded across multiple files.\"\"\"\n    model_state = nnx.state(model)\n\n    # Get all devices\n    devices = jax.devices()\n\n    # Shard model state across devices\n    # This enables parallel I/O\n    sharded_state = jax.tree.map(\n        lambda x: jax.device_put(x, devices[0]),\n        model_state\n    )\n\n    # Create save args with sharding\n    save_args = ocp.args.Composite(\n        model=ocp.args.StandardSave(sharded_state)\n    )\n\n    # Save (Orbax automatically shards large arrays)\n    checkpoint_manager.save(step, args=save_args)\n    checkpoint_manager.wait_until_finished()\n\n    return checkpoint_manager\n\n\n# Load sharded checkpoint\ndef load_sharded_checkpoint(\n    checkpoint_manager,\n    model_template,\n    step=None,\n):\n    \"\"\"Load sharded checkpoint.\"\"\"\n    if step is None:\n        step = checkpoint_manager.latest_step()\n\n    if step is None:\n        return None, None\n\n    model_state = nnx.state(model_template)\n\n    restore_args = ocp.args.Composite(\n        model=ocp.args.StandardRestore(model_state)\n    )\n\n    # Restore (Orbax automatically handles sharded loading)\n    restored_data = checkpoint_manager.restore(step, args=restore_args)\n\n    nnx.update(model_template, restored_data[\"model\"])\n\n    return model_template, step\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#checkpoint-validation","title":"Checkpoint Validation","text":"<p>Artifex provides a built-in function to validate that checkpoints save and load correctly:</p> <pre><code>from artifex.generative_models.core.checkpointing import (\n    setup_checkpoint_manager,\n    save_checkpoint,\n    validate_checkpoint,\n)\n\n# Setup and save checkpoint\ncheckpoint_manager, _ = setup_checkpoint_manager(\"./checkpoints\")\nsave_checkpoint(checkpoint_manager, model, step=100)\n\n# Validate the checkpoint loads correctly\nvalidation_data = jnp.ones((2, 10))  # Sample input for validation\nis_valid = validate_checkpoint(\n    checkpoint_manager,\n    model,\n    step=100,\n    validation_data=validation_data,\n    tolerance=1e-5,  # Maximum allowed difference\n)\n\nif is_valid:\n    print(\"Checkpoint validated successfully\")\nelse:\n    print(\"Checkpoint validation failed! Investigate before continuing.\")\n\n# Use in training loop\nif (step + 1) % save_every == 0:\n    save_checkpoint(checkpoint_manager, model, step + 1)\n\n    validation_sample = next(val_dataloader)\n    is_valid = validate_checkpoint(\n        checkpoint_manager,\n        model,\n        step=step + 1,\n        validation_data=validation_sample[\"data\"],\n    )\n\n    if not is_valid:\n        print(\"Warning: Checkpoint validation failed!\")\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#recovery-and-resumption","title":"Recovery and Resumption","text":"<p>Recover from failures and resume training.</p>"},{"location":"user-guide/advanced/checkpointing/#training-resumption","title":"Training Resumption","text":"<pre><code>from artifex.generative_models.core.checkpointing import (\n    setup_checkpoint_manager,\n    load_checkpoint,\n    save_checkpoint,\n)\nfrom flax import nnx\nimport optax\n\ndef setup_training_from_checkpoint(\n    checkpoint_dir: str,\n    config: dict,\n) -&gt; tuple:\n    \"\"\"Setup training, resuming from checkpoint if available.\"\"\"\n    # Setup checkpoint manager\n    checkpoint_manager, _ = setup_checkpoint_manager(checkpoint_dir)\n\n    # Create model and optimizer templates\n    model = create_vae_model(config, rngs=nnx.Rngs(0))\n    optimizer = nnx.Optimizer(model, optax.adam(config.learning_rate), wrt=nnx.Param)\n\n    # Try to load checkpoint\n    latest_step = checkpoint_manager.latest_step()\n\n    if latest_step is not None:\n        print(f\"Found checkpoint at step {latest_step}, resuming...\")\n\n        # Load model and optimizer\n        model_state = nnx.state(model)\n        optimizer_state = nnx.state(optimizer)\n\n        restore_args = ocp.args.Composite(\n            model=ocp.args.StandardRestore(model_state),\n            optimizer=ocp.args.StandardRestore(optimizer_state),\n        )\n\n        restored_data = checkpoint_manager.restore(\n            latest_step,\n            args=restore_args\n        )\n\n        nnx.update(model, restored_data[\"model\"])\n        nnx.update(optimizer, restored_data[\"optimizer\"])\n\n        start_step = latest_step + 1\n        print(f\"Resumed from step {latest_step}\")\n    else:\n        print(\"No checkpoint found, starting from scratch\")\n        start_step = 0\n\n    return model, optimizer, start_step, checkpoint_manager\n\n\n# Use in training script\nmodel, optimizer, start_step, checkpoint_manager = setup_training_from_checkpoint(\n    checkpoint_dir=\"./checkpoints/experiment_1\",\n    config=config,\n)\n\n# Continue training from start_step\nfor step in range(start_step, num_steps):\n    # Training step\n    loss, grads = nnx.value_and_grad(loss_fn)(model, batch)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n\n    # Save checkpoint\n    if (step + 1) % save_every == 0:\n        # Save both model and optimizer\n        model_state = nnx.state(model)\n        optimizer_state = nnx.state(optimizer)\n\n        save_args = ocp.args.Composite(\n            model=ocp.args.StandardSave(model_state),\n            optimizer=ocp.args.StandardSave(optimizer_state),\n        )\n\n        checkpoint_manager.save(step + 1, args=save_args)\n        checkpoint_manager.wait_until_finished()\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#checkpoint-corruption-recovery","title":"Checkpoint Corruption Recovery","text":"<p>Artifex provides a built-in function to recover from corrupted checkpoints. It tries loading checkpoints from newest to oldest until one succeeds:</p> <pre><code>from artifex.generative_models.core.checkpointing import recover_from_corruption\n\n# Create a model template with the same structure as the saved model\nmodel_template = create_vae_model(config, rngs=nnx.Rngs(0))\n\n# Attempt to recover from any available checkpoint\nmodel, step = recover_from_corruption(\n    checkpoint_dir=\"./checkpoints/experiment_1\",\n    model_template=model_template,\n)\n\nif model is not None:\n    print(f\"Recovered from step {step}, continuing training...\")\n    # Continue training from recovered state\n    for current_step in range(step + 1, num_steps):\n        # Training step...\n        pass\nelse:\n    print(\"Recovery failed, starting from scratch...\")\n    model = model_template\n    step = 0\n</code></pre>"},{"location":"user-guide/advanced/checkpointing/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/advanced/checkpointing/#model-checkpointing_1","title":"Model Checkpointing","text":""},{"location":"user-guide/advanced/checkpointing/#do","title":"DO","text":"<ul> <li>\u2705 Save checkpoints regularly - every N steps or epochs</li> <li>\u2705 Save optimizer state - needed for proper resumption</li> <li>\u2705 Use async checkpointing - don't block training</li> <li>\u2705 Validate checkpoints - ensure they load correctly</li> <li>\u2705 Keep multiple checkpoints - protect against corruption</li> <li>\u2705 Save before evaluation - preserve best models</li> <li>\u2705 Use absolute paths - avoid relative path issues</li> <li>\u2705 Document checkpoint structure - for reproducibility</li> <li>\u2705 Version checkpoint format - handle format changes</li> <li>\u2705 Monitor disk space - checkpoints can be large</li> </ul>"},{"location":"user-guide/advanced/checkpointing/#dont","title":"DON'T","text":"<ul> <li>\u274c Don't save too frequently - I/O overhead slows training</li> <li>\u274c Don't keep all checkpoints - wastes disk space</li> <li>\u274c Don't skip validation - corrupted checkpoints fail silently</li> <li>\u274c Don't modify checkpoint format - breaks compatibility</li> <li>\u274c Don't checkpoint on all ranks - only rank 0 in distributed</li> <li>\u274c Don't ignore save errors - check for failures</li> <li>\u274c Don't use checkpoint path in model - keep them separate</li> <li>\u274c Don't hardcode checkpoint paths - use configuration</li> <li>\u274c Don't forget to wait_until_finished - async saves need this</li> <li>\u274c Don't checkpoint during validation - separate concerns</li> </ul>"},{"location":"user-guide/advanced/checkpointing/#gradient-checkpointing_1","title":"Gradient Checkpointing","text":""},{"location":"user-guide/advanced/checkpointing/#do_1","title":"DO","text":"<ul> <li>\u2705 Profile before checkpointing - measure actual memory usage</li> <li>\u2705 Checkpoint expensive operations - attention, large matmuls</li> <li>\u2705 Use selective checkpointing - balance memory vs. compute</li> <li>\u2705 Checkpoint every N layers - for very deep models</li> <li>\u2705 Test memory savings - verify reduction</li> <li>\u2705 Monitor training speed - checkpointing adds overhead</li> <li>\u2705 Use with large batches - maximize throughput</li> <li>\u2705 Combine with model parallelism - for extreme scale</li> <li>\u2705 Document checkpoint strategy - for reproducibility</li> <li>\u2705 Benchmark different policies - find optimal trade-off</li> </ul>"},{"location":"user-guide/advanced/checkpointing/#dont_1","title":"DON'T","text":"<ul> <li>\u274c Don't checkpoint everything - excessive recomputation</li> <li>\u274c Don't checkpoint cheap operations - not worth overhead</li> <li>\u274c Don't assume memory savings - measure actual usage</li> <li>\u274c Don't ignore speed penalty - can be 30%+ slower</li> <li>\u274c Don't checkpoint randomly - use principled strategies</li> <li>\u274c Don't checkpoint I/O operations - data loading, logging</li> <li>\u274c Don't over-engineer policies - start simple</li> <li>\u274c Don't forget to profile - optimization without data is guessing</li> <li>\u274c Don't checkpoint non-deterministic ops - causes issues</li> <li>\u274c Don't mix checkpointing styles - keep consistent</li> </ul>"},{"location":"user-guide/advanced/checkpointing/#summary","title":"Summary","text":"<p>Checkpointing in Artifex provides:</p> <ol> <li>Model Checkpointing: Save/restore model state with Orbax</li> <li>Automatic state management</li> <li>Async saves for efficiency</li> <li>Validation and recovery</li> <li> <p>Flexible retention policies</p> </li> <li> <p>Gradient Checkpointing: Trade compute for memory</p> </li> <li>Recompute activations in backward pass</li> <li>Selective checkpointing strategies</li> <li>Policy-based automation</li> <li> <p>30-50% memory reduction</p> </li> <li> <p>Best Practices:</p> </li> <li>Regular model checkpoints (every N steps)</li> <li>Selective gradient checkpoints (expensive ops)</li> <li>Validation and recovery procedures</li> <li>Balance memory, speed, and reliability</li> </ol>"},{"location":"user-guide/advanced/checkpointing/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Custom Architectures</p> <p>Build custom model architectures with checkpointing</p> <p> Architecture guide</p> </li> <li> <p> Distributed Training</p> <p>Combine checkpointing with distributed training</p> <p> Distributed guide</p> </li> <li> <p> Model Parallelism</p> <p>Use checkpointing with model parallelism</p> <p> Parallelism guide</p> </li> <li> <p> Training Guide</p> <p>Return to the comprehensive training documentation</p> <p> Training guide</p> </li> </ul>"},{"location":"user-guide/advanced/distributed/","title":"Distributed Training","text":"<p>Distributed training enables training large models across multiple GPUs or nodes by parallelizing computation and distributing data. Artifex provides configuration and utilities for distributed training using JAX's native parallelization capabilities.</p> <ul> <li> <p> Data Parallelism</p> <p>Distribute data batches across devices while replicating the model</p> <p> Learn more</p> </li> <li> <p> Model Parallelism</p> <p>Split large models across devices when they don't fit in memory</p> <p> Learn more</p> </li> <li> <p>:material-pipeline:{ .lg .middle } Pipeline Parallelism</p> <p>Split model layers across devices and pipeline batches</p> <p> Learn more</p> </li> <li> <p> Device Meshes</p> <p>Organize devices with multi-dimensional parallelism strategies</p> <p> Learn more</p> </li> </ul>"},{"location":"user-guide/advanced/distributed/#overview","title":"Overview","text":"<p>Artifex uses JAX's <code>jax.sharding</code> API and device meshes for distributed training, providing:</p> <ul> <li>Automatic distribution: JAX handles device communication</li> <li>SPMD (Single Program Multiple Data): Same code runs on all devices</li> <li>Flexible strategies: Mix data, model, and pipeline parallelism</li> <li>XLA optimization: Automatic fusion and communication overlap</li> </ul>"},{"location":"user-guide/advanced/distributed/#artifex-distributed-training-module","title":"Artifex Distributed Training Module","text":"<p>Artifex provides a high-level distributed training module that simplifies common distributed training patterns:</p> <pre><code>from artifex.generative_models.training.distributed import (\n    DeviceMeshManager,  # Device mesh creation and management\n    DataParallel,       # Data parallel training utilities\n    DevicePlacement,    # Explicit device placement\n    DistributedMetrics, # Metrics aggregation across devices\n    HardwareType,       # Hardware type enumeration\n    BatchSizeRecommendation,  # Hardware-specific batch recommendations\n)\n</code></pre>"},{"location":"user-guide/advanced/distributed/#quick-start-with-artifex-utilities","title":"Quick Start with Artifex Utilities","text":"<pre><code>from artifex.generative_models.training.distributed import (\n    DeviceMeshManager,\n    DataParallel,\n    DistributedMetrics,\n)\nimport jax.numpy as jnp\n\n# Create device mesh manager\nmanager = DeviceMeshManager()\nprint(f\"Available devices: {manager.num_devices}\")\n\n# Create mesh for data parallelism\nmesh = manager.create_data_parallel_mesh()\n\n# Create data parallel utilities\ndp = DataParallel()\ndm = DistributedMetrics()\n\n# Create sharding for batch data\nsharding = dp.create_data_parallel_sharding(mesh)\n\n# Shard batch data\nbatch = {\"inputs\": jnp.ones((32, 784)), \"targets\": jnp.zeros((32,))}\nsharded_batch = dp.shard_batch(batch, sharding)\n\n# In training step, aggregate gradients and metrics\n# grads = dp.all_reduce_gradients(grads, reduce_type=\"mean\")\n# metrics = dm.reduce_mean(metrics)\n</code></pre> <p>For detailed documentation of each module, see:</p> <ul> <li>Device Mesh Management - <code>DeviceMeshManager</code> for creating device meshes</li> <li>Data Parallel Training - <code>DataParallel</code> for data parallelism</li> <li>Device Placement - <code>DevicePlacement</code> for explicit device placement</li> <li>Distributed Metrics - <code>DistributedMetrics</code> for aggregating metrics</li> </ul>"},{"location":"user-guide/advanced/distributed/#why-distributed-training","title":"Why Distributed Training?","text":"<p>Use distributed training when:</p> <ol> <li>Large Batches: Need bigger batch sizes than fit on one GPU</li> <li>Large Models: Model parameters exceed single device memory</li> <li>Faster Training: Reduce wall-clock time with more compute</li> <li>Multi-Node: Scale to cluster-level training</li> </ol>"},{"location":"user-guide/advanced/distributed/#distributed-configuration","title":"Distributed Configuration","text":"<p>Artifex provides a comprehensive configuration system for distributed training:</p> <pre><code>from artifex.configs.schema.distributed import DistributedConfig\n\n# Basic distributed configuration\nconfig = DistributedConfig(\n    enabled=True,\n    world_size=4,  # Total number of devices\n    backend=\"nccl\",  # Use NCCL for NVIDIA GPUs\n\n    # Device mesh configuration\n    mesh_shape=(2, 2),  # 2x2 device grid\n    mesh_axis_names=(\"data\", \"model\"),  # Axis semantics\n\n    # Parallelism settings\n    tensor_parallel_size=2,  # Tensor parallelism degree\n    pipeline_parallel_size=1,  # Pipeline parallelism degree\n)\n</code></pre>"},{"location":"user-guide/advanced/distributed/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Description <code>enabled</code> <code>bool</code> Enable distributed training <code>world_size</code> <code>int</code> Total number of processes <code>backend</code> <code>str</code> Backend: <code>nccl</code>, <code>gloo</code>, <code>mpi</code> <code>rank</code> <code>int</code> Global rank of this process <code>local_rank</code> <code>int</code> Local rank on this node <code>num_nodes</code> <code>int</code> Number of nodes in cluster <code>num_processes_per_node</code> <code>int</code> Processes per node <code>master_addr</code> <code>str</code> Master node address <code>master_port</code> <code>int</code> Communication port <code>tensor_parallel_size</code> <code>int</code> Tensor parallelism group size <code>pipeline_parallel_size</code> <code>int</code> Pipeline parallelism group size <code>mesh_shape</code> <code>tuple</code> Device mesh dimensions <code>mesh_axis_names</code> <code>tuple</code> Semantic names for mesh axes <code>mixed_precision</code> <code>str</code> Mixed precision mode: <code>no</code>, <code>fp16</code>, <code>bf16</code>"},{"location":"user-guide/advanced/distributed/#configuration-validation","title":"Configuration Validation","text":"<p>The configuration includes automatic validation:</p> <pre><code># This configuration is validated automatically\nconfig = DistributedConfig(\n    enabled=True,\n    world_size=8,\n    num_nodes=2,\n    num_processes_per_node=4,\n    tensor_parallel_size=2,\n    pipeline_parallel_size=2,\n    # Automatically validates:\n    # - world_size == num_nodes * num_processes_per_node\n    # - tensor_parallel * pipeline_parallel &lt;= world_size\n    # - rank &lt; world_size\n)\n\n# Get derived values\ndata_parallel_size = config.get_data_parallel_size()  # 2\nis_main = config.is_main_process()  # True if rank == 0\nis_local_main = config.is_local_main_process()  # True if local_rank == 0\n</code></pre>"},{"location":"user-guide/advanced/distributed/#data-parallelism","title":"Data Parallelism","text":"<p>Data parallelism replicates the model on each device and processes different data batches in parallel.</p>"},{"location":"user-guide/advanced/distributed/#basic-data-parallelism","title":"Basic Data Parallelism","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# Get available devices\ndevices = jax.devices()\nprint(f\"Available devices: {len(devices)}\")  # e.g., 4 GPUs\n\n# Create model\nmodel = create_vae_model(config, rngs=nnx.Rngs(0))\n\n# Replicate model across devices\nreplicated_model = jax.device_put_replicated(\n    nnx.state(model),\n    devices\n)\n\n# Training step with pmap\n@jax.pmap\ndef train_step(model_state, batch):\n    \"\"\"Training step replicated across devices.\"\"\"\n    # Reconstruct model from state\n    model = nnx.merge(model_graphdef, model_state)\n\n    # Forward pass\n    def loss_fn(model):\n        output = model(batch[\"data\"])\n        return output[\"loss\"]\n\n    # Compute gradients\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n\n    # Update parameters (NNX 0.11.0+ API)\n    optimizer.update(model, grads)\n\n    return nnx.state(model), loss\n\n# Store model structure\nmodel_graphdef, _ = nnx.split(model)\n\n# Prepare batched data (one batch per device)\nbatch_per_device = {\n    \"data\": jnp.array([...]),  # Shape: (num_devices, batch_size, ...)\n}\n\n# Run parallel training step\nupdated_state, losses = train_step(replicated_model, batch_per_device)\n\n# Average losses across devices\nmean_loss = jnp.mean(losses)\n</code></pre>"},{"location":"user-guide/advanced/distributed/#data-parallelism-with-device-mesh","title":"Data Parallelism with Device Mesh","text":"<p>Modern approach using <code>jax.sharding</code>:</p> <pre><code>import jax\nfrom jax.sharding import Mesh, PartitionSpec as P, NamedSharding\nfrom flax import nnx\n\n# Create device mesh for data parallelism\ndevices = jax.devices()\nmesh = Mesh(devices, axis_names=(\"data\",))\n\n# Define sharding for data (shard along batch dimension)\ndata_sharding = NamedSharding(mesh, P(\"data\", None, None, None))\n\n# Define sharding for model (replicate across all devices)\nmodel_sharding = NamedSharding(mesh, P())\n\n# Create model\nmodel = create_vae_model(config, rngs=nnx.Rngs(0))\nmodel_state = nnx.state(model)\n\n# Shard model state (replicate)\nsharded_model_state = jax.device_put(model_state, model_sharding)\n\n# JIT-compiled training step with sharding\n@jax.jit\ndef train_step(model_state, batch):\n    \"\"\"Training step with automatic distribution.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n\n    def loss_fn(model):\n        output = model(batch[\"data\"])\n        return output[\"loss\"]\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n\n    return nnx.state(model), loss\n\n# Store model structure\nmodel_graphdef, _ = nnx.split(model)\n\n# Training loop\nfor batch in dataloader:\n    # Shard batch data across devices\n    sharded_batch = jax.device_put(batch, data_sharding)\n\n    # Training step (automatically distributed)\n    sharded_model_state, loss = train_step(\n        sharded_model_state,\n        sharded_batch\n    )\n\n    print(f\"Loss: {loss}\")\n</code></pre>"},{"location":"user-guide/advanced/distributed/#gradient-aggregation","title":"Gradient Aggregation","text":"<p>When using data parallelism, gradients are automatically aggregated:</p> <pre><code>@jax.jit\ndef train_step_with_aggregation(model_state, batch):\n    \"\"\"Training step with explicit gradient aggregation.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n\n    def loss_fn(model):\n        output = model(batch[\"data\"])\n        return output[\"loss\"]\n\n    # Compute gradients on this device's data\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n\n    # Average gradients across devices (handled automatically by JAX)\n    # When using jax.pmap, use jax.lax.pmean:\n    # grads = jax.lax.pmean(grads, axis_name=\"batch\")\n    # loss = jax.lax.pmean(loss, axis_name=\"batch\")\n\n    # Update parameters (NNX 0.11.0+ API)\n    optimizer.update(model, grads)\n\n    return nnx.state(model), loss\n</code></pre>"},{"location":"user-guide/advanced/distributed/#model-parallelism","title":"Model Parallelism","text":"<p>Model parallelism (tensor parallelism) splits model layers across devices, useful when models don't fit in single-device memory.</p>"},{"location":"user-guide/advanced/distributed/#tensor-parallelism-basics","title":"Tensor Parallelism Basics","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec as P, NamedSharding\nfrom flax import nnx\n\n# Create 2D device mesh: (data_parallel, model_parallel)\ndevices = jax.devices()\nmesh = Mesh(\n    devices.reshape(2, 2),  # 2 data parallel, 2 model parallel\n    axis_names=(\"data\", \"model\")\n)\n\n# Define sharding for model parameters\n# Shard weights along model axis, replicate bias\nweight_sharding = NamedSharding(mesh, P(None, \"model\"))  # (in_features, out_features)\nbias_sharding = NamedSharding(mesh, P(\"model\"))  # (out_features,)\n\n# Create model with sharded parameters\nclass ShardedLinear(nnx.Module):\n    \"\"\"Linear layer with sharded weights.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        *,\n        rngs: nnx.Rngs,\n        mesh: Mesh,\n    ):\n        super().__init__()\n\n        # Create weight with sharding\n        self.weight = nnx.Param(\n            nnx.initializers.lecun_normal()(\n                rngs.params(),\n                (in_features, out_features)\n            )\n        )\n\n        # Apply sharding\n        self.weight = jax.device_put(\n            self.weight,\n            NamedSharding(mesh, P(None, \"model\"))\n        )\n\n        # Create bias with sharding\n        self.bias = nnx.Param(\n            jnp.zeros(out_features)\n        )\n        self.bias = jax.device_put(\n            self.bias,\n            NamedSharding(mesh, P(\"model\"))\n        )\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        # Computation automatically distributed\n        return x @ self.weight + self.bias\n</code></pre>"},{"location":"user-guide/advanced/distributed/#multi-layer-model-parallelism","title":"Multi-Layer Model Parallelism","text":"<pre><code>class ShardedVAEEncoder(nnx.Module):\n    \"\"\"VAE encoder with model parallelism.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        latent_dim: int,\n        *,\n        rngs: nnx.Rngs,\n        mesh: Mesh,\n    ):\n        super().__init__()\n\n        # First layer: replicated input, sharded output\n        self.layer1 = ShardedLinear(\n            input_dim, hidden_dim,\n            rngs=rngs, mesh=mesh\n        )\n\n        # Second layer: sharded input, sharded output\n        self.layer2 = ShardedLinear(\n            hidden_dim, hidden_dim,\n            rngs=rngs, mesh=mesh\n        )\n\n        # Output layers for mean and logvar\n        self.mean_layer = ShardedLinear(\n            hidden_dim, latent_dim,\n            rngs=rngs, mesh=mesh\n        )\n        self.logvar_layer = ShardedLinear(\n            hidden_dim, latent_dim,\n            rngs=rngs, mesh=mesh\n        )\n\n    def __call__(self, x: jax.Array) -&gt; dict[str, jax.Array]:\n        # Forward pass with automatic communication\n        h = nnx.relu(self.layer1(x))\n        h = nnx.relu(self.layer2(h))\n\n        mean = self.mean_layer(h)\n        logvar = self.logvar_layer(h)\n\n        return {\"mean\": mean, \"logvar\": logvar}\n\n# Create model with mesh\ndevices = jax.devices()\nmesh = Mesh(devices.reshape(2, 2), axis_names=(\"data\", \"model\"))\n\n# Initialize model\nencoder = ShardedVAEEncoder(\n    input_dim=784,\n    hidden_dim=512,\n    latent_dim=20,\n    rngs=nnx.Rngs(0),\n    mesh=mesh,\n)\n\n# Model parameters are automatically sharded\n</code></pre>"},{"location":"user-guide/advanced/distributed/#activation-sharding","title":"Activation Sharding","text":"<p>Control how activations are sharded between layers:</p> <pre><code>from jax.experimental import shard_map\n\n@jax.jit\ndef sharded_forward(model_state, x):\n    \"\"\"Forward pass with explicit activation sharding.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n\n    # x shape: (batch, features)\n    # Shard along batch dimension\n    x_sharding = NamedSharding(mesh, P(\"data\", None))\n    x = jax.device_put(x, x_sharding)\n\n    # Forward pass\n    h1 = model.layer1(x)  # Output sharded along (data, model)\n    h1 = nnx.relu(h1)\n\n    # Collect along model axis before next layer\n    h1 = jax.lax.all_gather(h1, \"model\", axis=1, tiled=True)\n\n    h2 = model.layer2(h1)\n\n    return h2\n</code></pre>"},{"location":"user-guide/advanced/distributed/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Pipeline parallelism splits model layers across devices and pipelines microbatches through stages.</p>"},{"location":"user-guide/advanced/distributed/#pipeline-stage-definition","title":"Pipeline Stage Definition","text":"<pre><code>from flax import nnx\nimport jax\nimport jax.numpy as jnp\n\nclass PipelineStage(nnx.Module):\n    \"\"\"A single stage in a pipeline parallel model.\"\"\"\n\n    def __init__(\n        self,\n        layer_specs: list,\n        stage_id: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n        self.stage_id = stage_id\n        self.layers = []\n\n        # Create layers for this stage\n        for spec in layer_specs:\n            layer = nnx.Linear(\n                in_features=spec[\"in_features\"],\n                out_features=spec[\"out_features\"],\n                rngs=rngs,\n            )\n            self.layers.append(layer)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass through this stage.\"\"\"\n        for layer in self.layers:\n            x = nnx.relu(layer(x))\n        return x\n\n# Define 4-stage pipeline\nstage_specs = [\n    # Stage 0: Input layers\n    [{\"in_features\": 784, \"out_features\": 512}],\n    # Stage 1: Middle layers\n    [{\"in_features\": 512, \"out_features\": 512}],\n    # Stage 2: Middle layers\n    [{\"in_features\": 512, \"out_features\": 256}],\n    # Stage 3: Output layers\n    [{\"in_features\": 256, \"out_features\": 10}],\n]\n\n# Create stages\nstages = [\n    PipelineStage(spec, stage_id=i, rngs=nnx.Rngs(i))\n    for i, spec in enumerate(stage_specs)\n]\n</code></pre>"},{"location":"user-guide/advanced/distributed/#microbatch-pipeline-execution","title":"Microbatch Pipeline Execution","text":"<pre><code>def pipeline_forward(stages, inputs, num_microbatches):\n    \"\"\"Execute forward pass with pipeline parallelism.\"\"\"\n    # Split batch into microbatches\n    microbatch_size = inputs.shape[0] // num_microbatches\n    microbatches = [\n        inputs[i * microbatch_size:(i + 1) * microbatch_size]\n        for i in range(num_microbatches)\n    ]\n\n    num_stages = len(stages)\n\n    # Pipeline state: activations at each stage\n    stage_activations = [None] * num_stages\n    outputs = []\n\n    # Pipeline schedule: (time_step, stage_id, microbatch_id)\n    for time_step in range(num_stages + num_microbatches - 1):\n        for stage_id in range(num_stages):\n            microbatch_id = time_step - stage_id\n\n            # Check if this stage should process a microbatch\n            if 0 &lt;= microbatch_id &lt; num_microbatches:\n                if stage_id == 0:\n                    # First stage: use input\n                    stage_input = microbatches[microbatch_id]\n                else:\n                    # Other stages: use previous stage output\n                    stage_input = stage_activations[stage_id - 1]\n\n                # Process through this stage\n                stage_output = stages[stage_id](stage_input)\n                stage_activations[stage_id] = stage_output\n\n                # If last stage, collect output\n                if stage_id == num_stages - 1:\n                    outputs.append(stage_output)\n\n    # Concatenate outputs\n    return jnp.concatenate(outputs, axis=0)\n\n# Use pipeline\ninputs = jnp.ones((32, 784))  # Batch of 32\noutput = pipeline_forward(stages, inputs, num_microbatches=4)\n</code></pre>"},{"location":"user-guide/advanced/distributed/#gpipe-style-pipeline","title":"GPipe-Style Pipeline","text":"<pre><code>class GPipePipeline(nnx.Module):\n    \"\"\"GPipe-style pipeline with gradient accumulation.\"\"\"\n\n    def __init__(\n        self,\n        num_stages: int,\n        layers_per_stage: int,\n        hidden_dim: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n        self.num_stages = num_stages\n        self.stages = []\n\n        for i in range(num_stages):\n            stage_layers = []\n            for j in range(layers_per_stage):\n                stage_layers.append(\n                    nnx.Linear(\n                        in_features=hidden_dim,\n                        out_features=hidden_dim,\n                        rngs=rngs,\n                    )\n                )\n            self.stages.append(stage_layers)\n\n    def forward_stage(self, stage_id: int, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass through one stage.\"\"\"\n        for layer in self.stages[stage_id]:\n            x = nnx.relu(layer(x))\n        return x\n\n    def __call__(\n        self,\n        x: jax.Array,\n        num_microbatches: int = 1\n    ) -&gt; jax.Array:\n        \"\"\"Forward pass with microbatching.\"\"\"\n        if num_microbatches == 1:\n            # No microbatching\n            for stage_id in range(self.num_stages):\n                x = self.forward_stage(stage_id, x)\n            return x\n\n        # Microbatched pipeline\n        return pipeline_forward(\n            [lambda x, i=i: self.forward_stage(i, x)\n             for i in range(self.num_stages)],\n            x,\n            num_microbatches\n        )\n</code></pre>"},{"location":"user-guide/advanced/distributed/#device-meshes","title":"Device Meshes","text":"<p>Device meshes organize devices with multi-dimensional parallelism.</p>"},{"location":"user-guide/advanced/distributed/#creating-device-meshes","title":"Creating Device Meshes","text":"<pre><code>import jax\nfrom jax.sharding import Mesh\n\n# Get available devices\ndevices = jax.devices()\nprint(f\"Total devices: {len(devices)}\")  # e.g., 8 GPUs\n\n# 1D mesh (data parallelism only)\nmesh_1d = Mesh(devices, axis_names=(\"data\",))\n\n# 2D mesh (data + model parallelism)\nmesh_2d = Mesh(\n    devices.reshape(4, 2),  # 4 data parallel, 2 model parallel\n    axis_names=(\"data\", \"model\")\n)\n\n# 3D mesh (data + model + pipeline parallelism)\nmesh_3d = Mesh(\n    devices.reshape(2, 2, 2),  # 2x2x2 grid\n    axis_names=(\"data\", \"model\", \"pipeline\")\n)\n\n# Check mesh properties\nprint(f\"Mesh shape: {mesh_2d.shape}\")  # (4, 2)\nprint(f\"Mesh axis names: {mesh_2d.axis_names}\")  # ('data', 'model')\n</code></pre>"},{"location":"user-guide/advanced/distributed/#using-mesh-context","title":"Using Mesh Context","text":"<pre><code>from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n\n# Create mesh\ndevices = jax.devices()\nmesh = Mesh(devices.reshape(2, 4), axis_names=(\"data\", \"model\"))\n\n# Use mesh context for automatic sharding\nwith mesh:\n    # Create model\n    model = create_vae_model(config, rngs=nnx.Rngs(0))\n\n    # Define sharding strategies\n    data_sharding = NamedSharding(mesh, P(\"data\", None))\n    param_sharding = NamedSharding(mesh, P(None, \"model\"))\n\n    # Shard model parameters\n    model_state = nnx.state(model)\n    sharded_state = jax.tree.map(\n        lambda x: jax.device_put(x, param_sharding),\n        model_state\n    )\n\n    # Training loop\n    for batch in dataloader:\n        # Shard batch\n        sharded_batch = jax.device_put(batch, data_sharding)\n\n        # Training step (automatically uses mesh)\n        sharded_state, loss = train_step(sharded_state, sharded_batch)\n</code></pre>"},{"location":"user-guide/advanced/distributed/#mesh-inspection","title":"Mesh Inspection","text":"<pre><code># Inspect sharding of arrays\ndef inspect_sharding(array, name=\"array\"):\n    \"\"\"Print sharding information for an array.\"\"\"\n    sharding = array.sharding\n    print(f\"{name}:\")\n    print(f\"  Shape: {array.shape}\")\n    print(f\"  Sharding: {sharding}\")\n    print(f\"  Devices: {len(sharding.device_set)} devices\")\n\n# Check model parameter sharding\nfor name, param in nnx.state(model).items():\n    inspect_sharding(param, name)\n\n# Visualize mesh\ndef visualize_mesh(mesh):\n    \"\"\"Visualize device mesh layout.\"\"\"\n    print(f\"Mesh shape: {mesh.shape}\")\n    print(f\"Axis names: {mesh.axis_names}\")\n    print(\"\\nDevice layout:\")\n\n    devices_array = mesh.devices\n    for i in range(devices_array.shape[0]):\n        for j in range(devices_array.shape[1]):\n            device = devices_array[i, j]\n            print(f\"  [{i},{j}]: {device}\")\n\nvisualize_mesh(mesh)\n</code></pre>"},{"location":"user-guide/advanced/distributed/#multi-node-training","title":"Multi-Node Training","text":"<p>Scaling training to multiple nodes requires coordination across machines.</p>"},{"location":"user-guide/advanced/distributed/#multi-node-setup","title":"Multi-Node Setup","text":"<pre><code># Node 0 (master)\nexport MASTER_ADDR=192.168.1.100\nexport MASTER_PORT=29500\nexport WORLD_SIZE=8  # 2 nodes x 4 GPUs\nexport RANK=0\nexport LOCAL_RANK=0\n\npython train.py --distributed\n\n# Node 1 (worker)\nexport MASTER_ADDR=192.168.1.100\nexport MASTER_PORT=29500\nexport WORLD_SIZE=8\nexport RANK=4  # Ranks 4-7 on node 1\nexport LOCAL_RANK=0\n\npython train.py --distributed\n</code></pre>"},{"location":"user-guide/advanced/distributed/#jax-multi-node-initialization","title":"JAX Multi-Node Initialization","text":"<pre><code>import jax\nimport os\n\ndef setup_multinode():\n    \"\"\"Initialize JAX for multi-node training.\"\"\"\n    # Get environment variables\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    rank = int(os.environ.get(\"RANK\", 0))\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    master_addr = os.environ.get(\"MASTER_ADDR\", \"localhost\")\n    master_port = int(os.environ.get(\"MASTER_PORT\", 29500))\n\n    # JAX automatically handles multi-host setup\n    # Just need to ensure CUDA_VISIBLE_DEVICES is set correctly\n    # and that the same code runs on all hosts\n\n    print(f\"Rank {rank}/{world_size} on {jax.device_count()} local devices\")\n    print(f\"Total devices: {jax.device_count()} local, {jax.device_count() * world_size} global\")\n\n    return {\n        \"world_size\": world_size,\n        \"rank\": rank,\n        \"local_rank\": local_rank,\n        \"is_master\": rank == 0,\n    }\n\n# Setup distributed training\ndist_info = setup_multinode()\n\n# Create mesh across all nodes\ndevices = jax.devices()  # All devices across all hosts\nmesh = Mesh(devices, axis_names=(\"data\",))\n\n# Training code identical to single-node\nwith mesh:\n    # Your training code here\n    pass\n</code></pre>"},{"location":"user-guide/advanced/distributed/#distributed-training-script","title":"Distributed Training Script","text":"<pre><code>from artifex.configs.schema.distributed import DistributedConfig\nfrom artifex.generative_models.training.trainer import Trainer\nimport jax\n\ndef main():\n    # Create distributed config\n    dist_config = DistributedConfig(\n        enabled=True,\n        world_size=8,\n        num_nodes=2,\n        num_processes_per_node=4,\n        master_addr=os.environ.get(\"MASTER_ADDR\", \"localhost\"),\n        master_port=int(os.environ.get(\"MASTER_PORT\", 29500)),\n        rank=int(os.environ.get(\"RANK\", 0)),\n        local_rank=int(os.environ.get(\"LOCAL_RANK\", 0)),\n        mesh_shape=(8,),  # Data parallel only\n        mesh_axis_names=(\"data\",),\n    )\n\n    # Create device mesh\n    devices = jax.devices()\n    mesh = Mesh(\n        devices.reshape(dist_config.mesh_shape),\n        axis_names=dist_config.mesh_axis_names\n    )\n\n    # Create model and training config\n    model_config = create_model_config()\n    training_config = create_training_config()\n\n    # Create trainer\n    trainer = Trainer(\n        model_config=model_config,\n        training_config=training_config,\n        distributed_config=dist_config,\n    )\n\n    # Train with automatic distribution\n    with mesh:\n        trainer.train(train_dataset, val_dataset)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user-guide/advanced/distributed/#performance-optimization","title":"Performance Optimization","text":"<p>Optimize distributed training for maximum efficiency.</p>"},{"location":"user-guide/advanced/distributed/#communication-overlap","title":"Communication Overlap","text":"<p>Overlap computation with communication:</p> <pre><code>@jax.jit\ndef optimized_train_step(model_state, batch, optimizer_state):\n    \"\"\"Training step with computation-communication overlap.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n\n    # Forward pass\n    def loss_fn(model):\n        output = model(batch[\"data\"])\n        return output[\"loss\"]\n\n    # Compute gradients\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n\n    # JAX automatically overlaps:\n    # 1. Gradient computation (backward pass)\n    # 2. Gradient all-reduce (across devices)\n    # 3. Parameter updates\n\n    # Update optimizer\n    updates, optimizer_state = optimizer.update(grads, optimizer_state)\n    model_state = optax.apply_updates(model_state, updates)\n\n    return model_state, optimizer_state, loss\n</code></pre>"},{"location":"user-guide/advanced/distributed/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Accumulate gradients across microbatches:</p> <pre><code>@jax.jit\ndef train_step_with_accumulation(\n    model_state,\n    batch,\n    optimizer_state,\n    num_microbatches: int = 4\n):\n    \"\"\"Training step with gradient accumulation.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n\n    # Split batch into microbatches\n    microbatch_size = batch[\"data\"].shape[0] // num_microbatches\n\n    # Initialize accumulated gradients\n    accumulated_grads = jax.tree.map(jnp.zeros_like, nnx.state(model))\n    total_loss = 0.0\n\n    # Process microbatches\n    for i in range(num_microbatches):\n        start_idx = i * microbatch_size\n        end_idx = (i + 1) * microbatch_size\n        microbatch = {\n            \"data\": batch[\"data\"][start_idx:end_idx]\n        }\n\n        # Compute gradients for this microbatch\n        def loss_fn(model):\n            output = model(microbatch[\"data\"])\n            return output[\"loss\"]\n\n        loss, grads = nnx.value_and_grad(loss_fn)(model)\n\n        # Accumulate gradients\n        accumulated_grads = jax.tree.map(\n            lambda acc, g: acc + g / num_microbatches,\n            accumulated_grads,\n            grads\n        )\n        total_loss += loss / num_microbatches\n\n    # Single optimizer update with accumulated gradients\n    updates, optimizer_state = optimizer.update(\n        accumulated_grads,\n        optimizer_state\n    )\n    model_state = optax.apply_updates(model_state, updates)\n\n    return model_state, optimizer_state, total_loss\n\n# Use with larger effective batch size\nfor batch in dataloader:  # batch_size = 32\n    # Effective batch size = 32 * 4 = 128\n    model_state, optimizer_state, loss = train_step_with_accumulation(\n        model_state, batch, optimizer_state, num_microbatches=4\n    )\n</code></pre> <p>GradientAccumulator Utility</p> <p>Artifex provides a dedicated <code>GradientAccumulator</code> class for cleaner gradient accumulation with automatic normalization. See Advanced Features for details.</p>"},{"location":"user-guide/advanced/distributed/#memory-efficient-training","title":"Memory-Efficient Training","text":"<p>Reduce memory usage in distributed training:</p> <pre><code># Use mixed precision\nfrom jax import numpy as jnp\n\n@jax.jit\ndef mixed_precision_train_step(model_state, batch):\n    \"\"\"Training step with mixed precision (bfloat16).\"\"\"\n    # Cast inputs to bfloat16\n    batch_bf16 = jax.tree.map(\n        lambda x: x.astype(jnp.bfloat16) if x.dtype == jnp.float32 else x,\n        batch\n    )\n\n    # Forward and backward in bfloat16\n    model = nnx.merge(model_graphdef, model_state)\n\n    def loss_fn(model):\n        output = model(batch_bf16[\"data\"])\n        return output[\"loss\"]\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n\n    # Cast gradients back to float32 for optimizer\n    grads_fp32 = jax.tree.map(\n        lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x,\n        grads\n    )\n\n    # Update in float32\n    optimizer.update(grads_fp32)\n\n    return nnx.state(model), loss.astype(jnp.float32)\n</code></pre> <p>Dynamic Loss Scaling</p> <p>For numerical stability in mixed-precision distributed training, use the <code>DynamicLossScaler</code> class which automatically adjusts loss scaling to prevent overflow/underflow. See Advanced Features for details.</p>"},{"location":"user-guide/advanced/distributed/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions in distributed training.</p>"},{"location":"user-guide/advanced/distributed/#out-of-memory-oom","title":"Out of Memory (OOM)","text":"<p>Problem: Model doesn't fit in GPU memory even with distribution.</p> <p>Solutions:</p> <ol> <li>Increase Model Parallelism:</li> </ol> <pre><code># Use more model parallel devices\nconfig = DistributedConfig(\n    tensor_parallel_size=4,  # Increase from 2\n    mesh_shape=(2, 4),  # 2 data, 4 model parallel\n)\n</code></pre> <ol> <li>Add Gradient Accumulation:</li> </ol> <pre><code># Reduce microbatch size, accumulate gradients\ntrain_step_with_accumulation(\n    model_state, batch, optimizer_state,\n    num_microbatches=8  # Smaller microbatches\n)\n</code></pre> <ol> <li>Use Gradient Checkpointing (see Checkpointing Guide)</li> </ol>"},{"location":"user-guide/advanced/distributed/#slow-training","title":"Slow Training","text":"<p>Problem: Training slower than expected with multiple devices.</p> <p>Solutions:</p> <ol> <li>Check Device Utilization:</li> </ol> <pre><code>import jax.profiler\n\n# Profile training step\njax.profiler.start_trace(\"/tmp/tensorboard\")\ntrain_step(model_state, batch)\njax.profiler.stop_trace()\n\n# View in TensorBoard\n# tensorboard --logdir=/tmp/tensorboard\n</code></pre> <ol> <li>Optimize Batch Size:</li> </ol> <pre><code># Increase batch size per device\n# Optimal: batch_size * num_devices fills GPU memory ~80%\noptimal_batch_size = 64  # Per device\ntotal_batch_size = optimal_batch_size * num_devices\n</code></pre> <ol> <li>Reduce Communication Overhead:</li> </ol> <pre><code># Use larger microbatches in pipeline parallelism\npipeline_forward(stages, inputs, num_microbatches=2)  # Instead of 8\n\n# Increase data parallelism, reduce model parallelism if possible\n</code></pre>"},{"location":"user-guide/advanced/distributed/#hanging-or-deadlocks","title":"Hanging or Deadlocks","text":"<p>Problem: Training hangs or deadlocks during execution.</p> <p>Solutions:</p> <ol> <li>Check Collective Operations:</li> </ol> <pre><code># Ensure all devices participate in collectives\n@jax.jit\ndef train_step(model_state, batch):\n    # Bad: Only some devices execute all-reduce\n    if jax.process_index() == 0:\n        grads = jax.lax.pmean(grads, \"batch\")  # Deadlock!\n\n    # Good: All devices execute all-reduce\n    grads = jax.lax.pmean(grads, \"batch\")  # OK\n\n    return model_state, loss\n</code></pre> <ol> <li>Verify World Size:</li> </ol> <pre><code># Check all processes are launched\nassert jax.device_count() == expected_devices\nassert jax.process_count() == expected_processes\n</code></pre>"},{"location":"user-guide/advanced/distributed/#numerical-instability","title":"Numerical Instability","text":"<p>Problem: Loss becomes NaN or diverges in distributed training.</p> <p>Solutions:</p> <ol> <li>Check Gradient Aggregation:</li> </ol> <pre><code># Ensure gradients are averaged, not summed\ngrads = jax.lax.pmean(grads, \"batch\")  # Mean\n# grads = jax.lax.psum(grads, \"batch\")  # Sum (wrong!)\n</code></pre> <ol> <li>Use Gradient Clipping:</li> </ol> <pre><code>import optax\n\n# Clip gradients before update\noptimizer = optax.chain(\n    optax.clip_by_global_norm(1.0),  # Clip to norm 1.0\n    optax.adam(learning_rate=1e-4),\n)\n</code></pre>"},{"location":"user-guide/advanced/distributed/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/advanced/distributed/#do","title":"DO","text":"<ul> <li>\u2705 Use jax.sharding for modern distributed training</li> <li>\u2705 Profile before optimizing - measure actual bottlenecks</li> <li>\u2705 Start with data parallelism - simplest and most efficient</li> <li>\u2705 Use mixed precision (bfloat16) for memory and speed</li> <li>\u2705 Test on single device first before distributing</li> <li>\u2705 Monitor device utilization with profiling tools</li> <li>\u2705 Use gradient accumulation for large effective batch sizes</li> <li>\u2705 Validate mesh configuration with DistributedConfig</li> <li>\u2705 Keep code identical across devices (SPMD principle)</li> <li>\u2705 Log only on rank 0 to avoid cluttered output</li> </ul>"},{"location":"user-guide/advanced/distributed/#dont","title":"DON'T","text":"<ul> <li>\u274c Don't use different code on different devices - breaks SPMD</li> <li>\u274c Don't skip validation - invalid configs cause cryptic errors</li> <li>\u274c Don't over-shard - communication overhead dominates</li> <li>\u274c Don't ignore profiling - assumptions often wrong</li> <li>\u274c Don't use pmap for new code - use jax.sharding instead</li> <li>\u274c Don't assume linear scaling - measure actual speedup</li> <li>\u274c Don't mix parallelism strategies without profiling</li> <li>\u274c Don't forget gradient averaging in data parallelism</li> <li>\u274c Don't use model parallelism if data parallelism works</li> <li>\u274c Don't checkpoint on all ranks - only rank 0 should save</li> </ul>"},{"location":"user-guide/advanced/distributed/#summary","title":"Summary","text":"<p>Distributed training in Artifex leverages JAX's native capabilities:</p> <ol> <li>Data Parallelism: Replicate model, distribute data batches</li> <li>Model Parallelism: Shard model parameters across devices</li> <li>Pipeline Parallelism: Split model layers, pipeline microbatches</li> <li>Device Meshes: Multi-dimensional parallelism strategies</li> <li>Automatic Distribution: JAX handles communication with jax.sharding</li> </ol> <p>Key APIs:</p> <ul> <li><code>DistributedConfig</code>: Configuration with validation</li> <li><code>jax.sharding.Mesh</code>: Multi-dimensional device organization</li> <li><code>PartitionSpec</code>: Specify sharding strategies</li> <li><code>NamedSharding</code>: Apply sharding to arrays</li> <li><code>@jax.jit</code>: Automatic distribution with XLA</li> </ul>"},{"location":"user-guide/advanced/distributed/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Model Parallelism</p> <p>Deep dive into tensor and pipeline parallelism strategies</p> <p> Read the guide</p> </li> <li> <p> Checkpointing</p> <p>Learn about gradient and model checkpointing for memory efficiency</p> <p> Checkpointing guide</p> </li> <li> <p> Custom Architectures</p> <p>Build custom distributed model architectures</p> <p> Architecture guide</p> </li> <li> <p> Training Guide</p> <p>Return to the comprehensive training documentation</p> <p> Training guide</p> </li> </ul>"},{"location":"user-guide/advanced/parallelism/","title":"Model Parallelism","text":"<p>Model parallelism techniques for training large models that don't fit on a single device. Artifex supports tensor parallelism, pipeline parallelism, and hybrid strategies using JAX's sharding capabilities.</p> <ul> <li> <p> Tensor Parallelism</p> <p>Split weight matrices across devices within a single layer</p> <p> Learn more</p> </li> <li> <p>:material-pipeline:{ .lg .middle } Pipeline Parallelism</p> <p>Distribute model layers across devices in a pipeline</p> <p> Learn more</p> </li> <li> <p> Hybrid Strategies</p> <p>Combine multiple parallelism techniques for maximum scale</p> <p> Learn more</p> </li> <li> <p> Memory Optimization</p> <p>Techniques to maximize model size on limited memory</p> <p> Learn more</p> </li> </ul>"},{"location":"user-guide/advanced/parallelism/#overview","title":"Overview","text":"<p>Model parallelism becomes necessary when:</p> <ul> <li>Model Too Large: Parameters exceed single device memory</li> <li>Activation Memory: Forward/backward activations don't fit</li> <li>Batch Size Constraints: Can't reduce batch size further</li> <li>Extreme Scale: Training models with billions of parameters</li> </ul>"},{"location":"user-guide/advanced/parallelism/#parallelism-strategies-comparison","title":"Parallelism Strategies Comparison","text":"Strategy When to Use Pros Cons Data Parallel Model fits on one device Simple, efficient Limited by model size Tensor Parallel Large layers, fast interconnect Balances compute Communication overhead Pipeline Parallel Many layers, slower interconnect Minimal communication Pipeline bubbles Hybrid Extreme scale Maximum efficiency Complex to implement"},{"location":"user-guide/advanced/parallelism/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>Tensor parallelism splits individual weight matrices across multiple devices.</p>"},{"location":"user-guide/advanced/parallelism/#megatron-style-tensor-parallelism","title":"Megatron-Style Tensor Parallelism","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh, PartitionSpec as P, NamedSharding\nfrom flax import nnx\n\nclass ColumnParallelLinear(nnx.Module):\n    \"\"\"Linear layer with column-parallel weight matrix.\n\n    Splits weight matrix along output dimension:\n        Y = X @ W  where W is split as [W1, W2]\n        Y = [X @ W1, X @ W2]  (concatenate outputs)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        *,\n        rngs: nnx.Rngs,\n        mesh: Mesh,\n        gather_output: bool = True,\n    ):\n        super().__init__()\n        self.gather_output = gather_output\n\n        # Create weight sharded along output dimension\n        self.weight = nnx.Param(\n            nnx.initializers.lecun_normal()(\n                rngs.params(),\n                (in_features, out_features)\n            )\n        )\n\n        # Shard along columns (output dimension)\n        weight_sharding = NamedSharding(mesh, P(None, \"model\"))\n        self.weight.value = jax.device_put(self.weight.value, weight_sharding)\n\n        # Bias sharded same way\n        self.bias = nnx.Param(jnp.zeros(out_features))\n        bias_sharding = NamedSharding(mesh, P(\"model\"))\n        self.bias.value = jax.device_put(self.bias.value, bias_sharding)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with column parallelism.\n\n        Args:\n            x: Input activations (batch, in_features)\n\n        Returns:\n            Output activations (batch, out_features)\n        \"\"\"\n        # Input is replicated across model parallel devices\n        # Output is sharded along out_features dimension\n\n        # Matrix multiplication (automatically parallelized)\n        output = x @ self.weight.value + self.bias.value\n\n        if self.gather_output:\n            # Gather output across model parallel devices\n            output = jax.lax.all_gather(\n                output,\n                axis_name=\"model\",\n                axis=1,  # Gather along feature dimension\n                tiled=True\n            )\n\n        return output\n\n\nclass RowParallelLinear(nnx.Module):\n    \"\"\"Linear layer with row-parallel weight matrix.\n\n    Splits weight matrix along input dimension:\n        Y = X @ W  where W is split as [W1; W2]\n        Y = X1 @ W1 + X2 @ W2  (sum partial results)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        *,\n        rngs: nnx.Rngs,\n        mesh: Mesh,\n        input_is_parallel: bool = True,\n    ):\n        super().__init__()\n        self.input_is_parallel = input_is_parallel\n\n        # Create weight sharded along input dimension\n        self.weight = nnx.Param(\n            nnx.initializers.lecun_normal()(\n                rngs.params(),\n                (in_features, out_features)\n            )\n        )\n\n        # Shard along rows (input dimension)\n        weight_sharding = NamedSharding(mesh, P(\"model\", None))\n        self.weight.value = jax.device_put(self.weight.value, weight_sharding)\n\n        # Bias replicated (only added once after reduce)\n        self.bias = nnx.Param(jnp.zeros(out_features))\n        bias_sharding = NamedSharding(mesh, P())\n        self.bias.value = jax.device_put(self.bias.value, bias_sharding)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with row parallelism.\n\n        Args:\n            x: Input activations (batch, in_features)\n                Either sharded or replicated depending on input_is_parallel\n\n        Returns:\n            Output activations (batch, out_features), replicated\n        \"\"\"\n        if not self.input_is_parallel:\n            # Split input across model parallel devices if not already split\n            x = jax.lax.all_split(x, axis_name=\"model\", split_axis=1)\n\n        # Matrix multiplication (each device has partial result)\n        partial_output = x @ self.weight.value\n\n        # All-reduce to sum partial results\n        output = jax.lax.psum(partial_output, axis_name=\"model\")\n\n        # Add bias (only once after reduction)\n        output = output + self.bias.value\n\n        return output\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#transformer-with-tensor-parallelism","title":"Transformer with Tensor Parallelism","text":"<pre><code>class ParallelTransformerLayer(nnx.Module):\n    \"\"\"Transformer layer with Megatron-style tensor parallelism.\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        ffn_hidden_size: int,\n        *,\n        rngs: nnx.Rngs,\n        mesh: Mesh,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n\n        # Attention: column-parallel for Q, K, V projections\n        self.qkv_proj = ColumnParallelLinear(\n            hidden_size,\n            3 * hidden_size,  # Q, K, V concatenated\n            rngs=rngs,\n            mesh=mesh,\n            gather_output=False,  # Keep sharded for attention\n        )\n\n        # Attention: row-parallel for output projection\n        self.output_proj = RowParallelLinear(\n            hidden_size,\n            hidden_size,\n            rngs=rngs,\n            mesh=mesh,\n            input_is_parallel=True,  # Input from sharded attention\n        )\n\n        # Feed-forward: column-parallel for first layer\n        self.ffn_layer1 = ColumnParallelLinear(\n            hidden_size,\n            ffn_hidden_size,\n            rngs=rngs,\n            mesh=mesh,\n            gather_output=False,  # Keep sharded\n        )\n\n        # Feed-forward: row-parallel for second layer\n        self.ffn_layer2 = RowParallelLinear(\n            ffn_hidden_size,\n            hidden_size,\n            rngs=rngs,\n            mesh=mesh,\n            input_is_parallel=True,\n        )\n\n        # Layer norms (replicated)\n        self.ln1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.ln2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n    def attention(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Multi-head attention with tensor parallelism.\n\n        Args:\n            x: (batch, seq_len, hidden_size)\n\n        Returns:\n            (batch, seq_len, hidden_size)\n        \"\"\"\n        batch_size, seq_len, hidden_size = x.shape\n        head_dim = hidden_size // self.num_heads\n\n        # Q, K, V projection (column-parallel)\n        qkv = self.qkv_proj(x)  # (batch, seq_len, 3 * hidden_size) sharded\n\n        # Split into Q, K, V\n        q, k, v = jnp.split(qkv, 3, axis=-1)\n\n        # Reshape for multi-head attention\n        # Each device has subset of heads\n        q = q.reshape(batch_size, seq_len, -1, head_dim)\n        k = k.reshape(batch_size, seq_len, -1, head_dim)\n        v = v.reshape(batch_size, seq_len, -1, head_dim)\n\n        # Attention computation (parallelized across heads)\n        scores = jnp.einsum(\"bqhd,bkhd-&gt;bhqk\", q, k) / jnp.sqrt(head_dim)\n        attention_weights = nnx.softmax(scores, axis=-1)\n        context = jnp.einsum(\"bhqk,bkhd-&gt;bqhd\", attention_weights, v)\n\n        # Reshape back\n        context = context.reshape(batch_size, seq_len, -1)\n\n        # Output projection (row-parallel)\n        output = self.output_proj(context)  # All-reduce happens here\n\n        return output\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass through transformer layer.\n\n        Args:\n            x: (batch, seq_len, hidden_size)\n\n        Returns:\n            (batch, seq_len, hidden_size)\n        \"\"\"\n        # Attention block with residual\n        residual = x\n        x = self.ln1(x)\n        x = self.attention(x)\n        x = residual + x\n\n        # Feed-forward block with residual\n        residual = x\n        x = self.ln2(x)\n        x = self.ffn_layer1(x)\n        x = nnx.gelu(x)\n        x = self.ffn_layer2(x)\n        x = residual + x\n\n        return x\n\n\n# Create model with tensor parallelism\ndevices = jax.devices()\nmesh = Mesh(devices.reshape(1, 4), axis_names=(\"data\", \"model\"))\n\nwith mesh:\n    # Create transformer layer parallelized across 4 devices\n    layer = ParallelTransformerLayer(\n        hidden_size=768,\n        num_heads=12,\n        ffn_hidden_size=3072,\n        rngs=nnx.Rngs(0),\n        mesh=mesh,\n    )\n\n    # Forward pass (automatic parallelization)\n    x = jnp.ones((2, 128, 768))  # (batch=2, seq_len=128, hidden=768)\n    output = layer(x)\n    print(f\"Output shape: {output.shape}\")  # (2, 128, 768)\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#sequence-parallelism","title":"Sequence Parallelism","text":"<p>For long sequences, also shard along sequence dimension:</p> <pre><code>class SequenceParallelTransformerLayer(nnx.Module):\n    \"\"\"Transformer with both tensor and sequence parallelism.\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        *,\n        rngs: nnx.Rngs,\n        mesh: Mesh,\n    ):\n        super().__init__()\n\n        # Same as before but with sequence parallelism annotations\n        self.qkv_proj = ColumnParallelLinear(\n            hidden_size, 3 * hidden_size,\n            rngs=rngs, mesh=mesh, gather_output=False\n        )\n        self.output_proj = RowParallelLinear(\n            hidden_size, hidden_size,\n            rngs=rngs, mesh=mesh, input_is_parallel=True\n        )\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with sequence parallelism.\n\n        Args:\n            x: (batch, seq_len, hidden_size)\n                Sharded along both seq_len and hidden_size dimensions\n\n        Returns:\n            (batch, seq_len, hidden_size) with same sharding\n        \"\"\"\n        # Layer norm computed independently on each sequence chunk\n        x_norm = nnx.LayerNorm(self.hidden_size)(x)\n\n        # Attention with sequence parallelism\n        # All-gather along sequence dimension for attention computation\n        x_gathered = jax.lax.all_gather(\n            x_norm,\n            axis_name=\"sequence\",\n            axis=1,  # Gather along sequence dimension\n            tiled=True\n        )\n\n        # Compute attention on full sequence\n        attn_output = self.attention(x_gathered)\n\n        # Split back along sequence dimension\n        attn_output = jax.lax.all_split(\n            attn_output,\n            axis_name=\"sequence\",\n            split_axis=1\n        )\n\n        # Residual connection\n        output = x + attn_output\n\n        return output\n\n\n# Create mesh with sequence parallelism\ndevices = jax.devices()  # 8 devices\nmesh = Mesh(\n    devices.reshape(2, 2, 2),\n    axis_names=(\"data\", \"model\", \"sequence\")\n)\n\n# Shard input along both model and sequence dimensions\nsharding = NamedSharding(mesh, P(\"data\", \"sequence\", \"model\"))\nx = jax.device_put(x, sharding)\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Pipeline parallelism splits model layers across devices and pipelines microbatches.</p>"},{"location":"user-guide/advanced/parallelism/#gpipe-style-pipeline","title":"GPipe-Style Pipeline","text":"<pre><code>from typing import Callable\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass PipelineParallelModel(nnx.Module):\n    \"\"\"Model with GPipe-style pipeline parallelism.\"\"\"\n\n    def __init__(\n        self,\n        layer_configs: list[dict],\n        num_microbatches: int = 4,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n        self.num_microbatches = num_microbatches\n\n        # Create stages (groups of layers)\n        self.stages = []\n        for config in layer_configs:\n            stage = self._create_stage(config, rngs)\n            self.stages.append(stage)\n\n    def _create_stage(self, config: dict, rngs: nnx.Rngs) -&gt; nnx.Module:\n        \"\"\"Create a pipeline stage from config.\"\"\"\n        layers = []\n        for layer_spec in config[\"layers\"]:\n            if layer_spec[\"type\"] == \"linear\":\n                layer = nnx.Linear(\n                    in_features=layer_spec[\"in_features\"],\n                    out_features=layer_spec[\"out_features\"],\n                    rngs=rngs,\n                )\n            elif layer_spec[\"type\"] == \"conv\":\n                layer = nnx.Conv(\n                    in_features=layer_spec[\"in_channels\"],\n                    out_features=layer_spec[\"out_channels\"],\n                    kernel_size=layer_spec[\"kernel_size\"],\n                    rngs=rngs,\n                )\n            layers.append(layer)\n\n        # Wrap layers in a sequential module\n        return nnx.Sequential(*layers)\n\n    def forward_stage(self, stage_id: int, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass through one pipeline stage.\"\"\"\n        return self.stages[stage_id](x)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with pipeline parallelism.\"\"\"\n        return self._pipeline_forward(x)\n\n    def _pipeline_forward(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Execute pipeline forward pass with microbatching.\"\"\"\n        batch_size = x.shape[0]\n        microbatch_size = batch_size // self.num_microbatches\n        num_stages = len(self.stages)\n\n        # Split input into microbatches\n        microbatches = [\n            x[i * microbatch_size:(i + 1) * microbatch_size]\n            for i in range(self.num_microbatches)\n        ]\n\n        # Pipeline execution buffer\n        # buffer[stage_id] holds the activation for that stage\n        buffer = [None] * num_stages\n        outputs = []\n\n        # Pipeline schedule: fill, steady-state, drain\n        for time_step in range(num_stages + self.num_microbatches - 1):\n            # Process each stage at this time step\n            for stage_id in range(num_stages - 1, -1, -1):\n                microbatch_id = time_step - stage_id\n\n                if 0 &lt;= microbatch_id &lt; self.num_microbatches:\n                    # Get input for this stage\n                    if stage_id == 0:\n                        stage_input = microbatches[microbatch_id]\n                    else:\n                        stage_input = buffer[stage_id - 1]\n\n                    # Compute this stage\n                    stage_output = self.forward_stage(stage_id, stage_input)\n\n                    # Store in buffer or output\n                    if stage_id == num_stages - 1:\n                        outputs.append(stage_output)\n                    else:\n                        buffer[stage_id] = stage_output\n\n        # Concatenate outputs\n        return jnp.concatenate(outputs, axis=0)\n\n\n# Create pipeline model\nlayer_configs = [\n    # Stage 0: Input layers\n    {\n        \"layers\": [\n            {\"type\": \"linear\", \"in_features\": 784, \"out_features\": 512},\n            {\"type\": \"linear\", \"in_features\": 512, \"out_features\": 512},\n        ]\n    },\n    # Stage 1: Middle layers\n    {\n        \"layers\": [\n            {\"type\": \"linear\", \"in_features\": 512, \"out_features\": 256},\n            {\"type\": \"linear\", \"in_features\": 256, \"out_features\": 256},\n        ]\n    },\n    # Stage 2: Output layers\n    {\n        \"layers\": [\n            {\"type\": \"linear\", \"in_features\": 256, \"out_features\": 128},\n            {\"type\": \"linear\", \"in_features\": 128, \"out_features\": 10},\n        ]\n    },\n]\n\nmodel = PipelineParallelModel(\n    layer_configs=layer_configs,\n    num_microbatches=4,\n    rngs=nnx.Rngs(0),\n)\n\n# Forward pass with pipeline parallelism\nx = jnp.ones((32, 784))  # Batch of 32\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")  # (32, 10)\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#1f1b-one-forward-one-backward-schedule","title":"1F1B (One Forward One Backward) Schedule","text":"<p>More memory-efficient pipeline schedule:</p> <pre><code>class OneFOneBPipeline(nnx.Module):\n    \"\"\"Pipeline with 1F1B (One Forward One Backward) schedule.\"\"\"\n\n    def __init__(\n        self,\n        stages: list[nnx.Module],\n        num_microbatches: int = 4,\n    ):\n        super().__init__()\n        self.stages = stages\n        self.num_microbatches = num_microbatches\n        self.num_stages = len(stages)\n\n    def forward_backward_step(\n        self,\n        stage_id: int,\n        forward_input: jax.Array | None,\n        backward_grad: jax.Array | None,\n    ) -&gt; tuple:\n        \"\"\"Perform one forward and one backward step for a stage.\"\"\"\n        outputs = {}\n\n        # Forward pass if input available\n        if forward_input is not None:\n            def forward_fn(stage, x):\n                return self.stages[stage_id](x)\n\n            # Compute forward and store for backward\n            outputs[\"forward_output\"], outputs[\"forward_vjp\"] = jax.vjp(\n                lambda x: forward_fn(stage_id, x),\n                forward_input\n            )\n\n        # Backward pass if gradient available\n        if backward_grad is not None and \"forward_vjp\" in outputs:\n            # Compute gradients\n            outputs[\"backward_grad\"] = outputs[\"forward_vjp\"](backward_grad)[0]\n\n        return outputs\n\n    def __call__(self, x: jax.Array) -&gt; tuple[jax.Array, dict]:\n        \"\"\"Forward-backward pass with 1F1B schedule.\"\"\"\n        batch_size = x.shape[0]\n        microbatch_size = batch_size // self.num_microbatches\n\n        # Split into microbatches\n        microbatches = [\n            x[i * microbatch_size:(i + 1) * microbatch_size]\n            for i in range(self.num_microbatches)\n        ]\n\n        # Execution state\n        forward_cache = [[None] * self.num_microbatches\n                        for _ in range(self.num_stages)]\n        backward_grads = [[None] * self.num_microbatches\n                         for _ in range(self.num_stages)]\n\n        outputs = []\n\n        # 1F1B Schedule:\n        # 1. Warmup: Fill pipeline with forward passes\n        # 2. Steady state: Alternate forward and backward\n        # 3. Cooldown: Drain backward passes\n\n        warmup_steps = self.num_stages - 1\n        steady_steps = self.num_microbatches - warmup_steps\n\n        # Warmup phase\n        for step in range(warmup_steps):\n            for stage_id in range(step + 1):\n                microbatch_id = step - stage_id\n\n                if stage_id == 0:\n                    stage_input = microbatches[microbatch_id]\n                else:\n                    stage_input = forward_cache[stage_id - 1][microbatch_id]\n\n                result = self.forward_backward_step(\n                    stage_id, stage_input, None\n                )\n                forward_cache[stage_id][microbatch_id] = result[\"forward_output\"]\n\n        # Steady state: 1 forward + 1 backward per step\n        for step in range(steady_steps):\n            microbatch_id = warmup_steps + step\n\n            # Forward pass for new microbatch\n            for stage_id in range(self.num_stages):\n                if stage_id == 0:\n                    stage_input = microbatches[microbatch_id]\n                else:\n                    stage_input = forward_cache[stage_id - 1][microbatch_id]\n\n                result = self.forward_backward_step(\n                    stage_id, stage_input, None\n                )\n                forward_cache[stage_id][microbatch_id] = result[\"forward_output\"]\n\n            # Backward pass for oldest microbatch in pipeline\n            backward_microbatch_id = step\n            for stage_id in range(self.num_stages - 1, -1, -1):\n                if stage_id == self.num_stages - 1:\n                    # Loss gradient (assume 1.0 for now)\n                    grad = jnp.ones_like(\n                        forward_cache[stage_id][backward_microbatch_id]\n                    )\n                else:\n                    grad = backward_grads[stage_id + 1][backward_microbatch_id]\n\n                result = self.forward_backward_step(\n                    stage_id,\n                    forward_cache[stage_id][backward_microbatch_id],\n                    grad\n                )\n                if \"backward_grad\" in result:\n                    backward_grads[stage_id][backward_microbatch_id] = result[\"backward_grad\"]\n\n        # Cooldown: Drain remaining backward passes\n        for step in range(warmup_steps):\n            backward_microbatch_id = steady_steps + step\n\n            for stage_id in range(self.num_stages - 1, -1, -1):\n                if stage_id == self.num_stages - 1:\n                    grad = jnp.ones_like(\n                        forward_cache[stage_id][backward_microbatch_id]\n                    )\n                else:\n                    grad = backward_grads[stage_id + 1][backward_microbatch_id]\n\n                result = self.forward_backward_step(\n                    stage_id,\n                    forward_cache[stage_id][backward_microbatch_id],\n                    grad\n                )\n                if \"backward_grad\" in result:\n                    backward_grads[stage_id][backward_microbatch_id] = result[\"backward_grad\"]\n\n        # Collect outputs\n        final_outputs = [\n            forward_cache[self.num_stages - 1][i]\n            for i in range(self.num_microbatches)\n        ]\n\n        return jnp.concatenate(final_outputs, axis=0), backward_grads[0]\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#hybrid-parallelism","title":"Hybrid Parallelism","text":"<p>Combine multiple parallelism strategies for maximum scale.</p>"},{"location":"user-guide/advanced/parallelism/#3d-parallelism","title":"3D Parallelism","text":"<p>Combine data, tensor, and pipeline parallelism:</p> <pre><code>from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\nfrom flax import nnx\n\nclass HybridParallelTransformer(nnx.Module):\n    \"\"\"Transformer with 3D parallelism (data + tensor + pipeline).\"\"\"\n\n    def __init__(\n        self,\n        num_layers: int,\n        hidden_size: int,\n        num_heads: int,\n        num_pipeline_stages: int,\n        *,\n        rngs: nnx.Rngs,\n        mesh: Mesh,\n    ):\n        super().__init__()\n\n        # Divide layers into pipeline stages\n        layers_per_stage = num_layers // num_pipeline_stages\n\n        self.stages = []\n        for stage_id in range(num_pipeline_stages):\n            stage_layers = []\n\n            for _ in range(layers_per_stage):\n                # Each layer uses tensor parallelism\n                layer = ParallelTransformerLayer(\n                    hidden_size=hidden_size,\n                    num_heads=num_heads,\n                    ffn_hidden_size=4 * hidden_size,\n                    rngs=rngs,\n                    mesh=mesh,\n                )\n                stage_layers.append(layer)\n\n            self.stages.append(nnx.Sequential(*stage_layers))\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with 3D parallelism.\n\n        Args:\n            x: (batch, seq_len, hidden_size)\n                Sharded along batch (data parallel),\n                hidden_size (tensor parallel),\n                and layers (pipeline parallel)\n\n        Returns:\n            (batch, seq_len, hidden_size) with same sharding\n        \"\"\"\n        # Pipeline forward through stages\n        for stage in self.stages:\n            x = stage(x)\n\n        return x\n\n\n# Create 3D parallel mesh\ndevices = jax.devices()  # e.g., 16 devices\nmesh = Mesh(\n    devices.reshape(2, 4, 2),  # (data, model, pipeline)\n    axis_names=(\"data\", \"model\", \"pipeline\")\n)\n\n# Create hybrid parallel model\nwith mesh:\n    model = HybridParallelTransformer(\n        num_layers=24,\n        hidden_size=1024,\n        num_heads=16,\n        num_pipeline_stages=2,  # 12 layers per stage\n        rngs=nnx.Rngs(0),\n        mesh=mesh,\n    )\n\n    # Define input sharding\n    input_sharding = NamedSharding(mesh, P(\"data\", None, \"model\"))\n\n    # Forward pass\n    x = jnp.ones((16, 512, 1024))  # (batch, seq_len, hidden)\n    x = jax.device_put(x, input_sharding)\n    output = model(x)\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#automatic-parallelism-selection","title":"Automatic Parallelism Selection","text":"<p>Choose parallelism strategy based on model size and available devices:</p> <pre><code>def select_parallelism_strategy(\n    model_params: int,\n    available_devices: int,\n    device_memory_gb: float,\n) -&gt; dict:\n    \"\"\"Select optimal parallelism strategy.\n\n    Args:\n        model_params: Number of model parameters (billions)\n        available_devices: Number of available devices\n        device_memory_gb: Memory per device (GB)\n\n    Returns:\n        Dictionary with parallelism configuration\n    \"\"\"\n    # Estimate memory requirements (rough approximation)\n    # Parameters: 4 bytes per param (fp32) or 2 bytes (fp16)\n    # Gradients: Same as parameters\n    # Optimizer states: 2x parameters (Adam)\n    # Activations: Depends on batch size, roughly 2x parameters\n    memory_per_param_bytes = 2  # fp16\n    total_memory_gb = model_params * memory_per_param_bytes * 5 / 1e9\n\n    params_per_device = model_params / available_devices\n\n    if total_memory_gb &lt;= device_memory_gb:\n        # Model fits on one device: use data parallelism\n        return {\n            \"strategy\": \"data_parallel\",\n            \"data_parallel_size\": available_devices,\n            \"tensor_parallel_size\": 1,\n            \"pipeline_parallel_size\": 1,\n            \"mesh_shape\": (available_devices,),\n            \"mesh_axis_names\": (\"data\",),\n        }\n\n    elif params_per_device * memory_per_param_bytes * 5 / 1e9 &lt;= device_memory_gb:\n        # Model fits with data parallelism: use it\n        return {\n            \"strategy\": \"data_parallel\",\n            \"data_parallel_size\": available_devices,\n            \"tensor_parallel_size\": 1,\n            \"pipeline_parallel_size\": 1,\n            \"mesh_shape\": (available_devices,),\n            \"mesh_axis_names\": (\"data\",),\n        }\n\n    else:\n        # Need model parallelism\n        # Prefer tensor parallelism for fast interconnect\n        # Fall back to pipeline for slower interconnect\n\n        # Heuristic: Use tensor parallelism up to 8 devices\n        # Then add pipeline parallelism\n        if available_devices &lt;= 8:\n            return {\n                \"strategy\": \"tensor_parallel\",\n                \"data_parallel_size\": 1,\n                \"tensor_parallel_size\": available_devices,\n                \"pipeline_parallel_size\": 1,\n                \"mesh_shape\": (1, available_devices),\n                \"mesh_axis_names\": (\"data\", \"model\"),\n            }\n\n        else:\n            # 3D parallelism\n            # Allocate: 25% data, 50% tensor, 25% pipeline\n            # Adjust to factors of available_devices\n\n            # Find factors\n            import math\n\n            def find_factors(n):\n                factors = []\n                for i in range(1, int(math.sqrt(n)) + 1):\n                    if n % i == 0:\n                        factors.append((i, n // i))\n                return factors\n\n            # Simple heuristic: balance tensor and pipeline\n            tensor_size = 4  # Typical good value\n            if available_devices % tensor_size == 0:\n                remaining = available_devices // tensor_size\n\n                # Split remaining between data and pipeline\n                data_size = max(1, remaining // 2)\n                pipeline_size = remaining // data_size\n\n                return {\n                    \"strategy\": \"hybrid_3d\",\n                    \"data_parallel_size\": data_size,\n                    \"tensor_parallel_size\": tensor_size,\n                    \"pipeline_parallel_size\": pipeline_size,\n                    \"mesh_shape\": (data_size, tensor_size, pipeline_size),\n                    \"mesh_axis_names\": (\"data\", \"model\", \"pipeline\"),\n                }\n\n            # Fallback: Just use available devices for tensor parallelism\n            return {\n                \"strategy\": \"tensor_parallel\",\n                \"data_parallel_size\": 1,\n                \"tensor_parallel_size\": available_devices,\n                \"pipeline_parallel_size\": 1,\n                \"mesh_shape\": (1, available_devices),\n                \"mesh_axis_names\": (\"data\", \"model\"),\n            }\n\n\n# Example usage\nstrategy = select_parallelism_strategy(\n    model_params=175,  # 175B parameters (GPT-3 scale)\n    available_devices=64,\n    device_memory_gb=40,  # A100 40GB\n)\n\nprint(f\"Selected strategy: {strategy['strategy']}\")\nprint(f\"Data parallel: {strategy['data_parallel_size']}\")\nprint(f\"Tensor parallel: {strategy['tensor_parallel_size']}\")\nprint(f\"Pipeline parallel: {strategy['pipeline_parallel_size']}\")\nprint(f\"Mesh shape: {strategy['mesh_shape']}\")\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#memory-optimization","title":"Memory Optimization","text":"<p>Techniques to maximize model size on limited memory.</p>"},{"location":"user-guide/advanced/parallelism/#activation-checkpointing","title":"Activation Checkpointing","text":"<p>Trade computation for memory by recomputing activations:</p> <pre><code>from jax.ad_checkpoint import checkpoint as jax_checkpoint\n\nclass CheckpointedTransformerLayer(nnx.Module):\n    \"\"\"Transformer layer with activation checkpointing.\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        *,\n        rngs: nnx.Rngs,\n        use_remat: bool = True,\n    ):\n        super().__init__()\n        self.use_remat = use_remat\n\n        # Standard transformer layer components\n        self.attention = MultiHeadAttention(hidden_size, num_heads, rngs=rngs)\n        self.ffn = FeedForward(hidden_size, 4 * hidden_size, rngs=rngs)\n        self.ln1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.ln2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n    def _forward_attention(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Attention block (may be checkpointed).\"\"\"\n        residual = x\n        x = self.ln1(x)\n        x = self.attention(x)\n        x = residual + x\n        return x\n\n    def _forward_ffn(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Feed-forward block (may be checkpointed).\"\"\"\n        residual = x\n        x = self.ln2(x)\n        x = self.ffn(x)\n        x = residual + x\n        return x\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with optional checkpointing.\"\"\"\n        if self.use_remat:\n            # Checkpoint both attention and FFN\n            # Activations will be recomputed during backward pass\n            x = jax_checkpoint(self._forward_attention)(x)\n            x = jax_checkpoint(self._forward_ffn)(x)\n        else:\n            # Standard forward pass\n            x = self._forward_attention(x)\n            x = self._forward_ffn(x)\n\n        return x\n\n\n# Compare memory usage\ndef measure_memory(use_checkpointing: bool):\n    \"\"\"Measure memory usage with/without checkpointing.\"\"\"\n    layer = CheckpointedTransformerLayer(\n        hidden_size=1024,\n        num_heads=16,\n        rngs=nnx.Rngs(0),\n        use_remat=use_checkpointing,\n    )\n\n    # Dummy forward-backward pass\n    x = jnp.ones((32, 512, 1024))  # Large batch and sequence\n\n    def loss_fn(layer, x):\n        output = layer(x)\n        return jnp.mean(output ** 2)\n\n    # Compute gradients (triggers backward pass)\n    loss, grads = nnx.value_and_grad(loss_fn)(layer, x)\n\n    return loss, grads\n\n# Without checkpointing: ~10GB peak memory\n# With checkpointing: ~5GB peak memory (50% reduction)\n# But ~30% slower due to recomputation\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#selective-checkpointing","title":"Selective Checkpointing","text":"<p>Checkpoint only memory-intensive operations:</p> <pre><code>class SelectiveCheckpointedLayer(nnx.Module):\n    \"\"\"Layer with selective activation checkpointing.\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        *,\n        rngs: nnx.Rngs,\n        checkpoint_attention: bool = True,\n        checkpoint_ffn: bool = False,\n    ):\n        super().__init__()\n        self.checkpoint_attention = checkpoint_attention\n        self.checkpoint_ffn = checkpoint_ffn\n\n        self.attention = MultiHeadAttention(hidden_size, 16, rngs=rngs)\n        self.ffn = FeedForward(hidden_size, 4 * hidden_size, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with selective checkpointing.\"\"\"\n        # Attention: Large activations (seq_len^2), worth checkpointing\n        if self.checkpoint_attention:\n            x = jax_checkpoint(self.attention)(x)\n        else:\n            x = self.attention(x)\n\n        # FFN: Smaller activations, maybe don't checkpoint\n        if self.checkpoint_ffn:\n            x = jax_checkpoint(self.ffn)(x)\n        else:\n            x = self.ffn(x)\n\n        return x\n\n# Rule of thumb:\n# - Checkpoint attention (quadratic memory in sequence length)\n# - Don't checkpoint FFN (linear memory, fast recompute)\n# - Checkpoint every N layers in deep models\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Simulate larger batches with gradient accumulation:</p> <pre><code>@jax.jit\ndef train_step_with_accumulation(\n    model_state,\n    batch,\n    optimizer_state,\n    num_accumulation_steps: int = 4\n):\n    \"\"\"Training step with gradient accumulation for memory efficiency.\"\"\"\n    model_graphdef, _ = nnx.split(model)\n\n    # Split batch into sub-batches\n    sub_batch_size = batch[\"data\"].shape[0] // num_accumulation_steps\n\n    # Initialize accumulated gradients\n    accumulated_grads = None\n    total_loss = 0.0\n\n    # Accumulate gradients over sub-batches\n    for i in range(num_accumulation_steps):\n        start_idx = i * sub_batch_size\n        end_idx = (i + 1) * sub_batch_size\n\n        sub_batch = {\n            \"data\": batch[\"data\"][start_idx:end_idx]\n        }\n\n        # Compute gradients for sub-batch\n        def loss_fn(state):\n            model = nnx.merge(model_graphdef, state)\n            output = model(sub_batch[\"data\"])\n            return output[\"loss\"]\n\n        loss, grads = nnx.value_and_grad(loss_fn)(model_state)\n\n        # Accumulate gradients\n        if accumulated_grads is None:\n            accumulated_grads = grads\n        else:\n            accumulated_grads = jax.tree.map(\n                lambda acc, g: acc + g,\n                accumulated_grads,\n                grads\n            )\n\n        total_loss += loss\n\n    # Average accumulated gradients\n    accumulated_grads = jax.tree.map(\n        lambda g: g / num_accumulation_steps,\n        accumulated_grads\n    )\n\n    # Single optimizer update\n    updates, optimizer_state = optimizer.update(\n        accumulated_grads,\n        optimizer_state\n    )\n    model_state = optax.apply_updates(model_state, updates)\n\n    return model_state, optimizer_state, total_loss / num_accumulation_steps\n\n# Memory usage: 1/num_accumulation_steps of full batch\n# Gradient noise: Same as full batch\n# Training speed: ~num_accumulation_steps slower\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions in model parallelism.</p>"},{"location":"user-guide/advanced/parallelism/#communication-overhead","title":"Communication Overhead","text":"<p>Problem: Training slower than expected due to communication.</p> <p>Solutions:</p> <ol> <li>Profile Communication:</li> </ol> <pre><code>import jax.profiler\n\n# Profile training step\nwith jax.profiler.trace(\"/tmp/trace\"):\n    train_step(model_state, batch)\n\n# Look for:\n# - All-reduce time (should be &lt;20% of step time)\n# - All-gather time\n# - Point-to-point communication\n</code></pre> <ol> <li>Reduce Tensor Parallelism:</li> </ol> <pre><code># If communication &gt; computation, reduce parallelism\n# Bad: 8-way tensor parallel on slow interconnect\nmesh = Mesh(devices.reshape(1, 8), (\"data\", \"model\"))\n\n# Good: 2-way tensor parallel, 4-way data parallel\nmesh = Mesh(devices.reshape(4, 2), (\"data\", \"model\"))\n</code></pre> <ol> <li>Use Larger Microbatches:</li> </ol> <pre><code># Larger microbatches amortize communication\n# Bad: Too many small microbatches\nmodel(x, num_microbatches=16)  # High overhead\n\n# Good: Fewer larger microbatches\nmodel(x, num_microbatches=4)  # Lower overhead\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#load-imbalance","title":"Load Imbalance","text":"<p>Problem: Some devices idle while others compute.</p> <p>Solutions:</p> <ol> <li>Balance Pipeline Stages:</li> </ol> <pre><code># Profile each stage\nfor stage_id, stage in enumerate(model.stages):\n    start = time.time()\n    stage(x)\n    duration = time.time() - start\n    print(f\"Stage {stage_id}: {duration:.3f}s\")\n\n# Rebalance: Move layers from slow stages to fast stages\n</code></pre> <ol> <li>Adjust Parallelism Dimensions:</li> </ol> <pre><code># If tensor parallel devices imbalanced, adjust mesh\n# Bad: Imbalanced load\nmesh = Mesh(devices.reshape(2, 4), (\"data\", \"model\"))\n\n# Good: More balanced\nmesh = Mesh(devices.reshape(4, 2), (\"data\", \"model\"))\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#memory-fragmentation","title":"Memory Fragmentation","text":"<p>Problem: Out of memory despite sufficient total memory.</p> <p>Solutions:</p> <ol> <li>Use Gradient Checkpointing:</li> </ol> <pre><code># Reduce peak memory with checkpointing\nlayer = CheckpointedTransformerLayer(\n    hidden_size=1024,\n    num_heads=16,\n    rngs=rngs,\n    use_remat=True,  # Enable checkpointing\n)\n</code></pre> <ol> <li>Increase Pipeline Stages:</li> </ol> <pre><code># More pipeline stages = less memory per device\n# But more communication and pipeline bubbles\nmodel = PipelineParallelModel(\n    layer_configs=layer_configs,\n    num_microbatches=8,  # Increase microbatches too\n    rngs=rngs,\n)\n</code></pre>"},{"location":"user-guide/advanced/parallelism/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/advanced/parallelism/#do","title":"DO","text":"<ul> <li>\u2705 Start with data parallelism - simplest and most efficient</li> <li>\u2705 Profile before optimizing - measure actual bottlenecks</li> <li>\u2705 Use tensor parallelism for large layers - effective for transformers</li> <li>\u2705 Use pipeline parallelism for many layers - good for deep models</li> <li>\u2705 Combine strategies for extreme scale - 3D parallelism</li> <li>\u2705 Use activation checkpointing - when memory-constrained</li> <li>\u2705 Balance pipeline stages - equal computation per stage</li> <li>\u2705 Match parallelism to interconnect - tensor needs fast links</li> <li>\u2705 Test on small scale first - validate before scaling</li> <li>\u2705 Monitor communication overhead - should be &lt;20%</li> </ul>"},{"location":"user-guide/advanced/parallelism/#dont","title":"DON'T","text":"<ul> <li>\u274c Don't over-parallelize - diminishing returns beyond 8-16 devices</li> <li>\u274c Don't mix strategies randomly - profile and measure</li> <li>\u274c Don't ignore load imbalance - causes bubbles and idle time</li> <li>\u274c Don't checkpoint everything - balance memory vs. compute</li> <li>\u274c Don't use pipeline for small models - overhead not worth it</li> <li>\u274c Don't use tensor parallel on slow interconnect - communication dominates</li> <li>\u274c Don't forget gradient averaging - affects convergence</li> <li>\u274c Don't assume linear scaling - measure actual speedup</li> <li>\u274c Don't ignore pipeline bubbles - can waste 20-30% of time</li> <li>\u274c Don't skip testing - parallelism bugs are subtle</li> </ul>"},{"location":"user-guide/advanced/parallelism/#summary","title":"Summary","text":"<p>Model parallelism enables training large models:</p> <ol> <li>Tensor Parallelism: Split weight matrices across devices</li> <li>Pipeline Parallelism: Distribute layers across devices</li> <li>Hybrid Parallelism: Combine strategies for extreme scale</li> <li>Memory Optimization: Checkpointing and gradient accumulation</li> </ol> <p>Key trade-offs:</p> <ul> <li>Tensor parallel: High communication, good for large layers</li> <li>Pipeline parallel: Low communication, pipeline bubbles</li> <li>Hybrid: Scales to extreme sizes, complex to implement</li> </ul> <p>Choose based on:</p> <ul> <li>Model size and architecture</li> <li>Available devices and interconnect</li> <li>Memory constraints</li> <li>Target throughput</li> </ul>"},{"location":"user-guide/advanced/parallelism/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Checkpointing</p> <p>Learn about gradient checkpointing and model checkpointing</p> <p> Checkpointing guide</p> </li> <li> <p> Custom Architectures</p> <p>Build custom model architectures with parallelism</p> <p> Architecture guide</p> </li> <li> <p> Distributed Training</p> <p>Return to distributed training overview</p> <p> Distributed guide</p> </li> <li> <p> Training Guide</p> <p>Complete training documentation and best practices</p> <p> Training guide</p> </li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/","title":"Autoregressive Models Explained","text":"<ul> <li> <p> Sequential Generation</p> <p>Generate data one element at a time, predicting each based on all previous elements</p> </li> <li> <p> Tractable Likelihood</p> <p>Compute exact probability through chain rule factorization with no approximations</p> </li> <li> <p> Flexible Architectures</p> <p>Use any architecture (RNNs, CNNs, Transformers) that respects the autoregressive property</p> </li> <li> <p> State-of-the-Art Performance</p> <p>Power modern language models (GPT) and achieve competitive results in image and audio generation</p> </li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#overview","title":"Overview","text":"<p>Autoregressive models are a fundamental class of generative models that decompose the joint probability distribution into a product of conditional distributions using the chain rule of probability. They generate data sequentially, predicting each element conditioned on all previously generated elements.</p> <p>What makes autoregressive models special?</p> <p>Unlike other generative models that learn data distributions through latent variables (VAEs), adversarial training (GANs), or energy functions (EBMs), autoregressive models directly model the conditional probability of each element given its predecessors. This approach offers:</p> <ul> <li>Exact likelihood computation - no variational bounds or approximations</li> <li>Simple training - standard maximum likelihood with cross-entropy loss</li> <li>Universal applicability - works for any ordered sequential data</li> <li>Flexible expressiveness - from simple next-token prediction to complex long-range dependencies</li> <li>Proven scalability - powers billion-parameter language models like GPT-4</li> </ul> <p>The core principle: order matters. By imposing a specific ordering on data dimensions and modeling each element conditionally, autoregressive models achieve tractable training and exact inference while maintaining high expressiveness.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#the-intuition-building-sequences-step-by-step","title":"The Intuition: Building Sequences Step-by-Step","text":"<p>Think of autoregressive models like an artist creating a painting:</p> <ol> <li> <p>Start with a Blank Canvas - The first element is predicted from a simple prior (often uniform or learned).</p> </li> <li> <p>Add One Brush Stroke at a Time - Each new element is predicted based on what's already been created. The model asks: \"Given what I've painted so far, what comes next?\"</p> </li> <li> <p>Build Complex Patterns Gradually - Simple local dependencies (adjacent pixels, consecutive words) compose into global structure (coherent images, meaningful sentences).</p> </li> <li> <p>No Going Back - The autoregressive property enforces a strict ordering: element \\(i\\) cannot depend on future elements \\(i+1, i+2, \\ldots\\). This constraint makes training tractable.</p> </li> </ol> <p>The critical insight: by breaking down a high-dimensional joint distribution into a sequence of simpler conditional distributions, autoregressive models make both training (likelihood computation) and generation (sequential sampling) tractable.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"user-guide/concepts/autoregressive-explained/#the-chain-rule-factorization","title":"The Chain Rule Factorization","text":"<p>The chain rule of probability is the cornerstone of all autoregressive models. Any joint distribution can be factored as:</p> \\[ p(x_1, x_2, \\ldots, x_n) = p(x_1) \\prod_{i=2}^{n} p(x_i \\mid x_1, \\ldots, x_{i-1}) \\] <p>Autoregressive models parameterize each conditional \\(p(x_i \\mid x_{&lt;i})\\) with a neural network:</p> \\[ p_\\theta(x_i \\mid x_{&lt;i}) = f_\\theta(x_i; x_1, \\ldots, x_{i-1}) \\] <p>where \\(\\theta\\) are learnable parameters and \\(x_{&lt;i} = (x_1, \\ldots, x_{i-1})\\) denotes all previous elements.</p> <pre><code>graph LR\n    X1[\"x\u2081\"] --&gt; P1[\"p(x\u2081)\"]\n    X1 --&gt; P2[\"p(x\u2082|x\u2081)\"]\n    X2[\"x\u2082\"] --&gt; P2\n    X1 --&gt; P3[\"p(x\u2083|x\u2081,x\u2082)\"]\n    X2 --&gt; P3\n    X3[\"x\u2083\"] --&gt; P3\n    P1 --&gt; Joint[\"Joint Distribution&lt;br/&gt;p(x\u2081,x\u2082,x\u2083)\"]\n    P2 --&gt; Joint\n    P3 --&gt; Joint\n\n    style P1 fill:#c8e6c9\n    style P2 fill:#fff3cd\n    style P3 fill:#ffccbc\n    style Joint fill:#e1f5ff</code></pre> <p>Example - Image with 3 pixels:</p> \\[ p(x_1, x_2, x_3) = p(x_1) \\cdot p(x_2 \\mid x_1) \\cdot p(x_3 \\mid x_1, x_2) \\] <p>For a 256\u00d7256 RGB image with discrete pixel values \\(\\{0, 1, \\ldots, 255\\}\\):</p> \\[ p(\\text{image}) = \\prod_{i=1}^{256 \\times 256 \\times 3} p(x_i \\mid x_{&lt;i}) \\] <p>This factorization reduces modeling \\((256)^{196608}\\) joint probabilities to modeling 196,608 conditional distributions\u2014a massive simplification.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#log-likelihood-and-training","title":"Log-Likelihood and Training","text":"<p>The log-likelihood decomposes additively:</p> \\[ \\log p_\\theta(x_1, \\ldots, x_n) = \\sum_{i=1}^{n} \\log p_\\theta(x_i \\mid x_{&lt;i}) \\] <p>This makes maximum likelihood training straightforward:</p> \\[ \\max_\\theta \\mathbb{E}_{x \\sim p_{\\text{data}}}\\left[\\sum_{i=1}^{n} \\log p_\\theta(x_i \\mid x_{&lt;i})\\right] \\] <p>Equivalently, minimize the negative log-likelihood (cross-entropy):</p> \\[ \\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{n} \\log p_\\theta(x_i^{(j)} \\mid x_{&lt;i}^{(j)}) \\] <p>where \\(N\\) is the dataset size.</p> <p>Why This is Beautiful</p> <p>Unlike VAEs (ELBO bound), GANs (minimax), or EBMs (intractable partition function), autoregressive models optimize the exact likelihood using standard supervised learning. Each conditional \\(p(x_i \\mid x_{&lt;i})\\) is a classification problem over the vocabulary.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#ordering-and-masking","title":"Ordering and Masking","text":"<p>Choosing an ordering is crucial. Different orderings lead to different models:</p> <p>Text (Natural Sequential Order):</p> \\[ p(\\text{\"hello\"}) = p(\\text{h}) \\cdot p(\\text{e}|\\text{h}) \\cdot p(\\text{l}|\\text{he}) \\cdot p(\\text{l}|\\text{hel}) \\cdot p(\\text{o}|\\text{hell}) \\] <p>Images (Raster Scan):</p> <p>Pixels generated left-to-right, top-to-bottom:</p> \\[ p(\\text{image}) = \\prod_{h=1}^{H} \\prod_{w=1}^{W} \\prod_{c=1}^{C} p(x_{h,w,c} \\mid x_{&lt;h}, x_{h,&lt;w}, x_{h,w,&lt;c}) \\] <p>where \\(x_{&lt;h}\\) denotes all rows above, \\(x_{h,&lt;w}\\) denotes pixels to the left in current row, and \\(x_{h,w,&lt;c}\\) denotes previous channels.</p> <pre><code>graph TD\n    subgraph \"Image Raster Scan Order\"\n    P00[\"(0,0)\"] --&gt; P01[\"(0,1)\"]\n    P01 --&gt; P02[\"(0,2)\"]\n    P02 --&gt; P03[\"...\"]\n    P03 --&gt; P10[\"(1,0)\"]\n    P10 --&gt; P11[\"(1,1)\"]\n    P11 --&gt; P12[\"(1,2)\"]\n    end\n\n    style P00 fill:#c8e6c9\n    style P01 fill:#fff3cd\n    style P02 fill:#ffccbc\n    style P10 fill:#e1f5ff</code></pre> <p>Masking ensures the autoregressive property. When computing \\(p(x_i \\mid x_{&lt;i})\\), the neural network must not access future elements \\(x_{\\geq i}\\).</p> <p>Causal Masking (for sequences):</p> <pre><code># Attention mask preventing position i from attending to positions &gt; i\nmask = jnp.tril(jnp.ones((seq_len, seq_len)))  # Lower triangular\n</code></pre> <p>Spatial Masking (for images):</p> <pre><code># PixelCNN mask: pixel (h,w) cannot see (h',w') where h' &gt; h or (h'=h and w' &gt; w)\n# Implemented via masked convolutions\n</code></pre>"},{"location":"user-guide/concepts/autoregressive-explained/#autoregressive-architectures","title":"Autoregressive Architectures","text":"<p>Autoregressive models can use various neural network architectures, each with different trade-offs between expressiveness, computational efficiency, and applicability.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#1-recurrent-neural-networks-rnns","title":"1. Recurrent Neural Networks (RNNs)","text":"<p>RNNs were the original architecture for autoregressive modeling, maintaining hidden state \\(h_t\\) across time steps:</p> \\[ h_t = f(h_{t-1}, x_{t-1}; \\theta) \\] \\[ p(x_t \\mid x_{&lt;t}) = g(h_t; \\theta) \\] <p>Variants:</p> <ul> <li>Vanilla RNN: Simple recurrence, suffers from vanishing gradients</li> <li>LSTM (Long Short-Term Memory): Gating mechanisms for long-range dependencies</li> <li>GRU (Gated Recurrent Unit): Simplified gating, fewer parameters</li> </ul> <pre><code>class AutoregressiveRNN(nnx.Module):\n    def __init__(self, vocab_size, hidden_dim, *, rngs):\n        super().__init__()\n        self.embedding = nnx.Embed(vocab_size, hidden_dim, rngs=rngs)\n        self.rnn = nnx.RNN(hidden_dim, hidden_dim, rngs=rngs)\n        self.output = nnx.Linear(hidden_dim, vocab_size, rngs=rngs)\n\n    def __call__(self, x, *, rngs=None):\n        # x: [batch, seq_len]\n        embeddings = self.embedding(x)  # [batch, seq_len, hidden_dim]\n        hidden_states = self.rnn(embeddings)  # [batch, seq_len, hidden_dim]\n        logits = self.output(hidden_states)  # [batch, seq_len, vocab_size]\n        return {\"logits\": logits}\n</code></pre> <p>Advantages:</p> <ul> <li>Variable-length sequences handled naturally</li> <li>Memory-efficient inference (constant memory)</li> <li>Well-understood theory and practice</li> </ul> <p>Disadvantages:</p> <ul> <li>Sequential computation (no parallelization during training)</li> <li>Limited context (gradients vanish for long sequences)</li> <li>Slow training compared to Transformers</li> </ul> <p>When to use: Text generation with moderate sequence lengths, real-time applications requiring low latency.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#2-masked-convolutional-networks-pixelcnn","title":"2. Masked Convolutional Networks (PixelCNN)","text":"<p>PixelCNN (van den Oord et al., 2016) uses masked convolutions for autoregressive image generation:</p> <p>Key idea: Apply convolution with a spatial mask ensuring pixel \\((i,j)\\) only depends on pixels above and to the left.</p> <p>Masked Convolution:</p> <pre><code>class MaskedConv2D(nnx.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, mask_type, *, rngs):\n        super().__init__()\n        self.conv = nnx.Conv(in_channels, out_channels,\n                            kernel_size=kernel_size, padding=\"SAME\", rngs=rngs)\n        self.mask = self._create_mask(kernel_size, mask_type)\n\n    def _create_mask(self, kernel_size, mask_type):\n        \"\"\"Create autoregressive mask for convolution.\"\"\"\n        kh, kw = kernel_size\n        mask = jnp.ones((kh, kw, self.in_channels, self.out_channels))\n\n        center_h, center_w = kh // 2, kw // 2\n\n        # Mask future pixels (below and to the right)\n        mask = mask.at[center_h + 1:, :, :, :].set(0)\n        mask = mask.at[center_h, center_w + 1:, :, :].set(0)\n\n        # For mask type A (first layer), also mask center\n        if mask_type == \"A\":\n            mask = mask.at[center_h, center_w, :, :].set(0)\n\n        return mask\n\n    def __call__(self, x):\n        masked_kernel = self.conv.kernel * self.mask\n        # Apply masked convolution\n        ...\n</code></pre> <p>Architecture:</p> <ol> <li>First layer: Masked Conv with type A (masks center pixel)</li> <li>Hidden layers: Masked Conv with type B (includes center pixel)</li> <li>Residual blocks: Stack masked convolutions with skip connections</li> <li>Output: Per-pixel categorical distribution over pixel values</li> </ol> <pre><code>graph TB\n    Input[\"Input Image\"] --&gt; MaskA[\"Masked Conv&lt;br/&gt;(Type A)\"]\n    MaskA --&gt; ReLU1[\"ReLU\"]\n    ReLU1 --&gt; ResBlock[\"Residual Blocks&lt;br/&gt;(Masked Conv Type B)\"]\n    ResBlock --&gt; Out[\"Output Conv&lt;br/&gt;256 logits per pixel\"]\n\n    style Input fill:#e1f5ff\n    style MaskA fill:#fff3cd\n    style ResBlock fill:#ffccbc\n    style Out fill:#c8e6c9</code></pre> <p>Advantages:</p> <ul> <li>Parallel training: All pixels computed simultaneously</li> <li>Spatial inductive bias: Local patterns learned efficiently</li> <li>Exact likelihood: No approximations</li> </ul> <p>Disadvantages:</p> <ul> <li>Slow generation: Sequential pixel-by-pixel (196,608 steps for 256\u00d7256\u00d73 image)</li> <li>Blind spot: Standard PixelCNN misses dependencies due to receptive field limitations (fixed in Gated PixelCNN)</li> <li>Limited long-range dependencies: Receptive field grows linearly with depth</li> </ul> <p>When to use: Image generation when exact likelihood matters, density estimation on images, image inpainting.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#3-transformer-based-autoregressive-models","title":"3. Transformer-Based Autoregressive Models","text":"<p>Transformers (Vaswani et al., 2017) use self-attention with causal masking for autoregressive modeling:</p> <p>Self-Attention:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right) V \\] <p>where \\(M\\) is a causal mask:</p> \\[ M_{ij} = \\begin{cases} 0 &amp; \\text{if } j \\leq i \\\\ -\\infty &amp; \\text{if } j &gt; i \\end{cases} \\] <p>This ensures position \\(i\\) only attends to positions \\(\\leq i\\).</p> <pre><code>class CausalSelfAttention(nnx.Module):\n    def __init__(self, hidden_dim, num_heads, *, rngs):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = hidden_dim // num_heads\n\n        self.qkv = nnx.Linear(hidden_dim, 3 * hidden_dim, rngs=rngs)\n        self.output = nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n\n    def __call__(self, x):\n        # x: [batch, seq_len, hidden_dim]\n        batch_size, seq_len, _ = x.shape\n\n        # Compute Q, K, V\n        qkv = self.qkv(x)  # [batch, seq_len, 3 * hidden_dim]\n        q, k, v = jnp.split(qkv, 3, axis=-1)\n\n        # Reshape for multi-head attention\n        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # Compute attention scores\n        scores = jnp.einsum('bqhd,bkhd-&gt;bhqk', q, k) / jnp.sqrt(self.head_dim)\n\n        # Apply causal mask\n        mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n        scores = jnp.where(mask, scores, -1e9)\n\n        # Attention weights and output\n        attn_weights = nnx.softmax(scores, axis=-1)\n        attn_output = jnp.einsum('bhqk,bkhd-&gt;bqhd', attn_weights, v)\n\n        # Concatenate heads and project\n        attn_output = attn_output.reshape(batch_size, seq_len, -1)\n        output = self.output(attn_output)\n\n        return output\n</code></pre> <p>GPT Architecture (Generative Pre-trained Transformer):</p> <ol> <li>Token Embedding + Positional Embedding</li> <li>Stack of Transformer Blocks:</li> <li>Causal Self-Attention</li> <li>Layer Normalization</li> <li>Feed-Forward Network (2-layer MLP)</li> <li>Residual connections</li> <li>Output projection to vocabulary</li> </ol> <p>Advantages:</p> <ul> <li>Parallel training: All positions computed simultaneously</li> <li>Long-range dependencies: Direct connections via attention</li> <li>Scalability: Powers models with billions of parameters (GPT-3: 175B, GPT-4: ~1.7T)</li> <li>State-of-the-art: Best performance on text, competitive on images (GPT-style AR models)</li> </ul> <p>Disadvantages:</p> <ul> <li>Quadratic complexity: \\(O(n^2)\\) in sequence length for self-attention</li> <li>Memory intensive: Storing attention matrices</li> <li>Sequential generation: Still generate one token at a time</li> </ul> <p>When to use: Text generation (GPT, LLaMA), code generation, any task requiring long-range dependencies.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#4-wavenet-autoregressive-audio-generation","title":"4. WaveNet: Autoregressive Audio Generation","text":"<p>WaveNet (van den Oord et al., 2016) is a deep autoregressive model for raw audio waveforms:</p> <p>Key innovation: Dilated causal convolutions for exponentially large receptive fields.</p> <p>Dilated Convolution:</p> \\[ y_t = \\sum_{k=0}^{K-1} w_k \\cdot x_{t - d \\cdot k} \\] <p>where \\(d\\) is the dilation factor. Stacking layers with dilations \\(1, 2, 4, 8, \\ldots, 512\\) achieves receptive field of 1024 time steps with only \\(\\log_2(1024) = 10\\) layers.</p> <pre><code>graph TB\n    Input[\"Input Waveform\"] --&gt; D1[\"Dilated Conv&lt;br/&gt;dilation=1\"]\n    D1 --&gt; D2[\"Dilated Conv&lt;br/&gt;dilation=2\"]\n    D2 --&gt; D4[\"Dilated Conv&lt;br/&gt;dilation=4\"]\n    D4 --&gt; D8[\"Dilated Conv&lt;br/&gt;dilation=8\"]\n    D8 --&gt; Out[\"Output&lt;br/&gt;256 logits per sample\"]\n\n    style Input fill:#e1f5ff\n    style D1 fill:#fff3cd\n    style D4 fill:#ffccbc\n    style Out fill:#c8e6c9</code></pre> <p>Gated activation units:</p> \\[ z = \\tanh(W_{f} * x) \\odot \\sigma(W_g * x) \\] <p>where \\(*\\) denotes convolution, \\(\\odot\\) is element-wise product, and \\(W_f\\), \\(W_g\\) are filter and gate weights.</p> <p>Residual and skip connections: Connect all layers to output for deep architectures (30-40 layers).</p> <p>Advantages:</p> <ul> <li>Raw waveform modeling: No hand-crafted features</li> <li>High-quality audio: State-of-the-art speech synthesis</li> <li>Large receptive field: Captures long-term dependencies efficiently</li> </ul> <p>Disadvantages:</p> <ul> <li>Extremely slow generation: 16kHz audio requires 16,000 sequential steps per second</li> <li>Specialized for audio: Architecture designed for 1D temporal data</li> </ul> <p>When to use: Text-to-speech, audio generation, music synthesis.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#5-modern-vision-transformers-visual-autoregressive-modeling-var","title":"5. Modern Vision Transformers: Visual Autoregressive Modeling (VAR)","text":"<p>VAR (Visual Autoregressive Modeling, NeurIPS 2024 Best Paper) applies GPT-style autoregressive modeling to images via next-scale prediction:</p> <p>Key innovation: Instead of predicting pixels in raster scan order, predict image tokens at progressively finer scales.</p> <p>Multi-scale tokenization:</p> <ol> <li>Encode image into tokens at multiple resolutions: \\(16 \\times 16\\), \\(32 \\times 32\\), \\(64 \\times 64\\), etc.</li> <li>Autoregressively predict tokens at scale \\(s+1\\) conditioned on all tokens at scales \\(\\leq s\\)</li> <li>Use Transformer to model \\(p(\\text{tokens}_{s+1} \\mid \\text{tokens}_{\\leq s})\\)</li> </ol> <p>Advantages over pixel-level AR:</p> <ul> <li>Faster generation: Fewer sequential steps (sum of tokens across scales vs. total pixels)</li> <li>Better quality: Multi-scale structure matches image hierarchies</li> <li>Scalable: Exhibits power-law scaling like LLMs (\\(R^2 \\approx -0.998\\))</li> </ul> <p>Results: First GPT-style AR model to surpass diffusion transformers on ImageNet generation.</p> <p>When to use: High-quality image generation, scaling autoregressive models to large datasets.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#training-autoregressive-models","title":"Training Autoregressive Models","text":""},{"location":"user-guide/concepts/autoregressive-explained/#maximum-likelihood-training","title":"Maximum Likelihood Training","text":"<p>Autoregressive models are trained via maximum likelihood estimation using teacher forcing:</p> <p>Teacher Forcing: During training, use ground truth previous tokens as input (not model's own predictions).</p> <p>Training loop:</p> <pre><code>def train_step(model, batch, optimizer):\n    # batch['sequences']: [batch_size, seq_len] ground truth sequences\n\n    def loss_fn(model):\n        # Forward pass with ground truth input\n        outputs = model(batch['sequences'])\n        logits = outputs['logits']  # [batch_size, seq_len, vocab_size]\n\n        # Shift targets: predict x_i given x_&lt;i\n        shifted_logits = logits[:, :-1, :]  # Remove last position\n        shifted_targets = batch['sequences'][:, 1:]  # Remove first position\n\n        # Cross-entropy loss\n        log_probs = nnx.log_softmax(shifted_logits, axis=-1)\n        one_hot_targets = nnx.one_hot(shifted_targets, vocab_size)\n        loss = -jnp.mean(jnp.sum(log_probs * one_hot_targets, axis=-1))\n\n        return loss\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n\n    return loss\n</code></pre> <p>Why teacher forcing?</p> <ul> <li>Stable training: Prevents error accumulation from model's mistakes</li> <li>Faster convergence: Model sees correct context</li> <li>Exact gradients: No need for reinforcement learning</li> </ul> <p>Exposure bias: At test time, model generates from its own predictions (different from training). Addressed by:</p> <ul> <li>Scheduled sampling: Gradually mix model predictions during training</li> <li>Curriculum learning: Start with teacher forcing, transition to self-generated</li> <li>Large-scale training: With enough data and capacity, models generalize well despite bias</li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#loss-functions-and-metrics","title":"Loss Functions and Metrics","text":"<p>Primary loss: Negative log-likelihood (NLL) / Cross-entropy:</p> \\[ \\mathcal{L}_{\\text{NLL}} = -\\frac{1}{N \\cdot T} \\sum_{n=1}^{N} \\sum_{t=1}^{T} \\log p_\\theta(x_t^{(n)} \\mid x_{&lt;t}^{(n)}) \\] <p>Perplexity: Exponentiated cross-entropy (lower is better):</p> \\[ \\text{PPL} = \\exp(\\mathcal{L}_{\\text{NLL}}) \\] <p>Bits per dimension (BPD): For images, normalized negative log-likelihood:</p> \\[ \\text{BPD} = \\frac{\\mathcal{L}_{\\text{NLL}}}{D \\cdot \\log 2} \\] <p>where \\(D\\) is the data dimensionality.</p> <p>Accuracy: Token-level prediction accuracy (for discrete data):</p> \\[ \\text{Acc} = \\frac{1}{N \\cdot T} \\sum_{n=1}^{N} \\sum_{t=1}^{T} \\mathbb{1}[\\arg\\max p_\\theta(x_t \\mid x_{&lt;t}) = x_t] \\]"},{"location":"user-guide/concepts/autoregressive-explained/#numerical-stability-and-best-practices","title":"Numerical Stability and Best Practices","text":"<p>Log-space computation: Always work in log-space to prevent underflow:</p> <pre><code># WRONG: Can underflow\nprobs = softmax(logits)\nloss = -jnp.mean(jnp.log(probs[targets]))\n\n# CORRECT: Numerically stable\nlog_probs = nnx.log_softmax(logits)\nloss = -jnp.mean(log_probs[targets])\n</code></pre> <p>Gradient clipping: Prevent exploding gradients in deep models:</p> <pre><code># Clip gradient norm to max value\ngrads = jax.tree_map(lambda g: jnp.clip(g, -clip_value, clip_value), grads)\n</code></pre> <p>Learning rate schedules: Use warmup + decay for Transformers:</p> <pre><code>def lr_schedule(step, warmup_steps=4000, d_model=512):\n    step = jnp.maximum(step, 1)  # Avoid division by zero\n    arg1 = step ** -0.5\n    arg2 = step * (warmup_steps ** -1.5)\n    return (d_model ** -0.5) * jnp.minimum(arg1, arg2)\n</code></pre> <p>Label smoothing: Reduce overconfidence:</p> <pre><code>def label_smoothing(one_hot_labels, smoothing=0.1):\n    num_classes = one_hot_labels.shape[-1]\n    smooth_labels = one_hot_labels * (1 - smoothing)\n    smooth_labels += smoothing / num_classes\n    return smooth_labels\n</code></pre>"},{"location":"user-guide/concepts/autoregressive-explained/#generation-and-sampling-strategies","title":"Generation and Sampling Strategies","text":""},{"location":"user-guide/concepts/autoregressive-explained/#greedy-decoding","title":"Greedy Decoding","text":"<p>Select the most likely token at each step:</p> \\[ x_t = \\arg\\max_{x} p_\\theta(x \\mid x_{&lt;t}) \\] <pre><code>def greedy_generation(model, max_length, *, rngs):\n    sequence = jnp.zeros((1, max_length), dtype=jnp.int32)\n\n    for t in range(max_length):\n        outputs = model(sequence, rngs=rngs)\n        logits = outputs['logits'][:, t, :]  # [1, vocab_size]\n\n        next_token = jnp.argmax(logits, axis=-1)\n        sequence = sequence.at[:, t].set(next_token)\n\n    return sequence\n</code></pre> <p>Pros: Deterministic, fast</p> <p>Cons: Repetitive, lacks diversity, not true sampling from \\(p_\\theta\\)</p>"},{"location":"user-guide/concepts/autoregressive-explained/#sampling-with-temperature","title":"Sampling with Temperature","text":"<p>Temperature \\(\\tau\\) controls randomness:</p> \\[ p_\\tau(x_t \\mid x_{&lt;t}) = \\frac{\\exp(f_\\theta(x_t \\mid x_{&lt;t}) / \\tau)}{\\sum_{x'} \\exp(f_\\theta(x' \\mid x_{&lt;t}) / \\tau)} \\] <ul> <li>\\(\\tau \\to 0\\): Greedy (deterministic)</li> <li>\\(\\tau = 1\\): Sample from model distribution</li> <li>\\(\\tau &gt; 1\\): More uniform (random)</li> </ul> <pre><code>def temperature_sampling(model, max_length, temperature=1.0, *, rngs):\n    sequence = jnp.zeros((1, max_length), dtype=jnp.int32)\n    sample_key = rngs.sample()\n\n    for t in range(max_length):\n        outputs = model(sequence, rngs=rngs)\n        logits = outputs['logits'][:, t, :] / temperature\n\n        sample_key, subkey = jax.random.split(sample_key)\n        next_token = jax.random.categorical(subkey, logits, axis=-1)\n        sequence = sequence.at[:, t].set(next_token)\n\n    return sequence\n</code></pre>"},{"location":"user-guide/concepts/autoregressive-explained/#top-k-sampling","title":"Top-k Sampling","text":"<p>Restrict sampling to the \\(k\\) most likely tokens:</p> <ol> <li>Find top-k logits: \\(\\text{top}_k(f_\\theta(x \\mid x_{&lt;t}))\\)</li> <li>Set all other logits to \\(-\\infty\\)</li> <li>Sample from renormalized distribution</li> </ol> <pre><code>def top_k_sampling(model, max_length, k=40, temperature=1.0, *, rngs):\n    sequence = jnp.zeros((1, max_length), dtype=jnp.int32)\n    sample_key = rngs.sample()\n\n    for t in range(max_length):\n        outputs = model(sequence, rngs=rngs)\n        logits = outputs['logits'][:, t, :] / temperature\n\n        # Get top-k\n        top_k_logits, top_k_indices = jax.lax.top_k(logits, k)\n\n        # Mask non-top-k\n        masked_logits = jnp.full_like(logits, -1e9)\n        masked_logits = masked_logits.at[0, top_k_indices[0]].set(top_k_logits[0])\n\n        # Sample\n        sample_key, subkey = jax.random.split(sample_key)\n        next_token = jax.random.categorical(subkey, masked_logits, axis=-1)\n        sequence = sequence.at[:, t].set(next_token)\n\n    return sequence\n</code></pre> <p>Typical \\(k\\) values: 10-50 for text, 40 is common.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#top-p-nucleus-sampling","title":"Top-p (Nucleus) Sampling","text":"<p>Sample from the smallest set of tokens whose cumulative probability exceeds \\(p\\):</p> <ol> <li>Sort tokens by probability in descending order</li> <li>Find cutoff where cumulative probability \\(\\geq p\\)</li> <li>Sample from this subset</li> </ol> <pre><code>def top_p_sampling(model, max_length, p=0.9, temperature=1.0, *, rngs):\n    sequence = jnp.zeros((1, max_length), dtype=jnp.int32)\n    sample_key = rngs.sample()\n\n    for t in range(max_length):\n        outputs = model(sequence, rngs=rngs)\n        logits = outputs['logits'][:, t, :] / temperature\n\n        # Sort by probability\n        probs = nnx.softmax(logits, axis=-1)\n        sorted_indices = jnp.argsort(-probs, axis=-1)\n        sorted_probs = jnp.take_along_axis(probs, sorted_indices, axis=-1)\n\n        # Cumulative probabilities\n        cumulative_probs = jnp.cumsum(sorted_probs, axis=-1)\n\n        # Find nucleus (keep at least one token)\n        cutoff_mask = cumulative_probs &lt;= p\n        cutoff_mask = cutoff_mask.at[:, 0].set(True)\n\n        # Mask and renormalize\n        masked_probs = jnp.where(cutoff_mask, sorted_probs, 0.0)\n        masked_probs /= jnp.sum(masked_probs, axis=-1, keepdims=True)\n\n        # Sample\n        sample_key, subkey = jax.random.split(sample_key)\n        sampled_idx = jax.random.categorical(subkey, jnp.log(masked_probs), axis=-1)\n        next_token = sorted_indices[0, sampled_idx[0]]\n        sequence = sequence.at[:, t].set(next_token)\n\n    return sequence\n</code></pre> <p>Typical \\(p\\) values: 0.9-0.95.</p> <p>Advantages: Adapts to probability distribution shape (varies nucleus size).</p>"},{"location":"user-guide/concepts/autoregressive-explained/#beam-search","title":"Beam Search","text":"<p>Maintain top-\\(B\\) most likely sequences:</p> <ol> <li>At each step, expand each of the \\(B\\) sequences with all possible next tokens</li> <li>Score all \\(B \\times V\\) candidates (where \\(V\\) is vocab size)</li> <li>Keep top-\\(B\\) by cumulative log-probability</li> <li>Return highest-scoring sequence at the end</li> </ol> <pre><code>def beam_search(model, max_length, beam_size=5, *, rngs):\n    # Initialize with start token\n    sequences = jnp.zeros((beam_size, max_length), dtype=jnp.int32)\n    scores = jnp.zeros(beam_size)\n    scores = scores.at[1:].set(-1e9)  # Only first beam is active initially\n\n    for t in range(max_length):\n        outputs = model(sequences, rngs=rngs)\n        logits = outputs['logits'][:, t, :]  # [beam_size, vocab_size]\n        log_probs = nnx.log_softmax(logits, axis=-1)\n\n        # Expand: [beam_size, vocab_size]\n        candidate_scores = scores[:, None] + log_probs\n\n        # Flatten and get top beam_size\n        flat_scores = candidate_scores.reshape(-1)\n        top_indices = jnp.argsort(-flat_scores)[:beam_size]\n\n        # Decode indices to (beam_idx, token_idx)\n        beam_indices = top_indices // vocab_size\n        token_indices = top_indices % vocab_size\n\n        # Update sequences and scores\n        sequences = sequences[beam_indices]\n        sequences = sequences.at[:, t].set(token_indices)\n        scores = flat_scores[top_indices]\n\n    # Return best sequence\n    best_idx = jnp.argmax(scores)\n    return sequences[best_idx:best_idx+1]\n</code></pre> <p>Beam size \\(B\\): Typical values 3-10. Larger = better likelihood, more computation.</p> <p>Use cases: Machine translation, caption generation (prefer high likelihood over diversity).</p>"},{"location":"user-guide/concepts/autoregressive-explained/#comparing-autoregressive-models-with-other-approaches","title":"Comparing Autoregressive Models with Other Approaches","text":""},{"location":"user-guide/concepts/autoregressive-explained/#autoregressive-vs-vaes-exact-likelihood-vs-latent-compression","title":"Autoregressive vs VAEs: Exact Likelihood vs Latent Compression","text":"Aspect Autoregressive Models VAEs Likelihood Exact Lower bound (ELBO) Training Cross-entropy (simple) ELBO (reconstruction + KL) Generation Speed Slow (sequential) Fast (single decoder pass) Sample Quality Sharp, high-fidelity Often blurry Latent Space No explicit latent Structured latent Interpolation Difficult Natural in latent space Use Cases Text, exact likelihood tasks Representation learning <p>When to use AR over VAE:</p> <ul> <li>Exact likelihood essential (density estimation, compression)</li> <li>Generation quality priority</li> <li>Sequential data (text, code)</li> <li>Willing to accept slower generation</li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#autoregressive-vs-gans-training-stability-vs-generation-speed","title":"Autoregressive vs GANs: Training Stability vs Generation Speed","text":"Aspect Autoregressive Models GANs Training Stability Stable (supervised learning) Unstable (minimax) Likelihood Exact None Generation Speed Slow (sequential) Fast (single pass) Sample Quality High (competitive with modern AR) High (sharp images) Mode Coverage Excellent Mode collapse common Diversity Controlled via sampling Variable <p>When to use AR over GAN:</p> <ul> <li>Training stability critical</li> <li>Exact likelihood needed</li> <li>Mode coverage essential</li> <li>Avoid adversarial training</li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#autoregressive-vs-diffusion-likelihood-vs-iterative-refinement","title":"Autoregressive vs Diffusion: Likelihood vs Iterative Refinement","text":"Aspect Autoregressive Models Diffusion Models Generation Process Sequential (one token/pixel) Iterative denoising Training Cross-entropy Denoising score matching Likelihood Exact Tractable via ODE Generation Speed Slow (sequential) Slow (50-1000 steps) Sample Quality Competitive (VAR 2024) State-of-the-art Architecture Ordered dependencies Flexible U-Net Parallelization Training: yes, Generation: no Training and generation: limited <p>Recent convergence: VAR (2024) shows AR models can match diffusion quality while maintaining exact likelihood.</p> <p>When to use AR over Diffusion:</p> <ul> <li>Exact likelihood computation required</li> <li>Natural sequential structure (text, code, music)</li> <li>Want to leverage Transformer scaling laws</li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#autoregressive-vs-flows-sequential-vs-invertible","title":"Autoregressive vs Flows: Sequential vs Invertible","text":"Aspect Autoregressive Models Normalizing Flows Likelihood Exact Exact Generation Speed Slow (sequential) Fast (single pass) Architecture Flexible (any network respecting order) Constrained (invertible) Training Cross-entropy Maximum likelihood via Jacobians Dimensionality No restrictions Input = output dimensionality <p>MAF/IAF: Masked Autoregressive Flow combines both\u2014autoregressive structure as normalizing flow.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#advanced-topics-and-recent-advances","title":"Advanced Topics and Recent Advances","text":""},{"location":"user-guide/concepts/autoregressive-explained/#masked-autoregressive-flows-maf","title":"Masked Autoregressive Flows (MAF)","text":"<p>MAF uses autoregressive transformations as invertible flow layers:</p> \\[ z_i = (x_i - \\mu_i(x_{&lt;i})) \\cdot \\exp(-\\alpha_i(x_{&lt;i})) \\] <p>where \\(\\mu_i\\) and \\(\\alpha_i\\) are outputs of a MADE (Masked Autoencoder) network.</p> <p>Jacobian is triangular:</p> \\[ \\frac{\\partial z}{\\partial x} = \\text{diag}(\\exp(-\\alpha_1(x_{&lt;1})), \\exp(-\\alpha_2(x_{&lt;2})), \\ldots) \\] <p>Log-determinant:</p> \\[ \\log \\left| \\det \\frac{\\partial z}{\\partial x} \\right| = -\\sum_i \\alpha_i(x_{&lt;i}) \\] <p>Trade-offs:</p> <ul> <li>Density estimation: \\(O(1)\\) forward pass (parallel)</li> <li>Sampling: \\(O(D)\\) sequential inverse</li> </ul> <p>IAF (Inverse Autoregressive Flow) reverses the trade-off: fast sampling, slow density.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#autoregressive-energy-based-models","title":"Autoregressive Energy-Based Models","text":"<p>Combine autoregressive and energy-based modeling:</p> \\[ p(x_1, \\ldots, x_n) = \\frac{1}{Z} \\prod_{i=1}^{n} \\exp(-E_i(x_i \\mid x_{&lt;i})) \\] <p>Train with contrastive divergence using autoregressive structure.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#sparse-transformers-and-efficient-attention","title":"Sparse Transformers and Efficient Attention","text":"<p>Problem: Standard self-attention is \\(O(n^2)\\) in sequence length.</p> <p>Sparse Transformers (Child et al., 2019) use sparse attention patterns:</p> <ul> <li>Strided attention: Attend to every \\(k\\)-th position</li> <li>Fixed attention: Attend to fixed positions (e.g., beginning of sequence)</li> <li>Local + global: Combine local windows with global tokens</li> </ul> <p>Complexity: \\(O(n \\sqrt{n})\\) or \\(O(n \\log n)\\) depending on pattern.</p> <p>Linear Transformers approximate attention with kernels:</p> \\[ \\text{Attention}(Q, K, V) \\approx \\phi(Q) (\\phi(K)^T V) \\] <p>achieving \\(O(n)\\) complexity.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#visual-autoregressive-modeling-var","title":"Visual Autoregressive Modeling (VAR)","text":"<p>VAR (2024, NeurIPS Best Paper) revolutionizes image generation:</p> <p>Multi-scale tokenization:</p> <ol> <li>Use VQ-VAE to tokenize images at scales \\(1, 2, 4, \\ldots, k\\)</li> <li>Flatten tokens across scales into a sequence</li> <li>Apply GPT-style Transformer to model \\(p(\\text{tokens}_{s+1} \\mid \\text{tokens}_{\\leq s})\\)</li> </ol> <p>Training: Standard next-token prediction</p> <p>Generation: Autoregressively predict scales</p> <p>Results:</p> <ul> <li>ImageNet 256\u00d7256: FID 1.92, surpassing diffusion transformers</li> <li>Scaling laws: Power-law relationship between loss and compute (\\(R^2 \\approx -0.998\\))</li> <li>Speed: Faster than pixel-level autoregressive, competitive with diffusion</li> </ul> <p>Significance: First GPT-style AR model to beat diffusion on image generation.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#autoregressive-for-protein-and-scientific-data","title":"Autoregressive for Protein and Scientific Data","text":"<p>ProtGPT2 (Ferruz et al., 2022): Autoregressive Transformer for protein sequences</p> <ul> <li>Generates novel, functional proteins</li> <li>50M parameters, trained on UniRef50</li> </ul> <p>AlphaFold 2 uses autoregressive structure prediction:</p> <ul> <li>Predicts protein structure token by token</li> <li>Iterative refinement via recycling</li> </ul> <p>Applications: Drug design, enzyme engineering, materials discovery.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#practical-implementation-in-artifex","title":"Practical Implementation in Artifex","text":""},{"location":"user-guide/concepts/autoregressive-explained/#basic-autoregressive-model","title":"Basic Autoregressive Model","text":"<pre><code>from artifex.generative_models.models.autoregressive import TransformerAR\n\n# Create Transformer autoregressive model\nmodel = TransformerAR(\n    vocab_size=10000,\n    sequence_length=512,\n    hidden_dim=512,\n    num_layers=6,\n    num_heads=8,\n    rngs=rngs\n)\n\n# Training\nbatch = {\"sequences\": sequences}  # [batch_size, seq_len]\noutputs = model(batch[\"sequences\"], rngs=rngs)\nloss_dict = model.loss_fn(batch, outputs, rngs=rngs)\n\n# Generation\nsamples = model.generate(\n    n_samples=10,\n    max_length=256,\n    temperature=0.8,\n    top_p=0.9,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/concepts/autoregressive-explained/#pixelcnn-for-images","title":"PixelCNN for Images","text":"<pre><code>from artifex.generative_models.models.autoregressive import PixelCNN\n\n# Create PixelCNN for MNIST (28\u00d728 grayscale)\nmodel = PixelCNN(\n    image_shape=(28, 28, 1),\n    num_layers=7,\n    hidden_channels=128,\n    num_residual_blocks=5,\n    rngs=rngs\n)\n\n# Training\nbatch = {\"images\": images}  # [batch_size, 28, 28, 1], values in [0, 255]\noutputs = model(batch[\"images\"], rngs=rngs, training=True)\nloss_dict = model.loss_fn(batch, outputs, rngs=rngs)\n\n# Generation\ngenerated_images = model.generate(\n    n_samples=16,\n    temperature=1.0,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/concepts/autoregressive-explained/#wavenet-for-audio","title":"WaveNet for Audio","text":"<pre><code>from artifex.generative_models.models.autoregressive import WaveNet\n\n# Create WaveNet for audio\nmodel = WaveNet(\n    num_layers=30,\n    num_stacks=3,\n    residual_channels=128,\n    dilation_channels=256,\n    skip_channels=512,\n    rngs=rngs\n)\n\n# Training\nbatch = {\"waveform\": waveform}  # [batch_size, time_steps]\noutputs = model(batch[\"waveform\"], rngs=rngs)\nloss_dict = model.loss_fn(batch, outputs, rngs=rngs)\n\n# Generation\ngenerated_audio = model.generate(\n    n_samples=1,\n    max_length=16000,  # 1 second at 16kHz\n    temperature=0.9,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/concepts/autoregressive-explained/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>Autoregressive models decompose joint distributions via the chain rule, enabling exact likelihood computation and straightforward maximum likelihood training. Their sequential generation, while slower than one-shot methods, achieves state-of-the-art results across modalities.</p>"},{"location":"user-guide/concepts/autoregressive-explained/#core-principles","title":"Core Principles","text":"<ul> <li> <p> Chain Rule Factorization</p> <p>Decompose \\(p(x_1, \\ldots, x_n) = \\prod_i p(x_i \\mid x_{&lt;i})\\) for tractable training</p> </li> <li> <p> Autoregressive Property</p> <p>Element \\(i\\) depends only on elements \\(&lt; i\\), enforced by masking</p> </li> <li> <p> Exact Likelihood</p> <p>No approximations\u2014log-likelihood decomposes additively over sequence</p> </li> <li> <p> Simple Training</p> <p>Standard supervised learning with cross-entropy loss</p> </li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#architecture-selection","title":"Architecture Selection","text":"Architecture Best For Generation Speed Likelihood Parallelization RNN/LSTM Text (legacy), real-time Moderate Exact Training: no, Generation: no PixelCNN Images (density estimation) Very slow Exact Training: yes, Generation: no Transformer Text, code, long-range Slow Exact Training: yes, Generation: no WaveNet Audio Very slow Exact Training: yes, Generation: no VAR Images (high-quality) Moderate Exact Training: yes, Generation: no"},{"location":"user-guide/concepts/autoregressive-explained/#sampling-strategies","title":"Sampling Strategies","text":"Strategy Use Case Diversity Quality Greedy Deterministic tasks Low High likelihood Temperature Controlled randomness Adjustable Variable Top-k Balanced diversity Medium Good Top-p (nucleus) Adaptive High Best overall Beam search Translation, captioning Low Highest likelihood"},{"location":"user-guide/concepts/autoregressive-explained/#when-to-use-autoregressive-models","title":"When to Use Autoregressive Models","text":"<p>Best suited for:</p> <ul> <li>Text generation (GPT, LLaMA, code models)</li> <li>Exact likelihood tasks (compression, density estimation)</li> <li>Sequential data with natural ordering (time series, audio)</li> <li>Long-range dependencies (via Transformers)</li> <li>Stable training (no adversarial dynamics)</li> </ul> <p>Avoid when:</p> <ul> <li>Real-time generation required (use GANs or fast flows)</li> <li>Latent representations needed (use VAEs)</li> <li>Order doesn't exist naturally (graph generation)</li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#future-directions","title":"Future Directions","text":"<ul> <li>Faster generation: Parallel decoding, non-autoregressive variants</li> <li>Hybrid models: Combining AR with diffusion or flows</li> <li>Efficiency: Sparse attention, linear transformers</li> <li>Scaling: Billion-parameter models across all modalities</li> <li>Multi-modal: Vision-language models (GPT-4V, Gemini)</li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#next-steps","title":"Next Steps","text":"<ul> <li> <p> AR User Guide</p> <p>Practical usage guide with implementation examples and training workflows</p> </li> <li> <p> AR API Reference</p> <p>Complete API documentation for Transformers, PixelCNN, and WaveNet</p> </li> <li> <p> Text Tutorial</p> <p>Step-by-step tutorial: train a Transformer language model</p> </li> <li> <p> Advanced Examples</p> <p>Explore PixelCNN, WaveNet, and state-of-the-art techniques</p> </li> </ul>"},{"location":"user-guide/concepts/autoregressive-explained/#further-reading","title":"Further Reading","text":""},{"location":"user-guide/concepts/autoregressive-explained/#seminal-papers-must-read","title":"Seminal Papers (Must Read)","text":"<p> Hochreiter, S., &amp; Schmidhuber, J. (1997). \"Long Short-Term Memory\" Neural Computation 9(8)  LSTM architecture enabling long-range dependencies in RNNs</p> <p> van den Oord, A., Kalchbrenner, N., &amp; Kavukcuoglu, K. (2016). \"Pixel Recurrent Neural Networks\" arXiv:1601.06759 | ICML 2016  PixelRNN and PixelCNN for autoregressive image generation</p> <p> van den Oord, A., et al. (2016). \"WaveNet: A Generative Model for Raw Audio\" arXiv:1609.03499  Dilated causal convolutions for high-quality audio synthesis</p> <p> Vaswani, A., et al. (2017). \"Attention Is All You Need\" arXiv:1706.03762 | NeurIPS 2017  Transformer architecture revolutionizing sequence modeling</p> <p> Radford, A., et al. (2018). \"Improving Language Understanding by Generative Pre-Training (GPT)\" OpenAI Technical Report  GPT demonstrating Transformer scaling for language</p> <p> Radford, A., et al. (2019). \"Language Models are Unsupervised Multitask Learners (GPT-2)\" OpenAI Technical Report  1.5B parameter model showing emergent capabilities</p>"},{"location":"user-guide/concepts/autoregressive-explained/#autoregressive-flows","title":"Autoregressive Flows","text":"<p> Papamakarios, G., Pavlakou, T., &amp; Murray, I. (2017). \"Masked Autoregressive Flow for Density Estimation\" arXiv:1705.07057 | NeurIPS 2017  Autoregressive transformations as normalizing flows</p> <p> Kingma, D. P., et al. (2016). \"Improved Variational Inference with Inverse Autoregressive Flow\" arXiv:1606.04934 | NeurIPS 2016  IAF for flexible variational posteriors</p>"},{"location":"user-guide/concepts/autoregressive-explained/#efficient-transformers","title":"Efficient Transformers","text":"<p> Child, R., et al. (2019). \"Generating Long Sequences with Sparse Transformers\" arXiv:1904.10509  Sparse attention patterns for \\(O(n \\sqrt{n})\\) complexity</p> <p> Katharopoulos, A., et al. (2020). \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\" arXiv:2006.16236 | ICML 2020  Linear attention achieving \\(O(n)\\) complexity</p>"},{"location":"user-guide/concepts/autoregressive-explained/#recent-advances-2023-2025","title":"Recent Advances (2023-2025)","text":"<p> Tian, K., et al. (2024). \"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction\" arXiv:2404.02905 | NeurIPS 2024 Best Paper  GPT-style AR surpassing diffusion on ImageNet</p> <p> Touvron, H., et al. (2023). \"LLaMA: Open and Efficient Foundation Language Models\" arXiv:2302.13971  7B-65B parameter open models competitive with GPT-3</p> <p> Brown, T., et al. (2020). \"Language Models are Few-Shot Learners (GPT-3)\" arXiv:2005.14165 | NeurIPS 2020  175B parameter model demonstrating in-context learning</p>"},{"location":"user-guide/concepts/autoregressive-explained/#tutorial-resources","title":"Tutorial Resources","text":"<p> UvA Deep Learning Tutorial 12: Autoregressive Image Modeling uvadlc-notebooks.readthedocs.io  Hands-on PixelCNN implementation with Colab notebooks</p> <p> Stanford CS236: Deep Generative Models (AR Lecture) deepgenerativemodels.github.io  Comprehensive course notes on autoregressive models</p> <p> The Illustrated Transformer jalammar.github.io/illustrated-transformer  Visual guide to understanding Transformers</p> <p> Hugging Face Transformers Library github.com/huggingface/transformers  State-of-the-art autoregressive models (GPT, LLaMA, etc.)</p> <p>Ready to build autoregressive models? Start with the AR User Guide for practical implementations, check the API Reference for complete documentation, or dive into tutorials to train your first language model or PixelCNN!</p>"},{"location":"user-guide/concepts/diffusion-explained/","title":"Diffusion Models Explained","text":"<ul> <li> <p> Progressive Denoising</p> <p>Learn to reverse a gradual noising process, iteratively refining random noise into coherent data</p> </li> <li> <p> Stable Training</p> <p>Straightforward MSE objective with no adversarial dynamics\u2014far more stable than GANs</p> </li> <li> <p> State-of-the-Art Quality</p> <p>Achieves the highest quality generative results, powering DALL-E 2, Stable Diffusion, and Sora</p> </li> <li> <p> Exceptional Controllability</p> <p>Natural framework for conditional generation, inpainting, editing, and guidance techniques</p> </li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#overview","title":"Overview","text":"<p>Diffusion models are a class of deep generative models that learn to generate data by reversing a gradual noising process. Unlike GANs which learn through adversarial training or VAEs which compress to latent codes, diffusion models systematically destroy data structure through noise addition, then learn to reverse this process for generation.</p> <p>What makes diffusion models special? They solve the generative modeling challenge through an elegant two-stage process: a fixed forward diffusion that gradually corrupts data into pure noise over many timesteps, and a learned reverse diffusion that progressively denoises random samples into realistic data. This approach offers unprecedented training stability, superior mode coverage, and exceptional sample quality.</p>"},{"location":"user-guide/concepts/diffusion-explained/#the-intuition-from-ink-to-water-and-back","title":"The Intuition: From Ink to Water and Back","text":"<p>Think of diffusion like watching a drop of ink dissolve in water:</p> <ol> <li> <p>The Forward Process is like dropping ink into a glass of water and watching it gradually diffuse. At first, you clearly see the ink drop. Over time, it spreads and mixes until the water appears uniformly tinted\u2014all structure is lost.</p> </li> <li> <p>The Reverse Process is like learning to run this process backwards: starting from uniformly tinted water and gradually reconstructing the original ink drop. This seems impossible by hand, but a neural network can learn the \"reverse physics.\"</p> </li> <li> <p>The Training teaches the network to predict: \"Given tinted water at some mixing stage, what did it look like one step earlier?\" Repeat this prediction many times, and you recover the original ink drop from fully mixed water.</p> </li> </ol> <p>The critical insight: while the forward diffusion is fixed and simple (just add noise), the reverse process is learned and powerful. The model learns to undo corruption at every noise level, enabling generation from pure random noise.</p>"},{"location":"user-guide/concepts/diffusion-explained/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"user-guide/concepts/diffusion-explained/#the-forward-diffusion-process","title":"The Forward Diffusion Process","text":"<p>The forward process defines a fixed Markov chain that gradually corrupts data \\(x_0\\) by adding Gaussian noise over \\(T\\) timesteps (typically \\(T=1000\\)):</p> \\[ q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} \\, x_{t-1}, \\beta_t \\mathbf{I}) \\] <p>where \\(\\beta_t \\in (0,1)\\) controls the variance of noise added at timestep \\(t\\). The complete forward chain factors as:</p> \\[ q(x_{1:T} | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1}) \\] <p>Key property: We can sample \\(x_t\\) at any arbitrary timestep directly without simulating the full chain. Defining \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\):</p> \\[ q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} \\, x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I}) \\] <p>This can be reparameterized as:</p> \\[ x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}) \\] <pre><code>graph LR\n    A[\"x\u2080&lt;br/&gt;(Clean Data)\"] --&gt;|\"\u03b2\u2081\"| B[\"x\u2081&lt;br/&gt;(Slightly Noisy)\"]\n    B --&gt;|\"\u03b2\u2082\"| C[\"x\u2082\"]\n    C --&gt;|\"\u03b2\u2083\"| D[\"...\"]\n    D --&gt;|\"\u03b2\u209c\"| E[\"x\u209c\"]\n    E --&gt;|\"...\"| F[\"x\u209c&lt;br/&gt;(Pure Noise)\"]\n\n    style A fill:#c8e6c9\n    style F fill:#ffccc9\n    style E fill:#fff3e0</code></pre> <p>Intuition: As \\(t \\to T\\), the distribution \\(q(x_T | x_0)\\) approaches an isotropic Gaussian \\(\\mathcal{N}(0, \\mathbf{I})\\), ensuring the endpoint is tractable pure noise. The forward process is designed so that \\(\\bar{\\alpha}_T \\approx 0\\).</p> <p>The posterior conditioned on the original data is also Gaussian with tractable parameters:</p> \\[ q(x_{t-1} | x_t, x_0) = \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t, x_0), \\tilde{\\beta}_t \\mathbf{I}) \\] <p>where:</p> \\[ \\tilde{\\mu}_t(x_t, x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} x_0 + \\frac{\\sqrt{\\alpha_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t \\] \\[ \\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t \\]"},{"location":"user-guide/concepts/diffusion-explained/#the-reverse-diffusion-process","title":"The Reverse Diffusion Process","text":"<p>The reverse process learns to invert the forward diffusion, starting from noise \\(x_T \\sim \\mathcal{N}(0, \\mathbf{I})\\) and progressively denoising to data \\(x_0\\):</p> \\[ p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)) \\] <p>The complete generative process:</p> \\[ p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} | x_t) \\] <pre><code>graph RL\n    F[\"x\u209c&lt;br/&gt;(Pure Noise)\"] --&gt;|\"Neural Network&lt;br/&gt;Denoising\"| E[\"x\u209c\u208b\u2081\"]\n    E --&gt;|\"Denoise\"| D[\"...\"]\n    D --&gt;|\"Denoise\"| C[\"x\u2082\"]\n    C --&gt;|\"Denoise\"| B[\"x\u2081\"]\n    B --&gt;|\"Denoise\"| A[\"x\u2080&lt;br/&gt;(Generated Data)\"]\n\n    style F fill:#ffccc9\n    style A fill:#c8e6c9\n    style E fill:#fff3e0</code></pre> <p>Three Equivalent Parameterizations:</p> <p>Noise Prediction (most common): The network predicts the noise \\(\\epsilon\\) that was added:</p> \\[ \\hat{\\epsilon} = \\epsilon_\\theta(x_t, t) \\] \\[ \\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) \\] <p>Data Prediction: The network directly predicts the clean image:</p> \\[ \\hat{x}_0 = f_\\theta(x_t, t) \\] <p>Score Prediction: The network predicts the gradient of the log probability:</p> \\[ s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t) \\] <p>These are mathematically equivalent through Tweedie's formula:</p> \\[ \\nabla_{x_t} \\log q(x_t) = -\\frac{\\epsilon}{\\sqrt{1-\\bar{\\alpha}_t}} = \\frac{\\sqrt{\\bar{\\alpha}_t}\\, x_0 - x_t}{1-\\bar{\\alpha}_t} \\] <p>Mathematical Equivalence</p> <p>Predicting noise is equivalent to predicting the score function, unifying diffusion models with score-based generative modeling. This connection reveals deep theoretical relationships between different approaches.</p>"},{"location":"user-guide/concepts/diffusion-explained/#the-elbo-derivation","title":"The ELBO Derivation","text":"<p>Diffusion models are Markovian hierarchical VAEs. The evidence lower bound decomposes as:</p> \\[ \\log p(x_0) \\geq \\mathbb{E}_q[\\log p_\\theta(x_0|x_1)] - D_{KL}(q(x_T|x_0) \\| p(x_T)) - \\sum_{t=2}^T \\mathbb{E}_{q(x_0)} D_{KL}(q(x_{t-1}|x_t,x_0) \\| p_\\theta(x_{t-1}|x_t)) \\] <p>For Gaussian posteriors, the KL divergence terms simplify. The key loss term becomes:</p> \\[ L_t = \\mathbb{E}_{q(x_0,x_t)}\\left[\\frac{1}{2\\sigma_t^2} \\|\\tilde{\\mu}_t(x_t,x_0) - \\mu_\\theta(x_t,t)\\|^2\\right] \\] <p>Substituting the reparameterization yields:</p> \\[ L_t \\propto \\mathbb{E}_{x_0,\\epsilon}\\left[\\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\, \\epsilon, t)\\|^2\\right] \\] <p>Ho et al.'s key empirical finding: The simplified objective works better:</p> \\[ L_{\\text{simple}} = \\mathbb{E}_{t \\sim U[1,T], \\, x_0, \\, \\epsilon \\sim \\mathcal{N}(0,\\mathbf{I})}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right] \\] <p>This reduces training to simple mean-squared error between predicted and actual noise!</p>"},{"location":"user-guide/concepts/diffusion-explained/#variance-schedules","title":"Variance Schedules","text":"<p>The noise schedule \\(\\{\\beta_1, \\ldots, \\beta_T\\}\\) fundamentally affects training and sampling quality:</p> <p>Linear Schedule (Ho et al. 2020):</p> \\[ \\beta_t = \\beta_1 + (\\beta_T - \\beta_1) \\cdot \\frac{t-1}{T-1} \\] <p>Typically \\(\\beta_1 = 0.0001\\), \\(\\beta_T = 0.02\\). Simple but can add too much noise early.</p> <p>Cosine Schedule (Nichol &amp; Dhariwal 2021):</p> \\[ \\bar{\\alpha}_t = \\frac{f(t)}{f(0)} \\quad \\text{where} \\quad f(t) = \\cos^2\\left(\\frac{t/T + s}{1+s} \\cdot \\frac{\\pi}{2}\\right) \\] \\[ \\beta_t = 1 - \\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}} \\] <p>with \\(s = 0.008\\). Provides smoother transitions and empirically superior performance.</p> <p>Schedule Selection</p> <p>The cosine schedule has become the de facto standard due to its superior empirical performance. It provides more balanced denoising across timesteps and avoids adding excessive noise in early steps.</p>"},{"location":"user-guide/concepts/diffusion-explained/#score-based-perspective","title":"Score-Based Perspective","text":"<p>The score function \\(\\nabla_x \\log p(x)\\) points toward regions of higher probability density. Score-based models train a network \\(s_\\theta(x, t)\\) to approximate this gradient field through denoising score matching:</p> \\[ \\mathcal{L}_{\\text{DSM}} = \\mathbb{E}_{p(x)}\\mathbb{E}_{p(\\tilde{x}|x)}\\left[\\|s_\\theta(\\tilde{x}, t) - \\nabla_{\\tilde{x}} \\log p(\\tilde{x}|x)\\|^2\\right] \\] <p>Given the learned score, generation proceeds via Langevin dynamics:</p> \\[ x_{i+1} = x_i + \\delta \\nabla_x \\log p(x) + \\sqrt{2\\delta} \\, z_i \\quad \\text{where } z_i \\sim \\mathcal{N}(0, \\mathbf{I}) \\] <p>The connection to diffusion: the score equals the negative scaled noise.</p>"},{"location":"user-guide/concepts/diffusion-explained/#stochastic-differential-equations","title":"Stochastic Differential Equations","text":"<p>The continuous-time formulation generalizes discrete diffusion as an SDE:</p> \\[ dx = f(x,t)dt + g(t)dw \\] <p>Variance Preserving (VP) SDE corresponds to DDPM:</p> \\[ dx = -\\frac{1}{2}\\beta(t)x \\, dt + \\sqrt{\\beta(t)} \\, dw \\] <p>The reverse-time SDE enables generation:</p> \\[ dx = \\left[f(x,t) - g(t)^2\\nabla_x \\log p_t(x)\\right] dt + g(t) d\\bar{w} \\] <p>There exists an equivalent probability flow ODE:</p> \\[ dx = \\left[f(x,t) - \\frac{1}{2}g(t)^2\\nabla_x \\log p_t(x)\\right] dt \\] <p>This ODE formulation enables exact likelihood computation and deterministic sampling.</p>"},{"location":"user-guide/concepts/diffusion-explained/#architecture-design","title":"Architecture Design","text":""},{"location":"user-guide/concepts/diffusion-explained/#u-net-backbone-with-skip-connections","title":"U-Net Backbone with Skip Connections","text":"<p>The U-Net architecture dominates diffusion models through its encoder-decoder structure with skip connections:</p> <pre><code>graph TB\n    subgraph \"Encoder (Downsampling)\"\n        A[\"Input Image&lt;br/&gt;256\u00d7256\u00d73\"] --&gt; B[\"Conv + ResBlock&lt;br/&gt;128\u00d7128\u00d7128\"]\n        B --&gt; C[\"Conv + ResBlock&lt;br/&gt;64\u00d764\u00d7256\"]\n        C --&gt; D[\"Conv + ResBlock&lt;br/&gt;32\u00d732\u00d7512\"]\n        D --&gt; E[\"Bottleneck&lt;br/&gt;16\u00d716\u00d71024\"]\n    end\n\n    subgraph \"Decoder (Upsampling)\"\n        E --&gt; F[\"Upsample + ResBlock&lt;br/&gt;32\u00d732\u00d7512\"]\n        F --&gt; G[\"Upsample + ResBlock&lt;br/&gt;64\u00d764\u00d7256\"]\n        G --&gt; H[\"Upsample + ResBlock&lt;br/&gt;128\u00d7128\u00d7128\"]\n        H --&gt; I[\"Output&lt;br/&gt;256\u00d7256\u00d73\"]\n    end\n\n    D -.-&gt;|\"Skip Connection\"| F\n    C -.-&gt;|\"Skip Connection\"| G\n    B -.-&gt;|\"Skip Connection\"| H\n\n    style E fill:#fff3e0\n    style A fill:#e1f5ff\n    style I fill:#c8e6c9</code></pre> <p>Key Components:</p> <ul> <li>Contracting path: Progressive downsampling (256\u2192128\u219264\u219232\u219216\u21928) while increasing channels</li> <li>Expanding path: Upsampling reconstructs output at original resolution</li> <li>Skip connections: Critical for propagating spatial details lost in bottleneck</li> <li>ResNet blocks: \\(\\text{output} = \\text{input} + F(\\text{input}, \\text{time\\_emb})\\)</li> <li>Group normalization: Dividing channels into groups (~32) for stability</li> </ul> <p>Why U-Net for Diffusion?</p> <ol> <li>Input and output have identical dimensions (essential for iterative refinement)</li> <li>Skip connections preserve fine details through bottleneck</li> <li>Multi-scale processing captures both coarse structure and fine texture</li> <li>No information bottleneck\u2014maintains full spatial information</li> </ol>"},{"location":"user-guide/concepts/diffusion-explained/#time-embedding-through-sinusoidal-encoding","title":"Time Embedding Through Sinusoidal Encoding","text":"<p>Timestep information must flow through the network since denoising behavior depends critically on noise level. Sinusoidal position embeddings provide the standard approach:</p> \\[ PE(t, 2i) = \\sin\\left(\\frac{t}{10000^{2i/d}}\\right) \\] \\[ PE(t, 2i+1) = \\cos\\left(\\frac{t}{10000^{2i/d}}\\right) \\] <p>where \\(t\\) is timestep, \\(i\\) is dimension index, and \\(d\\) is embedding dimension (typically 128-256).</p> <p>Implementation:</p> <ol> <li>Embed timestep \\(t\\) into 128-256 dimensions via sinusoidal encoding</li> <li>Pass through 2-layer MLP projecting to model dimension</li> <li>Inject via FiLM (Feature-wise Linear Modulation):</li> </ol> \\[ \\text{output} = \\gamma(t) \\odot \\text{features} + \\beta(t) \\] <p>where \\(\\gamma\\) and \\(\\beta\\) are learned functions of time embedding.</p> <p>Time Conditioning</p> <p>FiLM is the most effective injection method, allowing the network to adaptively modulate its processing for each noise level through learned affine transformations.</p>"},{"location":"user-guide/concepts/diffusion-explained/#attention-mechanisms","title":"Attention Mechanisms","text":"<p>Self-Attention captures long-range spatial dependencies:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>Applied selectively due to \\(O(N^2)\\) complexity where \\(N = H \\times W\\):</p> <ul> <li>Lower resolutions (16\u00d716, 8\u00d78): Apply multi-head self-attention</li> <li>Higher resolutions (32\u00d732, 64\u00d764): Skip attention (too expensive)</li> </ul> <p>Cross-Attention enables conditioning on external information (e.g., text):</p> <ul> <li>Queries \\(Q\\): From image features</li> <li>Keys \\(K\\) and Values \\(V\\): From conditioning signal (CLIP/T5 text embeddings)</li> </ul> <p>Different image regions attend to relevant text parts, enabling fine-grained control. Used extensively in Stable Diffusion.</p> <pre><code>graph LR\n    subgraph \"Image Processing\"\n        A[\"Image Features\"] --&gt; B[\"Query Q\"]\n    end\n\n    subgraph \"Text Conditioning\"\n        C[\"Text Embeddings&lt;br/&gt;(CLIP/T5)\"] --&gt; D[\"Keys K\"]\n        C --&gt; E[\"Values V\"]\n    end\n\n    B --&gt; F[\"Cross-Attention\"]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[\"Conditioned&lt;br/&gt;Features\"]\n\n    style A fill:#e1f5ff\n    style C fill:#fff9c4\n    style G fill:#c8e6c9</code></pre>"},{"location":"user-guide/concepts/diffusion-explained/#model-parameterization-choices","title":"Model Parameterization Choices","text":"<p>\u03b5-Prediction (Noise Prediction) \u2014 Most Common:</p> <p>Network predicts \\(\\epsilon_\\theta(x_t, t) = \\epsilon\\) where \\(x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\, \\epsilon\\)</p> <ul> <li>Loss: \\(L_{\\text{simple}} = \\mathbb{E}[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2]\\)</li> <li>Advantages: Most stable, used by DDPM, Stable Diffusion, most implementations</li> </ul> <p>x\u2080-Prediction (Data Prediction):</p> <p>Network directly predicts clean image \\(\\hat{x}_\\theta(x_t, t) = x_0\\)</p> <ul> <li>Advantages: Better quality in some cases</li> <li>Disadvantages: More prone to out-of-distribution predictions requiring clipping</li> </ul> <p>v-Prediction (Velocity Prediction):</p> <p>Predicts velocity \\(v_t = \\sqrt{\\bar{\\alpha}_t}\\, \\epsilon - \\sqrt{1 - \\bar{\\alpha}_t}\\, x_0\\)</p> <ul> <li>Loss: \\(L = \\mathbb{E}[\\|v_t - v_\\theta(x_t, t)\\|^2]\\)</li> <li>Advantages: More balanced learning across noise levels, better numerical stability</li> <li>Usage: Imagen, Google models</li> </ul> <p>Equivalence</p> <p>All three parameterizations are mathematically equivalent and can be converted between each other. Most implementations default to \u03b5-prediction, though v-prediction is gaining popularity.</p>"},{"location":"user-guide/concepts/diffusion-explained/#training-process","title":"Training Process","text":""},{"location":"user-guide/concepts/diffusion-explained/#the-simplified-training-objective","title":"The Simplified Training Objective","text":"<p>The simplified loss ignores theoretical weightings from ELBO:</p> \\[ L_{\\text{simple}} = \\mathbb{E}_{t \\sim U[1,T], \\, x_0 \\sim q(x_0), \\, \\epsilon \\sim \\mathcal{N}(0,\\mathbf{I})}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right] \\] <p>Training Algorithm:</p> <ol> <li>Sample training image \\(x_0 \\sim q(x_0)\\)</li> <li>Sample timestep \\(t \\sim \\text{Uniform}(1, T)\\)</li> <li>Sample noise \\(\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})\\)</li> <li>Compute noisy image \\(x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\, \\epsilon\\)</li> <li>Predict noise \\(\\hat{\\epsilon} = \\epsilon_\\theta(x_t, t)\\)</li> <li>Compute loss \\(L = \\|\\epsilon - \\hat{\\epsilon}\\|^2\\)</li> <li>Update \\(\\theta\\) via gradient descent</li> </ol> <p>Remarkably simple: Just MSE between predicted and actual noise!</p>"},{"location":"user-guide/concepts/diffusion-explained/#loss-function-variants","title":"Loss Function Variants","text":"<p>Variational Lower Bound (VLB):</p> <p>The full ELBO includes weighted terms for each timestep. While theoretically principled, optimizing full VLB is harder in practice.</p> <p>Hybrid Objective (Nichol &amp; Dhariwal 2021):</p> <p>Combines \\(L_{\\text{simple}}\\) for mean prediction with VLB terms for variance learning:</p> \\[ L_{\\text{hybrid}} = L_{\\text{simple}} + \\lambda L_{\\text{vlb}} \\] <p>Min-SNR-\u03b3 Weighting:</p> <p>Clips weights at \\(w_t = \\min(\\text{SNR}(t), \\gamma)\\) where \\(\\text{SNR}(t) = \\bar{\\alpha}_t / (1-\\bar{\\alpha}_t)\\):</p> \\[ L_{\\text{min-SNR}} = \\mathbb{E}\\left[\\min(\\text{SNR}(t), \\gamma) \\cdot \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right] \\] <p>Typical \\(\\gamma = 5\\). Achieves 3.4\u00d7 faster convergence by preventing over-weighting easy steps.</p>"},{"location":"user-guide/concepts/diffusion-explained/#training-stability-and-best-practices","title":"Training Stability and Best Practices","text":"<p>Essential Training Practices</p> <p>Exponential Moving Average (EMA): Critical for quality. Maintain running average:</p> \\[ \\theta_{\\text{ema}} = \\beta \\, \\theta_{\\text{ema}} + (1-\\beta) \\, \\theta \\] <p>with \\(\\beta = 0.9999\\). Always use EMA weights for inference, not raw training weights.</p> <p>Gradient Clipping: Prevents exploding gradients. Clip gradient norms to 1.0.</p> <p>Mixed Precision Training: FP16/BF16 provides 2-3\u00d7 speedup, 40-50% memory reduction.</p> <p>Normalization:</p> <ul> <li>Group Normalization: Divide channels into groups (~32) for stability</li> <li>Layer Normalization: Alternative for transformer-based models</li> <li>No Batch Normalization: Batch statistics interfere with noise conditioning</li> </ul> <p>Regularization:</p> <ul> <li>Weight Decay: \\(10^{-4}\\) to \\(10^{-6}\\) with AdamW optimizer</li> <li>Dropout: Sometimes used (rate 0.1-0.3) but less common than in other architectures</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#hyperparameter-selection","title":"Hyperparameter Selection","text":"<p>Timesteps: \\(T = 1000\\) is standard for training. More steps provide finer granularity but slower sampling.</p> <p>Noise Schedules:</p> <ul> <li>Cosine schedule outperforms linear empirically</li> <li>Critical: Ensure \\(\\bar{\\alpha}_T \\approx 0\\) for pure noise at final step</li> </ul> <p>Learning Rates:</p> <ul> <li>Standard: \\(1 \\times 10^{-4}\\) to \\(2 \\times 10^{-4}\\) with AdamW</li> <li>Sensitive domains (faces): \\(1 \\times 10^{-6}\\) to \\(2 \\times 10^{-6}\\)</li> <li>Use linear warmup over 500-1000 steps</li> </ul> <p>Batch Sizes:</p> <ul> <li>Small images (32\u00d732): 128-512</li> <li>Medium (256\u00d7256): 32-128</li> <li>Large (512\u00d7512): 8-32</li> <li>Use gradient accumulation to simulate larger batches</li> </ul> <p>Optimizer Configuration:</p> <pre><code>optimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-4,\n    betas=(0.9, 0.999),\n    weight_decay=1e-4\n)\n</code></pre>"},{"location":"user-guide/concepts/diffusion-explained/#training-dynamics-and-monitoring","title":"Training Dynamics and Monitoring","text":"<p>Common Training Issues</p> <p>Loss Plateaus: Normal behavior\u2014loss doesn't directly correlate with quality. Monitor visual samples!</p> <p>NaN Losses: Usually from exploding gradients. Enable gradient clipping and mixed precision loss scaling.</p> <p>Poor Sample Quality: Check EMA is enabled, noise schedule is correct, sufficient training steps completed.</p> <p>What to Monitor:</p> <ol> <li>Training Loss: Should decrease initially, then plateau</li> <li>Visual Samples: Generate every 5k-10k steps at fixed noise seeds</li> <li>FID Score: Compute on validation set every 25k-50k steps</li> <li>Gradient Norms: Should be stable, not exploding</li> <li>Learning Rate: Track warmup and decay schedules</li> </ol> <p>Checkpoint Management:</p> <ul> <li>Save both regular and EMA weights</li> <li>Keep checkpoints every 50k-100k steps</li> <li>Save best checkpoint based on FID score</li> <li>Include optimizer state for resuming training</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#computational-requirements","title":"Computational Requirements","text":"<p>GPU Requirements:</p> <ul> <li>Minimum: 10GB VRAM (RTX 3080)</li> <li>Recommended: 24GB VRAM (RTX 3090/4090)</li> <li>Large-scale: 40-80GB (A100/H100)</li> </ul> <p>Training Times:</p> <ul> <li>Small datasets (10k images): Days on single GPU</li> <li>Medium (100k images): Weeks on multiple GPUs</li> <li>Large-scale (millions): Months on hundreds of GPUs</li> <li>ImageNet 256\u00d7256 on 8\u00d7 A100: 7-14 days</li> </ul> <p>Memory Optimizations:</p> <ul> <li>Gradient Checkpointing: 30-50% memory reduction, 20% slowdown</li> <li>Mixed Precision: 40-50% memory reduction, 2-3\u00d7 speedup</li> <li>Smaller Batch Sizes: Use gradient accumulation to maintain effective batch size</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#sampling-methods","title":"Sampling Methods","text":""},{"location":"user-guide/concepts/diffusion-explained/#ddpm-sampling-the-iterative-reverse-process","title":"DDPM Sampling: The Iterative Reverse Process","text":"<p>The foundational DDPM sampling starts from pure noise \\(x_T \\sim \\mathcal{N}(0, \\mathbf{I})\\) and iteratively denoises:</p> \\[ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z \\] <p>where \\(z \\sim \\mathcal{N}(0, \\mathbf{I})\\) and \\(\\sigma_t\\) controls stochasticity.</p> <p>Algorithm:</p> <ol> <li>Sample \\(x_T \\sim \\mathcal{N}(0, \\mathbf{I})\\)</li> <li>For \\(t = T, T-1, \\ldots, 1\\):</li> <li>Predict noise: \\(\\hat{\\epsilon} = \\epsilon_\\theta(x_t, t)\\)</li> <li>Compute mean: \\(\\mu_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\hat{\\epsilon} \\right)\\)</li> <li>Sample: \\(x_{t-1} = \\mu_t + \\sigma_t z\\)</li> <li>Return \\(x_0\\)</li> </ol> <p>Characteristics:</p> <ul> <li>Stochastic: Introduces randomness at each step</li> <li>Slow: Requires \\(T=1000\\) neural network evaluations (seconds to minutes)</li> <li>High Quality: Excellent sample quality with sufficient steps</li> <li>1000-2000\u00d7 slower than single-pass generators like GANs</li> </ul> <pre><code>graph LR\n    A[\"x_T&lt;br/&gt;Pure Noise\"] --&gt;|\"Denoise Step T\"| B[\"x_{T-1}\"]\n    B --&gt;|\"Denoise Step T-1\"| C[\"x_{T-2}\"]\n    C --&gt;|\"...\"| D[\"x_t\"]\n    D --&gt;|\"...\"| E[\"x_1\"]\n    E --&gt;|\"Final Denoise\"| F[\"x_0&lt;br/&gt;Generated Image\"]\n\n    style A fill:#ffccc9\n    style F fill:#c8e6c9\n    style D fill:#fff3e0</code></pre>"},{"location":"user-guide/concepts/diffusion-explained/#ddim-fast-deterministic-sampling","title":"DDIM: Fast Deterministic Sampling","text":"<p>Paper: \"Denoising Diffusion Implicit Models\" (Song et al., 2021)</p> <p>DDIM constructs non-Markovian forward processes sharing DDPM's marginals but enabling much larger reverse steps:</p> \\[ x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\underbrace{\\left(\\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}}\\right)}_{\\text{\"predicted } x_0\\text{\"}} + \\underbrace{\\sqrt{1-\\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\epsilon_\\theta(x_t, t)}_{\\text{\"direction pointing to } x_t\\text{\"}} + \\underbrace{\\sigma_t \\epsilon}_{\\text{random noise}} \\] <p>When \\(\\sigma_t = 0\\), sampling becomes fully deterministic.</p> <p>Key Advantages:</p> <ul> <li>10-50\u00d7 speedup: Reduces from 1000 steps to 50-100 steps</li> <li>No retraining: Works with any pre-trained DDPM checkpoint</li> <li>Deterministic: When \\(\\eta = 0\\), enables consistent reconstructions</li> <li>Interpolation: Meaningful latent space interpolation</li> </ul> <p>Algorithm (Deterministic \\(\\sigma_t = 0\\)):</p> <ol> <li>Sample \\(x_T \\sim \\mathcal{N}(0, \\mathbf{I})\\)</li> <li>Choose subset of timesteps \\(\\{\\tau_1, \\tau_2, \\ldots, \\tau_S\\}\\) where \\(S \\ll T\\)</li> <li>For \\(i = S, S-1, \\ldots, 1\\):</li> <li>Predict \\(x_0\\): \\(\\hat{x}_0 = \\frac{x_{\\tau_i} - \\sqrt{1-\\bar{\\alpha}_{\\tau_i}} \\epsilon_\\theta(x_{\\tau_i}, \\tau_i)}{\\sqrt{\\bar{\\alpha}_{\\tau_i}}}\\)</li> <li>Compute \\(x_{\\tau_{i-1}} = \\sqrt{\\bar{\\alpha}_{\\tau_{i-1}}} \\hat{x}_0 + \\sqrt{1-\\bar{\\alpha}_{\\tau_{i-1}}} \\epsilon_\\theta(x_{\\tau_i}, \\tau_i)\\)</li> <li>Return \\(x_0\\)</li> </ol> <p>DDIM in Practice</p> <p>DDIM became the standard inference method for production systems. Stable Diffusion defaults to 50 DDIM steps for a good quality/speed trade-off. Fewer steps (20-25) work for quick previews.</p>"},{"location":"user-guide/concepts/diffusion-explained/#advanced-ode-solvers","title":"Advanced ODE Solvers","text":"<p>DPM-Solver (2022):</p> <p>Treats diffusion sampling as solving ODEs with specialized numerical methods:</p> <ul> <li>Higher-order solver (order 2-3) with convergence guarantees</li> <li>Achieves FID 4.70 in 10 steps, 2.87 in 20 steps on CIFAR-10</li> <li>4-16\u00d7 speedup over previous samplers</li> </ul> <p>DPM-Solver++ (2023):</p> <p>Addresses instability with large classifier-free guidance scales:</p> <ul> <li>Uses data prediction with dynamic thresholding</li> <li>Performs well with 15-20 steps for guided sampling</li> <li>Better numerical stability than DPM-Solver</li> </ul> <p>PNDM (Pseudo Numerical Methods):</p> <p>Treats DDPMs as solving differential equations on manifolds:</p> <ul> <li>Generates higher quality with 50 steps than 1000-step DDIM</li> <li>20\u00d7 speedup with quality improvement</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#consistency-models-one-step-generation","title":"Consistency Models: One-Step Generation","text":"<p>Papers:</p> <ul> <li>\"Consistency Models\" (Song et al., 2023)</li> <li>\"Improved Techniques for Training Consistency Models\" (Song et al., 2023)</li> </ul> <p>Paradigm shift: Learn a consistency function \\(f\\) that directly maps any point on a trajectory to its endpoint:</p> \\[ f(x_t, t) = x_0 \\quad \\text{for all } t \\] <p>The self-consistency property:</p> \\[ f(x_t, t) = f(x_s, s) \\quad \\text{for all } s, t \\] <p>Consistency Distillation:</p> <p>Train by distilling from pre-trained diffusion model:</p> \\[ \\mathcal{L}_{\\text{CD}} = \\mathbb{E}\\left[d(f_\\theta(x_{t_{n+1}}, t_{n+1}), f_{\\theta^-}(\\hat{x}_{t_n}^\\phi, t_n))\\right] \\] <p>where \\(\\hat{x}_{t_n}^\\phi\\) is one step of ODE solver from \\(x_{t_{n+1}}\\) using teacher model.</p> <p>Results:</p> <ul> <li>FID 3.55 on CIFAR-10 in one step</li> <li>Improved techniques: FID 2.51 in one step, 2.24 in two steps</li> <li>Consistency Trajectory Models (CTM): FID 1.73 in one step</li> <li>Zero-shot editing without task-specific training</li> </ul> <p>Revolutionary Speed</p> <p>Consistency models achieve 1000\u00d7 speedup over DDPM while maintaining competitive quality. This makes diffusion viable for real-time applications.</p>"},{"location":"user-guide/concepts/diffusion-explained/#guidance-techniques","title":"Guidance Techniques","text":"<p>Classifier Guidance:</p> <p>Modifies the score using gradients from a separately trained classifier:</p> \\[ \\tilde{\\epsilon}_\\theta(x_t, c) = \\epsilon_\\theta(x_t, c) - w \\cdot \\sigma_t \\cdot \\nabla_{x_t} \\log p_\\phi(c|x_t) \\] <p>where \\(w\\) is guidance scale, \\(c\\) is class label.</p> <p>Advantages: State-of-the-art results when well-tuned Disadvantages: Requires training noise-aware classifiers at all noise levels</p> <p>Classifier-Free Guidance (Ho &amp; Salimans, 2022):</p> <p>Eliminates the classifier by jointly training conditional and unconditional models:</p> \\[ \\tilde{\\epsilon}_\\theta(x_t, c) = (1 + w) \\cdot \\epsilon_\\theta(x_t, c) - w \\cdot \\epsilon_\\theta(x_t, \\emptyset) \\] <p>During training, randomly drop condition \\(c\\) with probability ~10%.</p> <p>Advantages:</p> <ul> <li>No auxiliary classifier needed</li> <li>Often better quality than classifier guidance</li> <li>Single guidance scale \\(w\\) controls trade-off</li> <li>Industry standard: Used by DALL-E 2, Stable Diffusion, Midjourney, Imagen</li> </ul> <p>Common Guidance Scales:</p> <ul> <li>\\(w = 1.0\\): No guidance (unconditional)</li> <li>\\(w = 3-5\\): Moderate guidance, balanced quality/diversity</li> <li>\\(w = 7-8\\): Standard guidance (Stable Diffusion default: 7.5)</li> <li>\\(w = 10-15\\): Strong guidance, high fidelity but lower diversity</li> <li>\\(w &gt; 20\\): Over-guided, saturated colors, artifacts</li> </ul> <pre><code>graph LR\n    A[\"Conditional&lt;br/&gt;\u03b5(x_t, c)\"] --&gt; C[\"Guidance&lt;br/&gt;Interpolation\"]\n    B[\"Unconditional&lt;br/&gt;\u03b5(x_t, \u2205)\"] --&gt; C\n    C --&gt; D[\"Guided Noise&lt;br/&gt;Prediction\"]\n    D --&gt; E[\"Denoising&lt;br/&gt;Step\"]\n\n    style A fill:#e1f5ff\n    style B fill:#ffccc9\n    style D fill:#c8e6c9</code></pre>"},{"location":"user-guide/concepts/diffusion-explained/#diffusion-model-variants","title":"Diffusion Model Variants","text":""},{"location":"user-guide/concepts/diffusion-explained/#latent-diffusion-models-ldm-stable-diffusion","title":"Latent Diffusion Models (LDM / Stable Diffusion)","text":"<p>Paper: \"High-Resolution Image Synthesis with Latent Diffusion Models\" (Rombach et al., 2022)</p> <p>Revolutionary insight: Run diffusion in VAE latent space instead of pixel space.</p> <p>Two-Stage Approach:</p> <ol> <li>Train autoencoder compressing images 8\u00d7 (512\u00d7512\u00d73 \u2192 64\u00d764\u00d74)</li> <li>Run diffusion in compressed latent space</li> </ol> <p>Architecture:</p> <pre><code>graph TB\n    A[\"Input Image&lt;br/&gt;512\u00d7512\u00d73\"] --&gt; B[\"VAE Encoder\"]\n    B --&gt; C[\"Latent Space&lt;br/&gt;64\u00d764\u00d74&lt;br/&gt;(8\u00d7 compression)\"]\n    C --&gt; D[\"Diffusion U-Net&lt;br/&gt;(with cross-attention)\"]\n    E[\"Text Prompt\"] --&gt; F[\"CLIP/T5&lt;br/&gt;Encoder\"]\n    F --&gt; D\n    D --&gt; G[\"Denoised Latent&lt;br/&gt;64\u00d764\u00d74\"]\n    G --&gt; H[\"VAE Decoder\"]\n    H --&gt; I[\"Generated Image&lt;br/&gt;512\u00d7512\u00d73\"]\n\n    style C fill:#fff3e0\n    style D fill:#e1f5ff\n    style I fill:#c8e6c9</code></pre> <p>Key Benefits:</p> <ul> <li>2.7\u00d7 training/inference speedup</li> <li>1.6\u00d7 FID improvement</li> <li>Massively reduced memory: ~10GB VRAM for 512\u00d7512 generation</li> <li>Cross-attention conditioning: Text embeddings guide generation</li> </ul> <p>Stable Diffusion Implementation:</p> <ul> <li>860M-parameter U-Net in latent space</li> <li>Trained on LAION-5B (5 billion text-image pairs)</li> <li>Open-source release democratized text-to-image generation</li> <li>Versions: 1.4, 1.5, 2.0, 2.1, SDXL (2.3B), SD3 (8B)</li> </ul> <p>SDXL (2023):</p> <ul> <li>2.3B parameters</li> <li>Dual text encoders (OpenCLIP + CLIP)</li> <li>Native 1024\u00d71024 resolution</li> <li>Two-stage: base model + refiner</li> </ul> <p>Stable Diffusion 3 (2024):</p> <ul> <li>Rectified Flow Transformer replacing U-Net</li> <li>Multimodal Diffusion Transformer</li> <li>Sizes: 800M, 2B, 8B parameters</li> <li>State-of-the-art text rendering in images</li> </ul> <p>Impact</p> <p>Latent diffusion made high-quality generation accessible. Stable Diffusion has 100M+ users and massive ecosystem of fine-tunes, LoRAs, and community tools.</p>"},{"location":"user-guide/concepts/diffusion-explained/#conditional-diffusion-models","title":"Conditional Diffusion Models","text":"<p>Class-Conditional Generation:</p> <p>Add class label \\(y\\) as conditioning:</p> \\[ p_\\theta(x_{t-1} | x_t, y) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t, y), \\Sigma_\\theta(x_t, t, y)) \\] <p>Typically implemented via:</p> <ul> <li>Class embeddings concatenated with time embeddings</li> <li>Conditional batch normalization: Modulate batch norm with class info</li> <li>Cross-attention: Attend to class token</li> </ul> <p>Text-to-Image Models:</p> <p>DALL-E 2 (OpenAI, 2022):</p> <ul> <li>Two-stage: CLIP prior diffusion + decoder</li> <li>CLIP prior maps text embeddings to image embeddings</li> <li>Decoder generates images from CLIP embeddings</li> <li>Up to 1024\u00d71024 resolution</li> </ul> <p>Imagen (Google, 2022):</p> <ul> <li>Frozen T5-XXL (4.6B parameters) as text encoder</li> <li>Cascaded diffusion: 64\u00d764 \u2192 256\u00d7256 \u2192 1024\u00d71024</li> <li>Key finding: Scaling text encoder improves quality more than scaling U-Net</li> <li>FID 7.27 on COCO (state-of-the-art at time)</li> <li>Not publicly released</li> </ul> <p>Midjourney (2022-2024):</p> <ul> <li>Proprietary diffusion model</li> <li>Exceptional aesthetic quality</li> <li>Versions V1 \u2192 V6 (2024)</li> <li>~15M users, multi-million dollar revenue</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#cascade-diffusion-models","title":"Cascade Diffusion Models","text":"<p>Generate through multiple resolution stages, each a separate diffusion model:</p> <ol> <li>Base model: Generate at 64\u00d764</li> <li>Super-resolution 1: Upscale to 256\u00d7256</li> <li>Super-resolution 2: Upscale to 1024\u00d71024</li> </ol> <p>Advantages:</p> <ul> <li>Each stage focuses on different detail scales</li> <li>More efficient than single high-resolution model</li> <li>Better quality through specialized models</li> </ul> <p>Disadvantages:</p> <ul> <li>Complexity of training multiple models</li> <li>Error accumulation across stages</li> </ul> <p>Used in DALL-E 2 and Imagen.</p>"},{"location":"user-guide/concepts/diffusion-explained/#video-diffusion-models","title":"Video Diffusion Models","text":"<p>Extend spatial generation to temporal dimension:</p> <p>3D U-Net Architecture:</p> <ul> <li>Spatial convolutions \u2192 Spatio-temporal convolutions</li> <li>Process video as Time \u00d7 Height \u00d7 Width tensor</li> <li>Temporal attention captures motion</li> </ul> <p>Sora (OpenAI, 2024):</p> <ul> <li>Diffusion Transformer on spacetime patches</li> <li>Videos as sequences of patches (like LLM tokens)</li> <li>Compressed into latent space via video VAE</li> <li>Up to 1 minute of 1080p video</li> <li>Variable aspect ratios</li> <li>Emergent physics understanding, object permanence</li> </ul> <p>Sora 2 (2025):</p> <ul> <li>Synchronized audio generation</li> <li>Improved physics simulation</li> <li>Multi-shot video with persistent world state</li> <li>Instruction-following for complex scenes</li> </ul> <p>Open-Source Video Models:</p> <ul> <li>CogVideoX: 2B and 5B parameter models</li> <li>HunyuanVideo: 7B and 14B parameters with scaling studies</li> <li>Stable Video Diffusion: Extension of Stable Diffusion</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#3d-diffusion-models","title":"3D Diffusion Models","text":"<p>DreamFusion (Google, 2022):</p> <p>Uses 2D text-to-image models as priors for 3D generation via Score Distillation Sampling (SDS):</p> <ol> <li>Initialize random 3D NeRF</li> <li>Render 2D views from random camera angles</li> <li>Apply noise and use Imagen to denoise</li> <li>Backpropagate through rendering to update NeRF</li> <li>Repeat</li> </ol> <p>Key Innovation: No 3D training data required\u2014leverages 2D diffusion models.</p> <p>Capabilities:</p> <ul> <li>Text-to-3D generation</li> <li>Viewable from any angle</li> <li>Relightable</li> <li>Exportable as meshes</li> </ul> <p>Stable-DreamFusion:</p> <ul> <li>Uses Stable Diffusion instead of Imagen</li> <li>Open-source implementation</li> <li>Enables text-to-3D and image-to-3D</li> </ul> <p>Applications: Game assets, VR/AR content, product design, 3D scene reconstruction</p>"},{"location":"user-guide/concepts/diffusion-explained/#comparison-with-other-generative-models","title":"Comparison with Other Generative Models","text":""},{"location":"user-guide/concepts/diffusion-explained/#diffusion-vs-gans","title":"Diffusion vs. GANs","text":"Aspect Diffusion Models GANs Sample Quality State-of-the-art (FID 1.81-7.72) High quality (FID ~2.97) Training Stability Very stable, straightforward MSE Unstable, adversarial balancing Mode Coverage Excellent, likelihood-based Prone to mode collapse Inference Speed Slow (25-1000 steps) Fast (1 forward pass) Training Ease Forgiving hyperparameters Requires careful tuning Controllability Excellent (guidance, editing) Limited, no natural framework Latent Space No explicit latent No encoder, implicit latent <p>Speed Comparison:</p> <ul> <li>GANs: ~0.01 seconds per image (single pass)</li> <li>DDPM: 10-60 seconds per image (1000 steps)</li> <li>DDIM: 1-5 seconds per image (50 steps)</li> <li>Consistency Models: 0.1-0.5 seconds per image (1-4 steps)</li> </ul> <p>When to Use Each</p> <p>Choose Diffusion: - Quality/diversity paramount - Training stability important - Denoising, super-resolution, inpainting tasks - Computational resources available</p> <p>Choose GANs: - Real-time generation required - Single-pass critical (e.g., video style transfer) - Limited inference compute - Interactive applications</p>"},{"location":"user-guide/concepts/diffusion-explained/#diffusion-vs-vaes","title":"Diffusion vs. VAEs","text":"Aspect Diffusion Models VAEs Sample Quality Sharp, high-fidelity Often blurry from MSE loss Latent Space No explicit low-dim latent Explicit interpretable latent Likelihood Tractable via ODE Explicit ELBO lower bound Training Straightforward, stable Straightforward, very stable Speed Slow (multi-step) Fast (single pass) Representation Implicit in noise trajectory Explicit learned encoding"},{"location":"user-guide/concepts/diffusion-explained/#hybrid-approach-latent-diffusion","title":"Hybrid Approach: Latent Diffusion","text":"<p>Combines strengths of both:</p> <ol> <li>VAE: Compresses images to latent space (8\u00d7 reduction)</li> <li>Diffusion: Operates in compressed latent space</li> </ol> <p>Results: 2.7\u00d7 speedup, better quality, significantly reduced memory.</p>"},{"location":"user-guide/concepts/diffusion-explained/#diffusion-vs-autoregressive-models","title":"Diffusion vs. Autoregressive Models","text":"Aspect Diffusion Autoregressive Speed Faster for long sequences (&gt;256 tokens) Faster for short (&lt;100 tokens) Quality Best for images/audio/video Best for text/language Parallelization Fully parallel generation Sequential (can't parallelize) Controllability Excellent (edit any step) Limited (left-to-right) Variable Length Fixed-length outputs Natural variable-length <p>Complementary Strengths:</p> <ul> <li>Diffusion: Images, audio, video, fixed-length sequences</li> <li>Autoregressive: Text, language, variable-length, sequential coherence</li> </ul> <p>Hybrid Models:</p> <p>HART (Hybrid Autoregressive Transformer):</p> <ul> <li>Autoregressive for coarse structure</li> <li>Diffusion for fine details</li> <li>9\u00d7 faster than pure diffusion</li> <li>31% less compute</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#advanced-topics","title":"Advanced Topics","text":""},{"location":"user-guide/concepts/diffusion-explained/#flow-matching-and-rectified-flow","title":"Flow Matching and Rectified Flow","text":"<p>Flow Matching (2022):</p> <p>Trains Continuous Normalizing Flows by regressing vector fields:</p> <ul> <li>Simulation-free training\u2014no ODE solving during training</li> <li>More stable dynamics than traditional diffusion</li> <li>Compatible with general Gaussian probability paths</li> <li>State-of-the-art ImageNet results</li> </ul> <p>Rectified Flow (2023):</p> <p>Learns ODEs following straight paths between noise and data:</p> \\[ \\frac{dx}{dt} = v_\\theta(x, t) \\] <p>where trajectories are straight lines from \\(x_0\\) to \\(x_1\\).</p> <p>Reflow Operation: Iteratively straightens trajectories:</p> <ol> <li>Sample pairs \\((x_0, x_1)\\) from data and noise</li> <li>Train model on straight paths</li> <li>Use trained model to generate new paired data</li> <li>Retrain on straightened paths</li> <li>Repeat</li> </ol> <p>Results:</p> <ul> <li>Can achieve high quality in 1-2 steps after reflow</li> <li>InstaFlow: Generates images in 0.12 seconds</li> <li>Used in Stable Diffusion 3 as core architecture</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#edm-elucidating-design-spaces","title":"EDM: Elucidating Design Spaces","text":"<p>Papers:</p> <ul> <li>\"Elucidating the Design Space\" (Karras et al., 2022)</li> <li>\"Analyzing and Improving Training Dynamics\" (Karras et al., 2024)</li> </ul> <p>EDM provides unified framework separating design choices:</p> <p>Optimal Preconditioning:</p> <p>Normalizing inputs/outputs of network for better training:</p> \\[ D_\\theta(x, \\sigma) = c_{\\text{skip}}(\\sigma) x + c_{\\text{out}}(\\sigma) F_\\theta(c_{\\text{in}}(\\sigma) x; c_{\\text{noise}}(\\sigma)) \\] <p>Karras Noise Schedule:</p> \\[ \\sigma(t) = \\sigma_{\\text{min}}^{1-t} \\cdot \\sigma_{\\text{max}}^t \\] <p>Widely adopted in practice, superior to linear/cosine.</p> <p>Results: FID 1.79 on CIFAR-10 (state-of-the-art)</p> <p>EDM2 (2024):</p> <ul> <li>Analyzed training dynamics at scale</li> <li>Redesigned architecture with magnitude preservation</li> <li>Post-hoc EMA: Set parameters after training without retraining</li> <li>FID 1.81 on ImageNet-512 (previous record: 2.41)</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#distillation-for-fast-sampling","title":"Distillation for Fast Sampling","text":"<p>Progressive Distillation (Salimans &amp; Ho, 2022):</p> <p>Iteratively halves the number of steps:</p> <ol> <li>Start with 8192-step teacher</li> <li>Train 4096-step student to match</li> <li>Train 2048-step student from previous student</li> <li>Continue halving to 4 steps</li> </ol> <p>Results: FID 3.0 on CIFAR-10 in 4 steps</p> <p>Distribution Matching Distillation:</p> <ul> <li>One-step FID 2.62 on ImageNet</li> <li>FID 11.49 on MS-COCO</li> <li>20 FPS generation</li> </ul> <p>Simple and Fast Distillation (NeurIPS 2024):</p> <ul> <li>1000\u00d7 faster fine-tuning</li> <li>FID 4.53 in 2 steps with only 0.64 hours on A100</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#diffusion-transformers-dit","title":"Diffusion Transformers (DiT)","text":"<p>Paper: \"Scalable Diffusion Models with Transformers\" (Peebles &amp; Xie, 2023)</p> <p>Replaces U-Net with transformer architecture:</p> <p>Architecture:</p> <ul> <li>Images as patch sequences (like ViT)</li> <li>Standard transformer blocks instead of U-Net</li> <li>Adaptive Layer Norm (adaLN-Zero) for conditioning:</li> </ul> \\[ \\text{LayerNorm}(x) \\cdot (1 + \\gamma(t)) + \\beta(t) \\] <p>Key Finding: Compute (Gflops) drives performance, not specific architecture.</p> <p>Results:</p> <ul> <li>DiT-XL/2: FID 2.27 on ImageNet 256\u00d7256</li> <li>Scales better than U-Nets to billions of parameters</li> <li>Influenced Sora and Stable Diffusion 3</li> </ul> <p>U-ViT (Bao et al., 2023):</p> <p>Combines ViT with U-Net principles:</p> <ul> <li>Token-based processing</li> <li>Long skip connections between encoder-decoder (crucial!)</li> <li>FID 2.29 on ImageNet 256\u00d7256</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#production-considerations","title":"Production Considerations","text":""},{"location":"user-guide/concepts/diffusion-explained/#deployment-challenges","title":"Deployment Challenges","text":"<p>Model Size:</p> <ul> <li>DDPM (256\u00d7256): ~200-500MB</li> <li>Stable Diffusion 1.5: 860M parameters (~3.5GB)</li> <li>SDXL: 2.3B parameters (~6.9GB)</li> <li>SD3: 800M to 8B parameters</li> </ul> <p>Optimization Strategies:</p> <ul> <li>Model Pruning: Remove 30-50% weights with &lt;5% quality loss</li> <li>Quantization: INT8/FP16 reduces size 2-4\u00d7 with minimal quality loss</li> <li>Knowledge Distillation: Train smaller student model</li> </ul> <p>Inference Optimization:</p> <ul> <li>ONNX Runtime: 10-30% speedup</li> <li>TensorRT: 2-5\u00d7 speedup on NVIDIA GPUs</li> <li>torch.compile(): 10-30% speedup with PyTorch 2.0+</li> <li>Flash Attention: 2-3\u00d7 speedup for attention layers</li> </ul> <p>Hardware Requirements:</p> <ul> <li>Inference: Minimum RTX 3060 (12GB) for 512\u00d7512</li> <li>Recommended: RTX 4090 (24GB) or professional GPUs</li> <li>Edge Deployment: Optimized models run on mobile (SD Turbo, LCM)</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#monitoring-and-quality-control","title":"Monitoring and Quality Control","text":"<p>Quality Drift:</p> <p>Monitor generated samples over time for:</p> <ul> <li>Artifacts or distortions</li> <li>Color shifts</li> <li>Mode collapse</li> <li>Prompt adherence degradation</li> </ul> <p>Metrics:</p> <ul> <li>FID: Track on validation set every N samples</li> <li>CLIP Score: For text-to-image, measure alignment</li> <li>Human Evaluation: A/B tests for subjective quality</li> <li>Diversity Metrics: Ensure mode coverage</li> </ul> <p>A/B Testing:</p> <p>Compare model versions using:</p> <ul> <li>FID/IS on held-out data</li> <li>Human preference studies (typically 1000+ comparisons)</li> <li>Production metrics (engagement, retention, quality reports)</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#ethical-considerations","title":"Ethical Considerations","text":"<p>Responsible Deployment</p> <p>Deepfakes and Misinformation:</p> <p>Diffusion models enable photorealistic fake images/videos. Mitigation strategies:</p> <ul> <li>Watermarking generated content</li> <li>Provenance tracking (C2PA metadata)</li> <li>Detection models for synthetic content</li> <li>Usage policies and terms of service</li> </ul> <p>Bias and Fairness:</p> <p>Models inherit biases from training data (LAION-5B, etc.):</p> <ul> <li>Underrepresentation of minorities</li> <li>Stereotypical associations</li> <li>Geographic/cultural biases</li> </ul> <p>Mitigation:</p> <ul> <li>Balanced training data curation</li> <li>Bias evaluation across demographics</li> <li>Red-teaming for harmful generations</li> </ul> <p>Copyright and Attribution:</p> <p>Training on copyrighted images raises questions:</p> <ul> <li>Fair use vs. infringement debates ongoing</li> <li>Artist consent and compensation</li> <li>Attribution for training data</li> </ul> <p>Best practices:</p> <ul> <li>Respect opt-out requests (Have I Been Trained)</li> <li>Consider ethical training data sources</li> <li>Transparent documentation of training data</li> </ul> <p>Environmental Impact:</p> <p>Large-scale training requires massive compute:</p> <ul> <li>ImageNet training: 1000s of GPU-hours</li> <li>Stable Diffusion: ~150,000 A100-hours</li> <li>SDXL: ~500,000 A100-hours</li> </ul> <p>Mitigation:</p> <ul> <li>Efficient architectures (Latent Diffusion)</li> <li>Distillation for deployment</li> <li>Carbon-aware training scheduling</li> <li>Renewable energy for data centers</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#safety-filters-and-content-moderation","title":"Safety Filters and Content Moderation","text":"<p>Safety Classifiers:</p> <p>Pre-deployment filters to prevent harmful content:</p> <ul> <li>NSFW Detection: Classify unsafe content</li> <li>Violence Detection: Flag graphic violence</li> <li>Hate Symbol Detection: Block extremist imagery</li> </ul> <p>Prompt Filtering:</p> <ul> <li>Block harmful prompt patterns</li> <li>Detect adversarial prompts</li> <li>Rate limiting for abuse prevention</li> </ul> <p>Post-Generation Filtering:</p> <ul> <li>Run safety classifier on outputs</li> <li>Block unsafe images before showing user</li> <li>Log violations for monitoring</li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>Diffusion models have revolutionized generative AI through an elegant approach: learning to reverse a gradual noising process. By systematically destroying data structure through fixed forward diffusion and learning the reverse process through neural networks, these models achieve state-of-the-art quality with remarkable training stability.</p> <p>Core Principles:</p> <ul> <li>Forward diffusion gradually corrupts data into pure noise over \\(T\\) timesteps</li> <li>Reverse diffusion learns to progressively denoise, reconstructing data from noise</li> <li>Training objective reduces to simple MSE between predicted and actual noise</li> <li>Sampling iteratively applies learned denoising, refining noise into data</li> </ul> <p>Key Variants:</p> <ul> <li>DDPM: Foundational stochastic sampling with 1000 steps</li> <li>DDIM: Deterministic fast sampling reducing to 50-100 steps</li> <li>Latent Diffusion: Operates in VAE latent space for 2.7\u00d7 speedup (Stable Diffusion)</li> <li>Consistency Models: One-step generation achieving 1000\u00d7 speedup</li> <li>Rectified Flow: Straight-line trajectories for efficient sampling (SD3)</li> </ul> <p>Architecture Innovations:</p> <ul> <li>U-Net backbone with skip connections preserving spatial details</li> <li>Sinusoidal time embeddings with FiLM conditioning</li> <li>Cross-attention enabling text-to-image and conditional generation</li> <li>Diffusion Transformers (DiT) scaling to billions of parameters</li> </ul> <p>Training Best Practices:</p> <ul> <li>Use cosine noise schedule for better dynamics</li> <li>Apply EMA with decay 0.9999\u2014critical for quality</li> <li>Gradient clipping and mixed precision for stability</li> <li>Monitor visual samples not just loss curves</li> <li>Min-SNR weighting for 3.4\u00d7 faster convergence</li> </ul> <p>Sampling Methods:</p> <ul> <li>DDPM: 1000 steps, highest quality, slowest</li> <li>DDIM: 50-100 steps, 10-20\u00d7 speedup, deterministic</li> <li>DPM-Solver: 10-20 steps with ODE solvers</li> <li>Consistency Models: 1-4 steps, near real-time</li> </ul> <p>Guidance Techniques:</p> <ul> <li>Classifier-free guidance as industry standard</li> <li>Guidance scale \\(w=7-8\\) balances quality and diversity</li> <li>Higher \\(w\\) increases fidelity, reduces diversity</li> </ul> <p>When to Use Diffusion Models:</p> <ul> <li>Quality and diversity are paramount</li> <li>Denoising, super-resolution, inpainting, editing tasks</li> <li>Text-to-image, text-to-video generation</li> <li>Scientific applications (protein design, drug discovery)</li> <li>Training stability more important than inference speed</li> </ul> <p>Current Landscape (2025):</p> <ul> <li>Diffusion dominates high-quality image generation</li> <li>Speed improvements enable near real-time generation</li> <li>Multimodal models handle text, image, video, 3D, audio</li> <li>Open-source ecosystem (Stable Diffusion) democratizes access</li> <li>Proprietary leaders (Midjourney, DALL-E) push quality boundaries</li> </ul> <p>Future Directions:</p> <ul> <li>Further sampling efficiency improvements</li> <li>Unified architectures for multiple modalities</li> <li>Better theoretical understanding of training dynamics</li> <li>Edge deployment and mobile generation</li> <li>Scientific applications in biology, chemistry, materials science</li> </ul> <p>Diffusion models represent a paradigm shift in generative modeling, offering unmatched quality, stability, and controllability. As architectures scale and sampling becomes more efficient, they will likely remain dominant for visual generation while expanding into new modalities and applications.</p>"},{"location":"user-guide/concepts/diffusion-explained/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Diffusion User Guide</p> <p>Practical usage guide with implementation examples and training workflows</p> </li> <li> <p> Diffusion API Reference</p> <p>Complete API documentation for DDPM, DDIM, Latent Diffusion, and variants</p> </li> <li> <p> MNIST Tutorial</p> <p>Step-by-step hands-on tutorial: train a diffusion model on MNIST from scratch</p> </li> <li> <p> Advanced Examples</p> <p>Explore Stable Diffusion, video diffusion, and state-of-the-art architectures</p> </li> </ul>"},{"location":"user-guide/concepts/diffusion-explained/#further-reading","title":"Further Reading","text":""},{"location":"user-guide/concepts/diffusion-explained/#seminal-papers-must-read","title":"Seminal Papers (Must Read)","text":"<p> Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., &amp; Ganguli, S. (2015). \"Deep Unsupervised Learning using Nonequilibrium Thermodynamics\" arXiv:1503.03585 | ICML 2015  The foundational paper introducing diffusion probabilistic models</p> <p> Ho, J., Jain, A., &amp; Abbeel, P. (2020). \"Denoising Diffusion Probabilistic Models\" arXiv:2006.11239 | NeurIPS 2020  DDPM: Made diffusion models practical with simplified training objective</p> <p> Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021). \"Score-Based Generative Modeling through Stochastic Differential Equations\" arXiv:2011.13456 | ICLR 2021 Outstanding Paper  Unified framework via SDEs, probability flow ODEs</p> <p> Song, J., Meng, C., &amp; Ermon, S. (2021). \"Denoising Diffusion Implicit Models\" arXiv:2010.02502 | ICLR 2021  DDIM: Fast deterministic sampling without retraining</p> <p> Nichol, A., &amp; Dhariwal, P. (2021). \"Improved Denoising Diffusion Probabilistic Models\" arXiv:2102.09672 | ICML 2021  Learned variances, cosine schedule, hybrid objective</p> <p> Dhariwal, P., &amp; Nichol, A. (2021). \"Diffusion Models Beat GANs on Image Synthesis\" arXiv:2105.05233 | NeurIPS 2021  Showed diffusion superiority, introduced classifier guidance</p> <p> Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). \"High-Resolution Image Synthesis with Latent Diffusion Models\" arXiv:2112.10752 | CVPR 2022  Latent diffusion / Stable Diffusion: 2.7\u00d7 speedup in VAE latent space</p>"},{"location":"user-guide/concepts/diffusion-explained/#tutorial-papers-and-surveys","title":"Tutorial Papers and Surveys","text":"<p> Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., ... &amp; Cui, B. (2023). \"Diffusion Models: A Comprehensive Survey of Methods and Applications\" arXiv:2209.00796  Comprehensive 150+ page survey covering theory and applications</p> <p> Cao, H., Tan, C., Gao, Z., Chen, G., Heng, P. A., &amp; Li, S. Z. (2023). \"A Survey on Generative Diffusion Models\" arXiv:2209.02646  Covers mathematical foundations, applications, and future directions</p> <p> Luo, C. (2022). \"Understanding Diffusion Models: A Unified Perspective\" arXiv:2208.11970  Excellent tutorial connecting ELBO, score matching, and SDEs</p>"},{"location":"user-guide/concepts/diffusion-explained/#important-variants-and-extensions","title":"Important Variants and Extensions","text":"<p> Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (2022). \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" arXiv:2204.06125  DALL-E 2: CLIP prior diffusion for text-to-image</p> <p> Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., ... &amp; Norouzi, M. (2022). \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\" arXiv:2205.11487 | NeurIPS 2022  Imagen: T5 text encoder with cascaded diffusion</p> <p> Ho, J., &amp; Salimans, T. (2022). \"Classifier-Free Diffusion Guidance\" arXiv:2207.12598  Industry-standard guidance without auxiliary classifiers</p> <p> Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M\u00fcller, J., ... &amp; Rombach, R. (2023). \"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\" arXiv:2307.01952  SDXL: 2.3B parameter upgrade to Stable Diffusion</p> <p> Esser, P., Kulal, S., Blattmann, A., Entezari, R., M\u00fcller, J., Saini, H., ... &amp; Rombach, R. (2024). \"Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\" arXiv:2403.03206  Stable Diffusion 3: Multimodal diffusion transformer</p>"},{"location":"user-guide/concepts/diffusion-explained/#sampling-and-acceleration","title":"Sampling and Acceleration","text":"<p> Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., &amp; Zhu, J. (2022). \"DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps\" arXiv:2206.00927 | NeurIPS 2022  10-20 step high-quality sampling via ODE solvers</p> <p> Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., &amp; Zhu, J. (2023). \"DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models\" arXiv:2211.01095  Improved stability for classifier-free guidance</p> <p> Liu, L., Ren, Y., Lin, Z., &amp; Zhao, Z. (2022). \"Pseudo Numerical Methods for Diffusion Models on Manifolds\" arXiv:2202.09778 | ICLR 2022  PNDM: 20\u00d7 speedup with quality improvement</p> <p> Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (2023). \"Consistency Models\" arXiv:2303.01469 | ICML 2023  One-step generation via consistency distillation</p> <p> Song, Y., Dhariwal, P., Chen, M., &amp; Sutskever, I. (2023). \"Improved Techniques for Training Consistency Models\" arXiv:2310.14189 | ICLR 2024 Oral  FID 2.51 in one step, 2.24 in two steps</p> <p> Salimans, T., &amp; Ho, J. (2022). \"Progressive Distillation for Fast Sampling of Diffusion Models\" arXiv:2202.00512 | ICLR 2022  Iteratively halve sampling steps through distillation</p>"},{"location":"user-guide/concepts/diffusion-explained/#architecture-innovations","title":"Architecture Innovations","text":"<p> Peebles, W., &amp; Xie, S. (2023). \"Scalable Diffusion Models with Transformers\" arXiv:2212.09748 | ICCV 2023  DiT: Replaces U-Net with transformer, scales to billions of parameters</p> <p> Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., &amp; Zhu, J. (2023). \"All are Worth Words: A ViT Backbone for Diffusion Models\" arXiv:2209.12152 | CVPR 2023  U-ViT: Combines ViT with U-Net skip connections</p> <p> Karras, T., Aittala, M., Aila, T., &amp; Laine, S. (2022). \"Elucidating the Design Space of Diffusion-Based Generative Models\" arXiv:2206.00364 | NeurIPS 2022  EDM: Unified framework, optimal preconditioning, FID 1.79</p> <p> Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., &amp; Laine, S. (2024). \"Analyzing and Improving the Training Dynamics of Diffusion Models\" arXiv:2312.02696 | CVPR 2024 Oral  EDM2: FID 1.81 on ImageNet-512, post-hoc EMA</p>"},{"location":"user-guide/concepts/diffusion-explained/#flow-matching-and-optimal-transport","title":"Flow Matching and Optimal Transport","text":"<p> Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., &amp; Le, M. (2023). \"Flow Matching for Generative Modeling\" arXiv:2210.02747 | ICLR 2023  Simulation-free training of continuous normalizing flows</p> <p> Liu, X., Gong, C., &amp; Liu, Q. (2023). \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\" arXiv:2209.03003 | ICLR 2023  Straight-line ODE paths, reflow operation</p> <p> Esser, P., Kulal, S., Blattmann, A., Entezari, R., M\u00fcller, J., Saini, H., ... &amp; Rombach, R. (2024). \"Scaling Rectified Flow Transformers for High-Resolution Image Synthesis\" arXiv:2403.03206  Stable Diffusion 3 technical report</p>"},{"location":"user-guide/concepts/diffusion-explained/#video-and-3d-generation","title":"Video and 3D Generation","text":"<p> Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., ... &amp; Salimans, T. (2022). \"Imagen Video: High Definition Video Generation with Diffusion Models\" arXiv:2210.02303  Cascaded video diffusion with temporal attention</p> <p> Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., ... &amp; Ramesh, A. (2024). \"Video Generation Models as World Simulators\" OpenAI Technical Report  Sora: Diffusion transformer on spacetime patches</p> <p> Poole, B., Jain, A., Barron, J. T., &amp; Mildenhall, B. (2022). \"DreamFusion: Text-to-3D using 2D Diffusion\" arXiv:2209.14988  Score Distillation Sampling for 3D generation</p> <p> Lin, C.-H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., ... &amp; Fidler, S. (2023). \"Magic3D: High-Resolution Text-to-3D Content Creation\" arXiv:2211.10440 | CVPR 2023  Two-stage coarse-to-fine 3D generation</p>"},{"location":"user-guide/concepts/diffusion-explained/#scientific-applications","title":"Scientific Applications","text":"<p> Watson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L., Yim, J., Eisenach, H. E., ... &amp; Baker, D. (2023). \"De novo design of protein structure and function with RFdiffusion\" Nature 620, 1089\u20131100  Protein design via diffusion achieving experimental validation</p> <p> Corso, G., St\u00e4rk, H., Jing, B., Barzilay, R., &amp; Jaakkola, T. (2022). \"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking\" arXiv:2210.01776 | ICLR 2023  Molecular docking as generative modeling</p> <p> Hoogeboom, E., Satorras, V. G., Vignac, C., &amp; Welling, M. (2022). \"Equivariant Diffusion for Molecule Generation in 3D\" arXiv:2203.17003 | ICML 2022  E(3)-equivariant diffusion for drug molecule design</p>"},{"location":"user-guide/concepts/diffusion-explained/#image-editing-and-control","title":"Image Editing and Control","text":"<p> Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., &amp; Van Gool, L. (2022). \"RePaint: Inpainting using Denoising Diffusion Probabilistic Models\" arXiv:2201.09865 | CVPR 2022  Mask-agnostic inpainting with pretrained models</p> <p> Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J. Y., &amp; Ermon, S. (2022). \"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations\" arXiv:2108.01073 | ICLR 2022  Edit images via noise addition and denoising</p> <p> Brooks, T., Holynski, A., &amp; Efros, A. A. (2023). \"InstructPix2Pix: Learning to Follow Image Editing Instructions\" arXiv:2211.09800 | CVPR 2023  Edit images from natural language instructions</p> <p> Zhang, L., Rao, A., &amp; Agrawala, M. (2023). \"Adding Conditional Control to Text-to-Image Diffusion Models\" arXiv:2302.05543 | ICCV 2023  ControlNet: Spatial conditioning with edges, depth, pose</p>"},{"location":"user-guide/concepts/diffusion-explained/#online-resources-and-code","title":"Online Resources and Code","text":"<p> Lilian Weng's Blog: \"What are Diffusion Models?\" lilianweng.github.io/posts/2021-07-11-diffusion-models  Comprehensive blog post with excellent visualizations and intuitions</p> <p> Yang Song's Blog: \"Generative Modeling by Estimating Gradients of the Data Distribution\" yang-song.net/blog/2021/score  Deep dive into score-based models and SDEs</p> <p> Hugging Face Diffusers Library github.com/huggingface/diffusers  Production-ready implementations: DDPM, DDIM, Stable Diffusion, ControlNet</p> <p> Stability AI: Stable Diffusion Official Repository github.com/Stability-AI/stablediffusion  Official implementation of Stable Diffusion models</p> <p> CompVis: Latent Diffusion Models github.com/CompVis/latent-diffusion  Original latent diffusion implementation</p> <p> Denoising Diffusion PyTorch github.com/lucidrains/denoising-diffusion-pytorch  Clean, well-documented PyTorch implementations</p>"},{"location":"user-guide/concepts/diffusion-explained/#books-and-comprehensive-tutorials","title":"Books and Comprehensive Tutorials","text":"<p> Prince, S. J. D. (2023). \"Understanding Deep Learning\"  Chapter on Diffusion Models | udlbook.github.io/udlbook  Excellent pedagogical treatment with visualizations</p> <p> Murphy, K. P. (2023). \"Probabilistic Machine Learning: Advanced Topics\"  Chapter on Score-Based and Diffusion Models | MIT Press  Rigorous mathematical treatment</p> <p> Hugging Face Diffusion Models Course huggingface.co/learn/diffusion-course  Hands-on tutorials from basics to advanced topics</p> <p>Ready to build with diffusion models? Start with the Diffusion User Guide for practical implementations, check the API Reference for complete documentation, or dive into the MNIST Tutorial to train your first diffusion model from scratch!</p>"},{"location":"user-guide/concepts/ebm-explained/","title":"Energy-Based Models (EBMs) Explained","text":"<ul> <li> <p> Energy Functions</p> <p>Learn distributions by assigning low energy to data and high energy to non-data, capturing complex dependencies</p> </li> <li> <p> Flexible Modeling</p> <p>No architectural constraints\u2014any neural network can define the energy landscape</p> </li> <li> <p> Unified Framework</p> <p>Simultaneously perform classification, generation, and out-of-distribution detection</p> </li> <li> <p> MCMC Sampling</p> <p>Generate samples by traversing the energy landscape using Markov Chain Monte Carlo methods</p> </li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#overview","title":"Overview","text":"<p>Energy-Based Models (EBMs) are a class of deep generative models that learn data distributions by assigning scalar energy values to every possible configuration. Unlike models that explicitly parameterize probability distributions, EBMs define probabilities implicitly through an energy function: lower energy corresponds to higher probability.</p> <p>What makes EBMs special?</p> <p>EBMs solve generative modeling from a fundamentally different perspective. Rather than learning to directly generate samples (GANs) or compress data through bottlenecks (VAEs), EBMs learn an energy landscape where:</p> <ul> <li>Data regions have low energy - the model assigns favorable (low) energy to realistic configurations</li> <li>Non-data regions have high energy - implausible configurations receive unfavorable (high) energy</li> <li>Sampling traverses the landscape - MCMC methods move from high to low energy regions</li> <li>No architectural constraints - any neural network architecture can define the energy function</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#the-intuition-probability-through-energy","title":"The Intuition: Probability Through Energy","text":"<p>Think of EBMs like a physical landscape with hills and valleys:</p> <ol> <li> <p>The Energy Function defines the terrain\u2014each point (data configuration) has an elevation (energy value). Data points sit in deep valleys (low energy), while meaningless noise occupies high peaks (high energy).</p> </li> <li> <p>Training Shapes the Landscape - The model learns to carve valleys where training data exists and raise peaks everywhere else, creating a terrain that naturally guides samples toward realistic outputs.</p> </li> <li> <p>Sampling Rolls Downhill - Like a ball released on a hillside, MCMC sampling iteratively moves toward lower energy, eventually settling in valleys that correspond to realistic data.</p> </li> <li> <p>Probability is Energy-Based - The probability of any configuration depends on its energy through the Boltzmann distribution: \\(p(x) \\propto e^{-E(x)}\\). Lower energy means exponentially higher probability.</p> </li> </ol> <p>The critical insight: by learning to distinguish data from non-data through energy assignment, EBMs capture complex distributions without explicit density modeling or generation mechanisms.</p>"},{"location":"user-guide/concepts/ebm-explained/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"user-guide/concepts/ebm-explained/#the-boltzmann-distribution","title":"The Boltzmann Distribution","text":"<p>EBMs define probability distributions through the Boltzmann distribution:</p> \\[ p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta} \\] <p>where:</p> <ul> <li>\\(E_\\theta(x)\\) is the energy function parameterized by neural network with weights \\(\\theta\\)</li> <li>\\(Z_\\theta = \\int e^{-E_\\theta(x')} dx'\\) is the partition function (normalizing constant)</li> <li>Lower energy \\(E_\\theta(x)\\) corresponds to higher probability \\(p_\\theta(x)\\)</li> </ul> <pre><code>graph TD\n    X[\"Data x\"] --&gt; E[\"Energy Function&lt;br/&gt;E_\u03b8(x)\"]\n    E --&gt; NegE[\"-E_\u03b8(x)\"]\n    NegE --&gt; Exp[\"exp(-E_\u03b8(x))\"]\n    Exp --&gt; Norm[\"\u00f7 Z_\u03b8\"]\n    Z[\"Partition Function&lt;br/&gt;Z_\u03b8 = \u222b exp(-E_\u03b8(x')) dx'\"] --&gt; Norm\n    Norm --&gt; P[\"Probability p_\u03b8(x)\"]\n\n    style X fill:#e1f5ff\n    style E fill:#fff3cd\n    style P fill:#c8e6c9\n    style Z fill:#ffccbc</code></pre> <p>Why the exponential? The exponential function ensures probabilities are positive and creates sharp distinctions between energy levels\u2014a small energy difference produces a large probability ratio.</p>"},{"location":"user-guide/concepts/ebm-explained/#the-intractable-partition-function","title":"The Intractable Partition Function","text":"<p>The partition function \\(Z_\\theta\\) poses the fundamental challenge in EBM training:</p> \\[ Z_\\theta = \\int e^{-E_\\theta(x)} dx \\] <p>This integral sums over all possible configurations of \\(x\\), which becomes intractable for high-dimensional data:</p> <ul> <li>Images (256\u00d7256\u00d73): \\(Z_\\theta\\) requires integration over \\(196{,}608\\)-dimensional space</li> <li>Text (512 tokens, vocab 50k): \\(Z_\\theta\\) requires summing over \\((50{,}000)^{512} \\approx 10^{2414}\\) configurations</li> </ul> <p>The intractability of \\(Z_\\theta\\) makes maximum likelihood training directly infeasible and necessitates alternative training methods.</p>"},{"location":"user-guide/concepts/ebm-explained/#energy-functions-and-neural-networks","title":"Energy Functions and Neural Networks","text":"<p>The energy function \\(E_\\theta(x)\\) can be implemented using any neural network architecture:</p> <p>Multi-Layer Perceptron (MLP) Energy:</p> \\[ E_\\theta(x) = \\text{MLP}_\\theta(x) \\in \\mathbb{R} \\] <pre><code>class MLPEnergyFunction(nnx.Module):\n    def __init__(self, hidden_dims, input_dim, *, rngs):\n        self.layers = []\n        for dim in hidden_dims:\n            self.layers.append(nnx.Linear(input_dim, dim, rngs=rngs))\n            input_dim = dim\n        self.output = nnx.Linear(input_dim, 1, rngs=rngs)\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = nnx.relu(layer(x))\n        return self.output(x).squeeze(-1)  # Scalar energy\n</code></pre> <p>Convolutional Energy (for images):</p> \\[ E_\\theta(x) = \\text{CNN}_\\theta(x) \\in \\mathbb{R} \\] <pre><code>class CNNEnergyFunction(nnx.Module):\n    def __init__(self, hidden_dims, input_channels, *, rngs):\n        self.conv_layers = []\n        for out_channels in hidden_dims:\n            self.conv_layers.append(\n                nnx.Conv(input_channels, out_channels,\n                        kernel_size=(3, 3), rngs=rngs)\n            )\n            input_channels = out_channels\n        self.global_pool = lambda x: jnp.mean(x, axis=(1, 2))\n        self.output = nnx.Linear(hidden_dims[-1], 1, rngs=rngs)\n\n    def __call__(self, x):\n        for conv in self.conv_layers:\n            x = nnx.relu(conv(x))\n        x = self.global_pool(x)\n        return self.output(x).squeeze(-1)\n</code></pre> <p>Key properties:</p> <ul> <li>Scalar output - energy function outputs single real number per input</li> <li>Unrestricted architecture - no invertibility, dimension matching, or structural requirements</li> <li>Learned landscape - network parameters define the energy surface topology</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#training-energy-based-models","title":"Training Energy-Based Models","text":""},{"location":"user-guide/concepts/ebm-explained/#maximum-likelihood-the-ideal-but-intractable-approach","title":"Maximum Likelihood: The Ideal but Intractable Approach","text":"<p>The maximum likelihood objective seeks to maximize the log-probability of observed data:</p> \\[ \\max_\\theta \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log p_\\theta(x)] \\] <p>Substituting the Boltzmann distribution:</p> \\[ \\log p_\\theta(x) = -E_\\theta(x) - \\log Z_\\theta \\] <p>Taking gradients with respect to \\(\\theta\\):</p> \\[ \\nabla_\\theta \\log p_\\theta(x) = -\\nabla_\\theta E_\\theta(x) + \\nabla_\\theta \\log Z_\\theta \\] <p>The first term \\(-\\nabla_\\theta E_\\theta(x)\\) is tractable\u2014just backpropagation through the energy function evaluated at data \\(x\\).</p> <p>The second term is problematic:</p> \\[ \\nabla_\\theta \\log Z_\\theta = \\frac{1}{Z_\\theta} \\nabla_\\theta Z_\\theta = \\frac{1}{Z_\\theta} \\int \\nabla_\\theta e^{-E_\\theta(x')} dx' \\] \\[ = -\\int \\frac{e^{-E_\\theta(x')}}{Z_\\theta} \\nabla_\\theta E_\\theta(x') dx' = -\\mathbb{E}_{x' \\sim p_\\theta}[\\nabla_\\theta E_\\theta(x')] \\] <p>This requires sampling from the model distribution \\(p_\\theta(x)\\), which is exactly what we're trying to learn! This chicken-and-egg problem necessitates approximate training methods.</p>"},{"location":"user-guide/concepts/ebm-explained/#contrastive-divergence-practical-approximation","title":"Contrastive Divergence: Practical Approximation","text":"<p>Contrastive Divergence (CD) (Hinton, 2002) approximates the intractable expectation using short MCMC chains:</p> <p>Algorithm:</p> <ol> <li>Positive phase: Sample data \\(x^+ \\sim p_{\\text{data}}\\), compute \\(\\nabla_\\theta E_\\theta(x^+)\\)</li> <li>Negative phase: Initialize \\(x^- = x^+\\), run \\(k\\) steps of MCMC to get \\(x^-\\), compute \\(\\nabla_\\theta E_\\theta(x^-)\\)</li> <li> <p>Gradient step:</p> \\[ \\nabla_\\theta \\mathcal{L}_{\\text{CD}} \\approx -\\nabla_\\theta E_\\theta(x^+) + \\nabla_\\theta E_\\theta(x^-) \\] </li> </ol> <pre><code>graph LR\n    Data[\"Data x\u207a&lt;br/&gt;(real sample)\"] --&gt; PosGrad[\"Positive Gradient&lt;br/&gt;-\u2207E(x\u207a)\"]\n    Data --&gt; Init[\"Initialize&lt;br/&gt;x\u207b = x\u207a\"]\n    Init --&gt; MCMC[\"k MCMC Steps&lt;br/&gt;(typically k=1-60)\"]\n    MCMC --&gt; Neg[\"Model Sample&lt;br/&gt;x\u207b\"]\n    Neg --&gt; NegGrad[\"Negative Gradient&lt;br/&gt;+\u2207E(x\u207b)\"]\n    PosGrad --&gt; Update[\"Parameter Update\"]\n    NegGrad --&gt; Update\n\n    style Data fill:#c8e6c9\n    style Neg fill:#ffccbc\n    style Update fill:#e1f5ff</code></pre> <p>Why it works:</p> <ul> <li>Positive gradient pushes down energy at data points</li> <li>Negative gradient pushes up energy at model samples</li> <li>Short chains approximate the full expectation with \\(k \\ll \\infty\\) steps</li> <li>Persistent chains (PCD) reuse samples across iterations for better mixing</li> </ul> <p>CD-k Limitations</p> <p>CD with very short chains (\\(k=1\\)) can be unstable. Persistent Contrastive Divergence (PCD) maintains a persistent pool of samples across training iterations, providing better gradient estimates.</p>"},{"location":"user-guide/concepts/ebm-explained/#persistent-contrastive-divergence-pcd","title":"Persistent Contrastive Divergence (PCD)","text":"<p>PCD (Tieleman, 2008) maintains a persistent sample buffer across training iterations:</p> <p>Algorithm:</p> <ol> <li>Initialize buffer: \\(\\mathcal{B} = \\{x_1, x_2, \\ldots, x_M\\}\\) with random samples</li> <li>Each training iteration:<ul> <li>Sample data batch \\(\\{x^+_i\\}\\)</li> <li>Sample buffer batch \\(\\{x^-_j\\} \\sim \\mathcal{B}\\)</li> <li>Run \\(k\\) MCMC steps from each \\(x^-_j\\) to get updated \\(\\tilde{x}^-_j\\)</li> <li>Update buffer: \\(\\mathcal{B} \\leftarrow \\mathcal{B} \\cup \\{\\tilde{x}^-_j\\}\\)</li> <li>Compute gradient: \\(\\nabla_\\theta \\mathcal{L} \\approx -\\frac{1}{N}\\sum_i \\nabla E(x^+_i) + \\frac{1}{N}\\sum_j \\nabla E(\\tilde{x}^-_j)\\)</li> <li>Update parameters</li> </ul> </li> </ol> <pre><code>def persistent_contrastive_divergence(\n    energy_fn, real_samples, sample_buffer, rng_key,\n    n_mcmc_steps=60, step_size=0.01, noise_scale=0.005\n):\n    \"\"\"Persistent Contrastive Divergence training step.\"\"\"\n    batch_size = real_samples.shape[0]\n    sample_shape = real_samples.shape[1:]\n\n    # Sample initial points from buffer\n    init_samples = sample_buffer.sample_initial(batch_size, rng_key, sample_shape)\n\n    # Run MCMC chain\n    final_samples = langevin_dynamics(\n        energy_fn, init_samples, n_mcmc_steps, step_size, noise_scale, rng_key\n    )\n\n    # Update buffer with new samples\n    sample_buffer.push(final_samples)\n\n    return init_samples, final_samples\n</code></pre> <p>Advantages over CD:</p> <ul> <li>Better mixing: Chains continue across iterations</li> <li>More accurate gradients: Samples closer to true model distribution</li> <li>Stable training: Reduces oscillations and mode collapse</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#langevin-dynamics-mcmc-sampling","title":"Langevin Dynamics: MCMC Sampling","text":"<p>Langevin dynamics provides the MCMC sampler for EBMs:</p> \\[ x_{t+1} = x_t - \\frac{\\epsilon}{2} \\nabla_x E_\\theta(x_t) + \\sqrt{\\epsilon} \\, \\xi_t \\] <p>where \\(\\xi_t \\sim \\mathcal{N}(0, I)\\) and \\(\\epsilon\\) is the step size.</p> <p>Two components:</p> <ol> <li>Gradient descent \\(-\\frac{\\epsilon}{2} \\nabla_x E_\\theta(x_t)\\) moves toward lower energy</li> <li>Brownian motion \\(\\sqrt{\\epsilon} \\, \\xi_t\\) adds noise to explore the space</li> </ol> <pre><code>def langevin_dynamics(\n    energy_fn, init_samples, n_steps, step_size, noise_scale, rng_key\n):\n    \"\"\"Sample from EBM using Langevin dynamics.\"\"\"\n    x = init_samples\n\n    for step in range(n_steps):\n        # Compute energy gradient\n        energy_grad = jax.grad(lambda x: jnp.sum(energy_fn(x)))(x)\n\n        # Gradient descent step\n        x = x - (step_size / 2) * energy_grad\n\n        # Add noise\n        rng_key, noise_key = jax.random.split(rng_key)\n        noise = jax.random.normal(noise_key, x.shape) * noise_scale\n        x = x + noise\n\n        # Clip to valid range\n        x = jnp.clip(x, -1.0, 1.0)\n\n    return x\n</code></pre> <p>Convergence: As \\(n_{\\text{steps}} \\to \\infty\\) and \\(\\epsilon \\to 0\\), Langevin dynamics samples from \\(p_\\theta(x)\\) exactly. In practice, finite steps and step sizes introduce bias.</p>"},{"location":"user-guide/concepts/ebm-explained/#score-matching-avoiding-sampling","title":"Score Matching: Avoiding Sampling","text":"<p>Score Matching (Hyv\u00e4rinen, 2005) bypasses MCMC sampling by matching the score function (gradient of log-density):</p> \\[ \\psi_\\theta(x) = \\nabla_x \\log p_\\theta(x) = -\\nabla_x E_\\theta(x) \\] <p>Objective - Minimize the Fisher divergence:</p> \\[ \\mathcal{L}_{\\text{SM}} = \\mathbb{E}_{p_{\\text{data}}}[\\|\\nabla_x E_\\theta(x) + \\nabla_x \\log p_{\\text{data}}(x)\\|^2] \\] <p>Since \\(\\nabla_x \\log p_{\\text{data}}(x)\\) is unknown, Hyv\u00e4rinen showed this is equivalent to:</p> \\[ \\mathcal{L}_{\\text{SM}} = \\mathbb{E}_{p_{\\text{data}}}[-\\text{tr}(\\nabla_x^2 E_\\theta(x)) + \\frac{1}{2}\\|\\nabla_x E_\\theta(x)\\|^2] \\] <p>Advantages:</p> <ul> <li>No sampling required during training</li> <li>No partition function computation needed</li> <li>Fast training compared to CD/PCD</li> </ul> <p>Disadvantages:</p> <ul> <li>Second derivatives \\(\\nabla_x^2 E_\\theta(x)\\) expensive to compute</li> <li>Generation still requires MCMC at test time</li> <li>Less effective for high-dimensional data</li> </ul> <p>Denoising Score Matching: A practical variant adds noise to data:</p> \\[ \\mathcal{L}_{\\text{DSM}} = \\mathbb{E}_{p_{\\text{data}}(x)} \\mathbb{E}_{p(\\tilde{x}|x)}[\\|\\nabla_{\\tilde{x}} E_\\theta(\\tilde{x}) + \\frac{x - \\tilde{x}}{\\sigma^2}\\|^2] \\] <p>where \\(\\tilde{x} = x + \\sigma \\epsilon\\) with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).</p>"},{"location":"user-guide/concepts/ebm-explained/#noise-contrastive-estimation-nce","title":"Noise Contrastive Estimation (NCE)","text":"<p>NCE (Gutmann &amp; Hyv\u00e4rinen, 2010) frames training as binary classification:</p> <p>Setup: Given data samples \\(\\{x_i\\}\\) and noise samples \\(\\{\\tilde{x}_j\\}\\) from distribution \\(p_n\\), train a classifier to distinguish them.</p> <p>Decision rule: Sample \\(x\\) comes from data if \\(\\frac{p_\\theta(x)}{p_n(x)} &gt; 1\\), equivalently:</p> \\[ h_\\theta(x) = \\sigma(-E_\\theta(x) - \\log p_n(x)) \\] <p>NCE objective:</p> \\[ \\mathcal{L}_{\\text{NCE}} = -\\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log h_\\theta(x)] - \\mathbb{E}_{\\tilde{x} \\sim p_n}[\\log(1 - h_\\theta(\\tilde{x}))] \\] <p>Advantages:</p> <ul> <li>No partition function in gradient</li> <li>Simple implementation like standard classification</li> <li>Flexible noise distribution \\(p_n\\) (typically Gaussian)</li> </ul> <p>When to use:</p> <ul> <li>High-dimensional problems where MCMC is slow</li> <li>When generation quality is secondary to density estimation</li> <li>Contrastive learning scenarios</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#joint-energy-based-models-jem","title":"Joint Energy-Based Models (JEM)","text":"<p>Joint Energy-Based Models (Grathwohl et al., 2020) unify generative and discriminative modeling in a single framework.</p>"},{"location":"user-guide/concepts/ebm-explained/#the-key-insight","title":"The Key Insight","text":"<p>A standard classifier \\(p(y|x)\\) can be reinterpreted as an energy model:</p> \\[ p_\\theta(y|x) = \\frac{e^{f_\\theta[x](y)}}{\\sum_{y'} e^{f_\\theta[x](y')}} \\] <p>where \\(f_\\theta(x) \\in \\mathbb{R}^C\\) are logits.</p> <p>Define energy as:</p> \\[ E_\\theta(x, y) = -f_\\theta[x](y) \\] <p>Then the joint distribution becomes:</p> \\[ p_\\theta(x, y) = \\frac{e^{f_\\theta[x](y)}}{Z_\\theta}, \\quad Z_\\theta = \\sum_{y'} \\int e^{f_\\theta[x'](y')} dx' \\] <p>And the marginal data distribution:</p> \\[ p_\\theta(x) = \\frac{\\sum_y e^{f_\\theta[x](y)}}{Z_\\theta} \\] <pre><code>graph TD\n    X[\"Input x\"] --&gt; F[\"Classifier f_\u03b8(x)&lt;br/&gt;(C logits)\"]\n    F --&gt; Cond[\"Conditional p(y|x)&lt;br/&gt;(softmax)\"]\n    F --&gt; LogSumExp[\"log-sum-exp\"]\n    LogSumExp --&gt; Energy[\"E_\u03b8(x) = -logsumexp(f_\u03b8(x))\"]\n    Energy --&gt; Joint[\"Joint p(x,y)&lt;br/&gt;\u221d exp(f_\u03b8(x)[y])\"]\n    Joint --&gt; Marg[\"Marginal p(x)&lt;br/&gt;\u221d exp(-E_\u03b8(x))\"]\n\n    style F fill:#fff3cd\n    style Cond fill:#e1f5ff\n    style Marg fill:#c8e6c9</code></pre>"},{"location":"user-guide/concepts/ebm-explained/#training-jem","title":"Training JEM","text":"<p>Hybrid objective combining classification and generation:</p> \\[ \\mathcal{L}_{\\text{JEM}} = \\underbrace{\\mathcal{L}_{\\text{class}}(f_\\theta)}_{\\text{discriminative}} + \\lambda \\underbrace{\\mathcal{L}_{\\text{gen}}(p_\\theta)}_{\\text{generative}} \\] <p>where:</p> <ul> <li>\\(\\mathcal{L}_{\\text{class}} = -\\mathbb{E}_{(x, y) \\sim p_{\\text{data}}}[\\log p_\\theta(y|x)]\\) (cross-entropy)</li> <li>\\(\\mathcal{L}_{\\text{gen}} = -\\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log p_\\theta(x)]\\) (MLE via SGLD)</li> </ul> <p>Training algorithm:</p> <ol> <li>Sample batch \\(\\{(x_i, y_i)\\}\\) from labeled data</li> <li>Classification update: Compute cross-entropy loss, backprop</li> <li>Generative update:</li> <li>Generate samples via SGLD</li> <li>Compute CD gradient</li> <li>Update parameters</li> <li>Alternate or combine both losses</li> </ol> <p>Capabilities of JEM:</p> <ul> <li> <p> Classification</p> <p>Achieves competitive accuracy on CIFAR-10, SVHN, ImageNet</p> </li> <li> <p> Generation</p> <p>Generates realistic samples comparable to GANs</p> </li> <li> <p> Out-of-Distribution Detection</p> <p>Uses \\(p(x)\\) to detect anomalies with AUROC &gt; 95%</p> </li> <li> <p> Adversarial Robustness</p> <p>More robust to adversarial examples than standard classifiers</p> </li> <li> <p> Calibration</p> <p>Better uncertainty estimates than standard softmax</p> </li> <li> <p> Hybrid Discriminative-Generative</p> <p>Leverages both labeled and unlabeled data</p> </li> </ul> <p>Challenges:</p> <ul> <li>Training instability: SGLD can fail to produce quality samples</li> <li>Computational cost: Requires MCMC sampling during training</li> <li>Hyperparameter sensitivity: Step size, noise scale, buffer size critical</li> </ul> <p>Recent work (2023-2024) has proposed improvements including better initialization, curriculum learning, and stabilized SGLD variants.</p>"},{"location":"user-guide/concepts/ebm-explained/#architecture-design","title":"Architecture Design","text":""},{"location":"user-guide/concepts/ebm-explained/#energy-function-design-principles","title":"Energy Function Design Principles","text":"<p>Key considerations when designing energy functions:</p> <ol> <li>Scalar output: \\(E_\\theta: \\mathcal{X} \\to \\mathbb{R}\\) must map inputs to single energy value</li> <li>Sufficient capacity: Deep networks capture complex dependencies</li> <li>Spectral normalization: Stabilizes training by constraining Lipschitz constant</li> <li>Residual connections: Enable deep architectures (10+ layers)</li> <li>Normalization layers: GroupNorm or LayerNorm (BatchNorm can interfere with MCMC)</li> </ol>"},{"location":"user-guide/concepts/ebm-explained/#mlp-energy-function-tabular-data","title":"MLP Energy Function (Tabular Data)","text":"<pre><code>class MLPEnergyFunction(nnx.Module):\n    def __init__(\n        self, hidden_dims, input_dim, activation=nnx.gelu,\n        dropout_rate=0.0, *, rngs\n    ):\n        super().__init__(rngs=rngs)\n        self.layers = []\n\n        for dim in hidden_dims:\n            self.layers.append(nnx.Linear(input_dim, dim, rngs=rngs))\n            if dropout_rate &gt; 0:\n                self.layers.append(nnx.Dropout(dropout_rate, rngs=rngs))\n            input_dim = dim\n\n        self.output_layer = nnx.Linear(input_dim, 1, rngs=rngs)\n        self.activation = activation\n\n    def __call__(self, x, *, deterministic=True):\n        for layer in self.layers:\n            if isinstance(layer, nnx.Linear):\n                x = self.activation(layer(x))\n            elif isinstance(layer, nnx.Dropout):\n                x = layer(x, deterministic=deterministic)\n        return self.output_layer(x).squeeze(-1)\n</code></pre> <p>Use cases: Tabular data, low-to-moderate dimensional problems (&lt; 1000 dimensions)</p>"},{"location":"user-guide/concepts/ebm-explained/#cnn-energy-function-images","title":"CNN Energy Function (Images)","text":"<pre><code>class CNNEnergyFunction(nnx.Module):\n    def __init__(\n        self, hidden_dims, input_channels=3,\n        activation=nnx.silu, *, rngs\n    ):\n        super().__init__(rngs=rngs)\n        self.conv_blocks = []\n\n        in_channels = input_channels\n        for out_channels in hidden_dims:\n            self.conv_blocks.append(EnergyBlock(\n                in_channels, out_channels, activation=activation, rngs=rngs\n            ))\n            in_channels = out_channels\n\n        self.global_pool = lambda x: jnp.mean(x, axis=(1, 2))\n        self.fc_layers = [\n            nnx.Linear(hidden_dims[-1], hidden_dims[-1]//2, rngs=rngs),\n            nnx.Linear(hidden_dims[-1]//2, 1, rngs=rngs),\n        ]\n        self.activation = activation\n\n    def __call__(self, x, *, deterministic=True):\n        for block in self.conv_blocks:\n            x = block(x)\n        x = self.global_pool(x)\n        for i, fc in enumerate(self.fc_layers):\n            x = fc(x)\n            if i &lt; len(self.fc_layers) - 1:\n                x = self.activation(x)\n        return x.squeeze(-1)\n\nclass EnergyBlock(nnx.Module):\n    \"\"\"Residual block with GroupNorm for energy function.\"\"\"\n    def __init__(\n        self, in_channels, out_channels,\n        kernel_size=3, stride=2,\n        use_residual=False, activation=nnx.silu, *, rngs\n    ):\n        super().__init__()\n        self.conv = nnx.Conv(\n            in_channels, out_channels,\n            kernel_size=(kernel_size, kernel_size),\n            strides=(stride, stride), padding=\"SAME\", rngs=rngs\n        )\n        self.norm = nnx.GroupNorm(\n            min(32, out_channels), out_channels, rngs=rngs\n        )\n        self.activation = activation\n        self.use_residual = use_residual and (in_channels == out_channels) and (stride == 1)\n\n    def __call__(self, x):\n        residual = x\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.activation(x)\n        if self.use_residual:\n            x = x + residual\n        return x\n</code></pre> <p>Use cases: Image data (MNIST, CIFAR-10, CelebA), spatial data</p>"},{"location":"user-guide/concepts/ebm-explained/#deep-energy-functions-with-spectral-normalization","title":"Deep Energy Functions with Spectral Normalization","text":"<p>For complex distributions, deeper architectures with spectral normalization improve stability:</p> <pre><code>class DeepCNNEnergyFunction(nnx.Module):\n    def __init__(\n        self, hidden_dims, input_channels=3,\n        use_spectral_norm=True, use_residual=True, *, rngs\n    ):\n        super().__init__(rngs=rngs)\n        # Build deeper architecture (8-12 layers)\n        # Apply spectral normalization to conv layers\n        # Use residual connections where dimensions match\n        ...\n</code></pre> <p>Benefits:</p> <ul> <li>Spectral norm: Constrains Lipschitz constant, prevents gradient explosion</li> <li>Residual connections: Enable training of 10+ layer networks</li> <li>Better sample quality: Captures finer details in distribution</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#training-dynamics-and-best-practices","title":"Training Dynamics and Best Practices","text":""},{"location":"user-guide/concepts/ebm-explained/#sample-buffer-management","title":"Sample Buffer Management","text":"<p>Persistent sample buffers are critical for stable EBM training:</p> <p>Buffer initialization:</p> <ul> <li>Random noise (Gaussian or uniform)</li> <li>Real data with added noise</li> <li>Pre-trained model samples</li> </ul> <p>Buffer operations:</p> <pre><code>class SampleBuffer:\n    def __init__(self, capacity=8192, reinit_prob=0.05):\n        self.capacity = capacity\n        self.reinit_prob = reinit_prob\n        self.buffer = []\n\n    def sample_initial(self, batch_size, rng_key, sample_shape):\n        \"\"\"Sample starting points for MCMC.\"\"\"\n        if len(self.buffer) &lt; batch_size:\n            # Not enough samples - return random noise\n            return jax.random.normal(rng_key, (batch_size, *sample_shape))\n\n        # Sample from buffer\n        indices = jax.random.choice(rng_key, len(self.buffer), (batch_size,))\n        samples = jnp.array([self.buffer[i] for i in indices])\n\n        # Reinitialize some samples with noise\n        reinit_mask = jax.random.uniform(rng_key, (batch_size,)) &lt; self.reinit_prob\n        noise = jax.random.normal(rng_key, samples.shape)\n        samples = jnp.where(reinit_mask[:, None, None, None], noise, samples)\n\n        return samples\n\n    def push(self, samples):\n        \"\"\"Add new samples to buffer.\"\"\"\n        for sample in samples:\n            if len(self.buffer) &gt;= self.capacity:\n                self.buffer.pop(0)  # Remove oldest\n            self.buffer.append(sample)\n</code></pre> <p>Reinitialiation probability: Setting \\(p_{\\text{reinit}} = 0.05\\) helps escape local minima.</p>"},{"location":"user-guide/concepts/ebm-explained/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":"<p>MCMC sampling:</p> Parameter Value Range Typical Notes MCMC steps 20-200 60 More steps = better samples, slower Step size 0.001-0.05 0.01 Too large: instability, too small: slow Noise scale 0.001-0.01 0.005 Exploration vs exploitation <p>Training:</p> Parameter Value Range Typical Notes Learning rate 1e-5 to 1e-3 1e-4 Lower than standard supervised Batch size 32-256 128 Larger batches stabilize Buffer capacity 2048-16384 8192 More = better mixing Alpha (regularization) 0.001-0.1 0.01 Prevents energy collapse <p>Data preprocessing:</p> <ul> <li>Normalize images to \\([-1, 1]\\) or \\([0, 1]\\)</li> <li>Add noise to real data during training (noise scale ~0.005)</li> <li>Clip samples after MCMC to valid range</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#common-training-issues","title":"Common Training Issues","text":"<ul> <li> <p> Mode Collapse</p> <p>Symptom: Generated samples lack diversity, all similar</p> <p>Solutions: Increase buffer size, use reinitialize probability, longer MCMC chains, reduce learning rate</p> </li> <li> <p> Energy Explosion</p> <p>Symptom: Energy values grow unbounded, NaN losses</p> <p>Solutions: Add regularization term \\(\\alpha \\mathbb{E}[E(x)^2]\\), spectral normalization, gradient clipping, smaller learning rate</p> </li> <li> <p> Poor Sample Quality</p> <p>Symptom: Samples look like noise or blurry averages</p> <p>Solutions: More MCMC steps (100+), better step size tuning, deeper energy function, larger model capacity</p> </li> <li> <p> Training Instability</p> <p>Symptom: Oscillating losses, sudden divergence</p> <p>Solutions: Persistent buffer, KL annealing on regularization, spectral norm, lower learning rate, gradient clipping</p> </li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#monitoring-and-diagnostics","title":"Monitoring and Diagnostics","text":"<p>Essential metrics:</p> <ol> <li>Energy values: Monitor \\(\\mathbb{E}[E(x_{\\text{data}})]\\) and \\(\\mathbb{E}[E(x_{\\text{gen}})]\\)</li> <li>Should satisfy \\(E(x_{\\text{data}}) &lt; E(x_{\\text{gen}})\\)</li> <li> <p>Gap should be positive and stable</p> </li> <li> <p>MCMC diagnostics:</p> </li> <li>Acceptance rate: Track how many proposed moves are accepted</li> <li> <p>Energy trajectory: Plot energy over MCMC steps (should decrease)</p> </li> <li> <p>Sample quality: Visual inspection, FID/IS scores</p> </li> <li> <p>Gradient norms: Should be stable, not exploding</p> </li> </ol> <pre><code>def training_step_with_diagnostics(model, batch, rngs):\n    # Forward pass\n    real_data = batch['x']\n    real_energy = model.energy(real_data).mean()\n\n    # Generate samples\n    fake_data = generate_samples_via_mcmc(model, batch_size, rngs)\n    fake_energy = model.energy(fake_data).mean()\n\n    # Compute loss\n    loss = real_energy - fake_energy + alpha * (real_energy**2).mean()\n\n    # Diagnostics\n    energy_gap = fake_energy - real_energy\n\n    print(f\"Real Energy: {real_energy:.3f}, \"\n          f\"Fake Energy: {fake_energy:.3f}, \"\n          f\"Gap: {energy_gap:.3f}, \"\n          f\"Loss: {loss:.3f}\")\n\n    return loss\n</code></pre>"},{"location":"user-guide/concepts/ebm-explained/#comparing-ebms-with-other-generative-models","title":"Comparing EBMs with Other Generative Models","text":""},{"location":"user-guide/concepts/ebm-explained/#ebms-vs-gans-stable-training-vs-sharp-samples","title":"EBMs vs GANs: Stable Training vs Sharp Samples","text":"Aspect Energy-Based Models GANs Training Stability Stable with PCD/CD Notorious instability, mode collapse Sample Quality Good (improving with recent methods) Excellent (sharp, realistic) Sampling Speed Slow (50-200 MCMC steps) Fast (1 forward pass) Mode Coverage Excellent (explicit density) Poor (mode collapse common) Likelihood Exact (modulo partition function) None (no explicit density) Architecture Any neural network Generator-discriminator pair Use Cases Density estimation, OOD detection High-quality image synthesis <p>When to use EBMs over GANs:</p> <ul> <li>Exact likelihood computation needed</li> <li>Out-of-distribution detection</li> <li>Training stability priority</li> <li>Avoiding mode collapse essential</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#ebms-vs-vaes-flexibility-vs-efficiency","title":"EBMs vs VAEs: Flexibility vs Efficiency","text":"Aspect Energy-Based Models VAEs Architecture Unrestricted Encoder-decoder with bottleneck Likelihood Exact density Lower bound (ELBO) Sample Quality Good Often blurry (reconstruction bias) Sampling Speed Slow (MCMC) Fast (single pass) Training CD/PCD (requires MCMC) Stable (reparameterization trick) Latent Space No explicit latent Structured latent representations Use Cases Density estimation Representation learning, compression <p>When to use EBMs over VAEs:</p> <ul> <li>No need for learned latent representations</li> <li>Exact density more important than speed</li> <li>Want architectural flexibility</li> <li>Avoid reconstruction-based blur</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#ebms-vs-diffusion-models-principled-energy-vs-iterative-denoising","title":"EBMs vs Diffusion Models: Principled Energy vs Iterative Denoising","text":"Aspect Energy-Based Models Diffusion Models Conceptual Framework Energy landscape Iterative denoising Training CD/PCD (challenging) Stable (MSE denoising) Sampling MCMC (50-200 steps) Denoising (50-1000 steps) Sample Quality Good State-of-the-art Likelihood Exact Tractable via ODE Flexibility Any architecture Flexible but typically U-Net Maturity Older field, renewed interest Rapidly advancing (2020-2025) <p>Relationship: Diffusion models can be viewed as energy-based models with time-dependent energy functions and specific noise schedules. Score-based models unify the two perspectives.</p>"},{"location":"user-guide/concepts/ebm-explained/#recent-advances-2023-2025","title":"Recent Advances (2023-2025)","text":""},{"location":"user-guide/concepts/ebm-explained/#improved-contrastive-divergence","title":"Improved Contrastive Divergence","text":"<p>Improved CD (Du et al., ICML 2021) addresses gradient bias in standard CD:</p> <p>Key innovations:</p> <ol> <li>KL divergence term: Add \\(\\mathbb{E}_{x \\sim p_{\\text{data}}}[D_{\\text{KL}}(q(x') | p_\\theta(x'))]\\) to loss</li> <li>Data augmentation: Augment both positive and negative samples</li> <li>Multi-scale processing: Process images at multiple resolutions</li> <li>Reservoir sampling: Maintain diverse buffer through stratified sampling</li> </ol> <p>Results: CIFAR-10 Inception Score of 8.30, substantial improvement over baseline CD.</p>"},{"location":"user-guide/concepts/ebm-explained/#diffusion-contrastive-divergence-dcd","title":"Diffusion Contrastive Divergence (DCD)","text":"<p>DCD (2023) replaces Langevin dynamics with diffusion processes:</p> <p>Instead of Langevin MCMC: $$ x_{t+1} = x_t - \\epsilon \\nabla E(x_t) + \\sqrt{2\\epsilon} \\xi_t $$</p> <p>Use general diffusion: $$ dx_t = f(x_t, t)dt + g(t)dw_t $$</p> <p>Benefits:</p> <ul> <li>More flexible: Not limited to Langevin</li> <li>More efficient: Better exploration of space</li> <li>Simulation-free: Train without explicit integration</li> </ul> <p>Applications: Higher quality image generation, faster convergence.</p>"},{"location":"user-guide/concepts/ebm-explained/#energy-matching-unifying-flows-and-ebms","title":"Energy Matching: Unifying Flows and EBMs","text":"<p>Energy Matching (2024) connects flow matching and energy-based modeling:</p> <p>Framework: Learn energy functions via flow matching objectives, avoiding MCMC during training entirely.</p> <p>Training: Match energy gradients to optimal transport flows between data and noise.</p> <p>Results: State-of-the-art likelihood on tabular datasets, competitive image generation.</p>"},{"location":"user-guide/concepts/ebm-explained/#stable-jem-training","title":"Stable JEM Training","text":"<p>Recent work addresses JEM training instability:</p> <ol> <li>Better initialization: Initialize from pre-trained classifier</li> <li>Curriculum learning: Gradually increase MCMC steps during training</li> <li>Improved SGLD: Adaptive step sizes, better noise schedules</li> <li>Separate buffer per class: Class-conditional buffers improve mixing</li> </ol> <p>Stabilized JEM (2023): Achieves stable training on ImageNet-scale datasets with competitive classification and generation.</p>"},{"location":"user-guide/concepts/ebm-explained/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>Energy-Based Models provide a principled framework for learning probability distributions through energy functions. While training challenges have historically limited their adoption, recent advances in contrastive divergence and hybrid methods are revitalizing the field.</p>"},{"location":"user-guide/concepts/ebm-explained/#core-principles","title":"Core Principles","text":"<ul> <li> <p> Energy Landscape</p> <p>Model assigns scalar energy to configurations\u2014low energy = high probability</p> </li> <li> <p> Boltzmann Distribution</p> <p>\\(p(x) \\propto e^{-E(x)}\\) connects energy to probability via exponential</p> </li> <li> <p> MCMC Sampling</p> <p>Generate samples by traversing energy landscape with Langevin dynamics</p> </li> <li> <p> Flexible Architecture</p> <p>Any neural network can define energy function\u2014no structural constraints</p> </li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#training-methods","title":"Training Methods","text":"Method Sampling During Training Pros Cons Contrastive Divergence Short MCMC chains (k=1-60) Practical, widely used Biased gradient estimates Persistent CD Persistent buffer + MCMC Better gradients, stable Requires careful buffer management Score Matching None No sampling needed Expensive second derivatives NCE Noise samples only Simple, no MCMC Requires good noise distribution"},{"location":"user-guide/concepts/ebm-explained/#when-to-use-ebms","title":"When to Use EBMs","text":"<p>Best suited for:</p> <ul> <li>Density estimation where exact likelihood matters</li> <li>Out-of-distribution detection leveraging \\(p(x)\\)</li> <li>Anomaly detection in high-stakes applications</li> <li>Hybrid tasks combining generation and classification (JEM)</li> <li>Structured prediction with complex dependencies</li> </ul> <p>Avoid when:</p> <ul> <li>Real-time generation required (use GANs or fast flows)</li> <li>Training data limited (VAEs/diffusion more sample-efficient)</li> <li>Computational resources constrained at training time</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#future-directions","title":"Future Directions","text":"<ul> <li>Faster sampling: Learned samplers, amortized MCMC</li> <li>Better training: Improved CD variants, score matching hybrids</li> <li>Scalability: Efficient large-scale training, distributed MCMC</li> <li>Applications: Scientific computing, protein design, molecular generation</li> <li>Unification: Bridging EBMs, diffusion, and score-based models</li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#next-steps","title":"Next Steps","text":"<ul> <li> <p> EBM User Guide</p> <p>Practical usage guide with implementation examples and training workflows</p> </li> <li> <p> EBM API Reference</p> <p>Complete API documentation for EBM, DeepEBM, and JEM classes</p> </li> <li> <p> EBM Tutorial</p> <p>Step-by-step hands-on tutorial: train an EBM on MNIST from scratch</p> </li> <li> <p> Advanced Examples</p> <p>Explore Joint Energy-Based Models, hybrid training, and applications</p> </li> </ul>"},{"location":"user-guide/concepts/ebm-explained/#further-reading","title":"Further Reading","text":""},{"location":"user-guide/concepts/ebm-explained/#seminal-papers-must-read","title":"Seminal Papers (Must Read)","text":"<p> LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., &amp; Huang, F. (2006). \"A Tutorial on Energy-Based Learning\" Technical Report  Foundational tutorial establishing the energy-based learning framework</p> <p> Hinton, G. E. (2002). \"Training Products of Experts by Minimizing Contrastive Divergence\" Neural Computation 14(8)  Introduced Contrastive Divergence, making EBM training practical</p> <p> Tieleman, T. (2008). \"Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient\" ICML 2008  Persistent Contrastive Divergence for improved training</p> <p> Hyv\u00e4rinen, A. (2005). \"Estimation of Non-Normalized Statistical Models by Score Matching\" JMLR 6  Score matching method avoiding partition function</p> <p> Gutmann, M., &amp; Hyv\u00e4rinen, A. (2010). \"Noise-contrastive estimation: A new estimation principle for unnormalized statistical models\" AISTATS 2010  NCE training via binary classification</p>"},{"location":"user-guide/concepts/ebm-explained/#modern-ebms-and-applications","title":"Modern EBMs and Applications","text":"<p> Du, Y., &amp; Mordatch, I. (2019). \"Implicit Generation and Modeling with Energy Based Models\" arXiv:1903.08689 | NeurIPS 2019  Modern EBMs for image generation with improved training</p> <p> Grathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., Norouzi, M., &amp; Swersky, K. (2020). \"Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One\" arXiv:1912.03263 | ICLR 2020  Joint Energy-Based Models unifying classification and generation</p> <p> Du, Y., Li, S., Tenenbaum, J., &amp; Mordatch, I. (2020). \"Improved Contrastive Divergence Training of Energy Based Models\" arXiv:2012.01316 | ICML 2021  Addresses gradient bias in CD with KL regularization</p> <p> Gao, R., Song, Y., Poole, B., Wu, Y. N., &amp; Kingma, D. P. (2021). \"Learning Energy-Based Models by Diffusion Recovery Likelihood\" arXiv:2012.08125 | ICLR 2021  Connects EBMs with diffusion models via recovery likelihood</p>"},{"location":"user-guide/concepts/ebm-explained/#recent-advances-2023-2025_1","title":"Recent Advances (2023-2025)","text":"<p> Wu, D., Wang, L., &amp; Hong, P. (2023). \"Training Energy-Based Models with Diffusion Contrastive Divergences\" arXiv:2307.01668  Replaces Langevin with general diffusion processes</p> <p> Gao, Y., Liu, X., &amp; Zha, H. (2024). \"Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling\" arXiv:2504.10612  Unifies flow matching and EBMs (2024 cutting-edge)</p> <p> Zhang, Y., et al. (2023). \"Stabilized Training of Joint Energy-based Models and their Practical Applications\" arXiv:2303.04187  Addresses JEM training instability</p>"},{"location":"user-guide/concepts/ebm-explained/#tutorial-resources","title":"Tutorial Resources","text":"<p> NYU Deep Learning Course: Energy-Based Models atcold.github.io/NYU-DLSP20/en/week07/07-1  Comprehensive lectures by Yann LeCun and Alfredo Canziani</p> <p> UvA Deep Learning Tutorial 8: Deep Energy-Based Generative Models uvadlc-notebooks.readthedocs.io  Hands-on Colab notebooks with implementations</p> <p> Stanford CS236: Deep Generative Models (EBM Lecture) Lecture Slides  Academic course materials covering EBM theory</p> <p> Energy-Based Models GitHub Repositories github.com/yilundu/improved_contrastive_divergence  Official implementation of Improved CD</p> <p>Ready to explore Energy-Based Models? Start with the EBM User Guide for practical implementations, check the API Reference for complete documentation, or dive into the MNIST Tutorial to train your first EBM!</p>"},{"location":"user-guide/concepts/flow-explained/","title":"Normalizing Flows Explained","text":"<ul> <li> <p> Exact Likelihood</p> <p>Compute exact log-likelihood through tractable Jacobian determinants, enabling precise density estimation</p> </li> <li> <p> Bijective Transformations</p> <p>Invertible mappings allow both efficient sampling and exact inference through forward and inverse passes</p> </li> <li> <p> Flexible Distributions</p> <p>Transform simple base distributions into complex target distributions through learned compositions</p> </li> <li> <p> Fast Generation</p> <p>Single-pass or few-step sampling with modern architectures achieving real-time performance</p> </li> </ul>"},{"location":"user-guide/concepts/flow-explained/#overview","title":"Overview","text":"<p>Normalizing flows have emerged as a uniquely powerful class of generative models that provide exact likelihood computation and efficient sampling through invertible transformations. Unlike VAEs that optimize approximate lower bounds or GANs that learn implicit distributions, flows transform simple base distributions into complex data distributions via learned bijective mappings with tractable Jacobian determinants.</p> <p>What makes normalizing flows special? Flows solve a fundamental challenge in generative modeling: simultaneously enabling precise density estimation and efficient sampling. By learning invertible transformations with structured Jacobians, flows:</p> <ul> <li>Compute exact likelihood for any data point without approximation</li> <li>Generate samples through fast inverse transformations</li> <li>Perform exact inference without variational bounds or adversarial training</li> <li>Train stably using straightforward maximum likelihood objectives</li> </ul> <p>Recent breakthroughs in 2023-2025\u2014including flow matching, rectified flows, and discrete flow variants\u2014have dramatically closed the performance gap with diffusion models while maintaining the core advantages of one-step generation and stable training.</p>"},{"location":"user-guide/concepts/flow-explained/#the-intuition-probability-transformations","title":"The Intuition: Probability Transformations","text":"<p>Think of normalizing flows like a sequence of coordinate transformations on a map:</p> <ol> <li> <p>Start with simple terrain (base distribution) - a flat, uniform grid easy to sample from</p> </li> <li> <p>Apply transformations - each step warps, stretches, and reshapes the terrain while maintaining a perfect one-to-one correspondence between original and transformed coordinates</p> </li> <li> <p>Track volume changes - the Jacobian determinant measures how much each region expands or contracts, ensuring probability mass is conserved</p> </li> <li> <p>Compose transformations - stack multiple simple warps to create arbitrarily complex landscapes (data distributions)</p> </li> </ol> <p>The critical insight: by carefully designing transformations where we can efficiently compute both the forward mapping and the volume change, we get a model that can both generate samples (apply the transformation) and evaluate probabilities (apply the inverse and account for volume changes).</p>"},{"location":"user-guide/concepts/flow-explained/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"user-guide/concepts/flow-explained/#the-change-of-variables-formula","title":"The Change of Variables Formula","text":"<p>The change of variables formula serves as the cornerstone of all normalizing flow architectures. Given a random variable \\(\\mathbf{z}\\) with known density \\(p_\\mathcal{Z}(\\mathbf{z})\\) and an invertible transformation \\(\\mathbf{x} = f(\\mathbf{z})\\), the density of \\(\\mathbf{x}\\) becomes:</p> \\[ p_\\mathcal{X}(\\mathbf{x}) = p_\\mathcal{Z}(f^{-1}(\\mathbf{x})) \\left| \\det \\frac{\\partial f^{-1}}{\\partial \\mathbf{x}} \\right| \\] <p>Or equivalently in log space:</p> \\[ \\log p_\\mathcal{X}(\\mathbf{x}) = \\log p_\\mathcal{Z}(\\mathbf{z}) + \\log \\left| \\det \\frac{\\partial f^{-1}}{\\partial \\mathbf{x}} \\right| \\] <p>where \\(\\mathbf{z} = f^{-1}(\\mathbf{x})\\).</p> <p>Geometric Intuition</p> <p>The Jacobian determinant \\(\\left| \\det \\frac{\\partial f}{\\partial \\mathbf{z}} \\right|\\) quantifies the relative change in volume of an infinitesimal neighborhood under transformation \\(f\\). When the transformation expands a region (\\(|\\det J| &gt; 1\\)), the probability density must decrease proportionally to conserve total probability mass. Conversely, contraction (\\(|\\det J| &lt; 1\\)) concentrates probability, increasing density.</p> <p>For \\(D\\)-dimensional vectors, the Jacobian matrix \\(J_f(\\mathbf{z})\\) is the \\(D \\times D\\) matrix of partial derivatives \\([\\frac{\\partial f_i}{\\partial z_j}]\\). Computing a general determinant requires \\(O(D^3)\\) operations, which becomes intractable for high-dimensional data like 256\u00d7256 RGB images with \\(D = 196{,}608\\) dimensions.</p> <p>The entire field of normalizing flows revolves around designing transformations with structured Jacobians\u2014triangular, diagonal, or block-structured matrices where determinants reduce to \\(O(D)\\) computations.</p>"},{"location":"user-guide/concepts/flow-explained/#composing-multiple-transformations","title":"Composing Multiple Transformations","text":"<p>A single invertible transformation typically provides limited modeling capacity. The power of flows emerges through composition: stacking \\(K\\) transformations:</p> \\[ \\mathbf{x} = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(\\mathbf{z}) \\] <p>The log-likelihood decomposes additively:</p> \\[ \\log p_\\mathcal{X}(\\mathbf{x}) = \\log p_\\mathcal{Z}(\\mathbf{z}_0) + \\sum_{k=1}^{K} \\log \\left| \\det \\frac{\\partial f_k}{\\partial \\mathbf{z}_{k-1}} \\right| \\] <p>where \\(\\mathbf{z}_0 = \\mathbf{z}\\) and \\(\\mathbf{z}_k = f_k(\\mathbf{z}_{k-1})\\) for \\(k=1,\\ldots,K\\).</p> <pre><code>graph LR\n    Z0[\"z\u2080&lt;br/&gt;(Base)\"] --&gt; F1[\"f\u2081\"]\n    F1 --&gt; Z1[\"z\u2081\"]\n    Z1 --&gt; F2[\"f\u2082\"]\n    F2 --&gt; Z2[\"z\u2082\"]\n    Z2 --&gt; Dots[\"...\"]\n    Dots --&gt; FK[\"f_K\"]\n    FK --&gt; X[\"x&lt;br/&gt;(Data)\"]\n\n    F1 -.-&gt;|\"log|det J\u2081|\"| LogDet1[\"\u03a3 log-det\"]\n    F2 -.-&gt;|\"log|det J\u2082|\"| LogDet1\n    FK -.-&gt;|\"log|det J_K|\"| LogDet1\n\n    style Z0 fill:#e1f5ff\n    style X fill:#ffe1e1\n    style LogDet1 fill:#fff3cd</code></pre> <p>Additive Structure in Log-Space</p> <p>The chain rule for Jacobians states \\(\\det J_{f_2 \\circ f_1}(\\mathbf{u}) = \\det J_{f_2}(f_1(\\mathbf{u})) \\cdot \\det J_{f_1}(\\mathbf{u})\\), so log-determinants simply add: \\(\\log|\\det J_\\text{total}| = \\sum_k \\log|\\det J_k|\\). This ensures numerical stability and makes total computational cost \\(O(KD)\\) when each layer has \\(O(D)\\) Jacobian computation.</p>"},{"location":"user-guide/concepts/flow-explained/#three-requirements-for-flow-layers","title":"Three Requirements for Flow Layers","text":"<p>For a transformation \\(f\\) to be a valid flow layer, it must satisfy:</p> <ol> <li>Invertibility: \\(f\\) must be bijective (one-to-one and onto)</li> <li>Efficient Jacobian: \\(\\log \\left| \\det \\frac{\\partial f}{\\partial \\mathbf{z}} \\right|\\) must be tractable to compute</li> <li>Efficient Inverse: \\(f^{-1}\\) must be computable efficiently (for sampling)</li> </ol> <p>Different flow architectures make different trade-offs among these requirements.</p>"},{"location":"user-guide/concepts/flow-explained/#base-distribution","title":"Base Distribution","text":"<p>The base distribution \\(p_\\mathcal{Z}(\\mathbf{z})\\) is typically chosen to be simple for efficient sampling:</p> <p>Standard Gaussian (most common):</p> \\[ p_\\mathcal{Z}(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}; \\mathbf{0}, \\mathbf{I}) = \\frac{1}{(2\\pi)^{D/2}} \\exp\\left(-\\frac{1}{2}\\|\\mathbf{z}\\|^2\\right) \\] <p>Uniform (less common):</p> \\[ p_\\mathcal{Z}(\\mathbf{z}) = \\mathcal{U}(\\mathbf{z}; -a, a) = \\frac{1}{(2a)^D} \\mathbb{1}_{[-a,a]^D}(\\mathbf{z}) \\]"},{"location":"user-guide/concepts/flow-explained/#flow-model-architectures","title":"Flow Model Architectures","text":"<p>Artifex provides implementations of several state-of-the-art flow architectures, each with different trade-offs between expressiveness, computational efficiency, and ease of use.</p>"},{"location":"user-guide/concepts/flow-explained/#1-nice-pioneering-coupling-layers","title":"1. NICE: Pioneering Coupling Layers","text":"<p>NICE (Non-linear Independent Components Estimation) introduced additive coupling layers that made normalizing flows practical for high-dimensional data.</p> <p>Coupling Layer Mechanism:</p> <p>Given input \\(\\mathbf{x} \\in \\mathbb{R}^D\\), partition into \\((\\mathbf{x}_1, \\mathbf{x}_2)\\):</p> \\[ \\begin{align} \\mathbf{y}_1 &amp;= \\mathbf{x}_1 \\\\ \\mathbf{y}_2 &amp;= \\mathbf{x}_2 + m(\\mathbf{x}_1) \\end{align} \\] <p>where \\(m\\) can be any arbitrary function (typically a neural network).</p> <p>Key Properties:</p> <ul> <li>Volume-preserving: \\(\\log|\\det(\\mathbf{J})| = 0\\) (determinant is exactly 1)</li> <li>Efficient inverse: \\(\\mathbf{x}_1 = \\mathbf{y}_1\\), \\(\\mathbf{x}_2 = \\mathbf{y}_2 - m(\\mathbf{y}_1)\\)</li> <li>No Jacobian computation: The triangular structure makes the determinant trivial</li> <li>Arbitrary coupling function: \\(m\\) can be arbitrarily complex without affecting computational cost</li> </ul> <pre><code>graph TB\n    X[\"Input x\"] --&gt; Split[\"Partition&lt;br/&gt;(x\u2081, x\u2082)\"]\n    Split --&gt; X1[\"x\u2081&lt;br/&gt;(unchanged)\"]\n    Split --&gt; X2[\"x\u2082\"]\n\n    X1 --&gt; NN[\"Neural Network&lt;br/&gt;m(x\u2081)\"]\n    NN --&gt; Add[\"y\u2082 = x\u2082 + m(x\u2081)\"]\n    X2 --&gt; Add\n\n    X1 --&gt; Concat[\"Concatenate\"]\n    Add --&gt; Concat\n    Concat --&gt; Y[\"Output y\"]\n\n    style X fill:#e1f5ff\n    style Y fill:#ffe1e1\n    style NN fill:#fff3cd</code></pre> <p>When to Use NICE:</p> <ul> <li>Fast forward and inverse computations required</li> <li>Volume-preserving transformations are acceptable</li> <li>Starting point for understanding coupling layers</li> <li>Lower-dimensional problems (hundreds of dimensions)</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#2-realnvp-adding-scale-for-expressiveness","title":"2. RealNVP: Adding Scale for Expressiveness","text":"<p>RealNVP (Real-valued Non-Volume Preserving) extends NICE with affine coupling layers:</p> \\[ \\begin{align} \\mathbf{y}_1 &amp;= \\mathbf{x}_1 \\\\ \\mathbf{y}_2 &amp;= \\mathbf{x}_2 \\odot \\exp(s(\\mathbf{x}_1)) + t(\\mathbf{x}_1) \\end{align} \\] <p>where \\(s(\\cdot)\\) and \\(t(\\cdot)\\) are neural networks outputting scale and translation, and \\(\\odot\\) denotes element-wise multiplication.</p> <p>Key Properties:</p> <ul> <li>Tractable Jacobian: \\(\\log|\\det(\\mathbf{J})| = \\sum_i s_i(\\mathbf{x}_1)\\)</li> <li>Efficient inverse:   $$   \\begin{align}   \\mathbf{x}_1 &amp;= \\mathbf{y}_1 \\   \\mathbf{x}_2 &amp;= (\\mathbf{y}_2 - t(\\mathbf{y}_1)) \\odot \\exp(-s(\\mathbf{y}_1))   \\end{align}   $$</li> <li>Alternating masks: Alternate which dimensions are transformed across layers</li> <li>No gradient through scale/translation: \\(s\\) and \\(t\\) can be arbitrarily complex ResNets</li> </ul> <pre><code>graph TB\n    X[\"Input x\"] --&gt; Split[\"Split&lt;br/&gt;(x\u2081, x\u2082)\"]\n    Split --&gt; X1[\"x\u2081&lt;br/&gt;(unchanged)\"]\n    Split --&gt; X2[\"x\u2082\"]\n\n    X1 --&gt; NN[\"Neural Networks&lt;br/&gt;s(x\u2081), t(x\u2081)\"]\n    NN --&gt; Scale[\"exp(s)\"]\n    NN --&gt; Trans[\"t\"]\n\n    X2 --&gt; Mult[\"\u2299\"]\n    Scale --&gt; Mult\n    Mult --&gt; Add[\"+ t\"]\n    Trans --&gt; Add\n    Add --&gt; Y2[\"y\u2082\"]\n\n    X1 --&gt; Concat[\"Concatenate\"]\n    Y2 --&gt; Concat\n    Concat --&gt; Y[\"Output y\"]\n\n    style X fill:#e1f5ff\n    style Y fill:#ffe1e1\n    style NN fill:#fff3cd</code></pre> <p>Multi-Scale Architecture:</p> <p>RealNVP introduced hierarchical structure that revolutionized flow-based modeling:</p> <ol> <li>Squeeze operation: Reshape \\(s \\times s \\times c\\) tensors into \\(\\frac{s}{2} \\times \\frac{s}{2} \\times 4c\\)</li> <li>Factor out: After several coupling layers, factor out half the channels to the prior</li> <li>Continue processing: Transform remaining channels at higher resolution</li> </ol> <p>This enables modeling 256\u00d7256 images by avoiding the prohibitive cost of applying dozens of layers to all 196,608 dimensions simultaneously.</p> <p>When to Use RealNVP:</p> <ul> <li>Need both fast sampling and density estimation</li> <li>Working with continuous data, especially images</li> <li>Image generation tasks at moderate to high resolution</li> <li>Moderate-dimensional data (hundreds to thousands of dimensions)</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#3-glow-learnable-permutations","title":"3. Glow: Learnable Permutations","text":"<p>Glow extends RealNVP with three key innovations that pushed flows to state-of-the-art density estimation:</p> <p>Glow Block Architecture:</p> <p>Each flow step combines three layers:</p> <pre><code>graph TB\n    X[\"Input\"] --&gt; AN[\"ActNorm&lt;br/&gt;(Activation Normalization)\"]\n    AN --&gt; Conv[\"Invertible 1\u00d71 Conv&lt;br/&gt;(Channel Mixing)\"]\n    Conv --&gt; Coup[\"Affine Coupling Layer&lt;br/&gt;(Transformation)\"]\n    Coup --&gt; Y[\"Output\"]\n\n    style X fill:#e1f5ff\n    style Y fill:#ffe1e1\n    style AN fill:#d4edda\n    style Conv fill:#d1ecf1\n    style Coup fill:#fff3cd</code></pre> <p>1. ActNorm (Activation Normalization):</p> <p>Per-channel affine transformation with trainable parameters:</p> \\[ y_{i,j,c} = s_c \\cdot x_{i,j,c} + b_c \\] <ul> <li>Data-dependent initialization: normalize first minibatch to zero mean, unit variance</li> <li>Enables training with batch size 1 (critical for high-resolution images)</li> <li>\\(\\log|\\det J| = H \\cdot W \\cdot \\sum_c \\log|s_c|\\) for \\(H \\times W\\) spatial dimensions</li> </ul> <p>2. Invertible 1\u00d71 Convolution:</p> <p>Learned linear mixing of channels using invertible matrix \\(\\mathbf{W}\\):</p> \\[ \\mathbf{y} = \\mathbf{W} \\mathbf{x} \\] <ul> <li>Replaces fixed permutations with learned channel mixing</li> <li>LU decomposition: \\(\\mathbf{W} = \\mathbf{P} \\cdot \\mathbf{L} \\cdot (\\mathbf{U} + \\text{diag}(\\mathbf{s}))\\)</li> <li>Determinant: \\(\\log|\\det \\mathbf{W}| = \\sum_i \\log|s_i|\\) (reduced to \\(O(c)\\))</li> <li>Improved log-likelihood by ~0.5 bits/dimension over fixed permutations</li> </ul> <p>3. Affine Coupling Layer:</p> <p>Similar to RealNVP but with the above improvements.</p> <p>When to Use Glow:</p> <ul> <li>High-resolution image generation (256\u00d7256 and above)</li> <li>Need state-of-the-art sample quality</li> <li>Have sufficient computational resources</li> <li>Want to leverage multi-scale processing</li> </ul> <p>Implementation Detail</p> <p>Glow achieves 3.35 bits/dimension on CIFAR-10 with 3 scales of 32 steps each (96 total transformations) and coupling networks using 512-channel 3-layer ResNets.</p>"},{"location":"user-guide/concepts/flow-explained/#4-maf-masked-autoregressive-flow","title":"4. MAF: Masked Autoregressive Flow","text":"<p>MAF uses autoregressive transformations where each dimension depends on all previous dimensions, providing maximum expressiveness at the cost of sequential sampling.</p> <p>Autoregressive Transformation:</p> \\[ z_i = (x_i - \\mu_i(x_{&lt;i})) \\cdot \\exp(-\\alpha_i(x_{&lt;i})) \\] <p>where \\(\\mu_i\\) and \\(\\alpha_i\\) are computed by a MADE (Masked Autoencoder for Distribution Estimation) network.</p> <p>MADE Architecture:</p> <p>Uses masked connections to ensure autoregressive property\u2014each output depends only on previous inputs:</p> <pre><code>graph TB\n    X1[\"x\u2081\"] --&gt; H1[\"h\u2081\"]\n    X2[\"x\u2082\"] --&gt; H1\n    X2 --&gt; H2[\"h\u2082\"]\n    X3[\"x\u2083\"] --&gt; H2\n    X3 --&gt; H3[\"h\u2083\"]\n\n    H1 --&gt; Z1[\"\u03bc\u2081, \u03b1\u2081\"]\n    H1 --&gt; Z2[\"\u03bc\u2082, \u03b1\u2082\"]\n    H2 --&gt; Z2\n    H2 --&gt; Z3[\"\u03bc\u2083, \u03b1\u2083\"]\n    H3 --&gt; Z3\n\n    style X1 fill:#e1f5ff\n    style X2 fill:#e1f5ff\n    style X3 fill:#e1f5ff\n    style Z1 fill:#ffe1e1\n    style Z2 fill:#ffe1e1\n    style Z3 fill:#ffe1e1</code></pre> <p>Trade-offs:</p> Direction Complexity Use Case Forward (density) \\(O(1)\\) passes All dimensions computed in parallel Inverse (sampling) \\(O(D)\\) passes Sequential computation required <p>When to Use MAF:</p> <ul> <li>Density estimation is the primary goal</li> <li>Sampling speed is less critical</li> <li>Tabular or low-to-moderate dimensional data</li> <li>Need highly expressive transformations</li> <li>All dimensions should interact</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#5-iaf-inverse-autoregressive-flow","title":"5. IAF: Inverse Autoregressive Flow","text":"<p>IAF is the \"inverse\" of MAF with opposite computational trade-offs:</p> \\[ y_i = x_i \\cdot \\exp(\\alpha_i(y_{&lt;i})) + \\mu_i(y_{&lt;i}) \\] <p>Trade-offs:</p> Direction Complexity Use Case Forward (density) \\(O(D)\\) passes Sequential computation Inverse (sampling) \\(O(1)\\) passes All dimensions computed in parallel <p>When to Use IAF:</p> <ul> <li>Fast sampling is the primary goal</li> <li>Density estimation is secondary or not needed</li> <li>Variational inference (amortized inference in VAEs)</li> <li>Real-time generation applications</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#6-neural-spline-flows","title":"6. Neural Spline Flows","text":"<p>Neural Spline Flows use monotonic rational-quadratic splines to create highly expressive yet tractable transformations.</p> <p>Rational-Quadratic Spline Transform:</p> <p>Each spline maps interval \\([-B, B]\\) to itself using \\(K\\) rational-quadratic segments, parameterized by:</p> <ul> <li>\\(K+1\\) knot positions \\(\\{(x^{(k)}, y^{(k)})\\}\\)</li> <li>\\(K+1\\) derivative values \\(\\{\\delta^{(k)}\\}\\)</li> </ul> <p>Within segment \\(k\\), the transformation applies a ratio of quadratic polynomials.</p> <p>Key Properties:</p> <ul> <li>Strict monotonicity: Ensures invertibility</li> <li>Smooth derivatives: No discontinuities (unlike piecewise-linear)</li> <li>Closed-form operations: Forward evaluation, analytic inverse (quadratic equation), closed-form derivative</li> <li>Universal approximation: With sufficient bins (8-16 typically suffice)</li> </ul> <pre><code>graph LR\n    Input[\"x\"] --&gt; Spline[\"Monotonic&lt;br/&gt;Rational-Quadratic&lt;br/&gt;Spline\"]\n    Spline --&gt; Output[\"y\"]\n    Params[\"Knot positions&lt;br/&gt;Derivatives\"] -.-&gt; Spline\n\n    style Input fill:#e1f5ff\n    style Output fill:#ffe1e1\n    style Params fill:#fff3cd</code></pre> <p>Compared to alternatives:</p> <ul> <li>vs Affine: ~23 parameters per dimension vs 2, much more expressive</li> <li>vs Neural Autoregressive Flows: No iterative root-finding needed</li> <li>vs Flow++: No bisection algorithms required</li> <li>vs Piecewise-linear: Smooth derivatives improve optimization</li> </ul> <p>Results:</p> <ul> <li>CIFAR-10: 3.38 bits/dimension using 10\u00d7 fewer parameters than Glow</li> <li>Best-in-class likelihood on multiple density estimation benchmarks</li> </ul> <p>When to Use Neural Spline Flows:</p> <ul> <li>Maximum expressiveness with tractability</li> <li>Density estimation on complex distributions</li> <li>Want fewer parameters than Glow</li> <li>Need smooth, differentiable transformations</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#training-normalizing-flows","title":"Training Normalizing Flows","text":""},{"location":"user-guide/concepts/flow-explained/#maximum-likelihood-objective","title":"Maximum Likelihood Objective","text":"<p>Flow training optimizes the straightforward objective of maximum likelihood:</p> \\[ \\max_\\theta \\mathbb{E}_{x \\sim p_\\text{data}}[\\log p_\\theta(x)] \\] <p>Equivalently, minimize negative log-likelihood:</p> \\[ \\mathcal{L}(\\theta) = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log p_u(f^{-1}_\\theta(x)) + \\log \\left| \\det \\frac{\\partial f^{-1}_\\theta}{\\partial x} \\right|\\right] \\] <p>This simplicity contrasts sharply with:</p> <ul> <li>GANs: Adversarial minimax optimization with mode collapse risks</li> <li>VAEs: ELBO with reconstruction-regularization trade-off</li> <li>Diffusion: Multi-step denoising with noise schedule design</li> </ul> <p>Training Stability</p> <p>Gradients flow through the entire composition automatically via backpropagation. Standard optimizers like Adam with learning rates around \\(10^{-3}\\) work reliably. The monotonic improvement of likelihood makes training highly stable.</p>"},{"location":"user-guide/concepts/flow-explained/#critical-preprocessing-steps","title":"Critical Preprocessing Steps","text":"<p>Proper preprocessing proves essential for successful training:</p> <p>1. Dequantization:</p> <p>Discrete data (e.g., uint8 images) create delta peaks in continuous space, allowing flows to assign arbitrarily high likelihood to exact discrete values while ignoring intermediate regions.</p> <pre><code># Uniform dequantization\nx_dequantized = x + torch.rand_like(x) / 256.0\n\n# Variational dequantization (more sophisticated)\nnoise = flow_model_for_noise(x)\nx_dequantized = x + noise / 256.0\n</code></pre> <p>2. Logit Transform:</p> <p>Maps bounded \\([0,1]\\) data to unbounded \\((-\\infty, +\\infty)\\) space matching Gaussian priors:</p> <pre><code># Add small constant for numerical stability\nalpha = 0.05\nx = alpha + (1 - 2*alpha) * x\n\n# Apply logit transform\nx_logit = torch.logit(x)  # log(x / (1-x))\n</code></pre> <p>Critical Importance</p> <p>Without these preprocessing steps, training diverges immediately as the model tries to match bounded data to Gaussian base distributions.</p>"},{"location":"user-guide/concepts/flow-explained/#numerical-stability-techniques","title":"Numerical Stability Techniques","text":"<p>1. Log-Space Computation:</p> <p>Never compute \\(\\det(J)\\) directly\u2014immediate overflow/underflow:</p> <pre><code># WRONG\ndet_J = torch.det(jacobian)\nlog_det = torch.log(det_J)  # Overflow!\n\n# CORRECT\nlog_det = torch.logdet(jacobian)\n# Or for triangular Jacobians:\nlog_det = torch.sum(torch.log(torch.abs(torch.diagonal(jacobian))))\n</code></pre> <p>2. Gradient Clipping:</p> <p>Prevents exploding gradients in deep architectures (10-15+ layers):</p> <pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre> <p>3. Normalization Layers:</p> <p>Batch normalization or ActNorm stabilizes intermediate representations:</p> <pre><code># ActNorm: data-dependent initialization\nscale, bias = compute_initial_stats(first_batch)\n# Then treat as learnable parameters\n</code></pre> <p>4. Learning Rate Schedules:</p> <p>Polynomial decay with warmup improves convergence:</p> <pre><code># Warmup: 2-10 epochs linear increase\n# Main training: polynomial decay from 1e-3 to 1e-4\nscheduler = PolynomialDecay(optimizer, warmup_steps=1000,\n                           total_steps=100000)\n</code></pre>"},{"location":"user-guide/concepts/flow-explained/#monitoring-training","title":"Monitoring Training","text":"<p>Watch these metrics:</p> <ol> <li>Negative log-likelihood: Should decrease steadily</li> <li>Per-layer log-determinants: Monitor for sudden spikes (numerical issues)</li> <li>Reconstruction error: \\(\\|x - f(f^{-1}(x))\\| &lt; 10^{-5}\\) for numerical stability</li> <li>Bits per dimension: For images, \\(\\text{bpd} = \\frac{\\text{NLL}}{D \\cdot \\log 2}\\)</li> </ol> <pre><code># Invertibility check\nx_reconstructed = flow.inverse(flow.forward(x))\nrecon_error = torch.mean(torch.abs(x - x_reconstructed))\nassert recon_error &lt; 1e-5, f\"Poor invertibility: {recon_error}\"\n</code></pre>"},{"location":"user-guide/concepts/flow-explained/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":"<ul> <li> <p> Missing Preprocessing</p> <p>Symptom: Immediate divergence or NaN losses</p> <p>Solution: Always dequantize discrete data and apply logit transform</p> </li> <li> <p> Numerical Instability</p> <p>Symptom: Sudden spikes in log-determinants or NaN gradients</p> <p>Solution: Use log-space computation, gradient clipping, monitor per-layer statistics</p> </li> <li> <p> Poor Invertibility</p> <p>Symptom: \\(\\|x - f^{-1}(f(x))\\| &gt; 10^{-3}\\)</p> <p>Solution: Use residual flows with soft-thresholding, reduce depth, check numerical precision</p> </li> <li> <p> Slow Convergence</p> <p>Symptom: Likelihood plateaus early</p> <p>Solution: Increase model capacity, add more layers, use spline flows, check preprocessing</p> </li> </ul>"},{"location":"user-guide/concepts/flow-explained/#advanced-architectures-and-recent-advances","title":"Advanced Architectures and Recent Advances","text":""},{"location":"user-guide/concepts/flow-explained/#continuous-normalizing-flows-neural-odes","title":"Continuous Normalizing Flows (Neural ODEs)","text":"<p>Continuous flows parameterize the derivative of the hidden state:</p> \\[ \\frac{d\\mathbf{z}_t}{dt} = f(\\mathbf{z}_t, t, \\theta) \\] <p>The output \\(\\mathbf{z}_1\\) at time \\(t=1\\) given initial condition \\(\\mathbf{z}_0\\) at \\(t=0\\) is computed using ODE solvers.</p> <p>Key Innovation (FFJORD):</p> <p>The change in log-density follows:</p> \\[ \\frac{d \\log p(\\mathbf{z}_t)}{dt} = -\\text{Tr}\\left(\\frac{\\partial f}{\\partial \\mathbf{z}_t}\\right) \\] <p>Hutchinson's trace estimator makes this tractable:</p> \\[ \\text{Tr}(A) \\approx \\mathbf{v}^T A \\mathbf{v} \\quad \\text{where } \\mathbf{v} \\sim \\mathcal{N}(0, I) \\] <p>This unbiased stochastic estimate requires only one Jacobian-vector product per sample, reducing complexity from \\(O(D^2)\\) to \\(O(D)\\).</p> <p>Advantages:</p> <ul> <li>No architectural constraints: Any neural network architecture works</li> <li>Flexible expressiveness: Can model disconnected regions and sharp boundaries</li> <li>Adjoint method: Memory-efficient training (\\(O(1)\\) memory vs \\(O(\\text{depth})\\))</li> </ul> <p>Challenges:</p> <ul> <li>Unpredictable cost: Number of function evaluations adapts to complexity</li> <li>Stiff dynamics: Can struggle with certain distributions</li> <li>Slower than discrete flows: Requires ODE integration</li> </ul> <p>When to Use:</p> <ul> <li>Modeling distributions with disconnected regions</li> <li>Physics simulation, molecular dynamics</li> <li>Scientific domains requiring flexible unrestricted networks</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#residual-flows-invertible-resnets","title":"Residual Flows: Invertible ResNets","text":"<p>Residual flows make standard ResNet architectures \\(F(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x})\\) invertible by constraining \\(g\\) to be contractive with Lipschitz constant \\(L &lt; 1\\).</p> <p>Invertibility via Fixed-Point Iteration:</p> <p>The Banach fixed-point theorem guarantees bijection with inverse computable via:</p> \\[ \\mathbf{x}_{k+1} = \\mathbf{y} - g(\\mathbf{x}_k) \\] <p>which converges exponentially to the true inverse.</p> <p>Spectral Normalization:</p> <p>Enforces the constraint by normalizing weight matrices:</p> \\[ \\mathbf{W} \\leftarrow \\frac{\\mathbf{W}}{\\|\\mathbf{W}\\|_2 / c} \\quad \\text{where } c &lt; 1 \\] <p>The spectral norm \\(\\|\\mathbf{W}\\|_2\\) is estimated via power iteration.</p> <p>Russian Roulette Estimator:</p> <p>For \\(F(\\mathbf{x}) = \\mathbf{x} + g(\\mathbf{x})\\), the log-determinant has power series:</p> \\[ \\log|\\det(I + J_g)| = \\sum_{k=1}^{\\infty} \\frac{(-1)^{k+1}}{k} \\text{Tr}(J_g^k) \\] <p>Rather than truncating (introducing bias), Russian roulette randomly terminates the series with probability ensuring unbiasedness while maintaining finite computation.</p> <p>When to Use:</p> <ul> <li>High-dimensional problems (&gt;1000 dimensions)</li> <li>Want free-form Jacobians (all dimensions interact)</li> <li>Need competitive density estimation with flexibility</li> <li>Extensions like Invertible DenseNets for parameter efficiency</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#flow-matching-simulation-free-training","title":"Flow Matching: Simulation-Free Training","text":"<p>Flow Matching (2022) introduced a paradigm shift for training continuous normalizing flows without ODE simulation.</p> <p>Key Idea:</p> <p>Rather than integrating forward dynamics during training (as in Neural ODEs), perform regression on the vector field of fixed conditional probability paths.</p> <p>Training Procedure:</p> <ol> <li>Given samples \\(\\mathbf{x}_0 \\sim p_0\\) and \\(\\mathbf{x}_1 \\sim p_1\\)</li> <li>Define interpolant: \\(\\mathbf{x}_t = (1-t)\\mathbf{x}_0 + t\\mathbf{x}_1\\)</li> <li>Train neural network \\(\\mathbf{v}_\\theta(\\mathbf{x}_t, t)\\) to match conditional vector field:</li> </ol> \\[ \\min_\\theta \\mathbb{E}_{t, \\mathbf{x}_0, \\mathbf{x}_1}\\left[\\|\\mathbf{v}_\\theta(\\mathbf{x}_t, t) - (\\mathbf{x}_1 - \\mathbf{x}_0)\\|^2\\right] \\] <p>This is simple L2 regression requiring no simulation.</p> <p>Inference:</p> <p>Integrate the learned field using standard ODE solvers:</p> \\[ \\frac{d\\mathbf{x}}{dt} = \\mathbf{v}_\\theta(\\mathbf{x}, t), \\quad t \\in [0, 1] \\] <p>Optimal Transport Flow Matching:</p> <p>Uses minibatch optimal transport to couple noise and data samples before interpolation, creating straighter paths that require fewer integration steps.</p> <p>Results:</p> <ul> <li>State-of-the-art on ImageNet</li> <li>Better likelihood and sample quality than simulation-based methods</li> <li>Extensions to Riemannian manifolds, discrete data, video generation</li> </ul> <p>When to Use:</p> <ul> <li>Training continuous flows efficiently</li> <li>Want simulation-free gradients</li> <li>Need state-of-the-art likelihood</li> <li>2023-2024 cutting-edge research</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#rectified-flows-learning-straight-trajectories","title":"Rectified Flows: Learning Straight Trajectories","text":"<p>Rectified Flow (2022) learns ODEs following straight-line paths connecting source and target distributions.</p> <p>Training:</p> <p>Given coupling between noise samples \\(\\mathbf{u} \\sim p_0\\) and data samples \\(\\mathbf{x} \\sim p_1\\):</p> <ol> <li>Linearly interpolate: \\(\\mathbf{x}_t = (1-t)\\mathbf{u} + t\\mathbf{x}\\)</li> <li>Learn velocity field: \\(\\frac{d\\mathbf{x}_t}{dt} = \\mathbf{v}_\\theta(\\mathbf{x}_t, t)\\)</li> </ol> <p>Reflow Process (Key Innovation):</p> <ol> <li>Train initial model</li> <li>Generate paired samples: \\((\\mathbf{u}, \\mathbf{x}_{\\text{gen}})\\) where \\(\\mathbf{x}_{\\text{gen}} = \\text{model}(\\mathbf{u})\\)</li> <li>Retrain on these pairs</li> </ol> <p>This iterative rectification progressively straightens trajectories.</p> <p>Benefits:</p> <ul> <li>Provably non-increasing convex transport costs</li> <li>One-step or few-step generation: Straight paths require minimal integration</li> <li>One reflow iteration typically suffices under realistic settings</li> </ul> <p>Applications:</p> <ul> <li>Stable Diffusion 3: Uses rectified flow formulation, outperforms pure diffusion</li> <li>InstaFlow: Achieves 0.1-second generation demonstrating practical viability</li> <li>One-step generation for real-time applications</li> </ul> <p>When to Use:</p> <ul> <li>Need few-step or one-step generation</li> <li>Real-time applications requiring fast inference</li> <li>Want to distill models for deployment</li> <li>State-of-the-art 2023-2024 research</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#discrete-flow-matching","title":"Discrete Flow Matching","text":"<p>Discrete Flow Matching (2024) extends flows to discrete data (text, molecules, code) using Continuous-Time Markov Chains (CTMC).</p> <p>Problem:</p> <p>Traditional flows designed for continuous data. Dequantization workarounds prove inadequate for inherently discrete data like language.</p> <p>Solution:</p> <p>CTMC process with learnable time-dependent transition rates:</p> \\[ \\frac{dp_t(x)}{dt} = \\sum_{x'} p_t(x') R_t(x' \\to x) - p_t(x) \\sum_{x'} R_t(x \\to x') \\] <p>Training:</p> <p>Regress on conditional flow:</p> \\[ \\min_\\theta \\mathbb{E}_{t, x_0, x_1}\\left[\\|R_\\theta(x_t, t) - R_{\\text{true}}(x_t, t | x_0, x_1)\\|^2\\right] \\] <p>Results:</p> <ul> <li>FlowMol-CTMC: State-of-the-art molecular validity</li> <li>Code generation: 1.7B parameter model achieves 13.4% Pass@10 on HumanEval</li> <li>DNA sequence design: Dirichlet Flow Matching</li> </ul> <p>When to Use:</p> <ul> <li>Text generation (alternative to autoregressive)</li> <li>Molecular generation with discrete atom types</li> <li>Code generation</li> <li>Any discrete structured data</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#geometric-flows-riemannian-manifolds","title":"Geometric Flows: Riemannian Manifolds","text":"<p>Riemannian Flow Matching extends flows to non-Euclidean geometries, critical for data on manifolds.</p> <p>Applications:</p> <ul> <li>Molecular conformations: SE(3) equivariant flows</li> <li>Protein structures: SO(3) rotations and translations</li> <li>Robotic configurations: Configuration space manifolds</li> <li>Materials: FlowMM for crystal structure generation (3\u00d7 efficiency improvement)</li> </ul> <p>Key Idea:</p> <p>Replace Euclidean straight-line interpolants with geodesics on the manifold:</p> \\[ \\frac{d\\mathbf{x}_t}{dt} = \\mathbf{v}_\\theta(\\mathbf{x}_t, t) \\quad \\text{on manifold } \\mathcal{M} \\] <p>When to Use:</p> <ul> <li>Data naturally lives on manifolds</li> <li>Symmetries and geometric constraints are important</li> <li>Protein design, molecular generation, materials discovery</li> <li>3D geometry and robotics applications</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#comparing-flows-with-other-generative-models","title":"Comparing Flows with Other Generative Models","text":""},{"location":"user-guide/concepts/flow-explained/#flows-vs-vaes-exact-likelihood-vs-learned-compression","title":"Flows vs VAEs: Exact Likelihood vs Learned Compression","text":"Aspect Normalizing Flows VAEs Likelihood Exact Approximate (ELBO lower bound) Dimensionality Input = Output Compressed latent (\\(\\dim(z) \\ll \\dim(x)\\)) Sample Quality Sharp (historically) Blurry (reconstruction loss) Training Maximum likelihood ELBO (reconstruction + KL) Mode Coverage Excellent (exact distribution) Can suffer posterior collapse Generation Speed Fast (single pass) Fast (single pass) Interpretability Limited Compressed representations <p>When to Choose:</p> <ul> <li>Use Flows when exact likelihood is essential (anomaly detection, density estimation, model comparison) or lossless reconstruction matters</li> <li>Use VAEs when compressed latent representations provide value for downstream tasks, interpretability matters, or computational constraints favor smaller latent spaces</li> <li>Hybrid f-VAEs: Combine both\u2014VAEs with flow-based posteriors or decoders</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#flows-vs-gans-mode-coverage-vs-sample-quality","title":"Flows vs GANs: Mode Coverage vs Sample Quality","text":"Aspect Normalizing Flows GANs Sample Quality Sharp, competitive with TarFlow (2024) Superior perceptual quality (historically) Training Stability Guaranteed convergence Notorious instability, mode collapse Likelihood Exact computation No likelihood evaluation Mode Coverage Complete Suffers from mode collapse Evaluation Negative log-likelihood FID, Inception Score Consistency Input-output consistency May hallucinate details <p>2020 Liu &amp; Gretton Study</p> <p>Empirical comparison on synthetic data showed several normalizing flows substantially outperformed WGAN in Wasserstein distance\u2014the very metric WGAN targets. No GAN tested could model simple distributions well.</p> <p>When to Choose:</p> <ul> <li>Use Flows for stable reliable training, guaranteed mode coverage, likelihood evaluation, consistent generation</li> <li>Use GANs when perceptual quality dominates all other concerns and substantial tuning expertise is available</li> <li>Modern landscape: TarFlow (2024) shows flows can match GAN quality with proper architecture</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#flows-vs-diffusion-speed-vs-quality-with-converging-trajectories","title":"Flows vs Diffusion: Speed vs Quality with Converging Trajectories","text":"<p>Historical Trade-off:</p> <ul> <li>Diffusion: Superior sample quality, but 50-1000 iterative denoising steps</li> <li>Flows: Fast single-step sampling, but lower perceptual quality</li> </ul> <p>2023-2024 Developments Disrupted This:</p> <ol> <li>Rectified flows: Straight paths enabling few-step generation</li> <li>Flow matching: Simulation-free training matching diffusion quality</li> <li>TarFlow (2024): Transformer-based flows matching diffusion quality while maintaining one-step generation</li> </ol> Aspect Normalizing Flows (Modern) Diffusion Models Sampling Speed 1-10 steps (TarFlow, Rectified) 50-1000 steps (DDPM) or 10-50 (DDIM) Sample Quality Matching diffusion (TarFlow 2024) Excellent Likelihood Exact Tractable (learned) Training Stability Stable (NLL) Stable (denoising) Jacobian Computation Required (\\(O(D^3)\\) \u2192 structured) Not required Architectural Constraints Invertibility, equal dimensions Flexible, no constraints <p>Diffusion Normalizing Flow (2021)</p> <p>Demonstrated 20\u00d7 speedup over standard diffusion with comparable quality by combining learnable flow-based forward processes with stochastic elements.</p> <p>When to Choose:</p> <ul> <li>Use Flows for real-time applications (audio synthesis, interactive systems), exact likelihood scoring (anomaly detection), computational constraints at inference</li> <li>Use Diffusion when sample quality is paramount and computational resources permit slower generation</li> <li>Hybrid Approaches: Flow matching unifies continuous flows and diffusion under common framework</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#practical-implementation-guidance","title":"Practical Implementation Guidance","text":""},{"location":"user-guide/concepts/flow-explained/#framework-and-package-selection","title":"Framework and Package Selection","text":"<p>Modern Flow Implementations:</p> <ul> <li> <p> PyTorch Ecosystem</p> </li> <li> <p>normflows (1000+ stars): Comprehensive architectures (RealNVP, Glow, NSF, MAF)</p> </li> <li>nflows: State-of-the-art methods from Edinburgh group (creators of spline flows)</li> <li> <p>Zuko: Modern PyTorch implementation with clean API</p> </li> <li> <p> TensorFlow Probability</p> </li> <li> <p>First-party flow support via composable bijectors</p> </li> <li>Production-stable with extensive testing</li> <li> <p>Integrates with TensorFlow ecosystem</p> </li> <li> <p>:simple-jax:{ .lg .middle } JAX Implementations</p> </li> <li> <p>Distrax: High-performance flows with JAX transformations</p> </li> <li>Artifex: This repository\u2014comprehensive flows with Flax/NNX</li> <li>Optimal for scientific computing and research</li> </ul> <p>Framework Choice:</p> <ul> <li>PyTorch: Dominates academic research, excellent debugging</li> <li>TensorFlow: Production stability, enterprise deployment</li> <li>JAX: High-performance scientific computing, automatic differentiation</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#architecture-selection-guide","title":"Architecture Selection Guide","text":"<pre><code>graph TD\n    Start{{\"What's your&lt;br/&gt;primary goal?\"}}\n    Start --&gt;|\"Density Estimation\"| Dense{{\"Dimensionality?\"}}\n    Start --&gt;|\"Fast Sampling\"| Sample{{\"Data Type?\"}}\n    Start --&gt;|\"Both Equally\"| Both[\"RealNVP or Glow\"]\n\n    Dense --&gt;|\"Low-Med&lt;br/&gt;(&lt; 100)\"| MAF[\"MAF&lt;br/&gt;(Masked Autoregressive)\"]\n    Dense --&gt;|\"High&lt;br/&gt;(&gt; 100)\"| Spline[\"Neural Spline Flows\"]\n    Dense --&gt;|\"Very High&lt;br/&gt;(&gt; 1000)\"| Residual[\"Residual Flows or&lt;br/&gt;Continuous Flows\"]\n\n    Sample --&gt;|\"Images\"| Glow[\"Glow\"]\n    Sample --&gt;|\"Tabular/Other\"| IAF[\"IAF or RealNVP\"]\n    Sample --&gt;|\"Real-time\"| Rectified[\"Rectified Flow\"]\n\n    style Start fill:#e1f5ff\n    style Dense fill:#fff3cd\n    style Sample fill:#fff3cd\n    style MAF fill:#d4edda\n    style Spline fill:#d4edda\n    style Glow fill:#d4edda\n    style IAF fill:#d4edda\n    style Both fill:#d4edda\n    style Residual fill:#d4edda\n    style Rectified fill:#d4edda</code></pre>"},{"location":"user-guide/concepts/flow-explained/#recommended-hyperparameters-by-task","title":"Recommended Hyperparameters by Task","text":"<p>Image Generation (Glow/RealNVP):</p> <pre><code>config = {\n    \"num_scales\": 3,\n    \"num_steps_per_scale\": 8-12,\n    \"hidden_channels\": 512,\n    \"num_layers_per_block\": 3,\n    \"batch_size\": 64-128,\n    \"learning_rate\": 1e-3,\n    \"lr_decay\": \"polynomial\",  # decay to 1e-4\n    \"preprocessing\": [\"dequantize\", \"logit_transform\"],\n}\n</code></pre> <p>Density Estimation on Tabular Data (MAF/Neural Spline Flows):</p> <pre><code>config = {\n    \"num_transforms\": 5-10,\n    \"hidden_dims\": [512, 512],  # Match or exceed data dimensionality\n    \"num_bins\": 8-16,  # For spline flows\n    \"batch_size\": 256,\n    \"learning_rate\": 5e-4,\n    \"preprocessing\": [\"standardization\"],\n}\n</code></pre> <p>Variational Inference (IAF/RealNVP):</p> <pre><code>config = {\n    \"num_steps\": 4-8,\n    \"hidden_dims\": [256, 256],\n    \"base_distribution\": \"gaussian\",  # Learned mean/std\n    \"learning_rate\": 1e-3,\n    \"annealing_schedule\": \"linear\",  # For KL term\n}\n</code></pre>"},{"location":"user-guide/concepts/flow-explained/#common-implementation-pitfalls","title":"Common Implementation Pitfalls","text":"<ul> <li> <p> Forgetting Preprocessing</p> <pre><code># WRONG: Direct training on uint8 images\nflow.train(images_uint8)\n\n# CORRECT: Dequantize + logit transform\nimages = dequantize(images_uint8)\nimages = logit_transform(images, alpha=0.05)\nflow.train(images)\n</code></pre> </li> <li> <p> Computing Determinants Directly</p> <pre><code># WRONG: Direct determinant (overflow!)\ndet = torch.det(jacobian)\nlog_det = torch.log(det)\n\n# CORRECT: Log-space computation\nlog_det = torch.logdet(jacobian)\n# Or for triangular matrices:\nlog_det = torch.sum(torch.log(torch.abs(\n    torch.diagonal(jacobian)\n)))\n</code></pre> </li> <li> <p> Ignoring Invertibility Checks</p> <pre><code># Monitor reconstruction error\nx_reconstructed = flow.inverse(flow.forward(x))\nerror = torch.mean(torch.abs(x - x_reconstructed))\n\n# Should be &lt; 1e-5 for numerical stability\nif error &gt; 1e-3:\n    warnings.warn(f\"Poor invertibility: {error}\")\n</code></pre> </li> <li> <p> Wrong Coupling Architecture</p> <pre><code># WRONG: Too deep coupling networks\ncoupling = MLP([512, 512, 512, 512, 512])  # Overkill!\n\n# CORRECT: 2-3 layers sufficient\ncoupling = MLP([512, 512, output_dim])\n</code></pre> </li> </ul>"},{"location":"user-guide/concepts/flow-explained/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>Normalizing flows provide a unique combination of exact likelihood computation, fast sampling, and stable training through invertible transformations with tractable Jacobians.</p>"},{"location":"user-guide/concepts/flow-explained/#core-principles","title":"Core Principles","text":"<ul> <li> <p> Exact Likelihood</p> <p>Flows compute exact probability through change of variables formula, enabling precise density estimation</p> </li> <li> <p> Invertible Architecture</p> <p>Bijective transformations allow both efficient sampling and exact inference</p> </li> <li> <p> Tractable Jacobians</p> <p>Structured Jacobians (triangular, diagonal) reduce complexity from \\(O(D^3)\\) to \\(O(D)\\)</p> </li> <li> <p> Stable Training</p> <p>Maximum likelihood provides clear, monotonic objective without adversarial dynamics</p> </li> <li> <p> Composable Design</p> <p>Stack simple transformations to build arbitrarily complex distributions</p> </li> </ul>"},{"location":"user-guide/concepts/flow-explained/#architecture-selection-matrix","title":"Architecture Selection Matrix","text":"Architecture Density Estimation Fast Sampling Best For RealNVP \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Balanced use, images Glow \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 High-res images, quality MAF \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 Density on tabular data IAF \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Fast sampling, VI Neural Spline \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Maximum expressiveness Continuous \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 No constraints, flexibility Rectified \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 One-step generation"},{"location":"user-guide/concepts/flow-explained/#recent-advances-2023-2025","title":"Recent Advances (2023-2025)","text":"<ol> <li>Flow Matching: Simulation-free training achieving state-of-the-art likelihood</li> <li>Rectified Flows: Straight paths enabling few-step generation (Stable Diffusion 3)</li> <li>TarFlow: First flow matching diffusion quality while maintaining one-step sampling</li> <li>Discrete Flows: CTMC-based flows for text, molecules, code</li> <li>Geometric Flows: Riemannian flow matching for manifold data (proteins, materials)</li> </ol>"},{"location":"user-guide/concepts/flow-explained/#when-to-use-normalizing-flows","title":"When to Use Normalizing Flows","text":"<p>Best Use Cases:</p> <ul> <li>Exact likelihood is essential (anomaly detection, model comparison)</li> <li>Fast generation required (real-time audio, interactive systems)</li> <li>Stable training preferred over adversarial methods</li> <li>Lossless reconstruction needed</li> <li>Mode coverage guarantees important</li> </ul> <p>Avoid When:</p> <ul> <li>Maximum perceptual quality is sole objective (use GANs/diffusion)</li> <li>Compressed representations needed (use VAEs)</li> <li>Architectural flexibility critical (diffusion has fewer constraints)</li> <li>Very high dimensions with limited resources (consider latent diffusion)</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#future-directions","title":"Future Directions","text":"<ul> <li>One-step generation via rectified flows and distillation</li> <li>Pyramidal structures for video and high-resolution media</li> <li>Hybrid models combining flows with diffusion, transformers</li> <li>Scientific applications in materials, proteins, molecular generation</li> <li>Geometric awareness for data on manifolds</li> </ul>"},{"location":"user-guide/concepts/flow-explained/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Flow User Guide</p> <p>Practical usage guide with implementation examples and training workflows</p> </li> <li> <p> Flow API Reference</p> <p>Complete API documentation for RealNVP, Glow, MAF, IAF, and Neural Spline Flows</p> </li> <li> <p> MNIST Tutorial</p> <p>Step-by-step hands-on tutorial: train a flow model on MNIST from scratch</p> </li> <li> <p> Advanced Examples</p> <p>Explore continuous flows, flow matching, and state-of-the-art architectures</p> </li> </ul>"},{"location":"user-guide/concepts/flow-explained/#references-and-further-reading","title":"References and Further Reading","text":""},{"location":"user-guide/concepts/flow-explained/#seminal-papers-must-read","title":"Seminal Papers (Must Read)","text":"<p> Dinh, L., Krueger, D., &amp; Bengio, Y. (2014). \"NICE: Non-linear Independent Components Estimation\" arXiv:1410.8516  First practical coupling layer architecture</p> <p> Dinh, L., Sohl-Dickstein, J., &amp; Bengio, S. (2016). \"Density estimation using Real NVP\" arXiv:1605.08803  Affine coupling layers and multi-scale architecture</p> <p> Kingma, D. P., &amp; Dhariwal, P. (2018). \"Glow: Generative Flow with Invertible 1\u00d71 Convolutions\" arXiv:1807.03039  State-of-the-art image generation with learnable permutations</p> <p> Papamakarios, G., Pavlakou, T., &amp; Murray, I. (2017). \"Masked Autoregressive Flow for Density Estimation\" arXiv:1705.07057  Autoregressive flows for maximum expressiveness</p> <p> Durkan, C., Bekasov, A., Murray, I., &amp; Papamakarios, G. (2019). \"Neural Spline Flows\" arXiv:1906.04032  Monotonic rational-quadratic splines for flexible transformations</p>"},{"location":"user-guide/concepts/flow-explained/#continuous-and-modern-flows","title":"Continuous and Modern Flows","text":"<p> Chen, R. T. Q., Rubanova, Y., Bettencourt, J., &amp; Duvenaud, D. (2018). \"Neural Ordinary Differential Equations\" arXiv:1806.07366  Continuous-time flows using ODE solvers</p> <p> Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., &amp; Duvenaud, D. (2019). \"FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models\" arXiv:1810.01367  Tractable continuous flows with Hutchinson's estimator</p> <p> Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., &amp; Le, M. (2022). \"Flow Matching for Generative Modeling\" arXiv:2210.02747  Simulation-free training paradigm</p> <p> Liu, X., Gong, C., &amp; Liu, Q. (2022). \"Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow\" arXiv:2209.03003  Straight paths for one-step generation</p>"},{"location":"user-guide/concepts/flow-explained/#recent-advances-2023-2025_1","title":"Recent Advances (2023-2025)","text":"<p> Gat, I., et al. (2024). \"Discrete Flow Matching\" arXiv:2407.15595  CTMC-based flows for discrete data (NeurIPS 2024 Spotlight)</p> <p> Zhai, S., et al. (2024). \"Normalizing Flows are Capable Generative Models (TarFlow)\" arXiv:2412.06329  First flow matching diffusion quality</p> <p> Esser, P., et al. (2024). \"Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (Stable Diffusion 3)\" arXiv:2403.03206  Rectified flows in production systems</p> <p> Chen, R. T. Q., &amp; Lipman, Y. (2024). \"Riemannian Flow Matching on General Geometries\" arXiv:2302.03660  Flows on manifolds for geometric data</p>"},{"location":"user-guide/concepts/flow-explained/#comprehensive-surveys","title":"Comprehensive Surveys","text":"<p> Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., &amp; Lakshminarayanan, B. (2021). \"Normalizing Flows for Probabilistic Modeling and Inference\" arXiv:1912.02762 | JMLR 22(57):1-64, 2021  Comprehensive tutorial covering theory and methods</p> <p> Kobyzev, I., Prince, S. J., &amp; Brubaker, M. A. (2020). \"Normalizing Flows: An Introduction and Review of Current Methods\" arXiv:1908.09257 | IEEE TPAMI 2020  Excellent introduction with taxonomy</p>"},{"location":"user-guide/concepts/flow-explained/#online-resources","title":"Online Resources","text":"<p> Lilian Weng's Blog: \"Flow-based Deep Generative Models\" lilianweng.github.io/posts/2018-10-13-flow-models  Comprehensive blog post with clear explanations and visualizations</p> <p> Eric Jang's Tutorial blog.evjang.com/2018/01/nf1.html  Two-part tutorial with code</p> <p> UvA Deep Learning Tutorial 11 uvadlc-notebooks.readthedocs.io  Complete Colab notebooks</p> <p> awesome-normalizing-flows github.com/janosh/awesome-normalizing-flows  Curated list with 700+ papers</p>"},{"location":"user-guide/concepts/gan-explained/","title":"Understanding Generative Adversarial Networks (GANs)","text":"<p>Generative Adversarial Networks (GANs) are a class of generative models that learn to produce realistic samples through an adversarial training process. Two neural networks\u2014a generator and a discriminator\u2014compete in a game-theoretic framework where the generator learns to create increasingly realistic samples while the discriminator learns to distinguish real from fake data.</p> <p>GANs revolutionized machine learning by achieving photorealistic image generation (StyleGAN2 FID: 2.84 on 1024\u00d71024 faces) and enabling unprecedented control through disentangled latent spaces. Despite diffusion models dominating recent headlines, GANs maintain critical advantages in inference speed (100\u00d7 faster) and controllability.</p>"},{"location":"user-guide/concepts/gan-explained/#overview","title":"Overview","text":"<ul> <li> <p> Adversarial Training</p> <p>Two networks compete: generator creates samples, discriminator evaluates them</p> </li> <li> <p> Implicit Density</p> <p>No explicit density model required\u2014learns through game dynamics</p> </li> <li> <p> High-Quality Samples</p> <p>Produces sharp, realistic images without blur common in other models</p> </li> <li> <p> Flexible Architecture</p> <p>Works with various architectures (MLP, CNN, ResNet) and data types</p> </li> </ul>"},{"location":"user-guide/concepts/gan-explained/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"user-guide/concepts/gan-explained/#the-adversarial-game","title":"The Adversarial Game","text":"<p>GANs operate through a minimax game between a generator \\(G\\) that creates synthetic data and a discriminator \\(D\\) that distinguishes real from fake samples. The generator learns to transform random noise \\(z\\) into realistic outputs \\(G(z)\\), while the discriminator estimates the probability that samples originated from real data.</p> <p>The mathematical foundation rests on the minimax objective:</p> \\[ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))] \\] <p>Where:</p> <ul> <li>\\(G\\) is the generator that maps latent vectors \\(z\\) to data space</li> <li>\\(D\\) is the discriminator that outputs probability of input being real</li> <li>\\(p_{data}\\) is the true data distribution</li> <li>\\(p_z\\) is the prior distribution over latent variables (typically Gaussian)</li> <li>\\(V(D,G)\\) measures how well the discriminator distinguishes real from fake</li> </ul> <p>The discriminator maximizes this objective by correctly classifying samples, while the generator minimizes it by producing convincing fakes that fool the discriminator.</p> <pre><code>%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#e1f5ff'}}}%%\ngraph TD\n    A[\"Random Noise&lt;br/&gt;z ~ \ud835\udca9(0,I)\"] --&gt;|Generator G| B[\"Fake Sample&lt;br/&gt;G(z)\"]\n    C[\"Real Data&lt;br/&gt;x ~ p_data\"] --&gt; D[\"Discriminator D\"]\n    B --&gt;|Try to fool D| D\n    D --&gt;|\"Real: D(x) \u2192 1\"| E[\"Loss: -log D(x)\"]\n    D --&gt;|\"Fake: D(G(z)) \u2192 0\"| F[\"Loss: -log(1-D(G(z)))\"]\n    E --&gt; G[\"Update D:&lt;br/&gt;Distinguish better\"]\n    F --&gt; G\n    D --&gt;|Feedback| H[\"Update G:&lt;br/&gt;Generate better\"]\n    H --&gt; B\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#e8f5e9\n    style D fill:#f3e5f5\n    style G fill:#ffebee\n    style H fill:#ffebee</code></pre>"},{"location":"user-guide/concepts/gan-explained/#theoretical-analysis","title":"Theoretical Analysis","text":"<p>Optimal Discriminator:</p> <p>For a fixed generator \\(G\\), the optimal discriminator takes the form:</p> \\[ D^*_G(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \\] <p>where \\(p_g\\) represents the generator's distribution. This result emerges from maximizing \\(V(D,G)\\) with respect to \\(D\\): the function \\(y \\rightarrow a \\cdot \\log(y) + b \\cdot \\log(1-y)\\) achieves its maximum at \\(y = \\frac{a}{a+b}\\), yielding the optimal discriminator formula when \\(a = p_{data}(x)\\) and \\(b = p_g(x)\\).</p> <p>Interpretation</p> <p>The optimal discriminator performs binary classification using maximum likelihood for the conditional probability that \\(x\\) originated from real data.</p> <p>Global Optimum:</p> <p>The global minimum occurs uniquely when the generator perfectly matches the data distribution (\\(p_g = p_{data}\\)), where the objective reaches:</p> \\[ C(G) = -\\log(4) \\] <p>By substituting the optimal discriminator into the objective:</p> \\[ C(G) = -\\log(4) + 2 \\cdot \\text{JSD}(p_{data} \\| p_g) \\] <p>where \\(\\text{JSD}\\) denotes Jensen-Shannon divergence. Since \\(\\text{JSD} \\geq 0\\) with equality only when distributions match, minimizing the GAN objective equivalently minimizes the JS divergence between real and generated distributions.</p>"},{"location":"user-guide/concepts/gan-explained/#training-dynamics","title":"Training Dynamics","text":"<p>The training alternates between two steps:</p> <p>1. Discriminator Update (maximize \\(V\\))</p> <p>Train \\(D\\) to maximize the probability of correctly classifying real and fake samples:</p> \\[ \\max_D \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))] \\] <p>2. Generator Update (minimize \\(V\\))</p> <p>Train \\(G\\) to maximize the probability of discriminator being wrong:</p> \\[ \\min_G \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))] \\] <p>In practice, we often use the non-saturating generator loss for better gradients:</p> \\[ \\max_G \\mathbb{E}_{z \\sim p_z}[\\log D(G(z))] \\] <p>Why Non-Saturating Loss?</p> <p>When the discriminator becomes too confident, the standard generator loss \\(\\log(1 - D(G(z)))\\) provides vanishing gradients. The non-saturating alternative \\(\\log D(G(z))\\) maintains strong gradients even when the discriminator is confident.</p>"},{"location":"user-guide/concepts/gan-explained/#nash-equilibrium","title":"Nash Equilibrium","text":"<p>At convergence, GANs reach a Nash equilibrium where:</p> <ol> <li>The generator produces samples indistinguishable from real data: \\(p_g = p_{data}\\)</li> <li>The discriminator outputs \\(\\frac{1}{2}\\) everywhere (cannot tell real from fake)</li> <li>Neither network can improve by changing its strategy alone</li> </ol> <p>Equilibrium vs. Reality</p> <p>While theoretically elegant, reaching true Nash equilibrium is rare in practice. Most GANs oscillate around an approximate equilibrium or get stuck in local equilibria.</p>"},{"location":"user-guide/concepts/gan-explained/#architecture-design","title":"Architecture Design","text":""},{"location":"user-guide/concepts/gan-explained/#generator-network","title":"Generator Network","text":"<p>The generator transforms random noise into realistic samples:</p> <pre><code>%%{init: {'theme':'base'}}%%\ngraph LR\n    A[\"Latent Vector z&lt;br/&gt;shape: (batch, latent_dim)\"] --&gt; B[\"Linear/Conv Layer\"]\n    B --&gt; C[\"Batch Norm&lt;br/&gt;(optional)\"]\n    C --&gt; D[\"Activation&lt;br/&gt;(ReLU/LeakyReLU)\"]\n    D --&gt; E[\"...\"]\n    E --&gt; F[\"Output Layer\"]\n    F --&gt; G[\"Activation&lt;br/&gt;(Tanh)\"]\n    G --&gt; H[\"Generated Sample&lt;br/&gt;shape: (batch, C, H, W)\"]\n\n    style A fill:#e1f5ff\n    style H fill:#fff4e1\n    style G fill:#f3e5f5</code></pre> <p>Key Design Choices:</p> <ul> <li>Input: Random latent vector \\(z \\sim \\mathcal{N}(0, I)\\) with dimension 64-512</li> <li>Architecture Options:</li> <li>MLP for simple data (MNIST, toy datasets)</li> <li>Transposed convolutions (DCGAN) for images</li> <li>ResNet blocks for complex images</li> <li>Normalization: Batch normalization helps stabilize training</li> <li>Activation: ReLU in hidden layers, Tanh at output (maps to \\([-1, 1]\\))</li> <li>Output: Same shape as target data</li> </ul> <p>DCGAN Guidelines</p> <ul> <li>Replace pooling with strided convolutions</li> <li>Use batch normalization in both networks (except output/input layers)</li> <li>Remove fully connected hidden layers for deeper architectures</li> <li>Use ReLU in generator (except output), LeakyReLU in discriminator</li> <li>Use Tanh activation in generator output</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#discriminator-network","title":"Discriminator Network","text":"<p>The discriminator classifies inputs as real or fake:</p> <pre><code>%%{init: {'theme':'base'}}%%\ngraph LR\n    A[\"Input Sample&lt;br/&gt;shape: (batch, C, H, W)\"] --&gt; B[\"Conv Layer&lt;br/&gt;(stride=2)\"]\n    B --&gt; C[\"Batch Norm&lt;br/&gt;(optional)\"]\n    C --&gt; D[\"LeakyReLU&lt;br/&gt;(\u03b1=0.2)\"]\n    D --&gt; E[\"...\"]\n    E --&gt; F[\"Flatten\"]\n    F --&gt; G[\"Linear Layer\"]\n    G --&gt; H[\"Sigmoid\"]\n    H --&gt; I[\"Probability&lt;br/&gt;shape: (batch, 1)\"]\n\n    style A fill:#e8f5e9\n    style I fill:#f3e5f5\n    style H fill:#fff3e0</code></pre> <p>Key Design Choices:</p> <ul> <li>Input: Real or generated samples</li> <li>Architecture: Convolutional layers with stride 2 (instead of pooling)</li> <li>Normalization: Batch normalization (except first layer)</li> <li>Activation: LeakyReLU (\\(\\alpha = 0.2\\)) throughout</li> <li>Output: Single probability via sigmoid activation</li> </ul> <p>Common Pitfalls</p> <ul> <li>Don't use pooling layers\u2014use strided convolutions instead</li> <li>Don't place batch norm in discriminator's first layer or generator's last layer</li> <li>Don't use sigmoid in generator output when data is in \\([-1, 1]\\)</li> <li>Don't make discriminator too powerful\u2014it will provide poor gradients</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#building-intuition","title":"Building Intuition","text":""},{"location":"user-guide/concepts/gan-explained/#the-counterfeiter-analogy","title":"The Counterfeiter Analogy","text":"<p>Think of GANs as a counterfeiter (generator) learning to produce fake currency by competing against a detective (discriminator) trying to identify fakes. Initially, the counterfeiter produces obvious forgeries that the detective easily catches. As the detective explains what gives away the fakes, the counterfeiter improves. Eventually, the counterfeiter becomes so skilled that the detective can only guess randomly\u2014they've reached an equilibrium where fake currency is indistinguishable from real currency.</p> <p>This adversarial dynamic, formalized through game theory and implemented with neural networks, drives the remarkable quality of modern GANs.</p>"},{"location":"user-guide/concepts/gan-explained/#key-insights","title":"Key Insights","text":"<p>The generator never sees real data directly\u2014it only receives gradient signals from the discriminator indicating how to improve. This indirect learning through an adversarial critic differs fundamentally from other generative models:</p> <ul> <li>VAEs: Directly minimize reconstruction loss comparing generated to real samples</li> <li>Autoregressive models: Directly maximize likelihood of training data</li> <li>GANs: Implicitly minimize distribution divergence through adversarial dynamics</li> </ul> <p>This enables sharp, realistic outputs without the blurring common in explicit reconstruction-based approaches.</p>"},{"location":"user-guide/concepts/gan-explained/#common-misconceptions","title":"Common Misconceptions","text":"<p>Training Myths</p> <ul> <li>Myth: Losses should decrease monotonically<ul> <li>Reality: Losses oscillate during healthy training</li> </ul> </li> <li>Myth: Generator \"wins\" when discriminator accuracy approaches 50%<ul> <li>Reality: This indicates equilibrium, not generator victory</li> </ul> </li> <li>Myth: Lower loss always means better samples<ul> <li>Reality: Losses are often non-indicative of sample quality</li> </ul> </li> <li>Myth: Training should be stable like supervised learning<ul> <li>Reality: GANs are inherently unstable, requiring careful balancing</li> </ul> </li> <li>Myth: Mode collapse indicates insufficient training<ul> <li>Reality: It often occurs despite long training without proper techniques</li> </ul> </li> </ul>"},{"location":"user-guide/concepts/gan-explained/#the-discriminators-role","title":"The Discriminator's Role","text":"<p>The discriminator's role extends beyond simply classifying real versus fake:</p> <ul> <li>Provides training signal to the generator</li> <li>Implicitly defines the loss function through its architecture and training</li> <li>Learns useful feature representations exploitable for downstream tasks</li> <li>Acts as a learned perceptual metric assessing sample quality</li> </ul> <p>In WGAN, the critic estimates Wasserstein distance rather than performing classification, fundamentally changing the training dynamics.</p>"},{"location":"user-guide/concepts/gan-explained/#training-challenges","title":"Training Challenges","text":""},{"location":"user-guide/concepts/gan-explained/#mode-collapse","title":"Mode Collapse","text":"<p>Mode collapse occurs when the generator produces limited variety despite a large, diverse data distribution.</p> <pre><code>graph TD\n    A[Diverse Training Data] --&gt; B[Generator]\n    B --&gt; C{Mode Collapse?}\n    C --&gt;|Yes| D[Limited Outputs&lt;br/&gt;All samples similar]\n    C --&gt;|No| E[Diverse Outputs&lt;br/&gt;Matches data variety]\n\n    style D fill:#ffccbc\n    style E fill:#c8e6c9</code></pre> <p>Symptoms:</p> <ul> <li>All outputs look nearly identical</li> <li>Generator only produces a few distinct samples</li> <li>Missing modes in the data distribution</li> </ul> <p>Solutions:</p> <ul> <li>Minibatch discrimination: Let discriminator see multiple samples at once</li> <li>Unrolled GANs: Look ahead several discriminator steps during generator updates</li> <li>Experience replay: Keep buffer of past generated samples</li> <li>Multiple GANs: Train ensemble and select best mode coverage</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#vanishing-gradients","title":"Vanishing Gradients","text":"<p>When the discriminator becomes too confident, it provides vanishing gradients to the generator.</p> <p>Problem: If \\(D(G(z)) \\approx 0\\), then \\(\\nabla_G \\log(1 - D(G(z))) \\approx 0\\)</p> <p>Solutions:</p> <ol> <li>Non-saturating loss: Use \\(\\max_G \\mathbb{E}[\\log D(G(z))]\\) instead</li> <li>Wasserstein GAN: Use Wasserstein distance with Lipschitz constraint</li> <li>Least squares GAN: Use \\(\\mathbb{E}[(D(G(z)) - 1)^2]\\) objective</li> <li>Relativistic GAN: Make discriminator estimate relative realism</li> </ol>"},{"location":"user-guide/concepts/gan-explained/#training-instability","title":"Training Instability","text":"<p>Training GANs differs fundamentally from standard supervised learning:</p> <ul> <li>No ground truth for generator's output\u2014quality is implicitly defined by discriminator</li> <li>Loss landscape continuously shifts as both networks update</li> <li>Local equilibria are common; global equilibrium may not exist</li> <li>Small hyperparameter changes can cause dramatic differences in convergence</li> <li>Two competing objectives must be balanced rather than minimizing a single loss</li> </ul> <p>Stability Techniques</p> <p>Architectural:</p> <ul> <li>Batch normalization (except discriminator input/generator output)</li> <li>Spectral normalization for Lipschitz constraint</li> <li>Self-attention for capturing long-range dependencies</li> </ul> <p>Algorithmic:</p> <ul> <li>Two-timescale update rule (TTUR): different learning rates for G and D</li> <li>Gradient penalty (WGAN-GP, R1/R2 regularization)</li> <li>Label smoothing (one-sided for real labels)</li> </ul> <p>Hyperparameter:</p> <ul> <li>Lower learning rates (1e-4 to 1e-5)</li> <li>Adam optimizer with \\(\\beta_1 = 0.5, \\beta_2 = 0.999\\)</li> <li>Multiple discriminator updates per generator update (typically 5:1)</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#gan-variants","title":"GAN Variants","text":""},{"location":"user-guide/concepts/gan-explained/#dcgan-deep-convolutional-gan","title":"DCGAN: Deep Convolutional GAN","text":"<p>DCGAN established the foundation for stable GAN training using convolutional architectures.</p> <p>Architecture Guidelines:</p> <ol> <li>Replace pooling layers with strided convolutions</li> <li>Use batch normalization in both G and D</li> <li>Remove fully connected hidden layers</li> <li>Use ReLU in G (except output), LeakyReLU in D</li> <li>Use Tanh in G output</li> </ol> <p>Impact: Made GAN training significantly more stable and enabled high-quality image generation.</p>"},{"location":"user-guide/concepts/gan-explained/#wgan-wasserstein-gan","title":"WGAN: Wasserstein GAN","text":"<p>WGAN replaces JS divergence with Wasserstein distance for improved training stability.</p> <p>Wasserstein Distance:</p> \\[ W(p_r, p_g) = \\inf_{\\gamma \\in \\Pi(p_r, p_g)} \\mathbb{E}_{(x,y) \\sim \\gamma}[\\|x - y\\|] \\] <p>Practical Objective (with weight clipping):</p> \\[ \\min_G \\max_{D \\in \\mathcal{D}} \\mathbb{E}_{x \\sim p_r}[D(x)] - \\mathbb{E}_{z \\sim p_z}[D(G(z))] \\] <p>where \\(\\mathcal{D}\\) is the set of 1-Lipschitz functions (enforced via weight clipping).</p> <p>Advantages:</p> <ul> <li>More meaningful loss metric correlating with sample quality</li> <li>Improved training stability</li> <li>No mode collapse</li> <li>Works with diverse architectures</li> </ul> <p>WGAN Terminology</p> <p>The discriminator in WGAN is called a critic since it doesn't output probabilities\u2014it estimates the Wasserstein distance.</p>"},{"location":"user-guide/concepts/gan-explained/#wgan-gp-gradient-penalty","title":"WGAN-GP: Gradient Penalty","text":"<p>WGAN-GP improves upon WGAN by replacing weight clipping with gradient penalty.</p> <p>Key Insight: The optimal 1-Lipschitz function maximizing the WGAN objective has gradient norm equal to 1 almost everywhere under \\(p_r\\) and \\(p_g\\).</p> <p>Gradient Penalty:</p> \\[ \\lambda \\cdot \\mathbb{E}_{\\hat{x} \\sim p_{\\hat{x}}}[(\\|\\nabla_{\\hat{x}} D(\\hat{x})\\|_2 - 1)^2] \\] <p>where \\(\\hat{x} = \\epsilon x + (1-\\epsilon)\\tilde{x}\\) with \\(\\epsilon \\sim \\text{Uniform}[0,1]\\), sampling along straight lines between real and generated data.</p> <p>Implementation Details:</p> <ul> <li>Set \\(\\lambda = 10\\) as standard (may require tuning)</li> <li>Remove batch normalization from critic (interferes with gradient penalty)</li> <li>Train critic for 5 iterations per generator update</li> </ul> <p>Results: On CIFAR-10, Inception Score improved from WGAN's ~3.8 to WGAN-GP's ~7.86. Works across diverse architectures including 101-layer ResNets.</p>"},{"location":"user-guide/concepts/gan-explained/#conditional-gan-cgan","title":"Conditional GAN (cGAN)","text":"<p>Conditional GANs extend the adversarial framework to enable controlled generation by conditioning both networks on additional information \\(y\\) (class labels, text, or images).</p> <p>Objective:</p> \\[ \\mathcal{L}_{cGAN}(G, D) = \\mathbb{E}_{x,y \\sim p_{\\text{data}}}[\\log D(x, y)] + \\mathbb{E}_{z \\sim p_z, y}[\\log(1 - D(G(z, y), y))] \\] <pre><code>graph TD\n    Z[\"Noise z\"] --&gt; G[\"Generator\"]\n    Y[\"Condition y&lt;br/&gt;(e.g., class label)\"] --&gt; G\n    G --&gt; F[\"Fake sample G(z, y)\"]\n    F --&gt; D[\"Discriminator\"]\n    Y --&gt; D\n    R[\"Real sample x\"] --&gt; D\n    D --&gt; P[\"Real / Fake\"]\n\n    style Y fill:#fff9c4\n    style G fill:#f3e5f5\n    style D fill:#fff3e0</code></pre> <p>Applications:</p> <ul> <li>Class-conditional generation: Generate specific digit classes in MNIST</li> <li>Attribute manipulation: Change hair color, age, expression in face images</li> <li>Text-to-image: Generate images matching text descriptions</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#pix2pix-image-to-image-translation","title":"Pix2Pix: Image-to-Image Translation","text":"<p>Pix2Pix learns paired image translation with both adversarial and reconstruction losses.</p> <p>Complete Objective:</p> \\[ G^* = \\arg\\min_G \\max_D \\mathcal{L}_{cGAN}(G, D) + \\lambda \\mathcal{L}_{L1}(G) \\] <p>where:</p> \\[ \\mathcal{L}_{L1}(G) = \\mathbb{E}_{x,y,z}[\\|y - G(x, z)\\|_1] \\] <p>and \\(\\lambda = 100\\) balances the terms.</p> <p>Architecture:</p> <ul> <li>Generator: U-Net with skip connections (prevents information loss)</li> <li>Discriminator: PatchGAN (classifies \\(N \\times N\\) patches as real/fake)</li> </ul> <p>Applications: Edges\u2192photos, labels\u2192scenes, day\u2192night, black-and-white\u2192color</p>"},{"location":"user-guide/concepts/gan-explained/#cyclegan-unpaired-translation","title":"CycleGAN: Unpaired Translation","text":"<p>CycleGAN enables translation between unpaired image domains using cycle consistency.</p> <p>Cycle Consistency Loss:</p> \\[ \\mathcal{L}_{cyc}(G, F) = \\mathbb{E}_{x \\sim p_X}[\\|F(G(x)) - x\\|_1] + \\mathbb{E}_{y \\sim p_Y}[\\|G(F(y)) - y\\|_1] \\] <p>Total Objective:</p> \\[ \\mathcal{L}(G, F, D_X, D_Y) = \\mathcal{L}_{GAN}(G, D_Y, X, Y) + \\mathcal{L}_{GAN}(F, D_X, Y, X) + \\lambda \\mathcal{L}_{cyc}(G, F) \\] <pre><code>graph LR\n    A[\"Domain X&lt;br/&gt;(e.g., horses)\"] --&gt;|G: X\u2192Y| B[\"Domain Y&lt;br/&gt;(e.g., zebras)\"]\n    B --&gt;|F: Y\u2192X| C[\"Reconstructed X\"]\n    A -.-&gt;|\"Should match\"| C\n\n    D[\"Domain Y\"] --&gt;|F: Y\u2192X| E[\"Domain X\"]\n    E --&gt;|G: X\u2192Y| F[\"Reconstructed Y\"]\n    D -.-&gt;|\"Should match\"| F\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style D fill:#fff4e1\n    style E fill:#e1f5ff</code></pre> <p>Applications: Horse\u2194zebra, photo\u2194painting, summer\u2194winter, without paired training data</p>"},{"location":"user-guide/concepts/gan-explained/#stylegan-style-based-generator","title":"StyleGAN: Style-Based Generator","text":"<p>StyleGAN introduces style-based architecture with unprecedented control over generated images.</p> <p>Key Innovations:</p> <ol> <li>Mapping Network: Maps \\(z \\in \\mathcal{Z}\\) to intermediate latent space \\(w \\in \\mathcal{W}\\)</li> <li>Adaptive Instance Normalization (AdaIN): Injects style at each resolution</li> <li>Stochastic Variation: Adds per-pixel noise for fine-grained details</li> <li>Progressive Growing: Starts with low resolution, gradually increases</li> </ol> <p>W-Space Benefits:</p> <p>StyleGAN's \\(\\mathcal{W}\\) space demonstrates superior disentanglement to the input \\(\\mathcal{Z}\\) space:</p> <ul> <li>Perceptual Path Length (PPL): 145 in \\(\\mathcal{W}\\) vs. 415 in \\(\\mathcal{Z}\\)</li> <li>Better linear separability of attributes</li> <li>More reliable manipulation transfer across samples</li> </ul> <p>The mapping network purposefully \"unwarps\" \\(\\mathcal{Z}\\) to create this better space.</p> <p>Results: StyleGAN2 achieves FID of 2.84 on FFHQ (1024\u00d71024 faces), setting new standards for image quality.</p>"},{"location":"user-guide/concepts/gan-explained/#latent-space-properties","title":"Latent Space Properties","text":""},{"location":"user-guide/concepts/gan-explained/#interpolation-and-continuity","title":"Interpolation and Continuity","text":"<p>In well-trained GANs, the latent space exhibits desirable properties:</p> <ul> <li>Nearby points decode to similar outputs</li> <li>Linear interpolation produces smooth semantic transitions</li> <li>Sampling generates coherent new samples</li> </ul> <pre><code># Interpolation between two points\nz1 = sample_latent()\nz2 = sample_latent()\nalphas = np.linspace(0, 1, 10)\nz_interp = [(1-\u03b1)*z1 + \u03b1*z2 for \u03b1 in alphas]\nimages = [generator(z) for z in z_interp]\n</code></pre>"},{"location":"user-guide/concepts/gan-explained/#semantic-vector-arithmetic","title":"Semantic Vector Arithmetic","text":"<p>Well-structured latent spaces support semantic manipulation through vector arithmetic:</p> \\[ z_{smiling\\_woman} \\approx z_{woman} + (z_{smiling\\_man} - z_{man}) \\] <p>Examples:</p> <ul> <li>Face with glasses = face + glasses vector</li> <li>Older face = face + aging vector</li> <li>Different expression = face + expression vector</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#latent-space-structure","title":"Latent Space Structure","text":"<p>The latent space structure determines controllability:</p> <ul> <li>Specific directions correspond to interpretable attributes (age, gender, lighting)</li> <li>Smooth manifolds enable continuous attribute manipulation</li> <li>Disentangled dimensions allow independent control of features</li> </ul> <p>Discovering Interpretable Directions</p> <p>Methods for finding semantic directions:</p> <ul> <li>Linear probes: Train classifiers on latent codes</li> <li>PCA: Find principal components of latent representations</li> <li>GANSpace: Identify directions in intermediate feature spaces</li> <li>Supervised methods: Use attribute labels to find control vectors</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"user-guide/concepts/gan-explained/#inception-score-is","title":"Inception Score (IS)","text":"<p>Inception Score measures quality and diversity using a pre-trained classifier.</p> \\[ \\text{IS} = \\exp\\left(\\mathbb{E}_{x \\sim p_g}[D_{KL}(p(y|x) \\| p(y))]\\right) \\] <ul> <li>High IS: Generated images are clear (low entropy \\(p(y|x)\\)) and diverse (high entropy \\(p(y)\\))</li> <li>Limitations: Doesn't compare to real data; can be gamed</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#frechet-inception-distance-fid","title":"Fr\u00e9chet Inception Distance (FID)","text":"<p>FID measures similarity between real and generated distributions in feature space.</p> \\[ \\text{FID} = \\|m_r - m_g\\|^2 + \\text{Tr}(C_r + C_g - 2\\sqrt{C_r C_g}) \\] <p>where \\(m_r, m_g\\) are feature means and \\(C_r, C_g\\) are covariance matrices.</p> <ul> <li>Lower is better: FID of 0 indicates identical distributions</li> <li>More reliable than IS: Compares against real data distribution</li> <li>Industry standard: Most widely used metric for GAN evaluation</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#precision-and-recall","title":"Precision and Recall","text":"<p>Precision: Fraction of generated samples that look realistic</p> <p>Recall: Fraction of real data modes covered by generator</p> <pre><code>graph LR\n    A[High Precision&lt;br/&gt;Low Recall] --&gt; B[Quality over Diversity]\n    C[Low Precision&lt;br/&gt;High Recall] --&gt; D[Diversity over Quality]\n    E[High Precision&lt;br/&gt;High Recall] --&gt; F[Ideal Balance]\n\n    style A fill:#fff9c4\n    style C fill:#fff9c4\n    style E fill:#c8e6c9</code></pre> <p>Trade-off: GANs often optimize precision at the expense of recall (mode collapse).</p>"},{"location":"user-guide/concepts/gan-explained/#human-evaluation","title":"Human Evaluation","text":"<p>Despite quantitative metrics, human evaluation remains the gold standard:</p> <ul> <li>Perceptual quality: Do samples look realistic?</li> <li>Diversity: Is there sufficient variety?</li> <li>Artifacts: Are there visible distortions or patterns?</li> </ul> <p>Metric Limitations</p> <ul> <li>FID: Measures overall distribution quality but doesn't separate precision from recall</li> <li>IS: Assesses generated distribution without comparing to real data</li> <li>PR curves: Separate quality from diversity but require more computation</li> </ul> <p>Use multiple complementary measures plus visual inspection.</p>"},{"location":"user-guide/concepts/gan-explained/#training-best-practices","title":"Training Best Practices","text":""},{"location":"user-guide/concepts/gan-explained/#development-workflow","title":"Development Workflow","text":"<p>Modern GAN development follows an established workflow:</p> <ol> <li>Start simple: DCGAN on MNIST to verify implementation</li> <li>Scale up: Move to CIFAR-10, CelebA for complexity</li> <li>Add stability: Implement WGAN-GP, spectral normalization, R1 GP as needed</li> <li>Monitor carefully: Track both metrics (FID, IS) and visual quality</li> <li>Save checkpoints: Training is unstable\u2014save frequently</li> <li>Iterate: Experiment with architectural variations once baseline works</li> <li>Transfer learning: Use pretrained models when possible</li> </ol>"},{"location":"user-guide/concepts/gan-explained/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":"<p>Learning Rates:</p> <ul> <li>Generator: \\(1 \\times 10^{-4}\\) to \\(5 \\times 10^{-5}\\)</li> <li>Discriminator: \\(4 \\times 10^{-4}\\) to \\(1 \\times 10^{-4}\\) (often 4\u00d7 generator)</li> </ul> <p>Optimizer:</p> <ul> <li>Adam with \\(\\beta_1 = 0.5\\) (or 0.0), \\(\\beta_2 = 0.999\\)</li> <li>RMSprop for WGAN</li> </ul> <p>Training Ratios:</p> <ul> <li>1 generator update : 5 discriminator updates (WGAN-GP)</li> <li>1:1 for well-tuned standard GANs</li> </ul> <p>Batch Size:</p> <ul> <li>Larger is better (32-128 typical, up to 256-512 if memory allows)</li> <li>Affects batch normalization statistics</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#debugging-strategies","title":"Debugging Strategies","text":"<pre><code>graph TD\n    A[GAN Not Training] --&gt; B{Check Losses}\n    B --&gt;|D loss \u2192 0, G loss \u2192 \u221e| C[D too strong]\n    B --&gt;|D loss \u2248 log 2| D[Mode collapse]\n    B --&gt;|Both oscillating wildly| E[Instability]\n    B --&gt;|G loss stuck high| F[Vanishing gradients]\n\n    C --&gt; C1[Reduce D learning rate&lt;br/&gt;Add noise to D inputs]\n    D --&gt; D1[Add minibatch discrimination&lt;br/&gt;Try unrolled GAN]\n    E --&gt; E1[Lower learning rates&lt;br/&gt;Add gradient penalty]\n    F --&gt; F1[Use non-saturating loss&lt;br/&gt;Try WGAN]\n\n    style C fill:#ffccbc\n    style D fill:#ffccbc\n    style E fill:#ffccbc\n    style F fill:#ffccbc</code></pre> <p>Common Issues:</p> Symptom Likely Cause Solution D loss \u2192 0 Discriminator too strong Reduce D LR, add label noise G loss stuck Vanishing gradients Non-saturating loss, WGAN All same output Mode collapse Minibatch discrimination NaN losses Numerical instability Lower LR, gradient clipping Blurry samples Architecture issues Check activations, add capacity"},{"location":"user-guide/concepts/gan-explained/#stability-techniques","title":"Stability Techniques","text":"<p>R1 Gradient Penalty:</p> \\[ R1 = \\frac{\\gamma}{2} \\mathbb{E}_{x \\sim p_r}[\\|\\nabla D(x)\\|^2] \\] <ul> <li>Apply only to real samples</li> <li>\\(\\gamma = 10\\) typical</li> <li>Lazy regularization: apply every 16 iterations</li> </ul> <p>Spectral Normalization:</p> <p>Normalize weight matrices by their spectral norm (largest singular value):</p> \\[ W_{SN} = \\frac{W}{\\sigma(W)} \\] <ul> <li>Enforces Lipschitz constraint without gradient penalty</li> <li>More computationally efficient than WGAN-GP</li> <li>Works well with standard GAN loss</li> </ul> <p>Label Smoothing:</p> <p>Replace binary labels with smoothed versions:</p> <ul> <li>Real labels: \\(0.9\\) instead of \\(1.0\\)</li> <li>Fake labels: \\(0.0\\) (no smoothing)</li> </ul> <p>One-sided smoothing prevents discriminator overconfidence.</p>"},{"location":"user-guide/concepts/gan-explained/#comparison-with-other-generative-models","title":"Comparison with Other Generative Models","text":""},{"location":"user-guide/concepts/gan-explained/#gans-vs-vaes","title":"GANs vs. VAEs","text":"Aspect GANs VAEs Sample Quality Sharp, realistic Blurry, smooth Training Stability Unstable, requires tuning Stable, straightforward Likelihood Implicit (no direct likelihood) Explicit lower bound (ELBO) Mode Coverage Often mode collapse Better mode coverage Latent Space Less structured by default Structured, continuous Speed Fast inference Fast inference"},{"location":"user-guide/concepts/gan-explained/#gans-vs-diffusion-models","title":"GANs vs. Diffusion Models","text":"Aspect GANs Diffusion Models Sample Quality High quality (FID ~2-5) Highest quality (FID ~1-3) Inference Speed Fast (1 forward pass) Slow (100-1000 steps) Training Stability Unstable Very stable Mode Coverage Prone to collapse Excellent coverage Controllability Excellent (latent editing) Improving (guidance) Text Conditioning Challenging Excellent (CLIP guidance) <p>2025 Landscape:</p> <ul> <li>Diffusion models: Excel at high-quality offline generation, creative applications, text-to-image</li> <li>GANs: Dominate real-time applications, interactive systems, latent space editing</li> </ul> <p>Complementary Use Cases:</p> <ul> <li>Use GANs when: Speed matters, controllability critical, resource constraints, 3D-aware generation</li> <li>Use Diffusion when: Highest quality required, complex text prompts, maximum diversity, stable training priority</li> </ul> <p>Speed Advantage</p> <p>GANs maintain a 100\u00d7 speed advantage over diffusion models, ensuring continued relevance for real-time applications despite diffusion quality leadership.</p>"},{"location":"user-guide/concepts/gan-explained/#advanced-topics","title":"Advanced Topics","text":""},{"location":"user-guide/concepts/gan-explained/#progressive-growing","title":"Progressive Growing","text":"<p>Train GANs by gradually increasing resolution:</p> <ol> <li>Start at 4\u00d74</li> <li>Stabilize training</li> <li>Add layers for 8\u00d78</li> <li>Fade in new layers</li> <li>Repeat to target resolution</li> </ol> <p>Benefits: More stable training, faster convergence, better quality</p>"},{"location":"user-guide/concepts/gan-explained/#self-attention","title":"Self-Attention","text":"<p>Add self-attention layers to capture long-range dependencies:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>SAGAN (Self-Attention GAN) improves modeling of geometric patterns and multi-object scenes.</p>"},{"location":"user-guide/concepts/gan-explained/#biggan","title":"BigGAN","text":"<p>BigGAN scales GANs to ImageNet with:</p> <ul> <li>Larger batch sizes (2048)</li> <li>Larger architectures (more channels)</li> <li>Truncated normal sampling</li> <li>Orthogonal regularization</li> <li>Class-conditional batch normalization</li> </ul> <p>Results: IS of 166.5 on ImageNet, unprecedented scale and quality</p>"},{"location":"user-guide/concepts/gan-explained/#3d-aware-generation","title":"3D-Aware Generation","text":"<p>EG3D generates 3D-consistent faces from 2D images:</p> <ul> <li>Tri-plane representation for efficient 3D</li> <li>Neural rendering from multi-view</li> <li>Adversarial training on 2D images</li> <li>Learns 3D structure without 3D supervision</li> </ul> <p>Applications: 3D face synthesis, novel view synthesis, relighting</p>"},{"location":"user-guide/concepts/gan-explained/#production-considerations","title":"Production Considerations","text":""},{"location":"user-guide/concepts/gan-explained/#deployment-challenges","title":"Deployment Challenges","text":"<p>Model Size:</p> <ul> <li>StyleGAN2: ~200MB for 1024\u00d71024 generator</li> <li>Optimization: Pruning, quantization, knowledge distillation</li> </ul> <p>Inference Speed:</p> <ul> <li>Real-time requirements: &lt;50ms per image</li> <li>Optimization: TensorRT, ONNX Runtime, model compression</li> </ul> <p>Hardware Requirements:</p> <ul> <li>Training: Multi-GPU (4-8 GPUs for large-scale)</li> <li>Inference: Single GPU or CPU with optimization</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<p>Quality Drift:</p> <p>Monitor generated samples over time for:</p> <ul> <li>Artifacts or distortions</li> <li>Mode collapse</li> <li>Distribution shift</li> </ul> <p>A/B Testing:</p> <p>Compare model versions using:</p> <ul> <li>FID scores on held-out data</li> <li>Human evaluation studies</li> <li>Production metrics (click-through, engagement)</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#ethical-considerations","title":"Ethical Considerations","text":"<p>Responsible Use</p> <p>Deepfakes and Misinformation:</p> <ul> <li>GANs can generate realistic fake images/videos</li> <li>Require watermarking and detection systems</li> <li>Need clear disclosure when using synthetic media</li> </ul> <p>Bias and Fairness:</p> <ul> <li>GANs inherit biases from training data</li> <li>May amplify stereotypes or lack diversity</li> <li>Require diverse, representative datasets</li> </ul> <p>Privacy:</p> <ul> <li>Memorization of training data possible</li> <li>Differential privacy techniques recommended</li> <li>Careful handling of sensitive data</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#future-directions-2025-and-beyond","title":"Future Directions (2025 and Beyond)","text":""},{"location":"user-guide/concepts/gan-explained/#open-research-problems","title":"Open Research Problems","text":"<ul> <li>Scaling: Compete with large-scale diffusion models (challenges beyond 1B parameters)</li> <li>3D Quality: Higher quality geometries with fine details and animation</li> <li>Video Generation: Temporal coherence while competing with Sora and Movie Gen</li> <li>Training Stability: Automatic hyperparameter selection</li> <li>Foundation Models: Integration with CLIP, DINO, and other pretrained features</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#emerging-trends","title":"Emerging Trends","text":"<p>Retrieval-Augmented Generation:</p> <ul> <li>Combine GANs with retrieval systems</li> <li>Improve quality through example retrieval</li> <li>Better few-shot adaptation</li> </ul> <p>Multimodal GANs:</p> <ul> <li>Integrate text-image-audio-video</li> <li>Cross-modal generation and translation</li> <li>Unified latent spaces</li> </ul> <p>Efficient Fine-Tuning:</p> <ul> <li>LoRA and adapter methods</li> <li>Few-shot adaptation with minimal data</li> <li>Transfer learning from foundation models</li> </ul> <p>Quality-Aware Training:</p> <ul> <li>Discriminators assessing perceptual quality</li> <li>Wavelet-driven approaches</li> <li>Learned perceptual losses</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#hybrid-approaches","title":"Hybrid Approaches","text":"<p>The future likely involves hybrid systems leveraging strengths of multiple paradigms:</p> <ul> <li>GAN + Diffusion: Fast inference with high quality</li> <li>GAN + VAE: Structured latent spaces with sharp samples</li> <li>GAN + Transformers: Scalability and flexibility</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>GANs revolutionized generative modeling through adversarial training, achieving remarkable image quality and enabling unprecedented control. Despite challenges with training stability, they remain essential for real-time applications and latent space manipulation.</p> <p>Core Principles:</p> <ul> <li>Minimax game between generator and discriminator drives learning</li> <li>Nash equilibrium reached when generator matches data distribution</li> <li>Implicit density modeling avoids explicit likelihood computation</li> <li>Sharp samples without reconstruction-based blurring</li> </ul> <p>Key Variants:</p> <ul> <li>DCGAN: Stable convolutional architecture</li> <li>WGAN/WGAN-GP: Wasserstein distance for training stability</li> <li>Pix2Pix/CycleGAN: Image translation with/without paired data</li> <li>StyleGAN: Style-based architecture with unprecedented control</li> </ul> <p>Training Challenges:</p> <ul> <li>Mode collapse, vanishing gradients, training instability</li> <li>Require careful hyperparameter tuning and monitoring</li> <li>Multiple techniques needed for stable convergence</li> </ul> <p>Best Practices:</p> <ul> <li>Start with DCGAN architecture on simple data</li> <li>Use WGAN-GP or spectral normalization for stability</li> <li>Monitor both quantitative metrics and visual quality</li> <li>Employ two-timescale updates (different learning rates)</li> <li>Save checkpoints frequently</li> </ul> <p>When to Use GANs:</p> <ul> <li>Real-time generation requirements</li> <li>Controllable latent space editing</li> <li>Resource-constrained environments</li> <li>3D-aware synthesis</li> </ul> <p>Current Landscape (2025):</p> <ul> <li>Diffusion models lead in quality and diversity</li> <li>GANs maintain 100\u00d7 speed advantage</li> <li>Complementary rather than competitive use cases</li> <li>Future involves hybrid approaches</li> </ul>"},{"location":"user-guide/concepts/gan-explained/#next-steps","title":"Next Steps","text":"<ul> <li> <p> GAN User Guide</p> <p>Practical usage guide with implementation examples and training workflows</p> </li> <li> <p> GAN API Reference</p> <p>Complete API documentation for DCGAN, WGAN, StyleGAN, and variants</p> </li> <li> <p> MNIST Tutorial</p> <p>Step-by-step hands-on tutorial: train a GAN on MNIST from scratch</p> </li> <li> <p> Advanced Examples</p> <p>Explore StyleGAN, CycleGAN, and state-of-the-art architectures</p> </li> </ul>"},{"location":"user-guide/concepts/gan-explained/#further-reading","title":"Further Reading","text":""},{"location":"user-guide/concepts/gan-explained/#seminal-papers-must-read","title":"Seminal Papers (Must Read)","text":"<p> Goodfellow, I., et al. (2014). \"Generative Adversarial Networks\" arXiv:1406.2661  The original GAN paper introducing the adversarial framework</p> <p> Radford, A., et al. (2015). \"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\" arXiv:1511.06434  DCGAN: Established stable convolutional GAN training</p> <p> Arjovsky, M., et al. (2017). \"Wasserstein GAN\" arXiv:1701.07875  WGAN: Improved stability through Wasserstein distance</p> <p> Gulrajani, I., et al. (2017). \"Improved Training of Wasserstein GANs\" arXiv:1704.00028  WGAN-GP: Gradient penalty for better convergence</p> <p> Karras, T., et al. (2019). \"A Style-Based Generator Architecture for Generative Adversarial Networks\" arXiv:1812.04948  StyleGAN: State-of-the-art quality and control</p>"},{"location":"user-guide/concepts/gan-explained/#application-papers","title":"Application Papers","text":"<p> Isola, P., et al. (2017). \"Image-to-Image Translation with Conditional Adversarial Networks\" arXiv:1611.07004  Pix2Pix: Paired image translation</p> <p> Zhu, J.-Y., et al. (2017). \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\" arXiv:1703.10593  CycleGAN: Unpaired domain translation</p> <p> Brock, A., et al. (2019). \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\" arXiv:1809.11096  BigGAN: Scaling to ImageNet</p>"},{"location":"user-guide/concepts/gan-explained/#theoretical-analysis_1","title":"Theoretical Analysis","text":"<p> Arora, S., et al. (2017). \"Generalization and Equilibrium in Generative Adversarial Nets\" arXiv:1703.00573  Theoretical understanding of GAN training</p> <p> Kodali, N., et al. (2017). \"On Convergence and Stability of GANs\" arXiv:1705.07215  Analysis of convergence properties</p>"},{"location":"user-guide/concepts/vae-explained/","title":"Variational Autoencoders (VAEs) Explained","text":"<ul> <li> <p> Probabilistic Framework</p> <p>Learn distributions over latent codes rather than deterministic encodings</p> </li> <li> <p> Structured Latent Space</p> <p>Continuous, smooth latent space enabling interpolation and controlled generation</p> </li> <li> <p> Principled Generation</p> <p>Sample from learned prior distribution to generate new, realistic data</p> </li> <li> <p> Differentiable Training</p> <p>End-to-end optimization using the reparameterization trick</p> </li> </ul>"},{"location":"user-guide/concepts/vae-explained/#overview","title":"Overview","text":"<p>Variational Autoencoders (VAEs) are a class of deep generative models that combine neural networks with variational inference to learn probabilistic representations of data. Unlike standard autoencoders that learn deterministic mappings, VAEs learn probability distributions over latent representations, enabling principled data generation and interpretable latent spaces.</p> <p>What makes VAEs special?</p> <p>VAEs solve a fundamental challenge in generative modeling: how to learn a structured, continuous latent space that can be sampled to generate new, realistic data. By imposing a probabilistic structure through variational inference, VAEs create smooth latent spaces where:</p> <ul> <li>Interpolation works naturally - moving between two points in latent space produces meaningful intermediate outputs</li> <li>Random sampling generates valid data - sampling from the prior produces realistic new samples</li> <li>The representation is interpretable - the latent space has structure that can be understood and controlled</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#the-intuition-compression-and-blueprints","title":"The Intuition: Compression and Blueprints","text":"<p>Think of VAEs like an architect creating blueprints:</p> <ol> <li> <p>The Encoder compresses a complex house (your data) into essential instructions (latent vector), capturing key features\u2014number of rooms, architectural style, materials\u2014while discarding minor details like exact nail positions.</p> </li> <li> <p>The Latent Space is a structured blueprint repository where similar designs cluster together: ranch houses near each other, Victorian mansions in another region, modern apartments elsewhere.</p> </li> <li> <p>The Decoder rebuilds houses from blueprints, reconstructing recognizable structures though minor details differ from the original.</p> </li> </ol> <p>The critical distinction: VAEs encode to probability distributions, not single points. Each house maps to a probability cloud of similar blueprints, ensuring the latent space remains smooth and continuous. This enables generation\u2014sample a random blueprint from the structured space, and the decoder builds a valid house, even one never seen before.</p>"},{"location":"user-guide/concepts/vae-explained/#mathematical-foundation","title":"Mathematical Foundation","text":""},{"location":"user-guide/concepts/vae-explained/#the-generative-story","title":"The Generative Story","text":"<p>VAEs model the data generation process as a two-step procedure:</p> <ol> <li>Sample latent code: \\(z \\sim p(z)\\) from a simple prior distribution (typically standard normal)</li> <li>Generate data: \\(x \\sim p_\\theta(x|z)\\) using a decoder network parameterized by \\(\\theta\\)</li> </ol> <p>The goal is to learn parameters \\(\\theta\\) (decoder) and \\(\\phi\\) (encoder) that maximize the likelihood of observed data \\(p_\\theta(x)\\).</p> <pre><code>graph LR\n    subgraph \"True Generative Model\"\n        A[\"Prior p(z)&lt;br/&gt;\ud835\udca9(0,I)\"] --&gt; B[\"Decoder p(x|z)\"]\n        B --&gt; C[\"Data x\"]\n    end\n\n    subgraph \"Inference Model\"\n        C2[\"Data x\"] --&gt; D[\"Encoder q(z|x)\"]\n        D --&gt; E[\"Approximate&lt;br/&gt;Posterior\"]\n    end\n\n    style A fill:#e1f5ff\n    style B fill:#fff3e0\n    style D fill:#f3e5f5</code></pre>"},{"location":"user-guide/concepts/vae-explained/#variational-inference-why-we-need-approximation","title":"Variational Inference: Why We Need Approximation","text":"<p>The true posterior \\(p_\\theta(z|x)\\) tells us what latent code likely generated our data. However, computing it requires:</p> \\[ p_\\theta(z|x) = \\frac{p_\\theta(x|z)p(z)}{p_\\theta(x)} = \\frac{p_\\theta(x|z)p(z)}{\\int p_\\theta(x|z')p(z') dz'} \\] <p>The integral in the denominator (the evidence \\(p_\\theta(x)\\)) is intractable for high-dimensional \\(z\\)\u2014we'd need to integrate over all possible latent codes. VAEs sidestep this by learning an approximate posterior \\(q_\\phi(z|x)\\) (the encoder) that's easy to compute.</p>"},{"location":"user-guide/concepts/vae-explained/#the-elbo-evidence-lower-bound","title":"The ELBO: Evidence Lower BOund","text":"<p>The key insight of VAEs is to maximize a tractable lower bound on the log-likelihood called the Evidence Lower BOund (ELBO):</p> \\[ \\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) \\] <p>This inequality states that the log-likelihood is always at least as large as the ELBO. The gap between them equals exactly \\(D_{\\text{KL}}(q_\\phi(z|x) \\| p_\\theta(z|x))\\)\u2014when our approximate posterior perfectly matches the true posterior, there's no gap and we achieve the true likelihood.</p>"},{"location":"user-guide/concepts/vae-explained/#derivation-from-first-principles","title":"Derivation from First Principles","text":"<p>Starting with the log-likelihood and introducing our approximate posterior:</p> \\[ \\begin{align} \\log p_\\theta(x) &amp;= \\log \\int p_\\theta(x, z) dz \\\\ &amp;= \\log \\int p_\\theta(x, z) \\frac{q_\\phi(z|x)}{q_\\phi(z|x)} dz \\\\ &amp;= \\log \\mathbb{E}_{q_\\phi(z|x)} \\left[\\frac{p_\\theta(x, z)}{q_\\phi(z|x)}\\right] \\end{align} \\] <p>Applying Jensen's inequality (since log is concave):</p> \\[ \\log \\mathbb{E}[f(z)] \\geq \\mathbb{E}[\\log f(z)] \\] <p>We get:</p> \\[ \\log p_\\theta(x) \\geq \\mathbb{E}_{q_\\phi(z|x)} \\left[\\log \\frac{p_\\theta(x, z)}{q_\\phi(z|x)}\\right] \\] <p>Expanding \\(p_\\theta(x, z) = p_\\theta(x|z)p(z)\\):</p> \\[ \\begin{align} \\text{ELBO} &amp;= \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log \\frac{p(z)}{q_\\phi(z|x)}\\right] \\\\ &amp;= \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) \\end{align} \\]"},{"location":"user-guide/concepts/vae-explained/#two-interpretable-terms","title":"Two Interpretable Terms","text":"<p>The ELBO naturally decomposes into two competing objectives:</p> <ol> <li>Reconstruction Term: \\(\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]\\)</li> <li>Measures how well we can reconstruct the input from sampled latent codes</li> <li>Encourages the model to preserve information</li> <li> <p>Higher is better (less negative)</p> </li> <li> <p>KL Divergence: \\(D_{\\text{KL}}(q_\\phi(z|x) \\| p(z))\\)</p> </li> <li>Measures how close our learned encoding is to the prior</li> <li>Regularizes the latent space to be smooth and structured</li> <li>Prevents \"cheating\" by spreading encodings arbitrarily far apart</li> <li>Lower is better (closer to prior)</li> </ol> <p>The fundamental trade-off: The reconstruction term wants to encode all information to perfectly reconstruct. The KL term wants to compress encodings to match the simple prior. Training finds the optimal balance, creating a structured latent space that retains essential information while remaining smooth for generation.</p>"},{"location":"user-guide/concepts/vae-explained/#architecture-components","title":"Architecture Components","text":""},{"location":"user-guide/concepts/vae-explained/#encoder-variational-posterior-q_phizx","title":"Encoder: Variational Posterior \\(q_\\phi(z|x)\\)","text":"<p>The encoder is a neural network that maps inputs to parameters of a probability distribution over latent codes:</p> \\[ q_\\phi(z|x) = \\mathcal{N}(z; \\mu_\\phi(x), \\sigma^2_\\phi(x) \\mathbf{I}) \\] <p>For a diagonal Gaussian (most common choice), the encoder outputs:</p> <ul> <li>Mean \\(\\mu_\\phi(x) \\in \\mathbb{R}^d\\) - the center of the latent distribution</li> <li>Log-variance \\(\\log \\sigma^2_\\phi(x) \\in \\mathbb{R}^d\\) - the spread/uncertainty</li> </ul> <pre><code>graph LR\n    A[\"Input x&lt;br/&gt;(e.g., 28\u00d728 image)\"] --&gt; B[\"Encoder Network&lt;br/&gt;(Conv layers or FC)\"]\n    B --&gt; C[\"Mean \u03bc&lt;br/&gt;(d dimensions)\"]\n    B --&gt; D[\"Log-variance log \u03c3\u00b2&lt;br/&gt;(d dimensions)\"]\n    C --&gt; E[\"Latent Distribution&lt;br/&gt;\ud835\udca9(\u03bc, \u03c3\u00b2I)\"]\n    D --&gt; E\n\n    style B fill:#f3e5f5\n    style E fill:#e8eaf6</code></pre> <p>Why output log-variance? Numerical stability. Variance must be positive, and learning \\(\\log \\sigma^2\\) allows the network to output any real number while ensuring \\(\\sigma^2 = \\exp(\\log \\sigma^2) &gt; 0\\).</p> <p>Why diagonal covariance? Full covariance matrices require \\(O(d^2)\\) parameters and are harder to optimize. Diagonal covariance assumes independence between dimensions, requiring only \\(O(d)\\) parameters while working well in practice.</p>"},{"location":"user-guide/concepts/vae-explained/#decoder-likelihood-p_thetaxz","title":"Decoder: Likelihood \\(p_\\theta(x|z)\\)","text":"<p>The decoder is a neural network that maps latent codes back to data space:</p> \\[ p_\\theta(x|z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma^2 \\mathbf{I}) \\quad \\text{or} \\quad \\text{Bernoulli}(x; f_\\theta(z)) \\] <p>The choice of output distribution depends on your data:</p> <ul> <li>Gaussian (continuous): For real-valued images (often simplified to MSE loss with fixed variance)</li> <li>Bernoulli (binary): For binary images or features (use sigmoid + BCE loss)</li> <li>Categorical: For discrete data (use softmax + cross-entropy)</li> </ul> <pre><code>graph LR\n    A[\"Latent z&lt;br/&gt;(d dimensions)\"] --&gt; B[\"Decoder Network&lt;br/&gt;(Transposed Conv or FC)\"]\n    B --&gt; C[\"Reconstruction \u03bc(z)&lt;br/&gt;(same shape as input)\"]\n\n    style B fill:#fff3e0\n    style C fill:#e8f5e9</code></pre>"},{"location":"user-guide/concepts/vae-explained/#the-reparameterization-trick","title":"The Reparameterization Trick","text":""},{"location":"user-guide/concepts/vae-explained/#the-problem-backpropagation-through-sampling","title":"The Problem: Backpropagation Through Sampling","text":"<p>We need to compute gradients of \\(\\mathbb{E}_{q_\\phi(z|x)}[f(z)]\\) with respect to \\(\\phi\\). Naively sampling \\(z \\sim q_\\phi(z|x)\\) and computing \\(\\nabla_\\phi f(z)\\) doesn't work because the sampling operation itself depends on \\(\\phi\\) but isn't differentiable.</p>"},{"location":"user-guide/concepts/vae-explained/#the-solution-separate-randomness-from-parameters","title":"The Solution: Separate Randomness from Parameters","text":"<p>Instead of sampling \\(z\\) directly from \\(q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\sigma^2_\\phi(x))\\), reparameterize as:</p> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}) \\] <p>where \\(\\odot\\) denotes element-wise multiplication.</p> <pre><code>graph TD\n    A[\"Input x\"] --&gt; B[\"Encoder\"]\n    B --&gt; C[\"\u03bc (mean)\"]\n    B --&gt; D[\"\u03c3 (std dev)\"]\n    E[\"\u03b5 ~ \ud835\udca9(0,I)&lt;br/&gt;(random noise)\"] --&gt; F[\"z = \u03bc + \u03c3 \u2299 \u03b5\"]\n    C --&gt; F\n    D --&gt; F\n    F --&gt; G[\"Decoder\"]\n    G --&gt; H[\"Reconstruction x\u0302\"]\n\n    style F fill:#ffebee\n    style E fill:#e1f5ff</code></pre> <p>Why this works:</p> <ol> <li>The randomness (\\(\\epsilon\\)) is now independent of our parameters \\(\\phi\\)</li> <li>Gradients flow through the deterministic operations \\(\\mu_\\phi\\) and \\(\\sigma_\\phi\\)</li> <li>The expectation becomes \\(\\mathbb{E}_{p(\\epsilon)}[f(g_\\phi(\\epsilon, x))]\\) where \\(g_\\phi\\) is deterministic</li> <li>We can approximate this expectation with Monte Carlo sampling: sample \\(\\epsilon\\), compute gradients, average</li> </ol> <p>This clever trick enabled practical VAE training and has since become fundamental to probabilistic deep learning.</p>"},{"location":"user-guide/concepts/vae-explained/#loss-function-and-training","title":"Loss Function and Training","text":"<p>The VAE loss is derived directly from the negative ELBO:</p> \\[ \\mathcal{L}(\\theta, \\phi; x) = -\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) \\]"},{"location":"user-guide/concepts/vae-explained/#practical-implementation","title":"Practical Implementation","text":"<p>For Gaussian encoder \\(q_\\phi(z|x) = \\mathcal{N}(\\mu, \\sigma^2\\mathbf{I})\\) and standard normal prior \\(p(z) = \\mathcal{N}(0, \\mathbf{I})\\):</p> <p>Reconstruction Loss (assuming Gaussian decoder with fixed variance):</p> \\[ \\mathcal{L}_{\\text{recon}} = \\frac{1}{N}\\sum_{i=1}^{N} \\|x_i - \\hat{x}_i\\|^2 = \\text{MSE}(x, \\hat{x}) \\] <p>KL Divergence (closed-form for Gaussians):</p> \\[ \\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum_{j=1}^{d} (1 + \\log \\sigma_j^2 - \\mu_j^2 - \\sigma_j^2) \\] <p>Total Loss:</p> \\[ \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{recon}} + \\mathcal{L}_{\\text{KL}} \\]"},{"location":"user-guide/concepts/vae-explained/#training-algorithm","title":"Training Algorithm","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\n\n# Create optimizer (wrt=nnx.Param required in NNX 0.11.0+)\noptimizer = nnx.Optimizer(vae, optax.adam(1e-3), wrt=nnx.Param)\n\nfor epoch in epochs:\n    for batch in dataloader:\n        def loss_fn(vae):\n            # Forward pass\n            mu, log_var = vae.encoder(batch)\n\n            # Reparameterization trick\n            epsilon = jax.random.normal(rng_key, mu.shape)\n            z = mu + jnp.exp(0.5 * log_var) * epsilon\n\n            # Decode\n            x_recon = vae.decoder(z)\n\n            # Compute losses\n            recon_loss = jnp.mean((x_recon - batch) ** 2)\n            kl_loss = -0.5 * jnp.sum(1 + log_var - mu ** 2 - jnp.exp(log_var))\n\n            # Total loss\n            return recon_loss + kl_loss\n\n        # Gradient update (NNX 0.11.0+ API)\n        loss, grads = nnx.value_and_grad(loss_fn)(vae)\n        optimizer.update(vae, grads)\n</code></pre>"},{"location":"user-guide/concepts/vae-explained/#key-training-metrics-to-monitor","title":"Key Training Metrics to Monitor","text":"<ol> <li>Reconstruction loss: Should decrease steadily (lower = better reconstruction)</li> <li>KL divergence: Should stabilize at a positive value (5-20 is typical for well-trained models)</li> <li>ELBO: Combination of both, the primary metric</li> <li>Per-dimension KL: Helps detect posterior collapse (all values near 0 indicates problem)</li> </ol>"},{"location":"user-guide/concepts/vae-explained/#vae-variants","title":"VAE Variants","text":""},{"location":"user-guide/concepts/vae-explained/#-vae-disentangled-representations","title":"\u03b2-VAE: Disentangled Representations","text":"<p>\u03b2-VAE modifies the objective to encourage disentanglement, where individual latent dimensions capture independent factors of variation:</p> \\[ \\mathcal{L}_{\\beta} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta \\cdot D_{\\text{KL}}(q_\\phi(z|x) \\| p(z)) \\] <p>Effect of \u03b2:</p> <ul> <li>\u03b2 = 1: Standard VAE (no additional emphasis on disentanglement)</li> <li>\u03b2 &gt; 1: Stronger regularization \u2192 encourages independent latent dimensions, improves disentanglement, but reduces reconstruction quality</li> <li>\u03b2 &lt; 1: Weaker regularization \u2192 better reconstruction, less structured latent space</li> </ul> <pre><code>graph LR\n    subgraph \"\u03b2 &lt; 1: Reconstruction Focus\"\n        A1[Sharp Images] --&gt; B1[Entangled Latents]\n    end\n\n    subgraph \"\u03b2 = 1: Standard VAE\"\n        A2[Balanced] --&gt; B2[Some Structure]\n    end\n\n    subgraph \"\u03b2 &gt; 1: Disentanglement Focus\"\n        A3[Blurrier Images] --&gt; B3[Disentangled Latents]\n    end\n\n    style A1 fill:#c8e6c9\n    style A3 fill:#ffccbc\n    style B3 fill:#c8e6c9</code></pre> <p>Practical \u03b2 values: Start with \u03b2=1, try \u03b2=4-10 for image disentanglement tasks (dSprites, CelebA), use \u03b2=0.1-0.5 for text (to avoid posterior collapse).</p> <p>Applications:</p> <ul> <li>Interpretable representations for analysis and visualization</li> <li>Fair AI by removing sensitive attributes from representations</li> <li>Controllable generation by manipulating specific latent factors</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#conditional-vae-cvae","title":"Conditional VAE (CVAE)","text":"<p>Conditional VAEs incorporate additional information \\(y\\) (class labels, attributes, text descriptions) to enable controlled generation:</p> \\[ \\begin{align} q_\\phi(z|x, y) &amp;= \\mathcal{N}(z; \\mu_\\phi(x, y), \\sigma^2_\\phi(x, y) \\mathbf{I}) \\\\ p_\\theta(x|z, y) &amp;= \\mathcal{N}(x; \\mu_\\theta(z, y), \\sigma^2 \\mathbf{I}) \\end{align} \\] <pre><code>graph TD\n    A[\"Input x\"] --&gt; E[\"Encoder\"]\n    B[\"Condition y&lt;br/&gt;(e.g., class label)\"] --&gt; E\n    E --&gt; C[\"Latent z\"]\n    C --&gt; D[\"Decoder\"]\n    B --&gt; D\n    D --&gt; F[\"Reconstruction x\u0302\"]\n\n    style B fill:#fff9c4\n    style E fill:#f3e5f5\n    style D fill:#fff3e0</code></pre> <p>How conditioning works:</p> <ul> <li>Concatenation: Append \\(y\\) to the input before encoding/after sampling before decoding</li> <li>Conditional Batch Normalization: Modulate batch norm parameters based on \\(y\\)</li> <li>FiLM (Feature-wise Linear Modulation): Scale and shift features based on \\(y\\)</li> </ul> <p>Applications:</p> <ul> <li>Class-conditional generation: Generate specific digit classes in MNIST</li> <li>Attribute manipulation: Change hair color, age, expression in face images</li> <li>Text-to-image: Generate images matching text descriptions</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#vector-quantized-vae-vq-vae","title":"Vector Quantized VAE (VQ-VAE)","text":"<p>VQ-VAE replaces continuous latent representations with discrete codes from a learned codebook:</p> \\[ z_q = \\arg\\min_{e_k \\in \\mathcal{C}} \\|z_e - e_k\\|^2 \\] <p>where \\(\\mathcal{C} = \\{e_1, ..., e_K\\}\\) is a learned codebook of \\(K\\) embedding vectors.</p> <pre><code>graph TD\n    A[\"Input x\"] --&gt; B[\"Encoder\"]\n    B --&gt; C[\"Continuous z_e\"]\n    C --&gt; D[\"Vector&lt;br/&gt;Quantization\"]\n    E[\"Learned&lt;br/&gt;Codebook\"] --&gt; D\n    D --&gt; F[\"Discrete z_q\"]\n    F --&gt; G[\"Decoder\"]\n    G --&gt; H[\"Reconstruction x\u0302\"]\n\n    style D fill:#ffebee\n    style E fill:#e1f5ff</code></pre> <p>VQ-VAE Loss Function:</p> \\[ \\mathcal{L} = \\|x - \\hat{x}\\|^2 + \\|sg[z_e] - z_q\\|^2 + \\beta \\|z_e - sg[z_q]\\|^2 \\] <p>where \\(sg[\\cdot]\\) is the stop-gradient operator. The three terms are:</p> <ol> <li>Reconstruction loss: Standard pixel-wise error</li> <li>Codebook loss: Updates codebook embeddings via exponential moving average</li> <li>Commitment loss: Encourages encoder to commit to codebook entries</li> </ol> <p>Key advantages:</p> <ul> <li>\u2705 No posterior collapse - discrete latents can't collapse to uninformative distributions</li> <li>\u2705 Better codebook utilization - all codebook entries get used</li> <li>\u2705 Powerful for hierarchical models - DALL-E, DALL-E 2 use VQ-VAE as foundation</li> <li>\u2705 Near-GAN quality - produces sharper images than standard VAEs</li> </ul> <p>Applications:</p> <ul> <li>DALL-E: Text-to-image generation using discrete visual codes</li> <li>Jukebox: High-fidelity music generation</li> <li>High-resolution image synthesis: VQ-GAN combines VQ-VAE with adversarial training</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#training-dynamics-and-common-challenges","title":"Training Dynamics and Common Challenges","text":""},{"location":"user-guide/concepts/vae-explained/#posterior-collapse","title":"Posterior Collapse","text":"<p>What is it?</p> <p>The encoder learns to ignore the input, producing latent codes that are essentially identical to the prior \\(q_\\phi(z|x) \\approx p(z)\\). The decoder learns to generate data without using latent information, defeating the purpose of the model.</p> <p>How to detect:</p> <ul> <li>KL divergence \u2248 0 across all dimensions</li> <li>Random samples from prior produce diverse outputs, but encoding-decoding produces generic/blurry results</li> <li>Reconstructions don't match inputs well despite low reconstruction loss</li> </ul> <p>Why does it happen?</p> <p>Powerful autoregressive decoders (especially in text VAEs) can model \\(p(x)\\) without needing latent information. The KL term drives encodings toward the prior, and if the decoder doesn't need \\(z\\), the KL term wins.</p> <pre><code>graph TD\n    A[Strong Decoder] --&gt; B{Can generate&lt;br/&gt;without z?}\n    B --&gt;|Yes| C[Ignores Latent Code]\n    B --&gt;|No| D[Uses Latent Code]\n    C --&gt; E[Posterior Collapse]\n    D --&gt; F[Healthy Training]\n\n    G[KL Annealing] --&gt; D\n    H[Weak Decoder] --&gt; D\n    I[Free Bits] --&gt; D\n\n    style E fill:#ffccbc\n    style F fill:#c8e6c9</code></pre> <p>Solutions ranked by effectiveness:</p> <ol> <li>KL Annealing (CRITICAL for text): Start with \u03b2=0, gradually increase to 1 over 20-40 epochs</li> <li>Linear: <code>\u03b2 = min(1.0, epoch / 40)</code></li> <li> <p>Cyclical (BEST): Cycle \u03b2 from 0\u21921 multiple times during training</p> </li> <li> <p>Free Bits: Only penalize KL when it drops below a threshold per dimension</p> </li> <li> <p><code>KL_constrained = max(KL_per_dim, \u03bb)</code> where \u03bb=0.5 works well</p> </li> <li> <p>\u03b2-VAE with \u03b2 &lt; 1: Reduce KL penalty (\u03b2=0.1-0.5 for text)</p> </li> <li> <p>Word Dropout (for text): Randomly replace 25-50% of input words with <code>&lt;UNK&gt;</code></p> </li> <li> <p>Weakening the Decoder: Use simpler decoder architecture or add noise</p> </li> </ol>"},{"location":"user-guide/concepts/vae-explained/#blurry-reconstructions","title":"Blurry Reconstructions","text":"<p>Why it happens:</p> <p>MSE loss encourages the decoder to output \\(\\mathbb{E}[x|z]\\), the average of all plausible outputs. Averaging sharp images produces blur\u2014this is a fundamental consequence of the Gaussian likelihood assumption, not a bug.</p> <p>Solutions:</p> <ol> <li>Perceptual Loss: Replace pixel-wise MSE with VGG/AlexNet feature matching</li> <li>Significantly improves sharpness while maintaining structure</li> <li> <p>Used in Deep Feature Consistent VAE (DFC-VAE)</p> </li> <li> <p>Adversarial Training: Add discriminator to penalize unrealistic outputs (VAE-GAN)</p> </li> <li>Used in Stable Diffusion's VAE component</li> <li> <p>Combines reconstruction, KL, and adversarial losses</p> </li> <li> <p>Multi-scale SSIM: Structural similarity loss instead of MSE</p> </li> <li> <p>Better captures perceptual quality</p> </li> <li> <p>VQ-VAE: Discrete latents naturally produce sharper outputs</p> </li> <li> <p>Learned Variance: Let decoder predict per-pixel variance instead of fixed \u03c3\u00b2</p> </li> </ol>"},{"location":"user-guide/concepts/vae-explained/#optimization-challenges","title":"Optimization Challenges","text":"<p>NaN losses:</p> <ul> <li>Check activation functions: ensure Sigmoid on decoder output for [0,1] images</li> <li>Add gradient clipping: <code>grads = jax.tree.map(lambda g: jnp.clip(g, -1.0, 1.0), grads)</code></li> <li>Use Softplus for log_var: <code>log_var = nnx.softplus(log_var_raw) + 1e-6</code></li> <li>Reduce learning rate if gradients explode</li> </ul> <p>Loss not decreasing:</p> <ul> <li>Verify loss signs: minimize negative ELBO</li> <li>Check data normalization: should be [0,1] or [-1,1]</li> <li>Ensure encoder-decoder dimension matching</li> <li>Monitor gradient norms: should be in range [0.1, 10]</li> </ul> <p>Imbalanced loss terms:</p> <ul> <li>Reconstruction loss sums over many pixels; KL sums over few latent dimensions</li> <li>Solution: normalize by dimension count or manually weight with \u03b2</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#advanced-topics","title":"Advanced Topics","text":""},{"location":"user-guide/concepts/vae-explained/#hierarchical-vaes","title":"Hierarchical VAEs","text":"<p>Stack multiple layers of latent variables for richer, more structured representations:</p> \\[ p(x, z_1, z_2) = p(z_2) p(z_1|z_2) p(x|z_1, z_2) \\] <p>Benefits:</p> <ul> <li>Coarse features (object class) at top levels</li> <li>Fine details (texture, color) at lower levels</li> <li>Better for complex, high-resolution data</li> </ul> <p>State-of-the-art: NVAE (Vahdat &amp; Kautz, 2020) uses 36 hierarchical groups, first VAE to successfully model 256\u00d7256 natural images.</p>"},{"location":"user-guide/concepts/vae-explained/#importance-weighted-vae-iwae","title":"Importance Weighted VAE (IWAE)","text":"<p>Use multiple samples to get tighter bounds on the log-likelihood:</p> \\[ \\mathcal{L}_{\\text{IWAE}} = \\mathbb{E}_{z_{1:K} \\sim q} \\left[ \\log \\frac{1}{K} \\sum_{k=1}^K \\frac{p(x, z_k)}{q(z_k|x)} \\right] \\] <p>With \\(K\\) samples, IWAE provides a strictly tighter bound than standard VAE (K=1). Typical values: K=5-50.</p>"},{"location":"user-guide/concepts/vae-explained/#normalizing-flow-vae","title":"Normalizing Flow VAE","text":"<p>Replace Gaussian posterior with flexible distributions via invertible transformations:</p> \\[ q_\\phi(z|x) = q_0(z_0|x) \\left|\\det \\frac{\\partial f}{\\partial z_0}\\right|^{-1} \\] <p>where \\(f\\) is an invertible function (Real NVP, MAF, IAF, Glow, etc.)</p> <p>Benefits:</p> <ul> <li>Arbitrarily complex posterior distributions</li> <li>Better approximation of true posterior</li> <li>Improved generation quality</li> </ul> <p>Trade-off: Increased computational cost during training</p>"},{"location":"user-guide/concepts/vae-explained/#latent-space-properties-and-interpretation","title":"Latent Space Properties and Interpretation","text":""},{"location":"user-guide/concepts/vae-explained/#continuity-and-interpolation","title":"Continuity and Interpolation","text":"<p>A well-trained VAE has a continuous latent space where:</p> <ul> <li>Nearby points decode to similar outputs</li> <li>Linear interpolation produces smooth transitions</li> <li>The space is \"covered\" - no holes where sampling produces garbage</li> </ul> <p>Testing interpolation:</p> <pre><code># Encode two images\nz1 = encoder(x1)[0]  # Take mean, ignore variance\nz2 = encoder(x2)[0]\n\n# Interpolate\nalphas = jnp.linspace(0, 1, num=10)\nz_interp = [(1-\u03b1)*z1 + \u03b1*z2 for \u03b1 in alphas]\n\n# Decode interpolated points\nx_interp = [decoder(z) for z in z_interp]\n</code></pre>"},{"location":"user-guide/concepts/vae-explained/#disentanglement-independent-factors-of-variation","title":"Disentanglement: Independent Factors of Variation","text":"<p>In a disentangled representation, each latent dimension captures a single, interpretable factor:</p> <ul> <li>\\(z_1\\): Object class (digit identity)</li> <li>\\(z_2\\): Rotation angle</li> <li>\\(z_3\\): Stroke width</li> <li>\\(z_4\\): Position</li> <li>...</li> </ul> <pre><code>graph TD\n    subgraph \"Disentangled Latent Space\"\n        A[\"z\u2081: Rotation\"] --&gt; E[\"Decoder\"]\n        B[\"z\u2082: Size\"] --&gt; E\n        C[\"z\u2083: Color\"] --&gt; E\n        D[\"z\u2084: Position\"] --&gt; E\n    end\n\n    E --&gt; F[\"Generated Image\"]\n\n    subgraph \"Entangled Latent Space\"\n        G[\"z\u2081: Mixed&lt;br/&gt;(rotation + size)\"] --&gt; H[\"Decoder\"]\n        I[\"z\u2082: Mixed&lt;br/&gt;(color + position)\"] --&gt; H\n    end\n\n    H --&gt; J[\"Generated Image\"]\n\n    style E fill:#c8e6c9\n    style H fill:#ffccbc</code></pre> <p>Achieving disentanglement:</p> <ul> <li>Train with \u03b2-VAE (\u03b2 &gt; 1)</li> <li>Use structured datasets (dSprites, 3D shapes)</li> <li>Apply supervision or weak supervision</li> <li>Consider Factor-VAE or TC-VAE variants</li> </ul> <p>Measuring disentanglement:</p> <ul> <li>MIG (Mutual Information Gap): Measures how informative each latent is about one specific factor</li> <li>SAP (Separated Attribute Predictability): Measures how predictable factors are from individual latents</li> <li>DCI (Disentanglement, Completeness, Informativeness): Three-metric framework</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#comparing-vaes-with-other-generative-models","title":"Comparing VAEs with Other Generative Models","text":"Aspect VAE GAN Diffusion Normalizing Flow Likelihood Lower bound (ELBO) Implicit Tractable Exact Training Stability Stable Unstable Stable Stable Sample Quality Good (blurry) Excellent (sharp) Excellent Good Sampling Speed Fast Fast Slow (50-1000 steps) Fast Latent Space Structured, smooth None (no encoder) Gradual diffusion Exact bijection Mode Coverage Excellent Poor (mode collapse) Excellent Excellent Architecture Constraints Flexible Flexible Flexible Invertible only"},{"location":"user-guide/concepts/vae-explained/#when-to-use-vaes","title":"When to Use VAEs","text":"<p>VAEs Excel When:</p> <ul> <li>You need structured latent representations for downstream tasks</li> <li>Training stability is more important than peak image quality</li> <li>You want both generation and reconstruction capabilities</li> <li>Interpretability matters (anomaly detection, representation learning)</li> <li>You're working with non-image data (text, graphs, molecules)</li> </ul> <p>Example Applications:</p> <ul> <li>Medical image anomaly detection via reconstruction error</li> <li>Molecular design with controllable chemical properties</li> <li>Semi-supervised learning with limited labels</li> <li>Data compression and denoising</li> <li>Recommendation systems</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#when-to-use-gans","title":"When to Use GANs","text":"<p>GANs Excel When:</p> <ul> <li>Image quality is paramount (super-resolution, photorealistic faces)</li> <li>You don't need an encoder (generation-only tasks)</li> <li>You're willing to handle training instability</li> <li>Mode coverage isn't critical</li> </ul> <p>Limitations:</p> <ul> <li>No structured latent space for interpolation/arithmetic</li> <li>Training instability (mode collapse, oscillation)</li> <li>No reconstruction capability</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#when-to-use-diffusion-models","title":"When to Use Diffusion Models","text":"<p>Diffusion Models Excel When:</p> <ul> <li>You want state-of-the-art quality (DALL-E 2, Imagen, Stable Diffusion)</li> <li>Computational cost is acceptable</li> <li>You need excellent mode coverage and diversity</li> </ul> <p>Limitations:</p> <ul> <li>Slow sampling (requires many iterative steps)</li> <li>Higher computational cost</li> <li>Often combined with VAEs (Latent Diffusion Models)</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#practical-implementation-guide","title":"Practical Implementation Guide","text":""},{"location":"user-guide/concepts/vae-explained/#architecture-recommendations","title":"Architecture Recommendations","text":"<p>For Images (MNIST, CIFAR-10, CelebA):</p> <pre><code># Encoder (using Flax NNX)\nnnx.Conv(3, 32, kernel_size=(4, 4), strides=2) \u2192 nnx.BatchNorm \u2192 nnx.relu\nnnx.Conv(32, 64, kernel_size=(4, 4), strides=2) \u2192 nnx.BatchNorm \u2192 nnx.relu\nnnx.Conv(64, 128, kernel_size=(4, 4), strides=2) \u2192 nnx.BatchNorm \u2192 nnx.relu\nFlatten \u2192 nnx.Linear(latent_dim \u00d7 2) \u2192 Split into \u03bc and log(\u03c3\u00b2)\n\n# Decoder (mirror)\nnnx.Linear(latent_dim, 128\u00d74\u00d74) \u2192 Reshape\nnnx.ConvTranspose(128, 64, kernel_size=(4, 4), strides=2) \u2192 nnx.BatchNorm \u2192 nnx.relu\nnnx.ConvTranspose(64, 32, kernel_size=(4, 4), strides=2) \u2192 nnx.BatchNorm \u2192 nnx.relu\nnnx.ConvTranspose(32, 3, kernel_size=(4, 4), strides=2) \u2192 nnx.sigmoid\n</code></pre> <p>For Text/Sequential Data:</p> <pre><code># Encoder (using Flax NNX)\nnnx.Embed(vocab_size, embed_dim) \u2192 Bidirectional nnx.LSTM/nnx.GRU (2-3 layers)\n\u2192 Take final hidden state \u2192 nnx.Linear(latent_dim \u00d7 2)\n\n# Decoder\nRepeat latent vector for each timestep\n\u2192 nnx.LSTM/nnx.GRU \u2192 nnx.Linear(vocab_size) \u2192 nnx.softmax\n</code></pre>"},{"location":"user-guide/concepts/vae-explained/#hyperparameter-recommendations","title":"Hyperparameter Recommendations","text":"<p>Latent Dimensions:</p> <ul> <li>MNIST (28\u00d728): 2-20 dimensions</li> <li>CIFAR-10 (32\u00d732): 128-256 dimensions</li> <li>CelebA (64\u00d764): 256-512 dimensions</li> <li>Text (sentences): 32-128 dimensions</li> </ul> <p>Learning Rates:</p> <ul> <li>Simple datasets (MNIST): 1e-3 to 5e-3</li> <li>Complex images: 1e-4 to 1e-3</li> <li>Text: 5e-4 to 1e-3</li> <li>Always use Adam or AdamW optimizer</li> </ul> <p>Batch Sizes:</p> <ul> <li>64-128 works well across domains</li> <li>Larger batches improve gradient estimates but require more memory</li> </ul> <p>Training Epochs:</p> <ul> <li>MNIST: 50-100 epochs</li> <li>CIFAR-10/CelebA: 100-300 epochs</li> <li>Text: 50-200 epochs</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#essential-training-techniques","title":"Essential Training Techniques","text":"<ol> <li>KL Annealing (CRITICAL for text, helpful for images):</li> </ol> <pre><code># Linear annealing\nbeta = min(1.0, epoch / 40)\nloss = recon_loss + beta * kl_loss\n\n# Cyclical annealing (BEST for NLP)\ncycle_length = 10\nt = epoch % cycle_length\nif t &lt;= 0.5 * cycle_length:\n    beta = t / (0.5 * cycle_length)\nelse:\n    beta = 1.0\n</code></pre> <ol> <li>Numerical Stability:</li> </ol> <pre><code># Use Softplus + epsilon for variance\nlog_var = nnx.softplus(log_var_raw) + 1e-6\nsigma = jnp.sqrt(jnp.exp(log_var))\n\n# Gradient clipping\ngrads = jax.tree.map(lambda g: jnp.clip(g, -1.0, 1.0), grads)\n</code></pre> <ol> <li>Loss Balancing:</li> </ol> <pre><code># Normalize by dimensions\nrecon_loss = jnp.mean((x_recon - x) ** 2)  # averages over pixels\nkl_loss = kl_divergence.mean()  # average over batch and dimensions\n</code></pre>"},{"location":"user-guide/concepts/vae-explained/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>VAEs are powerful generative models that combine deep learning with variational inference to learn structured, interpretable latent representations. Understanding VAEs provides essential foundations for modern generative modeling, from Stable Diffusion's latent space to DALL-E's discrete representations.</p> <p>Core Principles:</p> <ul> <li>ELBO objective balances reconstruction quality with latent space structure</li> <li>Reparameterization trick enables efficient gradient-based optimization</li> <li>Probabilistic framework creates smooth, continuous latent spaces suitable for generation</li> <li>Variational inference provides principled approximations to intractable posteriors</li> </ul> <p>Key Variants:</p> <ul> <li>\u03b2-VAE trades reconstruction for disentangled, interpretable representations</li> <li>VQ-VAE uses discrete latents for improved quality and codebook learning</li> <li>Conditional VAE enables controlled generation with auxiliary information</li> <li>Hierarchical VAE captures multi-scale structure in complex data</li> </ul> <p>Best Practices:</p> <ul> <li>Use KL annealing, especially for text</li> <li>Monitor both reconstruction and KL losses during training</li> <li>Consider perceptual or adversarial losses for sharper images</li> <li>Apply appropriate architecture choices for your data modality</li> <li>Start simple, add complexity as needed</li> </ul>"},{"location":"user-guide/concepts/vae-explained/#next-steps","title":"Next Steps","text":"<ul> <li> <p> VAE User Guide</p> <p>Practical usage guide with implementation examples and training workflows</p> </li> <li> <p> VAE API Reference</p> <p>Complete API documentation for VAE, \u03b2-VAE, CVAE, and VQ-VAE classes</p> </li> <li> <p> MNIST Tutorial</p> <p>Step-by-step hands-on tutorial: train a VAE on MNIST from scratch</p> </li> <li> <p> Advanced Examples</p> <p>Explore hierarchical VAEs, VQ-VAE applications, and multi-modal learning</p> </li> </ul>"},{"location":"user-guide/concepts/vae-explained/#additional-readings","title":"Additional Readings","text":""},{"location":"user-guide/concepts/vae-explained/#seminal-papers-must-read","title":"Seminal Papers (Must Read)","text":"<p> Kingma, D. P., &amp; Welling, M. (2013). \"Auto-Encoding Variational Bayes\" arXiv:1312.6114  The original VAE paper introducing the framework and reparameterization trick</p> <p> Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\" arXiv:1401.4082  Independent development of similar ideas with deep latent Gaussian models</p> <p> Higgins, I., et al. (2017). \"\u03b2-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework\" ICLR 2017  Introduces \u03b2-VAE for disentangled representations</p> <p> Van den Oord, A., Vinyals, O., &amp; Kavukcuoglu, K. (2017). \"Neural Discrete Representation Learning\" arXiv:1711.00937  VQ-VAE for discrete latent representations</p>"},{"location":"user-guide/concepts/vae-explained/#tutorial-papers-and-books","title":"Tutorial Papers and Books","text":"<p> Kingma, D. P., &amp; Welling, M. (2019). \"An Introduction to Variational Autoencoders\" arXiv:1906.02691  Authoritative modern tutorial by the original authors</p> <p> Doersch, C. (2016). \"Tutorial on Variational Autoencoders\" arXiv:1606.05908  Excellent intuitive introduction with minimal prerequisites</p> <p> Ghojogh, B., et al. (2021). \"Factor Analysis, Probabilistic PCA, Variational Inference, and VAE: Tutorial and Survey\" arXiv:2101.00734  Connects VAEs to classical dimensionality reduction methods</p>"},{"location":"user-guide/concepts/vae-explained/#important-vae-variants","title":"Important VAE Variants","text":"<p> Burda, Y., Grosse, R., &amp; Salakhutdinov, R. (2015). \"Importance Weighted Autoencoders\" arXiv:1509.00519  Tighter likelihood bounds using importance sampling</p> <p> Burgess, C. P., et al. (2018). \"Understanding Disentangling in \u03b2-VAE\" arXiv:1804.03599  Theory and practice of disentanglement in \u03b2-VAE</p> <p> S\u00f8nderby, C. K., et al. (2016). \"Ladder Variational Autoencoders\" arXiv:1602.02282  Hierarchical VAEs with bidirectional inference</p> <p> Vahdat, A., &amp; Kautz, J. (2020). \"NVAE: A Deep Hierarchical Variational Autoencoder\" arXiv:2007.03898  State-of-the-art deep hierarchical VAE for high-resolution images</p> <p> Rezende, D., &amp; Mohamed, S. (2015). \"Variational Inference with Normalizing Flows\" arXiv:1505.05770  Flexible posterior distributions using invertible transformations</p> <p> Kingma, D. P., et al. (2016). \"Improved Variational Inference with Inverse Autoregressive Flow\" arXiv:1606.04934  Scalable flexible posteriors for complex distributions</p> <p> Tomczak, J., &amp; Welling, M. (2017). \"VAE with a VampPrior\" arXiv:1705.07120  Learned mixture-of-posteriors prior for better modeling</p> <p> Makhzani, A., et al. (2015). \"Adversarial Autoencoders\" arXiv:1511.05644  Combining VAEs with adversarial training</p>"},{"location":"user-guide/concepts/vae-explained/#recent-advances-2023-2025","title":"Recent Advances (2023-2025)","text":"<p> Sadat, A., et al. (2024). \"LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models\" arXiv:2405.14477  6\u00d7 parameter reduction using wavelet transforms</p> <p> Li, Y., et al. (2024). \"WF-VAE: Enhancing Video VAE via Wavelet-Driven Energy Flow for Latent Video Diffusion Model\" arXiv:2411.17459  Efficient video VAE with wavelet flows</p> <p> Kouzelis, T., et al. (2025). \"EQ-VAE: Equivariance VAE with Application to Image Generation\" arXiv:2502.09509  Geometric equivariance for improved generation</p> <p> Anonymous (2025). \"Posterior Collapse as a Phase Transition\" arXiv:2510.01621  Statistical physics analysis of posterior collapse</p>"},{"location":"user-guide/concepts/vae-explained/#application-papers","title":"Application Papers","text":"<p> Bowman, S. R., et al. (2015). \"Generating Sentences from a Continuous Space\" arXiv:1511.06349  VAEs for text generation (pioneering work)</p> <p> Sohn, K., Lee, H., &amp; Yan, X. (2015). \"Learning Structured Output Representation using Deep Conditional Generative Models\" NeurIPS 2015  Conditional VAE framework</p> <p> Gomez-Bombarelli, R., et al. (2018). \"Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules\" ACS Central Science  VAEs for molecular design and drug discovery</p>"},{"location":"user-guide/concepts/vae-explained/#online-resources-and-code","title":"Online Resources and Code","text":"<p> Lil'Log: From Autoencoder to Beta-VAE lilianweng.github.io/posts/2018-08-12-vae  Comprehensive blog post with excellent visualizations</p> <p> Jaan Altosaar's VAE Tutorial jaan.io/what-is-variational-autoencoder-vae-tutorial  Clear mathematical derivations with intuitive explanations</p> <p> Pythae: Unifying VAE Framework github.com/clementchadebec/benchmark_VAE  Production-ready implementations with 15+ VAE variants</p> <p> AntixK/PyTorch-VAE github.com/AntixK/PyTorch-VAE  18+ VAE variants trained on CelebA for comparison</p> <p> Awesome VAEs Collection github.com/matthewvowels1/Awesome-VAEs  Curated list of ~900 papers on VAEs and disentanglement</p>"},{"location":"user-guide/concepts/vae-explained/#books-and-surveys","title":"Books and Surveys","text":"<p> Murphy, K. P. (2022). \"Probabilistic Machine Learning: Advanced Topics\"  Chapter on variational inference and deep generative models  Comprehensive treatment connecting theory and practice</p> <p> Foster, D. (2019). \"Generative Deep Learning\"  O'Reilly book with practical VAE implementations  Covers VAE, GAN, and autoregressive models</p> <p> Zhang, C., et al. (2021). \"An Overview of Variational Autoencoders for Source Separation, Finance, and Bio-Signal Applications\" PMC8774760  Survey of VAE applications across domains</p> <p>Ready to implement VAEs? Start with the VAE User Guide for practical usage, check the API Reference for complete documentation, or dive into the MNIST Tutorial for hands-on experience!</p>"},{"location":"user-guide/data/data-guide/","title":"Data Loading Guide","text":"<p>This guide provides practical instructions for loading and preprocessing data in Artifex, including custom datasets, augmentation strategies, and performance optimization.</p>"},{"location":"user-guide/data/data-guide/#quick-start","title":"Quick Start","text":"<p>Here's a minimal example to get you started with data loading:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.modalities.image import (\n    ImageModalityConfig,\n    ImageRepresentation\n)\nfrom artifex.generative_models.modalities.image.datasets import SyntheticImageDataset\n\n# Initialize RNG\nrngs = nnx.Rngs(0)\n\n# Configure and create dataset\nconfig = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=32,\n    width=32,\n    normalize=True\n)\n\ndataset = SyntheticImageDataset(\n    config=config,\n    dataset_size=1000,\n    pattern_type=\"random\",\n    rngs=rngs\n)\n\n# Load a batch\nbatch = dataset.get_batch(batch_size=32)\nprint(f\"Batch shape: {batch['images'].shape}\")  # (32, 32, 32, 3)\n</code></pre>"},{"location":"user-guide/data/data-guide/#loading-different-data-types","title":"Loading Different Data Types","text":""},{"location":"user-guide/data/data-guide/#loading-image-data","title":"Loading Image Data","text":""},{"location":"user-guide/data/data-guide/#from-synthetic-datasets","title":"From Synthetic Datasets","text":"<p>Artifex provides several synthetic dataset types for testing and development:</p> <pre><code>from artifex.generative_models.modalities.image.datasets import (\n    SyntheticImageDataset,\n    MNISTLikeDataset,\n    create_image_dataset\n)\n\n# Random patterns\nrandom_dataset = SyntheticImageDataset(\n    config=config,\n    dataset_size=5000,\n    pattern_type=\"random\",\n    rngs=rngs\n)\n\n# Gradient patterns\ngradient_dataset = SyntheticImageDataset(\n    config=config,\n    dataset_size=5000,\n    pattern_type=\"gradient\",\n    rngs=rngs\n)\n\n# Checkerboard patterns\ncheckerboard_dataset = SyntheticImageDataset(\n    config=config,\n    dataset_size=5000,\n    pattern_type=\"checkerboard\",\n    rngs=rngs\n)\n\n# Circle patterns\ncircles_dataset = SyntheticImageDataset(\n    config=config,\n    dataset_size=5000,\n    pattern_type=\"circles\",\n    rngs=rngs\n)\n\n# MNIST-like digit patterns\nmnist_config = ImageModalityConfig(\n    representation=ImageRepresentation.GRAYSCALE,\n    height=28,\n    width=28,\n    normalize=True\n)\n\nmnist_dataset = MNISTLikeDataset(\n    config=mnist_config,\n    dataset_size=60000,\n    num_classes=10,\n    rngs=rngs\n)\n\n# Using factory function\ndataset = create_image_dataset(\n    dataset_type=\"synthetic\",\n    config=config,\n    pattern_type=\"gradient\",\n    dataset_size=10000,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/data/data-guide/#iterating-over-datasets","title":"Iterating Over Datasets","text":"<pre><code># Iterate over individual samples\nfor sample in dataset:\n    image = sample[\"images\"]\n    print(f\"Image shape: {image.shape}\")  # (32, 32, 3)\n    break  # Process first sample\n\n# Batch iteration\nbatch_size = 128\nnum_batches = len(dataset) // batch_size\n\nfor i in range(num_batches):\n    batch = dataset.get_batch(batch_size)\n    images = batch[\"images\"]\n    # Process batch\n    print(f\"Batch {i}: {images.shape}\")\n</code></pre>"},{"location":"user-guide/data/data-guide/#loading-images-from-arrays","title":"Loading Images from Arrays","text":"<pre><code>import jax.numpy as jnp\nfrom artifex.generative_models.modalities.image.base import ImageModality\n\n# Create modality\nmodality = ImageModality(config=config, rngs=rngs)\n\n# Load from NumPy arrays (e.g., from PIL, OpenCV)\nraw_images = jnp.array([...])  # Shape: (N, H, W, C)\n\n# Process through modality\nprocessed_images = modality.process(raw_images)\n\n# Images are now:\n# - Resized to config dimensions\n# - Normalized to [0, 1]\n# - Ready for model input\n</code></pre>"},{"location":"user-guide/data/data-guide/#loading-text-data","title":"Loading Text Data","text":""},{"location":"user-guide/data/data-guide/#from-synthetic-datasets_1","title":"From Synthetic Datasets","text":"<pre><code>from artifex.generative_models.modalities.text.datasets import (\n    SyntheticTextDataset,\n    SimpleTextDataset,\n    create_text_dataset\n)\nfrom artifex.generative_models.core.configuration import ModalityConfiguration\n\n# Configure text modality\ntext_config = ModalityConfiguration(\n    name=\"text\",\n    modality_type=\"text\",\n    metadata={\n        \"text_params\": {\n            \"vocab_size\": 10000,\n            \"max_length\": 512,\n            \"pad_token_id\": 0,\n            \"unk_token_id\": 1,\n            \"bos_token_id\": 2,\n            \"eos_token_id\": 3,\n            \"case_sensitive\": False\n        }\n    }\n)\n\n# Random sentences\nrandom_text = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=5000,\n    pattern_type=\"random_sentences\",\n    rngs=rngs\n)\n\n# Repeated phrases\nrepeated_text = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=5000,\n    pattern_type=\"repeated_phrases\",\n    rngs=rngs\n)\n\n# Numerical sequences\nsequences = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=5000,\n    pattern_type=\"sequences\",\n    rngs=rngs\n)\n\n# Palindromes\npalindromes = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=5000,\n    pattern_type=\"palindromes\",\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/data/data-guide/#from-text-strings","title":"From Text Strings","text":"<pre><code># Create dataset from list of strings\ntexts = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Machine learning is transforming industries\",\n    \"Deep neural networks can learn complex patterns\",\n    \"Natural language processing enables text understanding\",\n    # ... more texts\n]\n\ntext_dataset = SimpleTextDataset(\n    config=text_config,\n    texts=texts,\n    split=\"train\",\n    rngs=rngs\n)\n\n# Get batch\nbatch = text_dataset.get_batch(batch_size=32)\nprint(batch[\"text_tokens\"].shape)  # (32, 512)\nprint(len(batch[\"texts\"]))  # 32 - list of original strings\n</code></pre>"},{"location":"user-guide/data/data-guide/#accessing-text-data","title":"Accessing Text Data","text":"<pre><code># Iterate over samples\nfor sample in text_dataset:\n    tokens = sample[\"text_tokens\"]  # JAX array of token IDs\n    text = sample[\"text\"]  # Original text string\n    index = sample[\"index\"]  # Sample index\n\n    print(f\"Text: {text}\")\n    print(f\"Tokens: {tokens.shape}\")\n\n# Get specific sample\nsample_text = text_dataset.get_sample_text(0)\nprint(f\"Sample 0: {sample_text}\")\n\n# Get vocabulary statistics\nstats = text_dataset.get_vocab_stats()\nprint(stats)\n# {\n#     'unique_tokens': 1523,\n#     'vocab_coverage': 0.1523,\n#     'total_sequences': 5000,\n#     'max_length': 512\n# }\n</code></pre>"},{"location":"user-guide/data/data-guide/#loading-audio-data","title":"Loading Audio Data","text":""},{"location":"user-guide/data/data-guide/#from-synthetic-datasets_2","title":"From Synthetic Datasets","text":"<pre><code>from artifex.generative_models.modalities.audio.datasets import (\n    SyntheticAudioDataset,\n    create_audio_dataset\n)\nfrom artifex.generative_models.modalities.audio.base import AudioModalityConfig\n\n# Configure audio modality\naudio_config = AudioModalityConfig(\n    sample_rate=16000,\n    duration=1.0,\n    n_mels=80,\n    hop_length=512,\n    normalize=True\n)\n\n# Create synthetic audio dataset\naudio_dataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=1000,\n    audio_types=[\"sine\", \"noise\", \"chirp\"]\n)\n\n# Access samples\nsample = audio_dataset[0]\nprint(sample[\"audio\"].shape)  # (16000,) - 1 second at 16kHz\nprint(sample[\"audio_type\"])  # \"sine\"\nprint(sample[\"sample_rate\"])  # 16000\nprint(sample[\"duration\"])  # 1.0\n\n# Using factory function\naudio_dataset = create_audio_dataset(\n    dataset_type=\"synthetic\",\n    config=audio_config,\n    n_samples=5000,\n    audio_types=[\"sine\", \"noise\"]\n)\n</code></pre>"},{"location":"user-guide/data/data-guide/#batching-audio-data","title":"Batching Audio Data","text":"<pre><code># Get batch of audio samples\nbatch = audio_dataset.collate_fn([\n    audio_dataset[0],\n    audio_dataset[1],\n    audio_dataset[2],\n    audio_dataset[3]\n])\n\nprint(batch[\"audio\"].shape)  # (4, 16000)\nprint(len(batch[\"audio_type\"]))  # 4\n</code></pre>"},{"location":"user-guide/data/data-guide/#loading-multi-modal-data","title":"Loading Multi-modal Data","text":""},{"location":"user-guide/data/data-guide/#creating-aligned-multi-modal-datasets","title":"Creating Aligned Multi-modal Datasets","text":"<pre><code>from artifex.generative_models.modalities.multi_modal.datasets import (\n    MultiModalDataset,\n    create_synthetic_multi_modal_dataset\n)\n\n# Create aligned multi-modal dataset\nmulti_dataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\", \"audio\"],\n    num_samples=1000,\n    alignment_strength=0.8,  # 0.0 = random, 1.0 = perfectly aligned\n    image_shape=(32, 32, 3),\n    text_vocab_size=1000,\n    text_sequence_length=50,\n    audio_sample_rate=16000,\n    audio_duration=1.0,\n    rngs=rngs\n)\n\n# Access multi-modal samples\nsample = multi_dataset[0]\nprint(sample.keys())\n# dict_keys(['image', 'text', 'audio', 'alignment_score', 'latent'])\n\nprint(sample[\"image\"].shape)  # (32, 32, 3)\nprint(sample[\"text\"].shape)  # (50,)\nprint(sample[\"audio\"].shape)  # (16000,)\nprint(sample[\"alignment_score\"])  # 0.8\nprint(sample[\"latent\"].shape)  # (32,) - shared latent\n</code></pre>"},{"location":"user-guide/data/data-guide/#creating-paired-datasets","title":"Creating Paired Datasets","text":"<pre><code>from artifex.generative_models.modalities.multi_modal.datasets import (\n    MultiModalPairedDataset\n)\n\n# Prepare paired data\nimage_data = jnp.array([...])  # (N, H, W, C)\ntext_data = jnp.array([...])  # (N, max_length)\naudio_data = jnp.array([...])  # (N, n_samples)\n\n# Define pairs\npairs = [\n    (\"image\", \"text\"),\n    (\"image\", \"audio\"),\n    (\"text\", \"audio\")\n]\n\n# Create paired dataset\npaired_dataset = MultiModalPairedDataset(\n    pairs=pairs,\n    data={\n        \"image\": image_data,\n        \"text\": text_data,\n        \"audio\": audio_data\n    },\n    alignments=jnp.ones((len(image_data),))  # Optional alignment scores\n)\n\n# Access paired sample\nsample = paired_dataset[0]\nprint(sample[\"image\"].shape)  # (H, W, C)\nprint(sample[\"text\"].shape)  # (max_length,)\nprint(sample[\"audio\"].shape)  # (n_samples,)\nprint(sample[\"alignment_scores\"])  # 1.0\n</code></pre>"},{"location":"user-guide/data/data-guide/#custom-datasets","title":"Custom Datasets","text":""},{"location":"user-guide/data/data-guide/#creating-a-custom-dataset","title":"Creating a Custom Dataset","text":"<p>To create a custom dataset, extend <code>BaseDataset</code> and implement the required methods:</p> <pre><code>from typing import Iterator\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.modalities.base import BaseDataset\nfrom artifex.generative_models.core.protocols.configuration import BaseModalityConfig\n\nclass CustomImageDataset(BaseDataset):\n    \"\"\"Custom image dataset loading from files.\"\"\"\n\n    def __init__(\n        self,\n        config: BaseModalityConfig,\n        image_paths: list[str],\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize custom dataset.\n\n        Args:\n            config: Modality configuration\n            image_paths: List of paths to image files\n            split: Dataset split\n            rngs: Random number generators\n        \"\"\"\n        super().__init__(config, split, rngs=rngs)\n        self.image_paths = image_paths\n        self.images = self._load_images()\n\n    def _load_images(self):\n        \"\"\"Load images from disk.\"\"\"\n        images = []\n        for path in self.image_paths:\n            # Load image using PIL, OpenCV, etc.\n            # For example with PIL:\n            # from PIL import Image\n            # img = Image.open(path)\n            # img_array = jnp.array(img)\n\n            # For demonstration, create synthetic data\n            img_array = jax.random.uniform(\n                jax.random.key(hash(path)),\n                (32, 32, 3)\n            )\n            images.append(img_array)\n\n        return images\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return dataset size.\"\"\"\n        return len(self.images)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        \"\"\"Iterate over dataset samples.\"\"\"\n        for i, image in enumerate(self.images):\n            yield {\n                \"images\": image,\n                \"index\": jnp.array(i),\n                \"path\": self.image_paths[i]\n            }\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get a batch of samples.\n\n        Args:\n            batch_size: Number of samples in batch\n\n        Returns:\n            Batch dictionary\n        \"\"\"\n        # Random sampling\n        key = self.rngs.sample() if \"sample\" in self.rngs else jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self))\n\n        # Gather samples\n        batch_images = [self.images[int(idx)] for idx in indices]\n        batch_paths = [self.image_paths[int(idx)] for idx in indices]\n\n        return {\n            \"images\": jnp.stack(batch_images),\n            \"indices\": indices,\n            \"paths\": batch_paths\n        }\n\n# Usage\nimage_paths = [\"/path/to/image1.jpg\", \"/path/to/image2.jpg\", ...]\n\ncustom_dataset = CustomImageDataset(\n    config=config,\n    image_paths=image_paths,\n    split=\"train\",\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/data/data-guide/#custom-dataset-with-caching","title":"Custom Dataset with Caching","text":"<p>For expensive preprocessing, implement caching:</p> <pre><code>class CachedDataset(BaseDataset):\n    \"\"\"Dataset with cached preprocessing.\"\"\"\n\n    def __init__(\n        self,\n        config: BaseModalityConfig,\n        data_source: list,\n        cache_dir: str = \"/tmp/cache\",\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(config, split, rngs=rngs)\n        self.data_source = data_source\n        self.cache_dir = cache_dir\n        self.cache = {}\n\n        # Create cache directory\n        import os\n        os.makedirs(cache_dir, exist_ok=True)\n\n    def _load_and_preprocess(self, index: int):\n        \"\"\"Load and preprocess single sample with caching.\"\"\"\n        # Check cache first\n        if index in self.cache:\n            return self.cache[index]\n\n        # Load raw data\n        raw_data = self.data_source[index]\n\n        # Expensive preprocessing\n        processed = self._expensive_preprocessing(raw_data)\n\n        # Cache result\n        self.cache[index] = processed\n\n        return processed\n\n    def _expensive_preprocessing(self, data):\n        \"\"\"Expensive preprocessing operation.\"\"\"\n        # Example: compute features, embeddings, etc.\n        return data\n\n    def __len__(self) -&gt; int:\n        return len(self.data_source)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        for i in range(len(self)):\n            processed = self._load_and_preprocess(i)\n            yield {\"data\": processed, \"index\": jnp.array(i)}\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        key = self.rngs.sample() if \"sample\" in self.rngs else jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self))\n\n        batch_data = [\n            self._load_and_preprocess(int(idx))\n            for idx in indices\n        ]\n\n        return {\n            \"data\": jnp.stack(batch_data),\n            \"indices\": indices\n        }\n</code></pre>"},{"location":"user-guide/data/data-guide/#custom-dataset-with-transformations","title":"Custom Dataset with Transformations","text":"<pre><code>class TransformDataset(BaseDataset):\n    \"\"\"Dataset with configurable transformations.\"\"\"\n\n    def __init__(\n        self,\n        config: BaseModalityConfig,\n        base_dataset: BaseDataset,\n        transforms: list = None,\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize transform dataset.\n\n        Args:\n            config: Modality configuration\n            base_dataset: Base dataset to transform\n            transforms: List of transformation functions\n            split: Dataset split\n            rngs: Random number generators\n        \"\"\"\n        super().__init__(config, split, rngs=rngs)\n        self.base_dataset = base_dataset\n        self.transforms = transforms or []\n\n    def _apply_transforms(self, sample: dict) -&gt; dict:\n        \"\"\"Apply transformations to sample.\"\"\"\n        for transform in self.transforms:\n            sample = transform(sample)\n        return sample\n\n    def __len__(self) -&gt; int:\n        return len(self.base_dataset)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        for sample in self.base_dataset:\n            yield self._apply_transforms(sample)\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        batch = self.base_dataset.get_batch(batch_size)\n        return self._apply_transforms(batch)\n\n# Define transformation functions\ndef normalize_images(sample):\n    \"\"\"Normalize images to [0, 1].\"\"\"\n    if \"images\" in sample:\n        images = sample[\"images\"]\n        sample[\"images\"] = (images - images.min()) / (images.max() - images.min() + 1e-8)\n    return sample\n\ndef add_noise(sample, noise_level=0.1):\n    \"\"\"Add Gaussian noise to images.\"\"\"\n    if \"images\" in sample:\n        key = jax.random.key(0)\n        noise = noise_level * jax.random.normal(key, sample[\"images\"].shape)\n        sample[\"images\"] = jnp.clip(sample[\"images\"] + noise, 0, 1)\n    return sample\n\ndef random_flip(sample):\n    \"\"\"Randomly flip images horizontally.\"\"\"\n    if \"images\" in sample:\n        key = jax.random.key(0)\n        flip = jax.random.bernoulli(key, 0.5)\n        if flip:\n            sample[\"images\"] = jnp.flip(sample[\"images\"], axis=-2)\n    return sample\n\n# Usage\ntransforms = [normalize_images, add_noise, random_flip]\n\ntransformed_dataset = TransformDataset(\n    config=config,\n    base_dataset=base_dataset,\n    transforms=transforms,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/data/data-guide/#data-augmentation","title":"Data Augmentation","text":""},{"location":"user-guide/data/data-guide/#image-augmentation","title":"Image Augmentation","text":"<pre><code>import jax\nimport jax.numpy as jnp\n\ndef augment_image(image, key):\n    \"\"\"Apply random augmentations to image.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n\n    Returns:\n        Augmented image\n    \"\"\"\n    keys = jax.random.split(key, 5)\n\n    # Random horizontal flip\n    if jax.random.bernoulli(keys[0], 0.5):\n        image = jnp.flip(image, axis=1)\n\n    # Random rotation (90, 180, 270 degrees)\n    k = jax.random.randint(keys[1], (), 0, 4)\n    image = jnp.rot90(image, k=int(k), axes=(0, 1))\n\n    # Random brightness adjustment\n    brightness_factor = jax.random.uniform(keys[2], minval=0.8, maxval=1.2)\n    image = jnp.clip(image * brightness_factor, 0, 1)\n\n    # Random contrast adjustment\n    contrast_factor = jax.random.uniform(keys[3], minval=0.8, maxval=1.2)\n    mean = jnp.mean(image)\n    image = jnp.clip((image - mean) * contrast_factor + mean, 0, 1)\n\n    # Random Gaussian noise\n    noise_level = jax.random.uniform(keys[4], minval=0, maxval=0.05)\n    noise = noise_level * jax.random.normal(keys[4], image.shape)\n    image = jnp.clip(image + noise, 0, 1)\n\n    return image\n\n# Apply to batch\ndef augment_batch(batch, key):\n    \"\"\"Apply augmentation to batch of images.\"\"\"\n    images = batch[\"images\"]\n    batch_size = images.shape[0]\n\n    keys = jax.random.split(key, batch_size)\n\n    augmented_images = []\n    for i in range(batch_size):\n        aug_image = augment_image(images[i], keys[i])\n        augmented_images.append(aug_image)\n\n    batch[\"images\"] = jnp.stack(augmented_images)\n    return batch\n\n# JIT-compiled version for performance\n@jax.jit\ndef augment_batch_jit(images, key):\n    \"\"\"JIT-compiled batch augmentation.\"\"\"\n    batch_size = images.shape[0]\n    keys = jax.random.split(key, batch_size)\n\n    def augment_single(carry, x):\n        image, key = x\n        augmented = augment_image(image, key)\n        return carry, augmented\n\n    _, augmented_images = jax.lax.scan(\n        augment_single,\n        None,\n        (images, keys)\n    )\n\n    return augmented_images\n\n# Usage in training\nkey = jax.random.key(0)\nfor batch in data_loader:\n    key, subkey = jax.random.split(key)\n    augmented_batch = augment_batch(batch, subkey)\n    # Use augmented_batch for training\n</code></pre>"},{"location":"user-guide/data/data-guide/#text-augmentation","title":"Text Augmentation","text":"<pre><code>def augment_text_tokens(tokens, vocab_size, key):\n    \"\"\"Apply augmentation to text tokens.\n\n    Args:\n        tokens: Token sequence (seq_len,)\n        vocab_size: Vocabulary size\n        key: Random key\n\n    Returns:\n        Augmented tokens\n    \"\"\"\n    keys = jax.random.split(key, 3)\n\n    # Random token masking (15% of tokens)\n    mask_prob = 0.15\n    mask = jax.random.bernoulli(keys[0], mask_prob, tokens.shape)\n    mask_token_id = 1  # UNK token\n    tokens = jnp.where(mask, mask_token_id, tokens)\n\n    # Random token replacement (5% of tokens)\n    replace_prob = 0.05\n    replace_mask = jax.random.bernoulli(keys[1], replace_prob, tokens.shape)\n    random_tokens = jax.random.randint(keys[2], tokens.shape, 4, vocab_size)\n    tokens = jnp.where(replace_mask, random_tokens, tokens)\n\n    return tokens\n\ndef augment_text_batch(batch, vocab_size, key):\n    \"\"\"Apply augmentation to batch of text.\"\"\"\n    tokens = batch[\"text_tokens\"]\n    batch_size = tokens.shape[0]\n\n    keys = jax.random.split(key, batch_size)\n\n    augmented_tokens = []\n    for i in range(batch_size):\n        aug_tokens = augment_text_tokens(tokens[i], vocab_size, keys[i])\n        augmented_tokens.append(aug_tokens)\n\n    batch[\"text_tokens\"] = jnp.stack(augmented_tokens)\n    return batch\n</code></pre>"},{"location":"user-guide/data/data-guide/#audio-augmentation","title":"Audio Augmentation","text":"<pre><code>def augment_audio(audio, sample_rate, key):\n    \"\"\"Apply augmentation to audio waveform.\n\n    Args:\n        audio: Audio waveform (n_samples,)\n        sample_rate: Sample rate in Hz\n        key: Random key\n\n    Returns:\n        Augmented audio\n    \"\"\"\n    keys = jax.random.split(key, 4)\n\n    # Random amplitude scaling\n    scale_factor = jax.random.uniform(keys[0], minval=0.7, maxval=1.3)\n    audio = audio * scale_factor\n\n    # Random time shift\n    max_shift = int(0.1 * len(audio))  # 10% shift\n    shift = jax.random.randint(keys[1], (), -max_shift, max_shift)\n    audio = jnp.roll(audio, int(shift))\n\n    # Random Gaussian noise\n    noise_level = jax.random.uniform(keys[2], minval=0, maxval=0.05)\n    noise = noise_level * jax.random.normal(keys[3], audio.shape)\n    audio = audio + noise\n\n    # Normalize\n    max_val = jnp.max(jnp.abs(audio))\n    audio = jnp.where(max_val &gt; 0, audio / max_val, audio)\n\n    return audio\n\ndef augment_audio_batch(batch, sample_rate, key):\n    \"\"\"Apply augmentation to batch of audio.\"\"\"\n    audio = batch[\"audio\"]\n    batch_size = audio.shape[0]\n\n    keys = jax.random.split(key, batch_size)\n\n    augmented_audio = []\n    for i in range(batch_size):\n        aug_audio = augment_audio(audio[i], sample_rate, keys[i])\n        augmented_audio.append(aug_audio)\n\n    batch[\"audio\"] = jnp.stack(augmented_audio)\n    return batch\n</code></pre>"},{"location":"user-guide/data/data-guide/#data-loaders","title":"Data Loaders","text":""},{"location":"user-guide/data/data-guide/#basic-data-loader","title":"Basic Data Loader","text":"<pre><code>def create_simple_data_loader(\n    dataset: BaseDataset,\n    batch_size: int,\n    shuffle: bool = True,\n    drop_last: bool = False\n):\n    \"\"\"Create a simple data loader.\n\n    Args:\n        dataset: Dataset to load from\n        batch_size: Batch size\n        shuffle: Whether to shuffle data\n        drop_last: Whether to drop last incomplete batch\n\n    Yields:\n        Batches of data\n    \"\"\"\n    num_samples = len(dataset)\n    num_batches = num_samples // batch_size\n    if not drop_last and num_samples % batch_size != 0:\n        num_batches += 1\n\n    for epoch in range(1):  # Single epoch\n        if shuffle:\n            key = jax.random.key(epoch)\n            indices = jax.random.permutation(key, num_samples)\n        else:\n            indices = jnp.arange(num_samples)\n\n        for i in range(num_batches):\n            start_idx = i * batch_size\n            end_idx = min(start_idx + batch_size, num_samples)\n            actual_batch_size = end_idx - start_idx\n\n            if actual_batch_size &lt; batch_size and drop_last:\n                continue\n\n            batch = dataset.get_batch(actual_batch_size)\n            yield batch\n\n# Usage\ndata_loader = create_simple_data_loader(\n    dataset=train_dataset,\n    batch_size=128,\n    shuffle=True,\n    drop_last=True\n)\n\nfor batch in data_loader:\n    # Training step\n    pass\n</code></pre>"},{"location":"user-guide/data/data-guide/#multi-epoch-data-loader","title":"Multi-Epoch Data Loader","text":"<pre><code>def create_multi_epoch_data_loader(\n    dataset: BaseDataset,\n    batch_size: int,\n    num_epochs: int,\n    shuffle: bool = True,\n    drop_last: bool = False\n):\n    \"\"\"Create a multi-epoch data loader.\n\n    Args:\n        dataset: Dataset to load from\n        batch_size: Batch size\n        num_epochs: Number of epochs\n        shuffle: Whether to shuffle data each epoch\n        drop_last: Whether to drop last incomplete batch\n\n    Yields:\n        Tuples of (epoch, batch)\n    \"\"\"\n    num_samples = len(dataset)\n\n    for epoch in range(num_epochs):\n        if shuffle:\n            key = jax.random.key(epoch)\n            indices = jax.random.permutation(key, num_samples)\n        else:\n            indices = jnp.arange(num_samples)\n\n        num_batches = num_samples // batch_size\n        if not drop_last and num_samples % batch_size != 0:\n            num_batches += 1\n\n        for i in range(num_batches):\n            start_idx = i * batch_size\n            end_idx = min(start_idx + batch_size, num_samples)\n            actual_batch_size = end_idx - start_idx\n\n            if actual_batch_size &lt; batch_size and drop_last:\n                continue\n\n            batch = dataset.get_batch(actual_batch_size)\n            yield epoch, batch\n\n# Usage\ndata_loader = create_multi_epoch_data_loader(\n    dataset=train_dataset,\n    batch_size=128,\n    num_epochs=10,\n    shuffle=True\n)\n\nfor epoch, batch in data_loader:\n    print(f\"Epoch {epoch}, batch shape: {batch['images'].shape}\")\n    # Training step\n</code></pre>"},{"location":"user-guide/data/data-guide/#prefetching-data-loader","title":"Prefetching Data Loader","text":"<p>For better performance, implement prefetching:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom queue import Queue\n\nclass PrefetchDataLoader:\n    \"\"\"Data loader with prefetching for better performance.\"\"\"\n\n    def __init__(\n        self,\n        dataset: BaseDataset,\n        batch_size: int,\n        shuffle: bool = True,\n        num_workers: int = 2,\n        prefetch_size: int = 2\n    ):\n        \"\"\"Initialize prefetching data loader.\n\n        Args:\n            dataset: Dataset to load from\n            batch_size: Batch size\n            shuffle: Whether to shuffle\n            num_workers: Number of worker threads\n            prefetch_size: Number of batches to prefetch\n        \"\"\"\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.num_workers = num_workers\n        self.prefetch_size = prefetch_size\n        self.queue = Queue(maxsize=prefetch_size)\n\n    def __iter__(self):\n        \"\"\"Iterate over prefetched batches.\"\"\"\n        num_samples = len(self.dataset)\n        num_batches = num_samples // self.batch_size\n\n        if self.shuffle:\n            key = jax.random.key(0)\n            indices = jax.random.permutation(key, num_samples)\n        else:\n            indices = jnp.arange(num_samples)\n\n        def load_batch(batch_idx):\n            \"\"\"Load single batch.\"\"\"\n            batch = self.dataset.get_batch(self.batch_size)\n            return batch\n\n        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:\n            # Submit initial batches\n            futures = []\n            for i in range(min(self.prefetch_size, num_batches)):\n                future = executor.submit(load_batch, i)\n                futures.append(future)\n\n            # Yield results and submit new batches\n            for i in range(num_batches):\n                # Wait for batch to be ready\n                batch = futures[i % len(futures)].result()\n                yield batch\n\n                # Submit next batch\n                next_idx = i + self.prefetch_size\n                if next_idx &lt; num_batches:\n                    future = executor.submit(load_batch, next_idx)\n                    futures.append(future)\n\n# Usage\nprefetch_loader = PrefetchDataLoader(\n    dataset=train_dataset,\n    batch_size=128,\n    shuffle=True,\n    num_workers=4,\n    prefetch_size=2\n)\n\nfor batch in prefetch_loader:\n    # Training step with prefetched data\n    pass\n</code></pre>"},{"location":"user-guide/data/data-guide/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/data/data-guide/#jit-compiled-preprocessing","title":"JIT-Compiled Preprocessing","text":"<pre><code>import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef preprocess_batch(batch):\n    \"\"\"JIT-compiled preprocessing for faster execution.\n\n    Args:\n        batch: Raw batch dictionary\n\n    Returns:\n        Preprocessed batch\n    \"\"\"\n    images = batch[\"images\"]\n\n    # Normalize to [0, 1]\n    images = (images - images.min()) / (images.max() - images.min() + 1e-8)\n\n    # Standardize (mean=0, std=1)\n    mean = jnp.mean(images, axis=(1, 2, 3), keepdims=True)\n    std = jnp.std(images, axis=(1, 2, 3), keepdims=True)\n    images = (images - mean) / (std + 1e-8)\n\n    batch[\"images\"] = images\n    return batch\n\n# Usage in training loop\nfor batch in data_loader:\n    preprocessed_batch = preprocess_batch(batch)\n    # Training step\n</code></pre>"},{"location":"user-guide/data/data-guide/#vectorized-operations","title":"Vectorized Operations","text":"<pre><code>@jax.jit\ndef vectorized_preprocessing(images):\n    \"\"\"Vectorized preprocessing using jax.vmap.\n\n    Args:\n        images: Batch of images (N, H, W, C)\n\n    Returns:\n        Preprocessed images\n    \"\"\"\n    def preprocess_single(image):\n        \"\"\"Preprocess single image.\"\"\"\n        # Normalize\n        image = (image - image.min()) / (image.max() - image.min() + 1e-8)\n        # Random flip\n        # ... more operations\n        return image\n\n    # Vectorize over batch dimension\n    preprocessed = jax.vmap(preprocess_single)(images)\n    return preprocessed\n\n# Usage\nimages = batch[\"images\"]\npreprocessed = vectorized_preprocessing(images)\n</code></pre>"},{"location":"user-guide/data/data-guide/#memory-efficient-loading","title":"Memory-Efficient Loading","text":"<pre><code>class MemoryEfficientDataset(BaseDataset):\n    \"\"\"Dataset that loads data on-demand to save memory.\"\"\"\n\n    def __init__(\n        self,\n        config: BaseModalityConfig,\n        data_paths: list[str],\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        \"\"\"Initialize memory-efficient dataset.\n\n        Args:\n            config: Modality configuration\n            data_paths: List of paths to data files\n            split: Dataset split\n            rngs: Random number generators\n        \"\"\"\n        super().__init__(config, split, rngs=rngs)\n        self.data_paths = data_paths\n        # Don't load all data into memory\n\n    def _load_sample(self, index: int):\n        \"\"\"Load single sample on-demand.\"\"\"\n        path = self.data_paths[index]\n        # Load from disk\n        # data = load_from_disk(path)\n        # For demonstration:\n        data = jax.random.uniform(jax.random.key(index), (32, 32, 3))\n        return data\n\n    def __len__(self) -&gt; int:\n        return len(self.data_paths)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        for i in range(len(self)):\n            data = self._load_sample(i)\n            yield {\"data\": data, \"index\": jnp.array(i)}\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        key = self.rngs.sample() if \"sample\" in self.rngs else jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self))\n\n        # Load samples on-demand\n        batch_data = [self._load_sample(int(idx)) for idx in indices]\n\n        return {\n            \"data\": jnp.stack(batch_data),\n            \"indices\": indices\n        }\n</code></pre>"},{"location":"user-guide/data/data-guide/#caching-frequently-used-data","title":"Caching Frequently Used Data","text":"<pre><code>from functools import lru_cache\n\nclass CachedLoader:\n    \"\"\"Data loader with LRU caching.\"\"\"\n\n    def __init__(self, dataset: BaseDataset, cache_size: int = 128):\n        \"\"\"Initialize cached loader.\n\n        Args:\n            dataset: Dataset to load from\n            cache_size: Maximum number of samples to cache\n        \"\"\"\n        self.dataset = dataset\n        self.cache_size = cache_size\n\n    @lru_cache(maxsize=128)\n    def _get_cached_sample(self, index: int):\n        \"\"\"Get sample with caching.\"\"\"\n        # Access dataset through iterator\n        for i, sample in enumerate(self.dataset):\n            if i == index:\n                return sample\n        return None\n\n    def get_batch(self, batch_size: int):\n        \"\"\"Get batch using cached samples.\"\"\"\n        # Generate random indices\n        key = jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self.dataset))\n\n        # Load samples (potentially from cache)\n        samples = [self._get_cached_sample(int(idx)) for idx in indices]\n\n        # Stack into batch\n        batch = {}\n        for key in samples[0].keys():\n            batch[key] = jnp.stack([s[key] for s in samples])\n\n        return batch\n\n# Usage\ncached_loader = CachedLoader(dataset, cache_size=256)\nbatch = cached_loader.get_batch(32)\n</code></pre>"},{"location":"user-guide/data/data-guide/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/data/data-guide/#trainingvalidation-split","title":"Training/Validation Split","text":"<pre><code>def split_dataset(\n    full_dataset: BaseDataset,\n    train_ratio: float = 0.8,\n    seed: int = 42\n):\n    \"\"\"Split dataset into training and validation sets.\n\n    Args:\n        full_dataset: Full dataset to split\n        train_ratio: Ratio of training data\n        seed: Random seed\n\n    Returns:\n        Tuple of (train_dataset, val_dataset)\n    \"\"\"\n    num_samples = len(full_dataset)\n    num_train = int(num_samples * train_ratio)\n\n    # Generate random permutation\n    key = jax.random.key(seed)\n    indices = jax.random.permutation(key, num_samples)\n\n    train_indices = indices[:num_train]\n    val_indices = indices[num_train:]\n\n    # Create subset datasets\n    class SubsetDataset(BaseDataset):\n        def __init__(self, dataset, indices, **kwargs):\n            super().__init__(dataset.config, dataset.split, rngs=dataset.rngs)\n            self.dataset = dataset\n            self.indices = indices\n\n        def __len__(self):\n            return len(self.indices)\n\n        def __iter__(self):\n            for idx in self.indices:\n                # Get sample from base dataset\n                for i, sample in enumerate(self.dataset):\n                    if i == int(idx):\n                        yield sample\n                        break\n\n        def get_batch(self, batch_size):\n            # Sample from subset indices\n            key = jax.random.key(0)\n            batch_indices = jax.random.choice(\n                key,\n                self.indices,\n                shape=(batch_size,),\n                replace=False\n            )\n            return self.dataset.get_batch(batch_size)\n\n    train_dataset = SubsetDataset(full_dataset, train_indices)\n    val_dataset = SubsetDataset(full_dataset, val_indices)\n\n    return train_dataset, val_dataset\n\n# Usage\ntrain_dataset, val_dataset = split_dataset(\n    full_dataset,\n    train_ratio=0.8,\n    seed=42\n)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")\n</code></pre>"},{"location":"user-guide/data/data-guide/#data-pipeline-with-transformations","title":"Data Pipeline with Transformations","text":"<pre><code>def create_training_pipeline(\n    dataset: BaseDataset,\n    batch_size: int,\n    augment: bool = True,\n    normalize: bool = True,\n    shuffle: bool = True\n):\n    \"\"\"Create complete training data pipeline.\n\n    Args:\n        dataset: Base dataset\n        batch_size: Batch size\n        augment: Whether to apply augmentation\n        normalize: Whether to normalize\n        shuffle: Whether to shuffle\n\n    Yields:\n        Preprocessed and augmented batches\n    \"\"\"\n    num_samples = len(dataset)\n    num_batches = num_samples // batch_size\n\n    for epoch in range(1):\n        if shuffle:\n            key = jax.random.key(epoch)\n            indices = jax.random.permutation(key, num_samples)\n        else:\n            indices = jnp.arange(num_samples)\n\n        for i in range(num_batches):\n            # Get batch\n            batch = dataset.get_batch(batch_size)\n\n            # Normalize\n            if normalize:\n                images = batch[\"images\"]\n                images = (images - images.min()) / (images.max() - images.min() + 1e-8)\n                batch[\"images\"] = images\n\n            # Augment\n            if augment:\n                key = jax.random.key(i)\n                batch = augment_batch(batch, key)\n\n            yield batch\n\n# Usage\npipeline = create_training_pipeline(\n    dataset=train_dataset,\n    batch_size=128,\n    augment=True,\n    normalize=True,\n    shuffle=True\n)\n\nfor batch in pipeline:\n    # Training step with preprocessed batch\n    pass\n</code></pre>"},{"location":"user-guide/data/data-guide/#multi-modal-data-loading","title":"Multi-Modal Data Loading","text":"<pre><code>def create_multi_modal_loader(\n    image_dataset: BaseDataset,\n    text_dataset: BaseDataset,\n    batch_size: int,\n    align: bool = True\n):\n    \"\"\"Create multi-modal data loader.\n\n    Args:\n        image_dataset: Image dataset\n        text_dataset: Text dataset\n        batch_size: Batch size\n        align: Whether to align samples (use same indices)\n\n    Yields:\n        Multi-modal batches\n    \"\"\"\n    assert len(image_dataset) == len(text_dataset), \\\n        \"Datasets must have same length for alignment\"\n\n    num_samples = len(image_dataset)\n    num_batches = num_samples // batch_size\n\n    for i in range(num_batches):\n        if align:\n            # Use same indices for both modalities\n            key = jax.random.key(i)\n            indices = jax.random.randint(key, (batch_size,), 0, num_samples)\n\n            # Would need to implement index-based loading\n            # For now, use get_batch which samples randomly\n            image_batch = image_dataset.get_batch(batch_size)\n            text_batch = text_dataset.get_batch(batch_size)\n        else:\n            # Independent sampling\n            image_batch = image_dataset.get_batch(batch_size)\n            text_batch = text_dataset.get_batch(batch_size)\n\n        # Combine batches\n        multi_modal_batch = {\n            \"images\": image_batch[\"images\"],\n            \"text_tokens\": text_batch[\"text_tokens\"]\n        }\n\n        yield multi_modal_batch\n\n# Usage\nmulti_modal_loader = create_multi_modal_loader(\n    image_dataset=image_dataset,\n    text_dataset=text_dataset,\n    batch_size=64,\n    align=True\n)\n\nfor batch in multi_modal_loader:\n    print(f\"Images: {batch['images'].shape}\")\n    print(f\"Text: {batch['text_tokens'].shape}\")\n</code></pre>"},{"location":"user-guide/data/data-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/data/data-guide/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Problem: Dataset too large to fit in memory.</p> <p>Solutions:</p> <pre><code># 1. Load data on-demand\nclass OnDemandDataset(BaseDataset):\n    \"\"\"Load data on-demand instead of all at once.\"\"\"\n\n    def __init__(self, data_paths, **kwargs):\n        super().__init__(**kwargs)\n        self.data_paths = data_paths\n        # Don't load data in __init__\n\n    def get_batch(self, batch_size):\n        # Load only requested samples\n        pass\n\n# 2. Use smaller batch sizes\nbatch_size = 32  # Instead of 128\n\n# 3. Clear cache periodically\nimport gc\nimport jax\n\nfor i, batch in enumerate(data_loader):\n    # Training step\n    if i % 100 == 0:\n        # Clear Python and JAX caches\n        gc.collect()\n        jax.clear_caches()\n\n# 4. Use float16 instead of float32\ndef convert_to_float16(batch):\n    \"\"\"Convert batch to float16 to save memory.\"\"\"\n    batch[\"images\"] = batch[\"images\"].astype(jnp.float16)\n    return batch\n</code></pre>"},{"location":"user-guide/data/data-guide/#slow-data-loading","title":"Slow Data Loading","text":"<p>Problem: Data loading is the bottleneck.</p> <p>Solutions:</p> <pre><code># 1. Use JIT compilation for preprocessing\n@jax.jit\ndef fast_preprocess(batch):\n    # JIT-compiled preprocessing\n    return batch\n\n# 2. Implement prefetching\nprefetch_loader = PrefetchDataLoader(\n    dataset,\n    batch_size=128,\n    num_workers=4,\n    prefetch_size=2\n)\n\n# 3. Cache preprocessed data\ncache = {}\ndef get_cached_batch(dataset, batch_size):\n    cache_key = (dataset, batch_size)\n    if cache_key not in cache:\n        cache[cache_key] = dataset.get_batch(batch_size)\n    return cache[cache_key]\n\n# 4. Reduce preprocessing complexity\n# Remove expensive operations from training loop\n# Preprocess once and save to disk\n</code></pre>"},{"location":"user-guide/data/data-guide/#inconsistent-batch-sizes","title":"Inconsistent Batch Sizes","text":"<p>Problem: Last batch has different size, causing shape mismatches.</p> <p>Solutions:</p> <pre><code># 1. Drop last incomplete batch\ndata_loader = create_simple_data_loader(\n    dataset,\n    batch_size=128,\n    drop_last=True  # Drop last batch if smaller\n)\n\n# 2. Pad last batch\ndef pad_batch(batch, target_size):\n    \"\"\"Pad batch to target size.\"\"\"\n    current_size = batch[\"images\"].shape[0]\n    if current_size &lt; target_size:\n        padding_size = target_size - current_size\n        # Repeat last samples\n        padding = jnp.repeat(\n            batch[\"images\"][-1:],\n            padding_size,\n            axis=0\n        )\n        batch[\"images\"] = jnp.concatenate([batch[\"images\"], padding])\n    return batch\n\n# 3. Handle variable batch sizes in model\ndef train_step(model, batch):\n    # Get actual batch size\n    batch_size = batch[\"images\"].shape[0]\n    # Use batch_size in computations\n    pass\n</code></pre>"},{"location":"user-guide/data/data-guide/#data-corruption","title":"Data Corruption","text":"<p>Problem: Some samples are corrupted or invalid.</p> <p>Solutions:</p> <pre><code>def validate_sample(sample):\n    \"\"\"Validate sample data.\"\"\"\n    # Check for NaN values\n    if jnp.any(jnp.isnan(sample[\"images\"])):\n        return False\n\n    # Check value range\n    if jnp.any(sample[\"images\"] &lt; 0) or jnp.any(sample[\"images\"] &gt; 1):\n        return False\n\n    # Check shape\n    if sample[\"images\"].shape != (32, 32, 3):\n        return False\n\n    return True\n\nclass ValidatedDataset(BaseDataset):\n    \"\"\"Dataset with validation.\"\"\"\n\n    def __init__(self, base_dataset, **kwargs):\n        super().__init__(**kwargs)\n        self.base_dataset = base_dataset\n\n    def get_batch(self, batch_size):\n        batch = self.base_dataset.get_batch(batch_size)\n\n        # Validate all samples\n        valid_samples = []\n        for i in range(batch[\"images\"].shape[0]):\n            sample = {\"images\": batch[\"images\"][i]}\n            if validate_sample(sample):\n                valid_samples.append(sample)\n\n        # Rebuild batch with valid samples only\n        if valid_samples:\n            batch[\"images\"] = jnp.stack([s[\"images\"] for s in valid_samples])\n\n        return batch\n\n# Usage\nvalidated_dataset = ValidatedDataset(raw_dataset, **kwargs)\n</code></pre>"},{"location":"user-guide/data/data-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/data/data-guide/#do","title":"DO","text":"<p>Dataset Design</p> <ul> <li>Implement all required methods (<code>__len__</code>, <code>__iter__</code>, <code>get_batch</code>)</li> <li>Return dictionaries with descriptive keys</li> <li>Use JAX arrays for all numeric data</li> <li>Validate data shapes and types</li> <li>Provide proper RNG handling for reproducibility</li> <li>Cache preprocessed data when appropriate</li> <li>Document expected data formats</li> <li>Use protocol-based interfaces</li> </ul> <p>Performance</p> <ul> <li>Use JIT compilation for preprocessing</li> <li>Implement prefetching for I/O-bound operations</li> <li>Vectorize operations with <code>jax.vmap</code></li> <li>Load data on-demand for large datasets</li> <li>Clear caches periodically for long training runs</li> <li>Profile data loading to identify bottlenecks</li> <li>Use appropriate batch sizes for hardware</li> </ul> <p>Data Quality</p> <ul> <li>Validate input data</li> <li>Handle missing or corrupted samples gracefully</li> <li>Normalize data to expected ranges</li> <li>Apply augmentation only during training</li> <li>Use consistent preprocessing across splits</li> <li>Document data statistics and distributions</li> </ul>"},{"location":"user-guide/data/data-guide/#dont","title":"DON'T","text":"<p>Common Mistakes</p> <ul> <li>Use PyTorch or TensorFlow tensors</li> <li>Load entire large dataset into memory</li> <li>Apply random augmentation during validation</li> <li>Ignore RNG seeding</li> <li>Mix different data types in batches</li> <li>Perform heavy I/O in tight loops</li> <li>Use non-deterministic operations without RNG</li> <li>Forget to handle edge cases (empty batches, etc.)</li> </ul> <p>Performance Pitfalls</p> <ul> <li>Recompute expensive preprocessing every batch</li> <li>Use Python loops for array operations</li> <li>Ignore batch size effects on memory</li> <li>Skip JIT compilation for repeated operations</li> <li>Load data synchronously without prefetching</li> </ul> <p>Data Issues</p> <ul> <li>Skip data validation</li> <li>Mix training and validation data</li> <li>Apply inconsistent preprocessing</li> <li>Ignore data distribution shifts</li> <li>Use invalid or out-of-range values</li> </ul>"},{"location":"user-guide/data/data-guide/#summary","title":"Summary","text":"<p>This guide covered:</p> <ul> <li>Loading different data types - Images, text, audio, and multi-modal data</li> <li>Custom datasets - Implementing custom dataset classes with caching and transformations</li> <li>Data augmentation - Augmentation strategies for images, text, and audio</li> <li>Data loaders - Simple, multi-epoch, and prefetching data loaders</li> <li>Performance optimization - JIT compilation, vectorization, and memory efficiency</li> <li>Common patterns - Training/validation splits, pipelines, and multi-modal loading</li> <li>Troubleshooting - Solutions for memory, speed, and data quality issues</li> <li>Best practices - DOs and DON'Ts for dataset design and implementation</li> </ul>"},{"location":"user-guide/data/data-guide/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Image Modality Guide</p> <p>Deep dive into image datasets, preprocessing, and augmentation techniques</p> </li> <li> <p> Text Modality Guide</p> <p>Learn about tokenization, vocabulary management, and text processing</p> </li> <li> <p> Audio Modality Guide</p> <p>Audio processing, spectrograms, and audio-specific augmentation</p> </li> <li> <p> Data API Reference</p> <p>Complete API documentation for all dataset classes and functions</p> </li> </ul>"},{"location":"user-guide/data/overview/","title":"Data Loading Overview","text":"<p>This guide provides an overview of Artifex's data loading system, including the modality framework, dataset classes, and data pipeline architecture.</p>"},{"location":"user-guide/data/overview/#key-features","title":"Key Features","text":"<ul> <li> <p> Modality System</p> <p>Unified interface for different data types (images, text, audio) with automatic preprocessing and validation</p> </li> <li> <p> Dataset Classes</p> <p>Protocol-based dataset interface compatible with JAX/Flax, supporting batching and iteration</p> </li> <li> <p> Efficient Pipeline</p> <p>JAX-native data loading with JIT compilation support and GPU acceleration</p> </li> <li> <p> Multi-modal Support</p> <p>Native support for multi-modal datasets with alignment and paired data handling</p> </li> <li> <p> Preprocessing</p> <p>Configurable preprocessing pipelines with normalization, augmentation, and transformation</p> </li> <li> <p> Extensible Design</p> <p>Easy to add custom datasets and modalities following protocol-based interfaces</p> </li> </ul>"},{"location":"user-guide/data/overview/#architecture-overview","title":"Architecture Overview","text":"<p>Artifex's data system is built around a modality-centric architecture that separates data type concerns from model implementations.</p>"},{"location":"user-guide/data/overview/#system-components","title":"System Components","text":"<pre><code>graph TB\n    A[Data Sources] --&gt; B[Modality System]\n    B --&gt; C[Image Modality]\n    B --&gt; D[Text Modality]\n    B --&gt; E[Audio Modality]\n    B --&gt; F[Multi-modal]\n\n    C --&gt; G[Image Datasets]\n    D --&gt; H[Text Datasets]\n    E --&gt; I[Audio Datasets]\n    F --&gt; J[Multi-modal Datasets]\n\n    G --&gt; K[Data Loaders]\n    H --&gt; K\n    I --&gt; K\n    J --&gt; K\n\n    K --&gt; L[Preprocessing]\n    L --&gt; M[Model Training]\n\n    style B fill:#e1f5ff\n    style C fill:#ffe1e1\n    style D fill:#e1ffe1\n    style E fill:#ffe1ff\n    style F fill:#fffbe1</code></pre>"},{"location":"user-guide/data/overview/#core-abstractions","title":"Core Abstractions","text":"<p>The data system uses protocol-based interfaces for maximum flexibility:</p> Component Purpose Key Methods Modality Defines data type interface <code>get_extensions()</code>, <code>get_adapter()</code> BaseDataset Dataset abstraction <code>__len__()</code>, <code>__iter__()</code>, <code>get_batch()</code> BaseProcessor Data preprocessing <code>process()</code>, <code>preprocess()</code>, <code>postprocess()</code> BaseEvaluationSuite Modality evaluation <code>evaluate_batch()</code>, <code>compute_quality_metrics()</code> ModelAdapter Model adaptation <code>create()</code>"},{"location":"user-guide/data/overview/#modality-system","title":"Modality System","text":"<p>The modality system provides a unified interface for working with different data types. Each modality encapsulates:</p> <ul> <li>Data representation and configuration</li> <li>Dataset implementations</li> <li>Preprocessing and augmentation</li> <li>Evaluation metrics</li> <li>Model adapters</li> </ul>"},{"location":"user-guide/data/overview/#modality-hierarchy","title":"Modality Hierarchy","text":"<pre><code>classDiagram\n    class Modality {\n        &lt;&lt;protocol&gt;&gt;\n        +name: str\n        +get_extensions(config, rngs)\n        +get_adapter(model_cls)\n    }\n\n    class BaseModalityImplementation {\n        +config: BaseModalityConfig\n        +rngs: Rngs\n        +validate_data_shape()\n        +create_batch_from_samples()\n    }\n\n    class ImageModality {\n        +image_shape: tuple\n        +output_shape: tuple\n        +generate()\n        +loss_fn()\n    }\n\n    class TextModality {\n        +vocab_size: int\n        +max_length: int\n        +tokenize()\n        +detokenize()\n    }\n\n    class AudioModality {\n        +sample_rate: int\n        +duration: float\n        +process_audio()\n        +compute_spectrogram()\n    }\n\n    Modality &lt;|.. BaseModalityImplementation\n    BaseModalityImplementation &lt;|-- ImageModality\n    BaseModalityImplementation &lt;|-- TextModality\n    BaseModalityImplementation &lt;|-- AudioModality</code></pre>"},{"location":"user-guide/data/overview/#supported-modalities","title":"Supported Modalities","text":""},{"location":"user-guide/data/overview/#image-modality","title":"Image Modality","text":"<pre><code>from artifex.generative_models.modalities import ImageModality, ImageModalityConfig, ImageRepresentation\n\n# Configure image modality\nconfig = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=64,\n    width=64,\n    channels=3,\n    normalize=True,\n    augmentation=False\n)\n\n# Create modality\nmodality = ImageModality(config=config, rngs=rngs)\n\n# Access properties\nprint(f\"Image shape: {modality.image_shape}\")  # (64, 64, 3)\nprint(f\"Output shape: {modality.output_shape}\")  # (64, 64, 3)\n</code></pre> <p>Supported representations:</p> <ul> <li><code>RGB</code>: 3-channel RGB images</li> <li><code>RGBA</code>: 4-channel RGB with alpha</li> <li><code>GRAYSCALE</code>: 1-channel grayscale</li> </ul>"},{"location":"user-guide/data/overview/#text-modality","title":"Text Modality","text":"<pre><code>from artifex.generative_models.modalities import TextModality\nfrom artifex.generative_models.core.configuration import ModalityConfiguration\n\n# Configure text modality\nconfig = ModalityConfiguration(\n    name=\"text\",\n    modality_type=\"text\",\n    metadata={\n        \"text_params\": {\n            \"vocab_size\": 10000,\n            \"max_length\": 512,\n            \"pad_token_id\": 0,\n            \"bos_token_id\": 2,\n            \"eos_token_id\": 3\n        }\n    }\n)\n\n# Create modality\nmodality = TextModality(config=config, rngs=rngs)\n\n# Tokenize text\ntokens = modality.tokenize(\"Hello world\")\nprint(f\"Tokens: {tokens.shape}\")  # (512,) - padded to max_length\n</code></pre> <p>Key features:</p> <ul> <li>Vocabulary management</li> <li>Special token handling (PAD, BOS, EOS, UNK)</li> <li>Sequence length management</li> <li>Case-sensitive/insensitive options</li> </ul>"},{"location":"user-guide/data/overview/#audio-modality","title":"Audio Modality","text":"<pre><code>from artifex.generative_models.modalities import AudioModality, AudioModalityConfig\n\n# Configure audio modality\nconfig = AudioModalityConfig(\n    sample_rate=16000,\n    duration=1.0,\n    n_mels=80,\n    hop_length=512,\n    normalize=True\n)\n\n# Create modality\nmodality = AudioModality(config=config, rngs=rngs)\n\n# Process audio\naudio_data = jnp.array([...])  # Raw waveform\nprocessed = modality.process(audio_data)\n</code></pre> <p>Key features:</p> <ul> <li>Waveform processing</li> <li>Spectrogram computation</li> <li>Sample rate conversion</li> <li>Duration management</li> </ul>"},{"location":"user-guide/data/overview/#multi-modal","title":"Multi-modal","text":"<pre><code>from artifex.generative_models.modalities.multi_modal import (\n    create_synthetic_multi_modal_dataset\n)\n\n# Create aligned multi-modal dataset\ndataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\", \"audio\"],\n    num_samples=1000,\n    alignment_strength=0.8,  # How strongly aligned\n    rngs=rngs\n)\n\n# Access multi-modal samples\nsample = dataset[0]\nprint(sample.keys())  # dict_keys(['image', 'text', 'audio', 'alignment_score', 'latent'])\n</code></pre> <p>Key features:</p> <ul> <li>Cross-modal alignment</li> <li>Paired datasets</li> <li>Shared latent representations</li> <li>Alignment strength control</li> </ul>"},{"location":"user-guide/data/overview/#dataset-interface","title":"Dataset Interface","text":"<p>All datasets in Artifex follow the <code>BaseDataset</code> protocol, providing a consistent interface regardless of modality.</p>"},{"location":"user-guide/data/overview/#base-dataset-protocol","title":"Base Dataset Protocol","text":"<pre><code>from artifex.generative_models.modalities.base import BaseDataset\nfrom flax import nnx\nimport jax.numpy as jnp\n\nclass CustomDataset(BaseDataset):\n    \"\"\"Custom dataset implementation.\"\"\"\n\n    def __init__(\n        self,\n        config: BaseModalityConfig,\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(config, split, rngs=rngs)\n        # Initialize your dataset\n        self.data = self._load_data()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return dataset size.\"\"\"\n        return len(self.data)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        \"\"\"Iterate over dataset samples.\"\"\"\n        for sample in self.data:\n            yield sample\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        \"\"\"Get a batch of samples.\"\"\"\n        # Sample random indices\n        key = self.rngs.sample() if \"sample\" in self.rngs else jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self))\n\n        # Gather samples\n        samples = [self.data[int(idx)] for idx in indices]\n\n        # Stack into batch\n        batch = {}\n        for key in samples[0].keys():\n            batch[key] = jnp.stack([s[key] for s in samples])\n\n        return batch\n\n    def _load_data(self):\n        \"\"\"Load dataset - implement your logic here.\"\"\"\n        pass\n</code></pre>"},{"location":"user-guide/data/overview/#built-in-dataset-types","title":"Built-in Dataset Types","text":""},{"location":"user-guide/data/overview/#image-datasets","title":"Image Datasets","text":"<p>SyntheticImageDataset - Generate synthetic image patterns:</p> <pre><code>from artifex.generative_models.modalities.image.datasets import (\n    SyntheticImageDataset\n)\n\n# Create synthetic dataset\ndataset = SyntheticImageDataset(\n    config=image_config,\n    dataset_size=1000,\n    pattern_type=\"gradient\",  # or \"random\", \"checkerboard\", \"circles\"\n    split=\"train\",\n    rngs=rngs\n)\n\n# Get batch\nbatch = dataset.get_batch(batch_size=32)\nprint(batch[\"images\"].shape)  # (32, 64, 64, 3)\n</code></pre> <p>Supported patterns:</p> <ul> <li><code>random</code>: Random noise patterns</li> <li><code>gradient</code>: Linear gradients with varying directions</li> <li><code>checkerboard</code>: Checkerboard patterns with random sizes</li> <li><code>circles</code>: Circular patterns with random positions/radii</li> </ul> <p>MNISTLikeDataset - Generate digit-like patterns:</p> <pre><code>from artifex.generative_models.modalities.image.datasets import (\n    MNISTLikeDataset\n)\n\n# Create MNIST-like dataset\ndataset = MNISTLikeDataset(\n    config=grayscale_config,  # Should be 28x28 grayscale\n    dataset_size=60000,\n    num_classes=10,\n    split=\"train\",\n    rngs=rngs\n)\n\n# Get labeled batch\nbatch = dataset.get_batch(batch_size=128)\nprint(batch[\"images\"].shape)  # (128, 28, 28, 1)\nprint(batch[\"labels\"].shape)  # (128,)\n</code></pre>"},{"location":"user-guide/data/overview/#text-datasets","title":"Text Datasets","text":"<p>SyntheticTextDataset - Generate synthetic text:</p> <pre><code>from artifex.generative_models.modalities.text.datasets import (\n    SyntheticTextDataset\n)\n\n# Create synthetic text dataset\ndataset = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=1000,\n    pattern_type=\"random_sentences\",  # or \"repeated_phrases\", \"sequences\", \"palindromes\"\n    split=\"train\",\n    rngs=rngs\n)\n\n# Get batch\nbatch = dataset.get_batch(batch_size=32)\nprint(batch[\"text_tokens\"].shape)  # (32, 512)\nprint(batch[\"texts\"])  # List of raw text strings\n</code></pre> <p>SimpleTextDataset - Load from text strings:</p> <pre><code>from artifex.generative_models.modalities.text.datasets import (\n    SimpleTextDataset\n)\n\n# Provide list of texts\ntexts = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Deep learning uses neural networks\"\n]\n\n# Create dataset\ndataset = SimpleTextDataset(\n    config=text_config,\n    texts=texts,\n    split=\"train\",\n    rngs=rngs\n)\n\n# Iterate over samples\nfor sample in dataset:\n    print(sample[\"text\"])\n    print(sample[\"text_tokens\"].shape)  # (512,)\n</code></pre>"},{"location":"user-guide/data/overview/#audio-datasets","title":"Audio Datasets","text":"<p>SyntheticAudioDataset - Generate synthetic audio:</p> <pre><code>from artifex.generative_models.modalities.audio.datasets import (\n    SyntheticAudioDataset\n)\n\n# Create synthetic audio dataset\ndataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=1000,\n    audio_types=[\"sine\", \"noise\", \"chirp\"],\n    name=\"SyntheticAudio\"\n)\n\n# Get sample\nsample = dataset[0]\nprint(sample[\"audio\"].shape)  # (16000,) - 1 second at 16kHz\nprint(sample[\"audio_type\"])  # \"sine\" or \"noise\" or \"chirp\"\n</code></pre> <p>Supported audio types:</p> <ul> <li><code>sine</code>: Sine waves with random frequencies (200-800 Hz)</li> <li><code>noise</code>: White noise</li> <li><code>chirp</code>: Linear frequency sweeps</li> </ul>"},{"location":"user-guide/data/overview/#data-pipeline-flow","title":"Data Pipeline Flow","text":"<p>The complete data flow from raw data to model training:</p> <pre><code>sequenceDiagram\n    participant DS as Dataset\n    participant PP as Preprocessor\n    participant DL as Data Loader\n    participant M as Model\n\n    Note over DS: Data Source\n    DS-&gt;&gt;DS: Load raw data\n    DS-&gt;&gt;DS: Apply transforms\n\n    Note over PP: Preprocessing\n    DS-&gt;&gt;PP: get_batch(batch_size)\n    PP-&gt;&gt;PP: Normalize\n    PP-&gt;&gt;PP: Augment (if enabled)\n    PP-&gt;&gt;PP: Validate shapes\n\n    Note over DL: Data Loading\n    PP-&gt;&gt;DL: Return batch dict\n    DL-&gt;&gt;DL: Convert to JAX arrays\n    DL-&gt;&gt;DL: Move to device\n\n    Note over M: Model Training\n    DL-&gt;&gt;M: Feed batch\n    M-&gt;&gt;M: Forward pass\n    M-&gt;&gt;M: Compute loss\n    M-&gt;&gt;M: Backward pass</code></pre>"},{"location":"user-guide/data/overview/#creating-a-data-loader","title":"Creating a Data Loader","text":"<p>Artifex provides utility functions for creating data loaders compatible with JAX training loops:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\ndef create_data_loader(\n    dataset: BaseDataset,\n    batch_size: int,\n    shuffle: bool = True,\n    drop_last: bool = False\n):\n    \"\"\"Create a simple data loader for JAX.\n\n    Args:\n        dataset: Dataset to load from\n        batch_size: Batch size\n        shuffle: Whether to shuffle data\n        drop_last: Whether to drop last incomplete batch\n\n    Yields:\n        Batches of data as dictionaries\n    \"\"\"\n    num_samples = len(dataset)\n\n    if shuffle:\n        # Generate random indices\n        key = jax.random.key(0)\n        indices = jax.random.permutation(key, num_samples)\n    else:\n        indices = jnp.arange(num_samples)\n\n    # Calculate number of batches\n    num_batches = num_samples // batch_size\n    if not drop_last and num_samples % batch_size != 0:\n        num_batches += 1\n\n    for i in range(num_batches):\n        start_idx = i * batch_size\n        end_idx = min(start_idx + batch_size, num_samples)\n        batch_indices = indices[start_idx:end_idx]\n\n        # Gather batch\n        batch = dataset.get_batch(len(batch_indices))\n        yield batch\n\n# Usage\ntrain_loader = create_data_loader(\n    dataset=train_dataset,\n    batch_size=128,\n    shuffle=True,\n    drop_last=True\n)\n\nfor batch in train_loader:\n    # Training step\n    loss = train_step(model, batch)\n</code></pre>"},{"location":"user-guide/data/overview/#preprocessing","title":"Preprocessing","text":"<p>Each modality provides preprocessing functionality through the <code>BaseProcessor</code> interface:</p>"},{"location":"user-guide/data/overview/#image-preprocessing","title":"Image Preprocessing","text":"<pre><code>from artifex.generative_models.modalities.image.base import ImageModality\n\n# Create modality with preprocessing\nconfig = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=64,\n    width=64,\n    normalize=True,  # Normalize to [0, 1]\n    augmentation=True  # Enable augmentation\n)\n\nmodality = ImageModality(config=config, rngs=rngs)\n\n# Process raw image data\nraw_images = jnp.array([...])  # Raw pixel values\nprocessed = modality.process(raw_images)\n\n# Processed images are:\n# - Resized to (64, 64)\n# - Normalized to [0, 1] (or [-1, 1] if normalize=False)\n# - Augmented (if enabled)\n</code></pre>"},{"location":"user-guide/data/overview/#text-preprocessing","title":"Text Preprocessing","text":"<pre><code>from artifex.generative_models.modalities.text.base import TextModality\n\n# Text preprocessing handles:\n# - Tokenization\n# - Vocabulary mapping\n# - Special token insertion (BOS/EOS)\n# - Padding/truncation to max_length\n\ntext = \"Hello world, this is a test sentence\"\ntokens = text_modality.tokenize(text)\nprint(tokens.shape)  # (512,) - padded to max_length\n\n# Detokenization\nrecovered_text = text_modality.detokenize(tokens)\n</code></pre>"},{"location":"user-guide/data/overview/#audio-preprocessing","title":"Audio Preprocessing","text":"<pre><code>from artifex.generative_models.modalities.audio.base import AudioModality\n\n# Audio preprocessing handles:\n# - Resampling to target sample rate\n# - Duration normalization\n# - Amplitude normalization\n# - Spectrogram computation\n\nraw_audio = load_audio_file(\"audio.wav\")\nprocessed = audio_modality.process(raw_audio)\n\n# Compute mel-spectrogram\nmel_spec = audio_modality.compute_mel_spectrogram(processed)\nprint(mel_spec.shape)  # (n_mels, n_frames)\n</code></pre>"},{"location":"user-guide/data/overview/#configuration","title":"Configuration","text":"<p>All modalities use configuration objects to manage their settings:</p>"},{"location":"user-guide/data/overview/#image-configuration","title":"Image Configuration","text":"<pre><code>from artifex.generative_models.modalities.image.base import (\n    ImageModalityConfig,\n    ImageRepresentation\n)\n\nconfig = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=256,\n    width=256,\n    channels=3,  # Auto-determined from representation if None\n    normalize=True,  # Normalize to [0, 1]\n    augmentation=False,  # Disable augmentation\n    resize_method=\"bilinear\"  # or \"nearest\"\n)\n</code></pre>"},{"location":"user-guide/data/overview/#text-configuration","title":"Text Configuration","text":"<pre><code>from artifex.generative_models.core.configuration import ModalityConfiguration\n\nconfig = ModalityConfiguration(\n    name=\"text\",\n    modality_type=\"text\",\n    metadata={\n        \"text_params\": {\n            \"vocab_size\": 50000,\n            \"max_length\": 1024,\n            \"pad_token_id\": 0,\n            \"unk_token_id\": 1,\n            \"bos_token_id\": 2,\n            \"eos_token_id\": 3,\n            \"case_sensitive\": False\n        }\n    }\n)\n</code></pre>"},{"location":"user-guide/data/overview/#audio-configuration","title":"Audio Configuration","text":"<pre><code>from artifex.generative_models.modalities.audio.base import AudioModalityConfig\n\nconfig = AudioModalityConfig(\n    sample_rate=16000,\n    duration=2.0,\n    n_mels=80,\n    n_fft=1024,\n    hop_length=256,\n    normalize=True,\n    spectrogram_type=\"mel\"  # or \"stft\"\n)\n</code></pre>"},{"location":"user-guide/data/overview/#complete-example","title":"Complete Example","text":"<p>Here's a complete example showing how to set up a data pipeline for training:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.modalities import ImageModality, ImageModalityConfig, ImageRepresentation\nfrom artifex.generative_models.modalities.image.datasets import SyntheticImageDataset\n\n# Initialize RNG\nrngs = nnx.Rngs(0)\n\n# Configure image modality\nimage_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=64,\n    width=64,\n    channels=3,\n    normalize=True,\n    augmentation=False\n)\n\n# Create modality\nmodality = ImageModality(config=image_config, rngs=rngs)\n\n# Create training dataset\ntrain_dataset = SyntheticImageDataset(\n    config=image_config,\n    dataset_size=10000,\n    pattern_type=\"gradient\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Create validation dataset\nval_dataset = SyntheticImageDataset(\n    config=image_config,\n    dataset_size=1000,\n    pattern_type=\"gradient\",\n    split=\"val\",\n    rngs=rngs\n)\n\n# Create data loader\ndef create_data_loader(dataset, batch_size, shuffle=True):\n    \"\"\"Simple data loader for JAX.\"\"\"\n    num_samples = len(dataset)\n\n    for epoch in range(num_epochs):\n        if shuffle:\n            key = jax.random.key(epoch)\n            indices = jax.random.permutation(key, num_samples)\n        else:\n            indices = jnp.arange(num_samples)\n\n        num_batches = num_samples // batch_size\n        for i in range(num_batches):\n            batch_indices = indices[i * batch_size:(i + 1) * batch_size]\n            batch = dataset.get_batch(batch_size)\n            yield batch\n\n# Training loop\nbatch_size = 128\nnum_epochs = 10\n\ntrain_loader = create_data_loader(train_dataset, batch_size, shuffle=True)\n\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        # Get images from batch\n        images = batch[\"images\"]\n\n        # Preprocess through modality\n        processed = modality.process(images)\n\n        # Training step\n        # ... (use processed images for training)\n\n    # Validation\n    val_loader = create_data_loader(val_dataset, batch_size, shuffle=False)\n    for val_batch in val_loader:\n        images = val_batch[\"images\"]\n        # Validation step\n        # ...\n</code></pre>"},{"location":"user-guide/data/overview/#modality-registry","title":"Modality Registry","text":"<p>Artifex provides a global registry for modalities:</p> <pre><code>from artifex.generative_models.modalities import (\n    register_modality,\n    get_modality,\n    list_modalities\n)\n\n# Register custom modality\nregister_modality(\"custom_image\", CustomImageModality)\n\n# Get modality by name\nmodality_class = get_modality(\"image\")\n\n# List all registered modalities\navailable = list_modalities()\nprint(available)  # ['image', 'text', 'audio', 'protein', 'molecular', ...]\n</code></pre>"},{"location":"user-guide/data/overview/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/data/overview/#dataset-design","title":"Dataset Design","text":"<p>DO</p> <ul> <li>Use protocol-based interfaces for extensibility</li> <li>Implement <code>__len__()</code>, <code>__iter__()</code>, and <code>get_batch()</code></li> <li>Return dictionaries with descriptive keys</li> <li>Use JAX arrays for all numeric data</li> <li>Provide proper RNG handling</li> <li>Validate data shapes and types</li> <li>Cache preprocessed data when possible</li> </ul> <p>DON'T</p> <ul> <li>Use PyTorch or TensorFlow tensors</li> <li>Return raw Python lists of arrays</li> <li>Perform heavy computation in <code>__iter__()</code></li> <li>Ignore RNG seeding for reproducibility</li> <li>Mix different data types in same batch</li> <li>Load entire dataset into memory (unless small)</li> </ul>"},{"location":"user-guide/data/overview/#preprocessing_1","title":"Preprocessing","text":"<p>DO</p> <ul> <li>Normalize data to expected range</li> <li>Apply augmentation during training only</li> <li>Use JIT-compiled preprocessing functions</li> <li>Cache computed features (spectrograms, embeddings)</li> <li>Validate preprocessed shapes</li> <li>Document expected input/output formats</li> </ul> <p>DON'T</p> <ul> <li>Apply random augmentation during validation</li> <li>Use non-deterministic operations without RNG</li> <li>Perform I/O operations in preprocessing</li> <li>Ignore batch dimension handling</li> <li>Mix preprocessing across modalities</li> </ul>"},{"location":"user-guide/data/overview/#configuration_1","title":"Configuration","text":"<p>DO</p> <ul> <li>Use dataclasses for configuration</li> <li>Provide sensible defaults</li> <li>Validate configuration values</li> <li>Document all configuration options</li> <li>Use enums for categorical choices</li> <li>Make configuration serializable</li> </ul> <p>DON'T</p> <ul> <li>Use raw dictionaries for configuration</li> <li>Allow invalid configuration combinations</li> <li>Hard-code magic numbers</li> <li>Mix configuration across components</li> <li>Forget to validate user inputs</li> </ul>"},{"location":"user-guide/data/overview/#summary","title":"Summary","text":"<p>Artifex's data system provides:</p> <ul> <li>Modality-centric architecture - Unified interface for different data types</li> <li>Protocol-based design - Easy to extend with custom datasets and modalities</li> <li>JAX-native - Full JAX compatibility with JIT and GPU support</li> <li>Preprocessing pipelines - Configurable normalization and augmentation</li> <li>Multi-modal support - Native support for aligned multi-modal data</li> <li>Type safety - Full type hints and validation</li> </ul>"},{"location":"user-guide/data/overview/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Data Loading Guide</p> <p>Learn how to load custom datasets, implement preprocessing pipelines, and optimize data loading</p> </li> <li> <p> Image Modality Guide</p> <p>Deep dive into image datasets, preprocessing, augmentation, and best practices</p> </li> <li> <p> Text Modality Guide</p> <p>Learn about text tokenization, vocabulary management, and sequence handling</p> </li> <li> <p> Data API Reference</p> <p>Complete API documentation for datasets, loaders, and preprocessing functions</p> </li> </ul>"},{"location":"user-guide/inference/optimization/","title":"Inference Optimization","text":"<p>Advanced techniques for optimizing generative model inference: JIT compilation, quantization, memory management, and hardware-specific optimizations.</p>"},{"location":"user-guide/inference/optimization/#overview","title":"Overview","text":"<p>Inference optimization is critical for deploying generative models in production. This guide covers techniques to maximize throughput, minimize latency, and reduce memory usage while maintaining generation quality.</p> <p>Performance Gains</p> <ul> <li>JIT Compilation: 10-100x speedup on first-time compilation</li> <li>Mixed Precision: 2-4x throughput improvement</li> <li>Quantization: 2-4x memory reduction</li> <li>Batching: Near-linear scaling with batch size</li> </ul> <ul> <li> <p> JIT Compilation</p> <p>Compile JAX functions for dramatic speedups</p> <p> JIT Guide</p> </li> <li> <p> Memory Optimization</p> <p>Reduce memory footprint for larger models</p> <p> Memory Guide</p> </li> <li> <p> Quantization</p> <p>Use lower precision for faster inference</p> <p> Quantization Guide</p> </li> <li> <p> Hardware Tuning</p> <p>Optimize for GPUs, TPUs, and multi-device setups</p> <p> Hardware Guide</p> </li> </ul>"},{"location":"user-guide/inference/optimization/#prerequisites","title":"Prerequisites","text":"<pre><code>from flax import nnx\nimport jax\nimport jax.numpy as jnp\nfrom artifex.generative_models.core import DeviceManager\n</code></pre>"},{"location":"user-guide/inference/optimization/#jit-compilation","title":"JIT Compilation","text":""},{"location":"user-guide/inference/optimization/#basic-jit-usage","title":"Basic JIT Usage","text":"<p>JAX's Just-In-Time compilation dramatically improves performance.</p> <pre><code>class JITOptimizedInference:\n    \"\"\"Inference with JIT compilation.\"\"\"\n\n    def __init__(self, model):\n        self.model = model\n\n        # Compile inference function\n        self.generate_jit = jax.jit(self._generate_impl)\n\n        # Warmup compilation\n        self._warmup()\n\n    def _generate_impl(self, z: jax.Array) -&gt; jax.Array:\n        \"\"\"Implementation to be JIT-compiled.\"\"\"\n        return self.model.decode(z)\n\n    def _warmup(self):\n        \"\"\"Warmup JIT compilation with dummy input.\"\"\"\n        dummy_z = jnp.zeros((1, self.model.latent_dim))\n        _ = self.generate_jit(dummy_z)\n        print(\"JIT compilation complete\")\n\n    def generate(self, z: jax.Array) -&gt; jax.Array:\n        \"\"\"Generate samples (uses compiled function).\"\"\"\n        return self.generate_jit(z)\n</code></pre>"},{"location":"user-guide/inference/optimization/#static-vs-dynamic-shapes","title":"Static vs Dynamic Shapes","text":"<p>JIT works best with static shapes. Handle dynamic shapes carefully.</p> <pre><code>def create_shape_specific_functions(model, batch_sizes: list[int]):\n    \"\"\"Compile separate functions for each batch size.\n\n    Args:\n        model: Generative model\n        batch_sizes: List of expected batch sizes\n\n    Returns:\n        Dictionary of compiled functions\n    \"\"\"\n    compiled_fns = {}\n\n    for batch_size in batch_sizes:\n        @jax.jit\n        def generate_fn(z):\n            return model.decode(z)\n\n        # Warmup with specific shape\n        dummy_z = jnp.zeros((batch_size, model.latent_dim))\n        _ = generate_fn(dummy_z)\n\n        compiled_fns[batch_size] = generate_fn\n\n    return compiled_fns\n\n\n# Usage\ncompiled_fns = create_shape_specific_functions(model, [1, 4, 16, 32])\n\n# Use appropriate function\nbatch_size = 16\nz = jax.random.normal(key, (batch_size, latent_dim))\nsamples = compiled_fns[batch_size](z)\n</code></pre>"},{"location":"user-guide/inference/optimization/#compilation-cache-management","title":"Compilation Cache Management","text":"<pre><code>import os\nfrom jax.experimental.compilation_cache import compilation_cache\n\nclass CachedInference:\n    \"\"\"Inference with persistent compilation cache.\"\"\"\n\n    def __init__(self, model, cache_dir: str = \"/tmp/jax_cache\"):\n        self.model = model\n\n        # Enable compilation caching\n        os.makedirs(cache_dir, exist_ok=True)\n        compilation_cache.set_cache_dir(cache_dir)\n\n        # Compile function (cache persists across runs)\n        self.generate = jax.jit(model.decode)\n\n        # First run compiles or loads from cache\n        self._warmup()\n\n    def _warmup(self):\n        dummy_z = jnp.zeros((1, self.model.latent_dim))\n        _ = self.generate(dummy_z)\n</code></pre>"},{"location":"user-guide/inference/optimization/#quantization","title":"Quantization","text":""},{"location":"user-guide/inference/optimization/#mixed-precision-inference","title":"Mixed Precision Inference","text":"<p>Use FP16 for faster inference on modern GPUs.</p> <pre><code>class MixedPrecisionModel(nnx.Module):\n    \"\"\"Model with mixed precision inference.\"\"\"\n\n    def __init__(\n        self,\n        base_model,\n        *,\n        compute_dtype: jnp.dtype = jnp.float16,\n        param_dtype: jnp.dtype = jnp.float32,\n    ):\n        super().__init__()\n        self.base_model = base_model\n        self.compute_dtype = compute_dtype\n        self.param_dtype = param_dtype\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with mixed precision.\"\"\"\n        # Convert input to compute dtype\n        x = x.astype(self.compute_dtype)\n\n        # Run model (parameters stay in param_dtype)\n        output = self.base_model(x)\n\n        # Convert back to float32 for output\n        return output.astype(jnp.float32)\n\n\n# Create mixed precision model\nmixed_model = MixedPrecisionModel(\n    model,\n    compute_dtype=jnp.float16,\n    param_dtype=jnp.float32,\n)\n</code></pre>"},{"location":"user-guide/inference/optimization/#dynamic-quantization","title":"Dynamic Quantization","text":"<p>Quantize activations dynamically at inference time.</p> <pre><code>def quantize_inference(\n    model,\n    x: jax.Array,\n    num_bits: int = 8,\n) -&gt; jax.Array:\n    \"\"\"Run inference with dynamic quantization.\n\n    Args:\n        model: Model to run\n        x: Input tensor\n        num_bits: Quantization bits (8 for INT8)\n\n    Returns:\n        Model output\n    \"\"\"\n    # Quantize input\n    x_min, x_max = x.min(), x.max()\n    scale = (x_max - x_min) / (2**num_bits - 1)\n    x_quant = jnp.round((x - x_min) / scale).astype(jnp.int8)\n\n    # Dequantize for computation\n    x_dequant = x_quant.astype(jnp.float32) * scale + x_min\n\n    # Run model\n    output = model(x_dequant)\n\n    return output\n</code></pre>"},{"location":"user-guide/inference/optimization/#int8-quantization","title":"INT8 Quantization","text":"<p>Full INT8 quantization for maximum efficiency.</p> <pre><code>class QuantizedLinear(nnx.Module):\n    \"\"\"INT8 quantized linear layer.\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__()\n\n        # Initialize weights in FP32\n        kernel = jax.random.normal(\n            rngs.params(),\n            (in_features, out_features)\n        ) * jnp.sqrt(2.0 / in_features)\n\n        # Quantize to INT8\n        self.scale = jnp.max(jnp.abs(kernel))\n        self.kernel_quant = jnp.round(\n            kernel / self.scale * 127\n        ).astype(jnp.int8)\n\n        # Bias stays in FP32\n        self.bias = nnx.Param(jnp.zeros(out_features))\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Quantized forward pass.\"\"\"\n        # Dequantize weights\n        kernel = self.kernel_quant.astype(jnp.float32) * self.scale / 127\n\n        # Compute\n        output = jnp.dot(x, kernel) + self.bias.value\n\n        return output\n</code></pre>"},{"location":"user-guide/inference/optimization/#memory-optimization","title":"Memory Optimization","text":""},{"location":"user-guide/inference/optimization/#gradient-checkpointing-for-large-models","title":"Gradient Checkpointing for Large Models","text":"<p>Save memory by recomputing activations during backward pass.</p> <pre><code>from jax.experimental import checkify\n\ndef memory_efficient_inference(model, x: jax.Array) -&gt; jax.Array:\n    \"\"\"Inference with gradient checkpointing (saves memory).\n\n    Args:\n        model: Large generative model\n        x: Input\n\n    Returns:\n        Model output with reduced memory usage\n    \"\"\"\n    # Use checkpoint to reduce memory\n    @jax.checkpoint\n    def checkpointed_forward(x_inner):\n        return model(x_inner)\n\n    return checkpointed_forward(x)\n</code></pre>"},{"location":"user-guide/inference/optimization/#batch-size-tuning","title":"Batch Size Tuning","text":"<p>Find optimal batch size for your hardware.</p> <pre><code>def find_optimal_batch_size(\n    model,\n    input_shape: tuple,\n    max_batch_size: int = 128,\n) -&gt; int:\n    \"\"\"Find largest batch size that fits in memory.\n\n    Args:\n        model: Model to test\n        input_shape: Shape of single input\n        max_batch_size: Maximum batch size to try\n\n    Returns:\n        Optimal batch size\n    \"\"\"\n    device_manager = DeviceManager()\n    device = device_manager.get_device()\n\n    batch_size = 1\n    while batch_size &lt;= max_batch_size:\n        try:\n            # Try inference with this batch size\n            x = jax.random.normal(\n                jax.random.key(0),\n                (batch_size, *input_shape)\n            )\n\n            # Move to device\n            x = jax.device_put(x, device)\n\n            # Run inference\n            _ = model(x)\n\n            # Success - try larger batch\n            batch_size *= 2\n\n        except RuntimeError as e:\n            if \"out of memory\" in str(e).lower():\n                # OOM - return previous batch size\n                return batch_size // 2\n            else:\n                raise\n\n    return max_batch_size\n</code></pre>"},{"location":"user-guide/inference/optimization/#activation-compression","title":"Activation Compression","text":"<p>Reduce memory by compressing intermediate activations.</p> <pre><code>class CompressedModel(nnx.Module):\n    \"\"\"Model with activation compression.\"\"\"\n\n    def __init__(self, base_model):\n        super().__init__()\n        self.base_model = base_model\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with activation compression.\"\"\"\n\n        # Process in chunks to reduce peak memory\n        chunk_size = 16\n        batch_size = x.shape[0]\n\n        outputs = []\n        for i in range(0, batch_size, chunk_size):\n            chunk = x[i:i + chunk_size]\n            chunk_output = self.base_model(chunk)\n            outputs.append(chunk_output)\n\n        return jnp.concatenate(outputs, axis=0)\n</code></pre>"},{"location":"user-guide/inference/optimization/#batching-strategies","title":"Batching Strategies","text":""},{"location":"user-guide/inference/optimization/#dynamic-batching","title":"Dynamic Batching","text":"<p>Accumulate requests into batches for efficiency.</p> <pre><code>import asyncio\nfrom collections import deque\n\nclass DynamicBatcher:\n    \"\"\"Dynamic batching for inference requests.\"\"\"\n\n    def __init__(\n        self,\n        model,\n        max_batch_size: int = 32,\n        timeout_ms: int = 100,\n    ):\n        self.model = model\n        self.max_batch_size = max_batch_size\n        self.timeout_ms = timeout_ms\n\n        # Request queue\n        self.queue = deque()\n        self.processing = False\n\n    async def infer_async(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Submit inference request.\n\n        Args:\n            x: Input tensor (single sample)\n\n        Returns:\n            Model output\n        \"\"\"\n        # Create future for result\n        future = asyncio.Future()\n        self.queue.append((x, future))\n\n        # Start processing if not already running\n        if not self.processing:\n            asyncio.create_task(self._process_batch())\n\n        # Wait for result\n        return await future\n\n    async def _process_batch(self):\n        \"\"\"Process batched requests.\"\"\"\n        self.processing = True\n\n        # Wait for timeout or max batch size\n        await asyncio.sleep(self.timeout_ms / 1000)\n\n        if not self.queue:\n            self.processing = False\n            return\n\n        # Collect batch\n        batch_inputs = []\n        futures = []\n\n        while self.queue and len(batch_inputs) &lt; self.max_batch_size:\n            x, future = self.queue.popleft()\n            batch_inputs.append(x)\n            futures.append(future)\n\n        # Run batched inference\n        batch_x = jnp.stack(batch_inputs)\n        batch_output = self.model(batch_x)\n\n        # Return results\n        for i, future in enumerate(futures):\n            future.set_result(batch_output[i])\n\n        self.processing = False\n\n        # Process remaining requests\n        if self.queue:\n            asyncio.create_task(self._process_batch())\n</code></pre>"},{"location":"user-guide/inference/optimization/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Split large models across multiple devices.</p> <pre><code>class PipelineParallelModel(nnx.Module):\n    \"\"\"Model with pipeline parallelism.\"\"\"\n\n    def __init__(\n        self,\n        encoder,\n        decoder,\n        devices: list,\n    ):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device_encoder = devices[0]\n        self.device_decoder = devices[1]\n\n    def __call__(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Forward pass with pipeline parallelism.\"\"\"\n\n        # Encoder on device 0\n        x = jax.device_put(x, self.device_encoder)\n        latent = self.encoder(x)\n\n        # Transfer to device 1\n        latent = jax.device_put(latent, self.device_decoder)\n        output = self.decoder(latent)\n\n        return output\n\n\n# Create pipeline parallel model\ndevice_manager = DeviceManager()\ndevices = device_manager.get_devices()\n\nif len(devices) &gt;= 2:\n    pipeline_model = PipelineParallelModel(\n        encoder=model.encoder,\n        decoder=model.decoder,\n        devices=devices[:2],\n    )\n</code></pre>"},{"location":"user-guide/inference/optimization/#hardware-specific-optimization","title":"Hardware-Specific Optimization","text":""},{"location":"user-guide/inference/optimization/#gpu-optimization","title":"GPU Optimization","text":"<p>Optimize for NVIDIA GPUs.</p> <pre><code>def optimize_for_gpu(model):\n    \"\"\"Apply GPU-specific optimizations.\n\n    Args:\n        model: Model to optimize\n\n    Returns:\n        Optimized model\n    \"\"\"\n    # Use XLA optimizations\n    os.environ['XLA_FLAGS'] = (\n        '--xla_gpu_enable_fast_min_max=true '\n        '--xla_gpu_enable_triton_gemm=true '\n        '--xla_gpu_triton_gemm_any=true'\n    )\n\n    # Enable tensor cores for mixed precision\n    jax.config.update('jax_enable_x64', False)\n\n    # JIT compile with aggressive optimization\n    @jax.jit\n    def optimized_forward(x):\n        return model(x)\n\n    return optimized_forward\n</code></pre>"},{"location":"user-guide/inference/optimization/#tpu-optimization","title":"TPU Optimization","text":"<p>Optimize for Google TPUs.</p> <pre><code>def optimize_for_tpu(model, num_devices: int = 8):\n    \"\"\"Apply TPU-specific optimizations.\n\n    Args:\n        model: Model to optimize\n        num_devices: Number of TPU cores\n\n    Returns:\n        Optimized model with data parallelism\n    \"\"\"\n    # Replicate model across TPU cores\n    @jax.pmap\n    def pmap_forward(x):\n        return model(x)\n\n    # Shard inputs across devices\n    def inference(x):\n        # Reshape to (num_devices, batch_per_device, ...)\n        batch_size = x.shape[0]\n        batch_per_device = batch_size // num_devices\n\n        x_sharded = x.reshape(\n            (num_devices, batch_per_device) + x.shape[1:]\n        )\n\n        # Run on all TPU cores\n        output_sharded = pmap_forward(x_sharded)\n\n        # Concatenate results\n        return output_sharded.reshape((batch_size,) + output_sharded.shape[2:])\n\n    return inference\n</code></pre>"},{"location":"user-guide/inference/optimization/#multi-device-inference","title":"Multi-Device Inference","text":"<p>Distribute inference across multiple devices.</p> <pre><code>class MultiDeviceInference:\n    \"\"\"Inference distributed across multiple devices.\"\"\"\n\n    def __init__(self, model):\n        self.model = model\n        self.device_manager = DeviceManager()\n        self.devices = self.device_manager.get_devices()\n        self.num_devices = len(self.devices)\n\n        # Replicate model on all devices\n        self.replicated_model = jax.pmap(\n            lambda x: model(x)\n        )\n\n    def infer_distributed(self, x: jax.Array) -&gt; jax.Array:\n        \"\"\"Run inference across all devices.\n\n        Args:\n            x: Input batch (must be divisible by num_devices)\n\n        Returns:\n            Model output\n        \"\"\"\n        batch_size = x.shape[0]\n        assert batch_size % self.num_devices == 0\n\n        # Reshape for pmap\n        batch_per_device = batch_size // self.num_devices\n        x_sharded = x.reshape(\n            (self.num_devices, batch_per_device) + x.shape[1:]\n        )\n\n        # Run on all devices\n        output_sharded = self.replicated_model(x_sharded)\n\n        # Merge results\n        return output_sharded.reshape((batch_size,) + output_sharded.shape[2:])\n</code></pre>"},{"location":"user-guide/inference/optimization/#performance-benchmarking","title":"Performance Benchmarking","text":""},{"location":"user-guide/inference/optimization/#throughput-measurement","title":"Throughput Measurement","text":"<pre><code>import time\n\ndef benchmark_throughput(\n    model,\n    input_shape: tuple,\n    batch_sizes: list[int],\n    num_iterations: int = 100,\n):\n    \"\"\"Benchmark model throughput.\n\n    Args:\n        model: Model to benchmark\n        input_shape: Shape of single input\n        batch_sizes: Batch sizes to test\n        num_iterations: Number of iterations per test\n\n    Returns:\n        Dictionary with throughput results\n    \"\"\"\n    results = {}\n\n    for batch_size in batch_sizes:\n        # Create dummy input\n        x = jax.random.normal(\n            jax.random.key(0),\n            (batch_size, *input_shape)\n        )\n\n        # Warmup\n        for _ in range(10):\n            _ = model(x)\n\n        # Benchmark\n        jax.block_until_ready(model(x))  # Ensure GPU work completes\n\n        start = time.time()\n        for _ in range(num_iterations):\n            output = model(x)\n            jax.block_until_ready(output)\n        elapsed = time.time() - start\n\n        # Compute metrics\n        samples_per_sec = (batch_size * num_iterations) / elapsed\n        latency_ms = (elapsed / num_iterations) * 1000\n\n        results[batch_size] = {\n            'throughput': samples_per_sec,\n            'latency_ms': latency_ms,\n            'batch_size': batch_size,\n        }\n\n    return results\n</code></pre>"},{"location":"user-guide/inference/optimization/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/inference/optimization/#do","title":"DO","text":"<p>Recommended Optimizations</p> <p>\u2705 Always JIT compile production inference functions</p> <p>\u2705 Use mixed precision (FP16) on modern GPUs for 2-4x speedup</p> <p>\u2705 Batch requests to maximize GPU utilization</p> <p>\u2705 Cache compiled functions across server restarts</p> <p>\u2705 Profile memory usage to find optimal batch size</p> <p>\u2705 Use pmap for multi-device inference when available</p>"},{"location":"user-guide/inference/optimization/#dont","title":"DON'T","text":"<p>Avoid These Mistakes</p> <p>\u274c Don't recompile on every request (cache JIT functions)</p> <p>\u274c Don't use FP64 unless absolutely necessary (2x slower)</p> <p>\u274c Don't ignore batch size (single samples waste resources)</p> <p>\u274c Don't mix device types (CPU/GPU transfers are slow)</p> <p>\u274c Don't skip warmup (first call is always slow)</p> <p>\u274c Don't quantize without testing quality impact</p>"},{"location":"user-guide/inference/optimization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/inference/optimization/#common-performance-issues","title":"Common Performance Issues","text":"Issue Symptom Solution Slow first inference 10-60s delay on first call Warmup JIT compilation during startup OOM errors Out of memory during inference Reduce batch size or use gradient checkpointing Low GPU utilization GPU usage &lt; 50% Increase batch size or use pipelining Recompilation on every call Consistent slow performance Use static shapes or cache per-shape functions Slow multi-device Linear speedup not achieved Check data transfer overhead, use pmap High latency Individual requests take too long Use dynamic batching or reduce model size"},{"location":"user-guide/inference/optimization/#summary","title":"Summary","text":"<p>Effective optimization can provide 10-100x speedups:</p> <ul> <li>JIT Compilation: Essential for production (10-100x faster)</li> <li>Mixed Precision: 2-4x throughput on modern GPUs</li> <li>Quantization: Reduce memory 2-4x with minimal quality loss</li> <li>Batching: Near-linear scaling with batch size</li> <li>Multi-Device: Distribute large workloads across GPUs/TPUs</li> </ul> <p>Choose optimizations based on your deployment constraints and quality requirements.</p>"},{"location":"user-guide/inference/optimization/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Sampling Methods</p> <p>Learn advanced sampling techniques for quality generation</p> <p> Sampling Guide</p> </li> <li> <p> Deployment</p> <p>Deploy optimized models to production</p> <p> Deployment Guide</p> </li> <li> <p> Benchmarking</p> <p>Measure and compare model performance</p> <p> Evaluation Framework</p> </li> <li> <p>:material-gpu:{ .lg .middle } Distributed Training</p> <p>Scale training across multiple devices</p> <p> Distributed Guide</p> </li> </ul>"},{"location":"user-guide/inference/overview/","title":"Inference Overview","text":"<p>This guide provides a complete overview of performing inference with Artifex models, covering model loading, batch processing, streaming inference, and performance optimization.</p>"},{"location":"user-guide/inference/overview/#overview","title":"Overview","text":"<ul> <li> <p> Model Loading</p> <p>Load trained models for inference efficiently</p> <p> Model Loading</p> </li> <li> <p> Batch Inference</p> <p>Process multiple inputs efficiently with batching</p> <p> Batch Inference</p> </li> <li> <p> Streaming Inference</p> <p>Real-time generation with streaming</p> <p> Streaming</p> </li> <li> <p> Performance</p> <p>Optimize inference speed and memory</p> <p> Performance</p> </li> </ul>"},{"location":"user-guide/inference/overview/#prerequisites","title":"Prerequisites","text":"<pre><code>uv pip install \"artifex[cuda]\"  # With GPU support\n</code></pre> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.core import DeviceManager\nfrom artifex.generative_models.core.checkpointing import load_checkpoint, setup_checkpoint_manager\n</code></pre>"},{"location":"user-guide/inference/overview/#model-loading","title":"Model Loading","text":""},{"location":"user-guide/inference/overview/#loading-from-checkpoint","title":"Loading from Checkpoint","text":"<pre><code>from artifex.generative_models.core.checkpointing import (\n    load_checkpoint,\n    setup_checkpoint_manager,\n)\nfrom artifex.generative_models.models.vae import VAE\n\ndef load_model_for_inference(\n    checkpoint_dir: str,\n    config: dict,\n) -&gt; VAE:\n    \"\"\"\n    Load a trained model from checkpoint for inference.\n\n    Args:\n        checkpoint_dir: Path to checkpoint directory\n        config: Model configuration dictionary\n\n    Returns:\n        Loaded model ready for inference\n    \"\"\"\n    # Initialize device manager\n    device_manager = DeviceManager()\n\n    # Create model template with same architecture\n    rngs = nnx.Rngs(0)  # Seed doesn't matter for loading\n    model_template = VAE(\n        input_shape=config[\"input_shape\"],\n        latent_dim=config[\"latent_dim\"],\n        encoder_features=config.get(\"encoder_features\", [32, 64, 128]),\n        decoder_features=config.get(\"decoder_features\", [128, 64, 32]),\n        rngs=rngs,\n    )\n\n    # Setup checkpoint manager\n    checkpoint_manager, _ = setup_checkpoint_manager(checkpoint_dir)\n\n    # Load weights\n    model, step = load_checkpoint(checkpoint_manager, model_template)\n\n    print(f\"Loaded model from step {step}\")\n\n    return model\n\n\n# Example usage\nconfig = {\n    \"input_shape\": (28, 28, 1),\n    \"latent_dim\": 64,\n    \"encoder_features\": [32, 64, 128],\n    \"decoder_features\": [128, 64, 32],\n}\n\nmodel = load_model_for_inference(\"./checkpoints/vae\", config)\n</code></pre>"},{"location":"user-guide/inference/overview/#loading-from-export-directory","title":"Loading from Export Directory","text":"<pre><code>import json\nfrom pathlib import Path\n\ndef load_exported_model(export_dir: str):\n    \"\"\"\n    Load model from export directory with metadata.\n\n    Args:\n        export_dir: Path to export directory\n\n    Returns:\n        Tuple of (model, config, metadata)\n    \"\"\"\n    export_path = Path(export_dir)\n\n    # Load metadata\n    with open(export_path / \"metadata.json\") as f:\n        metadata = json.load(f)\n\n    # Load config\n    with open(export_path / \"config.json\") as f:\n        config = json.load(f)\n\n    # Load model\n    model = load_model_for_inference(str(export_path / \"checkpoints\"), config)\n\n    print(f\"Loaded model: {metadata['model_type']}\")\n    print(f\"Trained for {metadata['training_steps']} steps\")\n    print(f\"Final loss: {metadata['final_loss']:.4f}\")\n\n    return model, config, metadata\n\n\n# Example\nmodel, config, metadata = load_exported_model(\"./exports/vae_mnist_final\")\n</code></pre>"},{"location":"user-guide/inference/overview/#jit-compilation-for-inference","title":"JIT Compilation for Inference","text":"<pre><code>@jax.jit\ndef generate_samples_jit(model: VAE, z: jax.Array) -&gt; jax.Array:\n    \"\"\"JIT-compiled sample generation.\"\"\"\n    return model.decode(z)\n\n\n@jax.jit\ndef encode_images_jit(model: VAE, images: jax.Array) -&gt; jax.Array:\n    \"\"\"JIT-compiled encoding.\"\"\"\n    latent_params = model.encode(images)\n    # Return mean for deterministic encoding\n    mean = latent_params[\"mean\"]\n    return mean\n\n\ndef inference_with_jit(model: VAE, num_samples: int = 16):\n    \"\"\"Perform inference with JIT compilation.\"\"\"\n\n    # First call compiles (may be slow)\n    print(\"Compiling...\")\n    rngs = nnx.Rngs(42)\n    z = jax.random.normal(rngs.sample(), (num_samples, model.latent_dim))\n\n    samples = generate_samples_jit(model, z)\n    print(f\"Compiled! Generated {samples.shape}\")\n\n    # Subsequent calls are fast\n    print(\"Running inference...\")\n    import time\n    start = time.time()\n\n    for _ in range(10):\n        z = jax.random.normal(rngs.sample(), (num_samples, model.latent_dim))\n        samples = generate_samples_jit(model, z)\n\n    elapsed = time.time() - start\n    print(f\"10 batches in {elapsed:.3f}s ({elapsed / 10 * 1000:.1f}ms per batch)\")\n\n    return samples\n</code></pre>"},{"location":"user-guide/inference/overview/#batch-inference","title":"Batch Inference","text":""},{"location":"user-guide/inference/overview/#processing-multiple-inputs","title":"Processing Multiple Inputs","text":"<pre><code>def batch_encode_images(\n    model: VAE,\n    images: jnp.ndarray,\n    batch_size: int = 32,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Encode images in batches.\n\n    Args:\n        model: Trained VAE model\n        images: Input images [N, H, W, C]\n        batch_size: Batch size for processing\n\n    Returns:\n        Encoded latents [N, latent_dim]\n    \"\"\"\n    num_samples = len(images)\n    latents = []\n\n    for i in range(0, num_samples, batch_size):\n        batch = images[i:i + batch_size]\n\n        # Encode batch\n        latent_params = model.encode(batch)\n        batch_latents = latent_params[\"mean\"]  # Use mean for deterministic\n\n        latents.append(batch_latents)\n\n    return jnp.concatenate(latents, axis=0)\n\n\ndef batch_generate_samples(\n    model: VAE,\n    num_samples: int,\n    batch_size: int = 32,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Generate samples in batches.\n\n    Args:\n        model: Trained VAE model\n        num_samples: Total number of samples to generate\n        batch_size: Batch size for generation\n        rngs: Random number generators\n\n    Returns:\n        Generated samples [num_samples, H, W, C]\n    \"\"\"\n    samples = []\n\n    for i in range(0, num_samples, batch_size):\n        current_batch_size = min(batch_size, num_samples - i)\n\n        # Sample latent codes\n        z = jax.random.normal(rngs.sample(), (current_batch_size, model.latent_dim))\n\n        # Generate\n        batch_samples = model.decode(z)\n        samples.append(batch_samples)\n\n    return jnp.concatenate(samples, axis=0)\n</code></pre>"},{"location":"user-guide/inference/overview/#parallel-batch-processing","title":"Parallel Batch Processing","text":"<pre><code>def parallel_batch_inference(\n    model: VAE,\n    images: jnp.ndarray,\n    num_devices: int = None,\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Process batches in parallel across devices.\n\n    Args:\n        model: Trained model\n        images: Input images\n        num_devices: Number of devices to use (None = all available)\n\n    Returns:\n        Processed outputs\n    \"\"\"\n    devices = jax.devices()[:num_devices] if num_devices else jax.devices()\n    num_devices = len(devices)\n\n    print(f\"Using {num_devices} devices for parallel inference\")\n\n    # Split data across devices\n    batch_size_per_device = len(images) // num_devices\n    image_shards = [\n        images[i * batch_size_per_device:(i + 1) * batch_size_per_device]\n        for i in range(num_devices)\n    ]\n\n    # Process in parallel\n    @jax.jit\n    def process_shard(shard):\n        latent_params = model.encode(shard)\n        return latent_params[\"mean\"]\n\n    # Map over shards\n    latents_shards = jax.tree_map(process_shard, image_shards)\n\n    # Concatenate results\n    return jnp.concatenate(latents_shards, axis=0)\n</code></pre>"},{"location":"user-guide/inference/overview/#streaming-inference","title":"Streaming Inference","text":""},{"location":"user-guide/inference/overview/#real-time-generation","title":"Real-Time Generation","text":"<pre><code>class StreamingGenerator:\n    \"\"\"Stream samples one at a time for real-time applications.\"\"\"\n\n    def __init__(\n        self,\n        model: VAE,\n        seed: int = 42,\n    ):\n        self.model = model\n        self.rngs = nnx.Rngs(seed)\n\n        # Pre-compile generation function\n        self._generate_fn = jax.jit(self._generate_single)\n\n    def _generate_single(self, z: jax.Array) -&gt; jax.Array:\n        \"\"\"Generate single sample (JIT-compiled).\"\"\"\n        return self.model.decode(z)\n\n    def __iter__(self):\n        \"\"\"Iterator interface for streaming.\"\"\"\n        return self\n\n    def __next__(self) -&gt; jax.Array:\n        \"\"\"Generate next sample.\"\"\"\n        z = jax.random.normal(self.rngs.sample(), (1, self.model.latent_dim))\n        sample = self._generate_fn(z)\n        return sample[0]  # Remove batch dimension\n\n\n# Usage\ndef stream_samples(model: VAE, num_samples: int = 100):\n    \"\"\"Stream samples in real-time.\"\"\"\n    import time\n\n    generator = StreamingGenerator(model, seed=42)\n\n    print(\"Streaming samples...\")\n    for i, sample in enumerate(generator):\n        if i &gt;= num_samples:\n            break\n\n        # Process sample (e.g., display, save, send over network)\n        print(f\"Sample {i + 1}: shape {sample.shape}\")\n\n        # Simulate processing time\n        time.sleep(0.01)\n</code></pre>"},{"location":"user-guide/inference/overview/#asynchronous-inference","title":"Asynchronous Inference","text":"<pre><code>import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass AsyncInferenceServer:\n    \"\"\"Asynchronous inference server for concurrent requests.\"\"\"\n\n    def __init__(\n        self,\n        model: VAE,\n        max_workers: int = 4,\n    ):\n        self.model = model\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n\n        # Pre-compile functions\n        self.encode_fn = jax.jit(lambda x: model.encode(x)[\"mean\"])\n        self.decode_fn = jax.jit(model.decode)\n\n    async def encode_async(self, images: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Asynchronously encode images.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, self.encode_fn, images)\n\n    async def decode_async(self, latents: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Asynchronously decode latents.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, self.decode_fn, latents)\n\n    async def generate_async(self, num_samples: int, rngs: nnx.Rngs) -&gt; jnp.ndarray:\n        \"\"\"Asynchronously generate samples.\"\"\"\n        z = jax.random.normal(rngs.sample(), (num_samples, self.model.latent_dim))\n        return await self.decode_async(z)\n\n    def shutdown(self):\n        \"\"\"Shutdown executor.\"\"\"\n        self.executor.shutdown(wait=True)\n\n\n# Usage\nasync def process_concurrent_requests():\n    \"\"\"Process multiple inference requests concurrently.\"\"\"\n    model = load_model_for_inference(\"./checkpoints/vae\", config)\n    server = AsyncInferenceServer(model, max_workers=4)\n\n    # Simulate concurrent requests\n    tasks = [\n        server.generate_async(16, nnx.Rngs(i))\n        for i in range(10)\n    ]\n\n    # Wait for all to complete\n    results = await asyncio.gather(*tasks)\n\n    print(f\"Processed {len(results)} concurrent requests\")\n\n    server.shutdown()\n    return results\n\n\n# Run async inference\n# results = asyncio.run(process_concurrent_requests())\n</code></pre>"},{"location":"user-guide/inference/overview/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/inference/overview/#memory-management","title":"Memory Management","text":"<pre><code>def memory_efficient_inference(\n    model: VAE,\n    images: jnp.ndarray,\n    batch_size: int = 16,\n):\n    \"\"\"\n    Memory-efficient inference with explicit cleanup.\n\n    Args:\n        model: Trained model\n        images: Input images\n        batch_size: Small batch size to reduce memory usage\n\n    Returns:\n        Processed outputs\n    \"\"\"\n    device_manager = DeviceManager()\n    results = []\n\n    for i in range(0, len(images), batch_size):\n        batch = images[i:i + batch_size]\n\n        # Process batch\n        latent_params = model.encode(batch)\n        latents = latent_params[\"mean\"]\n\n        # Convert to numpy to free device memory\n        results.append(jnp.array(latents))\n\n        # Explicit cleanup every N batches\n        if (i // batch_size) % 10 == 0:\n            device_manager.cleanup()\n\n    return jnp.concatenate(results, axis=0)\n</code></pre>"},{"location":"user-guide/inference/overview/#inference-benchmarking","title":"Inference Benchmarking","text":"<pre><code>import time\nfrom typing import Callable\n\ndef benchmark_inference(\n    inference_fn: Callable,\n    num_runs: int = 100,\n    warmup_runs: int = 10,\n) -&gt; dict:\n    \"\"\"\n    Benchmark inference performance.\n\n    Args:\n        inference_fn: Function to benchmark\n        num_runs: Number of benchmark runs\n        warmup_runs: Number of warmup runs (not counted)\n\n    Returns:\n        Dictionary with timing statistics\n    \"\"\"\n    # Warmup\n    for _ in range(warmup_runs):\n        _ = inference_fn()\n\n    # Benchmark\n    times = []\n    for _ in range(num_runs):\n        start = time.time()\n        result = inference_fn()\n\n        # Force synchronization (important for JAX)\n        result.block_until_ready() if hasattr(result, 'block_until_ready') else None\n\n        elapsed = time.time() - start\n        times.append(elapsed)\n\n    times = jnp.array(times)\n\n    return {\n        \"mean_ms\": float(jnp.mean(times) * 1000),\n        \"std_ms\": float(jnp.std(times) * 1000),\n        \"min_ms\": float(jnp.min(times) * 1000),\n        \"max_ms\": float(jnp.max(times) * 1000),\n        \"median_ms\": float(jnp.median(times) * 1000),\n    }\n\n\n# Example usage\ndef benchmark_vae_inference(model: VAE):\n    \"\"\"Benchmark VAE inference.\"\"\"\n\n    rngs = nnx.Rngs(42)\n\n    # Benchmark encoding\n    test_images = jax.random.normal(rngs.sample(), (32, 28, 28, 1))\n\n    def encode_fn():\n        return model.encode(test_images)[\"mean\"]\n\n    encode_stats = benchmark_inference(encode_fn, num_runs=100)\n    print(\"Encoding performance:\")\n    print(f\"  Mean: {encode_stats['mean_ms']:.2f}ms \u00b1 {encode_stats['std_ms']:.2f}ms\")\n    print(f\"  Median: {encode_stats['median_ms']:.2f}ms\")\n\n    # Benchmark decoding\n    z = jax.random.normal(rngs.sample(), (32, model.latent_dim))\n\n    def decode_fn():\n        return model.decode(z)\n\n    decode_stats = benchmark_inference(decode_fn, num_runs=100)\n    print(\"\\nDecoding performance:\")\n    print(f\"  Mean: {decode_stats['mean_ms']:.2f}ms \u00b1 {decode_stats['std_ms']:.2f}ms\")\n    print(f\"  Median: {decode_stats['median_ms']:.2f}ms\")\n\n    return encode_stats, decode_stats\n</code></pre>"},{"location":"user-guide/inference/overview/#throughput-optimization","title":"Throughput Optimization","text":"<pre><code>def optimize_batch_size(\n    model: VAE,\n    max_batch_size: int = 256,\n    step: int = 16,\n) -&gt; int:\n    \"\"\"\n    Find optimal batch size for throughput.\n\n    Args:\n        model: Trained model\n        max_batch_size: Maximum batch size to try\n        step: Batch size increment\n\n    Returns:\n        Optimal batch size\n    \"\"\"\n    rngs = nnx.Rngs(42)\n    best_batch_size = step\n    best_throughput = 0.0\n\n    print(\"Testing batch sizes for optimal throughput...\")\n\n    for batch_size in range(step, max_batch_size + 1, step):\n        try:\n            # Generate test batch\n            z = jax.random.normal(rngs.sample(), (batch_size, model.latent_dim))\n\n            # Benchmark\n            def inference_fn():\n                return model.decode(z)\n\n            stats = benchmark_inference(inference_fn, num_runs=20, warmup_runs=5)\n\n            # Calculate throughput (samples per second)\n            throughput = (batch_size / stats[\"mean_ms\"]) * 1000\n\n            print(f\"  Batch size {batch_size}: {throughput:.1f} samples/sec\")\n\n            if throughput &gt; best_throughput:\n                best_throughput = throughput\n                best_batch_size = batch_size\n\n        except Exception as e:\n            print(f\"  Batch size {batch_size}: Failed ({e})\")\n            break\n\n    print(f\"\\nOptimal batch size: {best_batch_size}\")\n    print(f\"Peak throughput: {best_throughput:.1f} samples/sec\")\n\n    return best_batch_size\n</code></pre>"},{"location":"user-guide/inference/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/inference/overview/#inference-pipeline","title":"Inference Pipeline","text":"<pre><code>class InferencePipeline:\n    \"\"\"Complete inference pipeline with preprocessing and postprocessing.\"\"\"\n\n    def __init__(\n        self,\n        model: VAE,\n        preprocess_fn: Callable = None,\n        postprocess_fn: Callable = None,\n    ):\n        self.model = model\n        self.preprocess_fn = preprocess_fn or (lambda x: x)\n        self.postprocess_fn = postprocess_fn or (lambda x: x)\n\n        # JIT-compile core functions\n        self.encode_fn = jax.jit(lambda x: model.encode(x)[\"mean\"])\n        self.decode_fn = jax.jit(model.decode)\n\n    def encode(self, images: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Full encoding pipeline.\"\"\"\n        # Preprocess\n        preprocessed = self.preprocess_fn(images)\n\n        # Encode\n        latents = self.encode_fn(preprocessed)\n\n        return latents\n\n    def decode(self, latents: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Full decoding pipeline.\"\"\"\n        # Decode\n        samples = self.decode_fn(latents)\n\n        # Postprocess\n        postprocessed = self.postprocess_fn(samples)\n\n        return postprocessed\n\n    def reconstruct(self, images: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Reconstruct images through encode-decode.\"\"\"\n        latents = self.encode(images)\n        return self.decode(latents)\n\n    def generate(self, num_samples: int, *, rngs: nnx.Rngs) -&gt; jnp.ndarray:\n        \"\"\"Generate new samples.\"\"\"\n        z = jax.random.normal(rngs.sample(), (num_samples, self.model.latent_dim))\n        return self.decode(z)\n\n\n# Example with preprocessing/postprocessing\ndef normalize_images(images):\n    \"\"\"Normalize to [-1, 1].\"\"\"\n    return (images / 255.0) * 2.0 - 1.0\n\n\ndef denormalize_images(images):\n    \"\"\"Denormalize to [0, 255].\"\"\"\n    return ((images + 1.0) / 2.0 * 255.0).astype(jnp.uint8)\n\n\npipeline = InferencePipeline(\n    model=model,\n    preprocess_fn=normalize_images,\n    postprocess_fn=denormalize_images,\n)\n\n# Use pipeline\nsamples = pipeline.generate(16, rngs=nnx.Rngs(42))\n</code></pre>"},{"location":"user-guide/inference/overview/#best-practices","title":"Best Practices","text":"<p>DO</p> <ul> <li>Always JIT-compile inference functions</li> <li>Use appropriate batch sizes for your hardware</li> <li>Pre-compile during initialization (warmup)</li> <li>Monitor memory usage with large batches</li> <li>Use asynchronous inference for servers</li> <li>Benchmark different batch sizes</li> </ul> <p>DON'T</p> <ul> <li>Don't skip JIT compilation for production</li> <li>Don't use batch size 1 unless necessary</li> <li>Don't forget to call <code>block_until_ready()</code> in benchmarks</li> <li>Don't load models in hot paths</li> <li>Don't ignore device memory limits</li> <li>Don't use floating point for final outputs</li> </ul>"},{"location":"user-guide/inference/overview/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution Slow first inference JIT compilation Pre-compile with warmup runs Out of memory Batch too large Reduce batch size, use memory-efficient mode Inconsistent timing Async execution Use <code>block_until_ready()</code> for accurate timing Low throughput Small batches Increase batch size, optimize for hardware High latency Not using JIT JIT-compile all inference functions"},{"location":"user-guide/inference/overview/#summary","title":"Summary","text":"<p>This guide covered:</p> <ol> <li>Model Loading: Load from checkpoints or exports</li> <li>Batch Inference: Process multiple inputs efficiently</li> <li>Streaming: Real-time generation for interactive applications</li> <li>Performance: Optimize speed, memory, and throughput</li> </ol> <p>Key Takeaways:</p> <ul> <li>JIT compilation is essential for performance</li> <li>Batch size significantly impacts throughput</li> <li>Memory management matters for large-scale inference</li> <li>Benchmarking helps find optimal configurations</li> </ul>"},{"location":"user-guide/inference/overview/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Sampling Methods</p> <p>Advanced sampling techniques</p> <p> Sampling Guide</p> </li> <li> <p> Optimization</p> <p>Deep dive into inference optimization</p> <p> Optimization Guide</p> </li> <li> <p> Deployment</p> <p>Deploy models to production</p> <p> Deployment Guide</p> </li> <li> <p> Benchmarks</p> <p>Evaluate model performance</p> <p> Benchmarks</p> </li> </ul>"},{"location":"user-guide/inference/sampling/","title":"Sampling Methods","text":"<p>A comprehensive guide to sampling techniques for generating high-quality outputs from VAE, GAN, Diffusion, and Flow models.</p>"},{"location":"user-guide/inference/sampling/#overview","title":"Overview","text":"<p>Sampling is the process of generating new data from trained generative models. Different model architectures require different sampling strategies, each with unique trade-offs between quality, diversity, and computational cost.</p> <p>Key Concepts</p> <ul> <li>Deterministic Sampling: Same latent code produces same output</li> <li>Stochastic Sampling: Introduces randomness for diversity</li> <li>Temperature: Controls sampling randomness</li> <li>Truncation: Trades diversity for quality</li> </ul> <ul> <li> <p> VAE Sampling</p> <p>Sample from learned latent distributions with interpolation and conditional generation</p> <p> VAE Methods</p> </li> <li> <p> GAN Sampling</p> <p>Generate from latent codes with truncation trick and style mixing</p> <p> GAN Methods</p> </li> <li> <p> Diffusion Sampling</p> <p>Iterative denoising with DDPM, DDIM, and DPM-Solver for speed/quality tradeoffs</p> <p> Diffusion Methods</p> </li> <li> <p> Flow Sampling</p> <p>Invertible transformations with temperature scaling and rejection sampling</p> <p> Flow Methods</p> </li> </ul>"},{"location":"user-guide/inference/sampling/#prerequisites","title":"Prerequisites","text":"<p>Before using sampling methods, ensure you have:</p> <pre><code>from flax import nnx\nimport jax\nimport jax.numpy as jnp\nfrom artifex.generative_models.models.vae import VAE\nfrom artifex.generative_models.models.gan import GAN\nfrom artifex.generative_models.models.diffusion import DiffusionModel\nfrom artifex.generative_models.models.flow import FlowModel\n</code></pre> <p>Model Checkpoint</p> <p>All sampling examples assume you have a trained model checkpoint loaded. See Inference Overview for loading instructions.</p>"},{"location":"user-guide/inference/sampling/#vae-sampling-methods","title":"VAE Sampling Methods","text":""},{"location":"user-guide/inference/sampling/#latent-space-sampling","title":"Latent Space Sampling","text":"<p>VAEs learn a probabilistic latent space, typically modeled as a Gaussian distribution.</p> <pre><code>class VAESampler:\n    \"\"\"Sampling utilities for Variational Autoencoders.\"\"\"\n\n    def __init__(self, vae: VAE):\n        self.vae = vae\n\n    def sample_prior(\n        self,\n        num_samples: int,\n        temperature: float = 1.0,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample from the prior distribution.\n\n        Args:\n            num_samples: Number of samples to generate\n            temperature: Controls randomness (default: 1.0)\n            rngs: Random number generators\n\n        Returns:\n            Generated samples with shape (num_samples, *image_shape)\n        \"\"\"\n        # Sample from standard normal\n        z = jax.random.normal(\n            rngs.sample(),\n            (num_samples, self.vae.latent_dim)\n        )\n\n        # Apply temperature scaling\n        z = z * temperature\n\n        # Decode to image space\n        samples = self.vae.decode(z)\n\n        return samples\n\n    def reconstruct(self, images: jax.Array) -&gt; jax.Array:\n        \"\"\"Reconstruct images through the VAE.\n\n        Args:\n            images: Input images with shape (batch_size, *image_shape)\n\n        Returns:\n            Reconstructed images\n        \"\"\"\n        # Encode to latent space\n        z_mean, z_logvar = self.vae.encode(images)\n\n        # Decode (using mean for reconstruction)\n        reconstructed = self.vae.decode(z_mean)\n\n        return reconstructed\n</code></pre>"},{"location":"user-guide/inference/sampling/#conditional-vae-sampling","title":"Conditional VAE Sampling","text":"<p>Generate samples conditioned on class labels or other attributes.</p> <pre><code>class ConditionalVAESampler:\n    \"\"\"Sampling for conditional VAEs.\"\"\"\n\n    def __init__(self, vae: VAE):\n        self.vae = vae\n\n    def sample_conditional(\n        self,\n        labels: jax.Array,\n        temperature: float = 1.0,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample conditioned on labels.\n\n        Args:\n            labels: Class labels with shape (batch_size,)\n            temperature: Sampling temperature\n            rngs: Random number generators\n\n        Returns:\n            Conditional samples\n        \"\"\"\n        batch_size = labels.shape[0]\n\n        # Sample latent codes\n        z = jax.random.normal(\n            rngs.sample(),\n            (batch_size, self.vae.latent_dim)\n        ) * temperature\n\n        # Decode with conditioning\n        samples = self.vae.decode(z, labels=labels)\n\n        return samples\n</code></pre>"},{"location":"user-guide/inference/sampling/#latent-space-interpolation","title":"Latent Space Interpolation","text":"<p>Create smooth transitions between samples by interpolating in latent space.</p> <pre><code>def interpolate_latent(\n    sampler: VAESampler,\n    start_image: jax.Array,\n    end_image: jax.Array,\n    num_steps: int = 10,\n) -&gt; jax.Array:\n    \"\"\"Interpolate between two images in latent space.\n\n    Args:\n        sampler: VAE sampler instance\n        start_image: Starting image (1, *image_shape)\n        end_image: Ending image (1, *image_shape)\n        num_steps: Number of interpolation steps\n\n    Returns:\n        Interpolated images (num_steps, *image_shape)\n    \"\"\"\n    # Encode both images\n    z_start, _ = sampler.vae.encode(start_image)\n    z_end, _ = sampler.vae.encode(end_image)\n\n    # Linear interpolation in latent space\n    alphas = jnp.linspace(0, 1, num_steps)[:, None]\n    z_interp = z_start * (1 - alphas) + z_end * alphas\n\n    # Decode interpolated latents\n    interpolated = sampler.vae.decode(z_interp)\n\n    return interpolated\n</code></pre> <p>Spherical Interpolation (SLERP)</p> <p>For better interpolation in high-dimensional spaces:</p> <pre><code>def slerp(v0, v1, t):\n    \"\"\"Spherical linear interpolation.\"\"\"\n    # Normalize vectors\n    v0_norm = v0 / jnp.linalg.norm(v0)\n    v1_norm = v1 / jnp.linalg.norm(v1)\n\n    # Calculate angle\n    omega = jnp.arccos(jnp.clip(jnp.dot(v0_norm, v1_norm), -1, 1))\n\n    # Interpolate\n    sin_omega = jnp.sin(omega)\n    return (jnp.sin((1 - t) * omega) / sin_omega * v0 +\n            jnp.sin(t * omega) / sin_omega * v1)\n\n# Use in interpolation\nz_interp = jax.vmap(lambda alpha: slerp(z_start[0], z_end[0], alpha))(alphas)\n</code></pre>"},{"location":"user-guide/inference/sampling/#gan-sampling-methods","title":"GAN Sampling Methods","text":""},{"location":"user-guide/inference/sampling/#basic-latent-code-sampling","title":"Basic Latent Code Sampling","text":"<p>GANs generate samples by mapping random latent codes through the generator.</p> <pre><code>class GANSampler:\n    \"\"\"Sampling utilities for Generative Adversarial Networks.\"\"\"\n\n    def __init__(self, gan: GAN):\n        self.gan = gan\n\n    def sample(\n        self,\n        num_samples: int,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Generate samples from random latent codes.\n\n        Args:\n            num_samples: Number of samples to generate\n            rngs: Random number generators\n\n        Returns:\n            Generated images\n        \"\"\"\n        # Sample latent codes from standard normal\n        z = jax.random.normal(\n            rngs.sample(),\n            (num_samples, self.gan.latent_dim)\n        )\n\n        # Generate images\n        samples = self.gan.generator(z)\n\n        return samples\n</code></pre>"},{"location":"user-guide/inference/sampling/#truncation-trick","title":"Truncation Trick","text":"<p>Improve sample quality at the cost of diversity by truncating the latent distribution.</p> <pre><code>class TruncatedGANSampler:\n    \"\"\"GAN sampling with truncation trick.\"\"\"\n\n    def __init__(self, gan: GAN):\n        self.gan = gan\n\n    def sample_truncated(\n        self,\n        num_samples: int,\n        truncation: float = 0.7,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample with truncation trick.\n\n        Args:\n            num_samples: Number of samples\n            truncation: Truncation factor (0 &lt; t &lt;= 1)\n                       Lower values = higher quality, less diversity\n            rngs: Random number generators\n\n        Returns:\n            High-quality samples\n        \"\"\"\n        # Sample latent codes\n        z = jax.random.normal(\n            rngs.sample(),\n            (num_samples, self.gan.latent_dim)\n        )\n\n        # Truncate to reduce diversity\n        z = z * truncation\n\n        # Generate images\n        samples = self.gan.generator(z)\n\n        return samples\n\n    def sample_adaptive_truncation(\n        self,\n        num_samples: int,\n        threshold: float = 2.0,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample with adaptive truncation.\n\n        Resamples latent dimensions that exceed threshold.\n\n        Args:\n            num_samples: Number of samples\n            threshold: Threshold for resampling (typically 2-3)\n            rngs: Random number generators\n\n        Returns:\n            Adaptively truncated samples\n        \"\"\"\n        z = jax.random.normal(\n            rngs.sample(),\n            (num_samples, self.gan.latent_dim)\n        )\n\n        # Resample dimensions exceeding threshold\n        mask = jnp.abs(z) &gt; threshold\n\n        while jnp.any(mask):\n            z_new = jax.random.normal(rngs.sample(), z.shape)\n            z = jnp.where(mask, z_new, z)\n            mask = jnp.abs(z) &gt; threshold\n\n        samples = self.gan.generator(z)\n\n        return samples\n</code></pre>"},{"location":"user-guide/inference/sampling/#style-mixing-stylegan","title":"Style Mixing (StyleGAN)","text":"<p>Mix styles from different latent codes for creative generation.</p> <pre><code>class StyleGANSampler:\n    \"\"\"Advanced sampling for StyleGAN architectures.\"\"\"\n\n    def __init__(self, stylegan):\n        self.stylegan = stylegan\n\n    def mix_styles(\n        self,\n        num_samples: int,\n        mix_layer: int,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Generate samples with style mixing.\n\n        Args:\n            num_samples: Number of samples\n            mix_layer: Layer index to switch styles (0 to num_layers)\n            rngs: Random number generators\n\n        Returns:\n            Style-mixed samples\n        \"\"\"\n        # Sample two sets of latent codes\n        z1 = jax.random.normal(\n            rngs.sample(),\n            (num_samples, self.stylegan.latent_dim)\n        )\n        z2 = jax.random.normal(\n            rngs.sample(),\n            (num_samples, self.stylegan.latent_dim)\n        )\n\n        # Map to W space\n        w1 = self.stylegan.mapping_network(z1)\n        w2 = self.stylegan.mapping_network(z2)\n\n        # Generate with style mixing\n        samples = self.stylegan.synthesis_network(\n            w1, w2, mix_layer=mix_layer\n        )\n\n        return samples\n</code></pre>"},{"location":"user-guide/inference/sampling/#diffusion-sampling-methods","title":"Diffusion Sampling Methods","text":""},{"location":"user-guide/inference/sampling/#ddpm-sampling","title":"DDPM Sampling","text":"<p>Standard denoising diffusion probabilistic model sampling.</p> <pre><code>class DDPMSampler:\n    \"\"\"DDPM sampling with full denoising process.\"\"\"\n\n    def __init__(self, diffusion: DiffusionModel):\n        self.diffusion = diffusion\n        self.num_timesteps = diffusion.num_timesteps\n\n    def sample(\n        self,\n        num_samples: int,\n        image_shape: tuple,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample using full DDPM process.\n\n        Args:\n            num_samples: Number of samples\n            image_shape: Shape of output images (C, H, W)\n            rngs: Random number generators\n\n        Returns:\n            Generated samples\n        \"\"\"\n        # Start from pure noise\n        x = jax.random.normal(\n            rngs.sample(),\n            (num_samples, *image_shape)\n        )\n\n        # Reverse diffusion process\n        for t in reversed(range(self.num_timesteps)):\n            # Create timestep array\n            timesteps = jnp.full((num_samples,), t)\n\n            # Predict noise\n            noise_pred = self.diffusion(x, timesteps)\n\n            # Get schedule parameters\n            alpha_t = self.diffusion.alpha_schedule[t]\n            alpha_prev = self.diffusion.alpha_schedule[t - 1] if t &gt; 0 else 1.0\n            beta_t = 1 - alpha_t / alpha_prev\n\n            # Compute mean\n            x_mean = (x - beta_t * noise_pred / jnp.sqrt(1 - alpha_t))\n            x_mean = x_mean / jnp.sqrt(1 - beta_t)\n\n            # Add noise (except final step)\n            if t &gt; 0:\n                noise = jax.random.normal(rngs.sample(), x.shape)\n                sigma_t = jnp.sqrt(beta_t)\n                x = x_mean + sigma_t * noise\n            else:\n                x = x_mean\n\n        return x\n</code></pre>"},{"location":"user-guide/inference/sampling/#ddim-sampling","title":"DDIM Sampling","text":"<p>Deterministic sampling for faster generation with fewer steps.</p> <pre><code>class DDIMSampler:\n    \"\"\"DDIM sampling for fast inference.\"\"\"\n\n    def __init__(self, diffusion: DiffusionModel):\n        self.diffusion = diffusion\n\n    def sample(\n        self,\n        num_samples: int,\n        image_shape: tuple,\n        num_steps: int = 50,\n        eta: float = 0.0,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample using DDIM with configurable steps.\n\n        Args:\n            num_samples: Number of samples\n            image_shape: Output shape\n            num_steps: Number of denoising steps (&lt; num_timesteps)\n            eta: Stochasticity parameter (0 = deterministic)\n            rngs: Random number generators\n\n        Returns:\n            Generated samples\n        \"\"\"\n        # Create timestep schedule\n        timesteps = jnp.linspace(\n            self.diffusion.num_timesteps - 1,\n            0,\n            num_steps,\n            dtype=jnp.int32\n        )\n\n        # Start from noise\n        x = jax.random.normal(\n            rngs.sample(),\n            (num_samples, *image_shape)\n        )\n\n        # DDIM sampling loop\n        for i, t in enumerate(timesteps):\n            t_batch = jnp.full((num_samples,), t)\n\n            # Predict noise\n            noise_pred = self.diffusion(x, t_batch)\n\n            # Get alphas\n            alpha_t = self.diffusion.alpha_schedule[t]\n            if i &lt; len(timesteps) - 1:\n                alpha_prev = self.diffusion.alpha_schedule[timesteps[i + 1]]\n            else:\n                alpha_prev = 1.0\n\n            # Predict x0\n            x0_pred = (x - jnp.sqrt(1 - alpha_t) * noise_pred) / jnp.sqrt(alpha_t)\n            x0_pred = jnp.clip(x0_pred, -1, 1)\n\n            # Direction pointing to x_t\n            dir_xt = jnp.sqrt(1 - alpha_prev - eta**2) * noise_pred\n\n            # Compute x_{t-1}\n            x = jnp.sqrt(alpha_prev) * x0_pred + dir_xt\n\n            # Add stochasticity\n            if eta &gt; 0 and i &lt; len(timesteps) - 1:\n                noise = jax.random.normal(rngs.sample(), x.shape)\n                sigma_t = eta * jnp.sqrt((1 - alpha_prev) / (1 - alpha_t))\n                sigma_t = sigma_t * jnp.sqrt(1 - alpha_t / alpha_prev)\n                x = x + sigma_t * noise\n\n        return x\n</code></pre>"},{"location":"user-guide/inference/sampling/#dpm-solver","title":"DPM-Solver","text":"<p>Advanced solver for high-quality samples with very few steps.</p> <pre><code>class DPMSolver:\n    \"\"\"DPM-Solver for efficient diffusion sampling.\"\"\"\n\n    def __init__(self, diffusion: DiffusionModel):\n        self.diffusion = diffusion\n\n    def sample(\n        self,\n        num_samples: int,\n        image_shape: tuple,\n        num_steps: int = 20,\n        order: int = 2,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample using DPM-Solver.\n\n        Args:\n            num_samples: Number of samples\n            image_shape: Output shape\n            num_steps: Sampling steps (10-20 often sufficient)\n            order: Solver order (1, 2, or 3)\n            rngs: Random number generators\n\n        Returns:\n            High-quality samples\n        \"\"\"\n        # Timestep schedule\n        timesteps = jnp.linspace(\n            self.diffusion.num_timesteps - 1,\n            0,\n            num_steps + 1,\n            dtype=jnp.int32\n        )\n\n        # Start from noise\n        x = jax.random.normal(\n            rngs.sample(),\n            (num_samples, *image_shape)\n        )\n\n        # DPM-Solver iterations\n        for i in range(num_steps):\n            t = timesteps[i]\n            t_next = timesteps[i + 1]\n\n            # First-order update\n            noise_pred = self.diffusion(x, jnp.full((num_samples,), t))\n            x = self._dpm_solver_step(x, noise_pred, t, t_next, order)\n\n        return x\n\n    def _dpm_solver_step(\n        self,\n        x: jax.Array,\n        noise: jax.Array,\n        t: int,\n        t_next: int,\n        order: int,\n    ) -&gt; jax.Array:\n        \"\"\"Single DPM-Solver step.\"\"\"\n        lambda_t = self._log_snr(t)\n        lambda_next = self._log_snr(t_next)\n\n        h = lambda_next - lambda_t\n        alpha_t = self.diffusion.alpha_schedule[t]\n        alpha_next = self.diffusion.alpha_schedule[t_next]\n\n        # First-order exponential integrator\n        x_next = (\n            jnp.sqrt(alpha_next / alpha_t) * x\n            - jnp.sqrt(alpha_next) * jnp.expm1(h) * noise\n        )\n\n        return x_next\n\n    def _log_snr(self, t: int) -&gt; float:\n        \"\"\"Compute log signal-to-noise ratio.\"\"\"\n        alpha_t = self.diffusion.alpha_schedule[t]\n        return jnp.log(alpha_t / (1 - alpha_t))\n</code></pre>"},{"location":"user-guide/inference/sampling/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<p>Improve sample quality with guidance.</p> <pre><code>def sample_with_guidance(\n    diffusion: DiffusionModel,\n    num_samples: int,\n    labels: jax.Array,\n    guidance_scale: float = 7.5,\n    num_steps: int = 50,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; jax.Array:\n    \"\"\"Sample with classifier-free guidance.\n\n    Args:\n        diffusion: Conditional diffusion model\n        num_samples: Number of samples\n        labels: Class labels for conditioning\n        guidance_scale: Guidance strength (1.0 = no guidance, 7.5 = strong)\n        num_steps: Sampling steps\n        rngs: Random number generators\n\n    Returns:\n        Guided samples\n    \"\"\"\n    # Create null labels for unconditional prediction\n    null_labels = jnp.full_like(labels, -1)\n\n    # Sampling loop (using DDIM)\n    timesteps = jnp.linspace(diffusion.num_timesteps - 1, 0, num_steps, dtype=jnp.int32)\n    x = jax.random.normal(rngs.sample(), (num_samples, *diffusion.image_shape))\n\n    for t in timesteps:\n        t_batch = jnp.full((num_samples,), t)\n\n        # Conditional prediction\n        noise_cond = diffusion(x, t_batch, labels)\n\n        # Unconditional prediction\n        noise_uncond = diffusion(x, t_batch, null_labels)\n\n        # Apply guidance\n        noise_pred = noise_uncond + guidance_scale * (noise_cond - noise_uncond)\n\n        # Update (simplified DDIM step)\n        alpha_t = diffusion.alpha_schedule[t]\n        x = (x - jnp.sqrt(1 - alpha_t) * noise_pred) / jnp.sqrt(alpha_t)\n\n    return x\n</code></pre>"},{"location":"user-guide/inference/sampling/#flow-sampling-methods","title":"Flow Sampling Methods","text":""},{"location":"user-guide/inference/sampling/#inverse-transformation","title":"Inverse Transformation","text":"<p>Flows sample by inverting the learned transformation.</p> <pre><code>class FlowSampler:\n    \"\"\"Sampling for normalizing flows.\"\"\"\n\n    def __init__(self, flow: FlowModel):\n        self.flow = flow\n\n    def sample(\n        self,\n        num_samples: int,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample by inverting the flow.\n\n        Args:\n            num_samples: Number of samples\n            rngs: Random number generators\n\n        Returns:\n            Generated samples\n        \"\"\"\n        # Sample from base distribution (standard normal)\n        z = jax.random.normal(\n            rngs.sample(),\n            (num_samples, self.flow.data_dim)\n        )\n\n        # Invert flow: z -&gt; x\n        x = self.flow.inverse(z)\n\n        return x\n</code></pre>"},{"location":"user-guide/inference/sampling/#temperature-scaling","title":"Temperature Scaling","text":"<p>Control sample diversity with temperature.</p> <pre><code>class TemperatureFlowSampler:\n    \"\"\"Flow sampling with temperature control.\"\"\"\n\n    def __init__(self, flow: FlowModel):\n        self.flow = flow\n\n    def sample_with_temperature(\n        self,\n        num_samples: int,\n        temperature: float = 1.0,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample with temperature scaling.\n\n        Args:\n            num_samples: Number of samples\n            temperature: Controls diversity (&lt; 1 = less diverse, &gt; 1 = more diverse)\n            rngs: Random number generators\n\n        Returns:\n            Temperature-scaled samples\n        \"\"\"\n        # Sample from scaled base distribution\n        z = jax.random.normal(\n            rngs.sample(),\n            (num_samples, self.flow.data_dim)\n        ) * temperature\n\n        # Invert to data space\n        x = self.flow.inverse(z)\n\n        return x\n</code></pre>"},{"location":"user-guide/inference/sampling/#rejection-sampling","title":"Rejection Sampling","text":"<p>Improve quality by rejecting low-probability samples.</p> <pre><code>class RejectionFlowSampler:\n    \"\"\"Flow sampling with rejection for quality control.\"\"\"\n\n    def __init__(self, flow: FlowModel):\n        self.flow = flow\n\n    def sample_with_rejection(\n        self,\n        num_samples: int,\n        threshold: float = -10.0,\n        max_attempts: int = 1000,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Sample with rejection based on log-likelihood.\n\n        Args:\n            num_samples: Number of samples to generate\n            threshold: Minimum log-likelihood threshold\n            max_attempts: Maximum sampling attempts\n            rngs: Random number generators\n\n        Returns:\n            High-quality samples (may be fewer than num_samples)\n        \"\"\"\n        accepted_samples = []\n        attempts = 0\n\n        while len(accepted_samples) &lt; num_samples and attempts &lt; max_attempts:\n            # Generate candidates\n            z = jax.random.normal(rngs.sample(), (num_samples, self.flow.data_dim))\n            x = self.flow.inverse(z)\n\n            # Compute log-likelihood\n            log_prob = self.flow.log_prob(x)\n\n            # Accept samples above threshold\n            mask = log_prob &gt; threshold\n            accepted_samples.append(x[mask])\n\n            attempts += 1\n\n        # Concatenate and return\n        if accepted_samples:\n            samples = jnp.concatenate(accepted_samples, axis=0)\n            return samples[:num_samples]\n        else:\n            # Fallback: return best samples\n            return x[jnp.argsort(log_prob)[-num_samples:]]\n</code></pre>"},{"location":"user-guide/inference/sampling/#quality-vs-speed-tradeoffs","title":"Quality vs Speed Tradeoffs","text":""},{"location":"user-guide/inference/sampling/#sampling-steps-comparison","title":"Sampling Steps Comparison","text":"<p>Different methods offer different speed/quality tradeoffs.</p> <pre><code>def compare_sampling_methods(\n    diffusion: DiffusionModel,\n    image_shape: tuple,\n    *,\n    rngs: nnx.Rngs,\n) -&gt; dict:\n    \"\"\"Compare sampling methods on speed and quality.\n\n    Returns:\n        Dictionary with timing and sample results\n    \"\"\"\n    import time\n\n    results = {}\n\n    # DDPM (1000 steps) - Highest quality, slowest\n    start = time.time()\n    ddpm_sampler = DDPMSampler(diffusion)\n    samples_ddpm = ddpm_sampler.sample(1, image_shape, rngs=rngs)\n    results['ddpm'] = {\n        'time': time.time() - start,\n        'steps': 1000,\n        'samples': samples_ddpm,\n    }\n\n    # DDIM (50 steps) - Good quality, fast\n    start = time.time()\n    ddim_sampler = DDIMSampler(diffusion)\n    samples_ddim = ddim_sampler.sample(1, image_shape, num_steps=50, rngs=rngs)\n    results['ddim_50'] = {\n        'time': time.time() - start,\n        'steps': 50,\n        'samples': samples_ddim,\n    }\n\n    # DDIM (20 steps) - Lower quality, faster\n    start = time.time()\n    samples_ddim_fast = ddim_sampler.sample(1, image_shape, num_steps=20, rngs=rngs)\n    results['ddim_20'] = {\n        'time': time.time() - start,\n        'steps': 20,\n        'samples': samples_ddim_fast,\n    }\n\n    # DPM-Solver (20 steps) - Best quality/speed tradeoff\n    start = time.time()\n    dpm_sampler = DPMSolver(diffusion)\n    samples_dpm = dpm_sampler.sample(1, image_shape, num_steps=20, rngs=rngs)\n    results['dpm_20'] = {\n        'time': time.time() - start,\n        'steps': 20,\n        'samples': samples_dpm,\n    }\n\n    return results\n</code></pre>"},{"location":"user-guide/inference/sampling/#guidance-scale-effects","title":"Guidance Scale Effects","text":"<p>Higher guidance improves quality but reduces diversity.</p> Guidance Scale Quality Diversity Use Case 1.0 Low High Exploration 3.0 Medium Medium Balanced generation 7.5 High Low Production quality 15.0 Very High Very Low Maximum quality"},{"location":"user-guide/inference/sampling/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/inference/sampling/#do","title":"DO","text":"<p>Recommended Practices</p> <p>\u2705 Use DDIM or DPM-Solver for production (10-20x faster than DDPM)</p> <p>\u2705 Apply truncation to GANs for higher quality samples</p> <p>\u2705 Cache compiled functions with <code>@jax.jit</code> for repeated sampling</p> <p>\u2705 Batch samples to maximize GPU utilization</p> <p>\u2705 Use temperature &lt; 1.0 for conservative, high-quality samples</p> <p>\u2705 Validate samples with quality metrics (FID, IS)</p>"},{"location":"user-guide/inference/sampling/#dont","title":"DON'T","text":"<p>Avoid These Mistakes</p> <p>\u274c Don't use DDPM for interactive applications (too slow)</p> <p>\u274c Don't use guidance &gt; 10 (over-saturated, artifacts)</p> <p>\u274c Don't ignore batch size (single samples waste GPU)</p> <p>\u274c Don't skip warmup compilation on first call</p> <p>\u274c Don't use temperature &gt; 2.0 (unstable, poor quality)</p> <p>\u274c Don't mix sampling strategies without understanding tradeoffs</p>"},{"location":"user-guide/inference/sampling/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/inference/sampling/#production-sampling-pipeline","title":"Production Sampling Pipeline","text":"<pre><code>class ProductionSampler:\n    \"\"\"Production-ready sampling with caching and batching.\"\"\"\n\n    def __init__(self, model, sampler_type: str = 'ddim'):\n        self.model = model\n\n        # Select sampler\n        if sampler_type == 'ddim':\n            self.sampler = DDIMSampler(model)\n            self.sample_fn = self.sampler.sample\n        elif sampler_type == 'dpm':\n            self.sampler = DPMSolver(model)\n            self.sample_fn = self.sampler.sample\n        else:\n            raise ValueError(f\"Unknown sampler: {sampler_type}\")\n\n        # JIT compile for speed\n        self.sample_fn_jit = jax.jit(self.sample_fn)\n\n        # Warmup compilation\n        self._warmup()\n\n    def _warmup(self):\n        \"\"\"Warmup JIT compilation.\"\"\"\n        dummy_rngs = nnx.Rngs(0)\n        _ = self.sample_fn_jit(1, self.model.image_shape, rngs=dummy_rngs)\n\n    def generate_batch(\n        self,\n        batch_size: int,\n        *,\n        rngs: nnx.Rngs,\n    ) -&gt; jax.Array:\n        \"\"\"Generate a batch of samples efficiently.\"\"\"\n        return self.sample_fn_jit(\n            batch_size,\n            self.model.image_shape,\n            rngs=rngs\n        )\n</code></pre>"},{"location":"user-guide/inference/sampling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/inference/sampling/#common-issues","title":"Common Issues","text":"Issue Cause Solution Blurry samples Too many DDIM steps or low temperature Reduce steps to 20-50, increase temperature Mode collapse GAN training issue or too much truncation Retrain GAN, reduce truncation to 0.7-0.8 Slow sampling Using DDPM with 1000 steps Switch to DDIM (50 steps) or DPM (20 steps) OOM during sampling Batch size too large Reduce batch size or use gradient checkpointing Artifacts with guidance Guidance scale too high Reduce to 5.0-7.5 range Low diversity Truncation too aggressive Increase truncation or use adaptive method Unstable flows Temperature too high Reduce temperature to 0.8-1.2 range"},{"location":"user-guide/inference/sampling/#summary","title":"Summary","text":"<p>Effective sampling is crucial for generating high-quality outputs:</p> <ul> <li>VAEs: Sample from learned distributions with temperature and interpolation</li> <li>GANs: Use truncation trick and style mixing for quality and creativity</li> <li>Diffusion: Trade off quality and speed with DDPM, DDIM, or DPM-Solver</li> <li>Flows: Control diversity with temperature and rejection sampling</li> <li>Production: Use JIT compilation and batching for efficiency</li> </ul> <p>Choose your sampling method based on your quality requirements and computational budget.</p>"},{"location":"user-guide/inference/sampling/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Optimization</p> <p>Learn advanced optimization techniques for faster inference</p> <p> Inference Optimization</p> </li> <li> <p> Inference Overview</p> <p>Return to model loading and batch processing</p> <p> Inference Overview</p> </li> <li> <p> Model Training</p> <p>Improve sampling quality by training better models</p> <p> Training Guide</p> </li> <li> <p> Benchmarking</p> <p>Evaluate sample quality with metrics like FID and IS</p> <p> Evaluation Framework</p> </li> </ul>"},{"location":"user-guide/integrations/deployment/","title":"Model Deployment","text":"<p>Deploy trained Artifex models to production environments, including model export, serving, and optimization for inference.</p> <ul> <li> <p> Model Export</p> <p>Export models for deployment and inference</p> <p> Learn more</p> </li> <li> <p> Model Serving</p> <p>Serve models with REST APIs and batch processing</p> <p> Learn more</p> </li> <li> <p> Optimization</p> <p>Optimize models for production performance</p> <p> Learn more</p> </li> <li> <p> Cloud Deployment</p> <p>Deploy to cloud platforms and containers</p> <p> Learn more</p> </li> </ul>"},{"location":"user-guide/integrations/deployment/#model-export","title":"Model Export","text":"<p>Export trained models for deployment.</p>"},{"location":"user-guide/integrations/deployment/#export-model-state","title":"Export Model State","text":"<pre><code>from artifex.generative_models.core.checkpointing import save_checkpoint\nfrom flax import nnx\nimport orbax.checkpoint as ocp\n\n# Save trained model\ncheckpoint_manager, checkpoint_dir = setup_checkpoint_manager(\n    base_dir=\"./models/production/vae_v1\"\n)\n\nsave_checkpoint(checkpoint_manager, model, step=final_step)\n\nprint(f\"Model exported to {checkpoint_dir}\")\n</code></pre>"},{"location":"user-guide/integrations/deployment/#export-with-metadata","title":"Export with Metadata","text":"<pre><code>import json\nimport orbax.checkpoint as ocp\nfrom flax import nnx\n\ndef export_model_with_metadata(\n    model: nnx.Module,\n    config: dict,\n    metrics: dict,\n    export_dir: str,\n):\n    \"\"\"Export model with configuration and metrics.\"\"\"\n    # Save model checkpoint\n    checkpoint_manager, _ = setup_checkpoint_manager(export_dir)\n    save_checkpoint(checkpoint_manager, model, step=0)\n\n    # Save configuration\n    config_path = f\"{export_dir}/config.json\"\n    with open(config_path, \"w\") as f:\n        json.dump(config, f, indent=2)\n\n    # Save metrics\n    metrics_path = f\"{export_dir}/metrics.json\"\n    with open(metrics_path, \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # Save model info\n    info = {\n        \"model_type\": config.get(\"model_type\"),\n        \"input_shape\": config.get(\"input_shape\"),\n        \"latent_dim\": config.get(\"latent_dim\"),\n        \"trained_steps\": metrics.get(\"total_steps\"),\n        \"final_loss\": float(metrics.get(\"final_loss\", 0.0)),\n    }\n\n    info_path = f\"{export_dir}/model_info.json\"\n    with open(info_path, \"w\") as f:\n        json.dump(info, f, indent=2)\n\n    return export_dir\n\n# Export\nexport_dir = export_model_with_metadata(\n    model=model,\n    config={\"model_type\": \"vae\", \"latent_dim\": 20, \"input_shape\": [28, 28, 1]},\n    metrics={\"total_steps\": 10000, \"final_loss\": 0.15},\n    export_dir=\"./models/production/vae_v1\",\n)\n</code></pre>"},{"location":"user-guide/integrations/deployment/#load-exported-model","title":"Load Exported Model","text":"<pre><code>import json\nfrom artifex.generative_models.core.checkpointing import (\n    setup_checkpoint_manager,\n    load_checkpoint,\n)\n\ndef load_exported_model(export_dir: str):\n    \"\"\"Load exported model with metadata.\"\"\"\n    # Load configuration\n    with open(f\"{export_dir}/config.json\") as f:\n        config = json.load(f)\n\n    # Create model template\n    from artifex.generative_models.models.vae import create_vae_model\n    from artifex.generative_models.core.configuration import ModelConfig\n\n    model_config = ModelConfig(**config)\n    model_template = create_vae_model(model_config, rngs=nnx.Rngs(0))\n\n    # Load checkpoint\n    checkpoint_manager, _ = setup_checkpoint_manager(export_dir)\n    model, step = load_checkpoint(checkpoint_manager, model_template)\n\n    # Load metrics\n    with open(f\"{export_dir}/metrics.json\") as f:\n        metrics = json.load(f)\n\n    return model, config, metrics\n\n# Load\nmodel, config, metrics = load_exported_model(\"./models/production/vae_v1\")\nprint(f\"Loaded model trained for {metrics['total_steps']} steps\")\n</code></pre>"},{"location":"user-guide/integrations/deployment/#model-serving","title":"Model Serving","text":"<p>Serve models for inference requests.</p>"},{"location":"user-guide/integrations/deployment/#simple-inference-server","title":"Simple Inference Server","text":"<pre><code>from flask import Flask, request, jsonify\nimport jax.numpy as jnp\nimport numpy as np\nfrom PIL import Image\nimport io\n\napp = Flask(__name__)\n\n# Load model at startup\nmodel, config, metrics = load_exported_model(\"./models/production/vae_v1\")\n\n@app.route(\"/health\", methods=[\"GET\"])\ndef health():\n    \"\"\"Health check endpoint.\"\"\"\n    return jsonify({\"status\": \"healthy\", \"model_loaded\": model is not None})\n\n@app.route(\"/generate\", methods=[\"POST\"])\ndef generate():\n    \"\"\"Generate samples from the model.\"\"\"\n    data = request.get_json()\n\n    # Get parameters\n    num_samples = data.get(\"num_samples\", 1)\n    seed = data.get(\"seed\", 0)\n\n    # Generate samples\n    key = jax.random.key(seed)\n    z = jax.random.normal(key, (num_samples, config[\"latent_dim\"]))\n\n    samples = model.decode(z)\n    samples = np.array(samples)\n\n    # Convert to list for JSON\n    samples_list = samples.tolist()\n\n    return jsonify({\n        \"samples\": samples_list,\n        \"num_samples\": num_samples,\n        \"shape\": list(samples.shape),\n    })\n\n@app.route(\"/encode\", methods=[\"POST\"])\ndef encode():\n    \"\"\"Encode image to latent representation.\"\"\"\n    # Get image from request\n    file = request.files[\"image\"]\n    image = Image.open(io.BytesIO(file.read()))\n\n    # Preprocess\n    image_array = np.array(image).astype(np.float32) / 255.0\n    image_array = image_array.reshape(1, *config[\"input_shape\"])\n\n    # Encode\n    output = model.encode(jnp.array(image_array))\n    latent = np.array(output[\"mean\"][0])\n\n    return jsonify({\n        \"latent\": latent.tolist(),\n        \"latent_dim\": len(latent),\n    })\n\n@app.route(\"/reconstruct\", methods=[\"POST\"])\ndef reconstruct():\n    \"\"\"Reconstruct image from input.\"\"\"\n    file = request.files[\"image\"]\n    image = Image.open(io.BytesIO(file.read()))\n\n    # Preprocess\n    image_array = np.array(image).astype(np.float32) / 255.0\n    image_array = image_array.reshape(1, *config[\"input_shape\"])\n\n    # Reconstruct\n    output = model(jnp.array(image_array))\n    reconstruction = np.array(output[\"reconstruction\"][0])\n\n    return jsonify({\n        \"reconstruction\": reconstruction.tolist(),\n        \"shape\": list(reconstruction.shape),\n    })\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"user-guide/integrations/deployment/#batch-inference","title":"Batch Inference","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom typing import Iterator\n\ndef batch_inference(\n    model: nnx.Module,\n    data_iterator: Iterator,\n    batch_size: int = 32,\n) -&gt; list:\n    \"\"\"Process data in batches for efficient inference.\"\"\"\n    results = []\n\n    batch = []\n    for sample in data_iterator:\n        batch.append(sample)\n\n        if len(batch) &gt;= batch_size:\n            # Process batch\n            batch_array = jnp.array(batch)\n            output = model(batch_array)\n\n            # Store results\n            results.extend(np.array(output[\"reconstruction\"]))\n\n            # Clear batch\n            batch = []\n\n    # Process remaining samples\n    if batch:\n        batch_array = jnp.array(batch)\n        output = model(batch_array)\n        results.extend(np.array(output[\"reconstruction\"]))\n\n    return results\n\n# Usage\ndef data_generator():\n    \"\"\"Generator for inference data.\"\"\"\n    for i in range(1000):\n        yield np.random.randn(28, 28, 1)\n\nresults = batch_inference(model, data_generator(), batch_size=64)\nprint(f\"Processed {len(results)} samples\")\n</code></pre>"},{"location":"user-guide/integrations/deployment/#optimization","title":"Optimization","text":"<p>Optimize models for production performance.</p>"},{"location":"user-guide/integrations/deployment/#jit-compilation","title":"JIT Compilation","text":"<pre><code>import jax\n\n# JIT-compile inference functions\n@jax.jit\ndef generate_jit(model_state, z):\n    \"\"\"JIT-compiled generation.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n    return model.decode(z)\n\n@jax.jit\ndef encode_jit(model_state, x):\n    \"\"\"JIT-compiled encoding.\"\"\"\n    model = nnx.merge(model_graphdef, model_state)\n    return model.encode(x)\n\n# Split model once\nmodel_graphdef, model_state = nnx.split(model)\n\n# Fast inference\nz = jax.random.normal(jax.random.key(0), (10, 20))\nsamples = generate_jit(model_state, z)\n\n# First call: compilation + execution (~slow)\n# Subsequent calls: cached execution (~fast)\n</code></pre>"},{"location":"user-guide/integrations/deployment/#batched-generation","title":"Batched Generation","text":"<pre><code>@jax.jit\ndef batched_generate(model_state, keys):\n    \"\"\"Generate multiple samples in parallel.\"\"\"\n    # Vectorize over batch\n    def generate_single(key):\n        z = jax.random.normal(key, (latent_dim,))\n        model = nnx.merge(model_graphdef, model_state)\n        return model.decode(z[None, :])[0]\n\n    # vmap over keys\n    samples = jax.vmap(generate_single)(keys)\n    return samples\n\n# Generate 100 samples in parallel\nkeys = jax.random.split(jax.random.key(0), 100)\nsamples = batched_generate(model_state, keys)\n</code></pre>"},{"location":"user-guide/integrations/deployment/#mixed-precision","title":"Mixed Precision","text":"<pre><code>import jax.numpy as jnp\n\ndef convert_to_bfloat16(model_state):\n    \"\"\"Convert model to bfloat16 for faster inference.\"\"\"\n    def to_bf16(x):\n        if x.dtype == jnp.float32:\n            return x.astype(jnp.bfloat16)\n        return x\n\n    return jax.tree.map(to_bf16, model_state)\n\n# Convert model\nmodel_state_bf16 = convert_to_bfloat16(model_state)\n\n# Inference in bfloat16 (2x faster on modern GPUs)\n@jax.jit\ndef generate_bf16(model_state, z):\n    z = z.astype(jnp.bfloat16)\n    model = nnx.merge(model_graphdef, model_state)\n    output = model.decode(z)\n    return output.astype(jnp.float32)  # Convert back for output\n\nz = jax.random.normal(jax.random.key(0), (10, 20))\nsamples = generate_bf16(model_state_bf16, z)\n</code></pre>"},{"location":"user-guide/integrations/deployment/#cloud-deployment","title":"Cloud Deployment","text":"<p>Deploy models to cloud platforms.</p>"},{"location":"user-guide/integrations/deployment/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy model and code\nCOPY models/ models/\nCOPY serve.py .\n\n# Expose port\nEXPOSE 8000\n\n# Run server\nCMD [\"python\", \"serve.py\"]\n</code></pre> <pre><code># Build image\ndocker build -t artifex-vae-server .\n\n# Run container\ndocker run -p 8000:8000 artifex-vae-server\n\n# Test\ncurl -X POST http://localhost:8000/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"num_samples\": 5}'\n</code></pre>"},{"location":"user-guide/integrations/deployment/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: artifex-vae-server\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: artifex-vae-server\n  template:\n    metadata:\n      labels:\n        app: artifex-vae-server\n    spec:\n      containers:\n      - name: server\n        image: artifex-vae-server:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: artifex-vae-service\nspec:\n  selector:\n    app: artifex-vae-server\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000\n  type: LoadBalancer\n</code></pre> <pre><code># Deploy to Kubernetes\nkubectl apply -f deployment.yaml\n\n# Check status\nkubectl get pods\nkubectl get services\n</code></pre>"},{"location":"user-guide/integrations/deployment/#cloud-functions-serverless","title":"Cloud Functions (Serverless)","text":"<pre><code># Google Cloud Function\nimport functions_framework\nimport jax.numpy as jnp\nimport numpy as np\n\n# Load model once (cold start)\nmodel = None\n\ndef load_model():\n    \"\"\"Load model on cold start.\"\"\"\n    global model\n    if model is None:\n        model, _, _ = load_exported_model(\"gs://my-bucket/models/vae_v1\")\n    return model\n\n@functions_framework.http\ndef generate_samples(request):\n    \"\"\"Cloud Function for generation.\"\"\"\n    # Load model\n    model = load_model()\n\n    # Parse request\n    request_json = request.get_json()\n    num_samples = request_json.get(\"num_samples\", 1)\n    seed = request_json.get(\"seed\", 0)\n\n    # Generate\n    key = jax.random.key(seed)\n    z = jax.random.normal(key, (num_samples, 20))\n    samples = model.decode(z)\n\n    return {\n        \"samples\": np.array(samples).tolist(),\n        \"num_samples\": num_samples,\n    }\n</code></pre>"},{"location":"user-guide/integrations/deployment/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>Monitor deployed models in production.</p>"},{"location":"user-guide/integrations/deployment/#logging-inference-metrics","title":"Logging Inference Metrics","text":"<pre><code>import time\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef monitored_inference(model, input_data):\n    \"\"\"Inference with monitoring.\"\"\"\n    start_time = time.time()\n\n    try:\n        # Inference\n        output = model(input_data)\n\n        # Log success\n        duration = time.time() - start_time\n        logger.info(\n            f\"Inference successful: \"\n            f\"batch_size={input_data.shape[0]}, \"\n            f\"duration={duration:.3f}s\"\n        )\n\n        return output\n\n    except Exception as e:\n        # Log error\n        duration = time.time() - start_time\n        logger.error(\n            f\"Inference failed: \"\n            f\"error={str(e)}, \"\n            f\"duration={duration:.3f}s\"\n        )\n        raise\n</code></pre>"},{"location":"user-guide/integrations/deployment/#prometheus-metrics","title":"Prometheus Metrics","text":"<pre><code>from prometheus_client import Counter, Histogram, start_http_server\n\n# Define metrics\ninference_counter = Counter(\n    \"model_inference_total\",\n    \"Total number of inference requests\"\n)\n\ninference_duration = Histogram(\n    \"model_inference_duration_seconds\",\n    \"Inference duration in seconds\"\n)\n\ninference_errors = Counter(\n    \"model_inference_errors_total\",\n    \"Total number of inference errors\"\n)\n\ndef monitored_inference_prometheus(model, input_data):\n    \"\"\"Inference with Prometheus metrics.\"\"\"\n    inference_counter.inc()\n\n    with inference_duration.time():\n        try:\n            output = model(input_data)\n            return output\n        except Exception as e:\n            inference_errors.inc()\n            raise\n\n# Start Prometheus metrics server\nstart_http_server(9090)\n</code></pre>"},{"location":"user-guide/integrations/deployment/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/integrations/deployment/#do","title":"DO","text":"<ul> <li>\u2705 Export models with metadata - include config and metrics</li> <li>\u2705 Use JIT compilation - significant speedup for inference</li> <li>\u2705 Implement health checks - monitor server status</li> <li>\u2705 Add logging - track inference requests and errors</li> <li>\u2705 Use batching - process multiple requests efficiently</li> <li>\u2705 Set resource limits - prevent out-of-memory errors</li> <li>\u2705 Version models - track deployed model versions</li> <li>\u2705 Monitor latency - track inference performance</li> <li>\u2705 Use load balancers - distribute traffic across replicas</li> <li>\u2705 Test before deploying - validate in staging environment</li> </ul>"},{"location":"user-guide/integrations/deployment/#dont","title":"DON'T","text":"<ul> <li>\u274c Don't skip JIT - leave performance on the table</li> <li>\u274c Don't ignore errors - log and handle gracefully</li> <li>\u274c Don't process one at a time - use batching</li> <li>\u274c Don't deploy without health checks - can't monitor status</li> <li>\u274c Don't hardcode configurations - use environment variables</li> <li>\u274c Don't skip resource limits - can crash containers</li> <li>\u274c Don't deploy untested models - validate first</li> <li>\u274c Don't ignore monitoring - can't debug production issues</li> <li>\u274c Don't use debug mode - slow and verbose</li> <li>\u274c Don't expose internal errors - sanitize error messages</li> </ul>"},{"location":"user-guide/integrations/deployment/#summary","title":"Summary","text":"<p>Model deployment in Artifex:</p> <ol> <li>Export: Save models with checkpoints and metadata</li> <li>Serve: REST APIs, batch processing, cloud functions</li> <li>Optimize: JIT compilation, batching, mixed precision</li> <li>Deploy: Docker, Kubernetes, serverless platforms</li> <li>Monitor: Logging, metrics, health checks</li> </ol> <p>Key considerations:</p> <ul> <li>Performance: JIT, batching, mixed precision</li> <li>Reliability: Health checks, error handling, monitoring</li> <li>Scalability: Load balancing, auto-scaling, replicas</li> <li>Maintainability: Versioning, logging, configuration</li> </ul>"},{"location":"user-guide/integrations/deployment/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Inference Guide</p> <p>Learn more about inference and generation</p> <p> Inference guide</p> </li> <li> <p> Training Guide</p> <p>Return to training documentation</p> <p> Training guide</p> </li> <li> <p> API Reference</p> <p>Explore the complete API documentation</p> <p> API docs</p> </li> <li> <p> FAQ</p> <p>Common questions about deployment</p> <p> FAQ</p> </li> </ul>"},{"location":"user-guide/integrations/huggingface/","title":"HuggingFace Integration","text":"<p>Complete guide to integrating Artifex with HuggingFace Hub for model sharing, dataset loading, and deployment to Spaces.</p>"},{"location":"user-guide/integrations/huggingface/#overview","title":"Overview","text":"<p>HuggingFace provides a powerful ecosystem for sharing models, datasets, and demos. Artifex integrates seamlessly with HuggingFace Hub, enabling you to share your generative models with the community and leverage thousands of existing datasets.</p> <p>HuggingFace Benefits</p> <ul> <li>Model Hub: Share and discover pretrained models</li> <li>Datasets: Access 50,000+ datasets</li> <li>Spaces: Deploy interactive demos</li> <li>Community: Connect with researchers worldwide</li> </ul> <ul> <li> <p> Model Hub</p> <p>Upload and download models from HuggingFace Hub</p> <p> Model Hub Guide</p> </li> <li> <p> Datasets</p> <p>Load and preprocess HuggingFace datasets</p> <p> Datasets Guide</p> </li> <li> <p> Spaces</p> <p>Deploy interactive demos with Gradio or Streamlit</p> <p> Spaces Guide</p> </li> <li> <p> Model Cards</p> <p>Document models with metadata and examples</p> <p> Model Cards</p> </li> </ul>"},{"location":"user-guide/integrations/huggingface/#prerequisites","title":"Prerequisites","text":"<p>Install HuggingFace libraries:</p> <pre><code># Install HuggingFace ecosystem\npip install huggingface_hub datasets gradio\n\n# Or using uv\nuv pip install huggingface_hub datasets gradio\n</code></pre> <p>Authenticate with HuggingFace Hub:</p> <pre><code># Login to HuggingFace\nhuggingface-cli login\n\n# Or set token in code\nfrom huggingface_hub import login\nlogin(token=\"hf_...\")\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#model-hub-integration","title":"Model Hub Integration","text":""},{"location":"user-guide/integrations/huggingface/#uploading-models-to-hub","title":"Uploading Models to Hub","text":"<p>Share your trained models with the community.</p> <pre><code>from huggingface_hub import HfApi, create_repo\nfrom flax import nnx\nimport jax.numpy as jnp\nfrom pathlib import Path\nimport json\n\nclass ModelUploader:\n    \"\"\"Upload Artifex models to HuggingFace Hub.\"\"\"\n\n    def __init__(self, model, model_name: str):\n        self.model = model\n        self.model_name = model_name\n        self.api = HfApi()\n\n    def upload_to_hub(\n        self,\n        repo_id: str,\n        commit_message: str = \"Upload model\",\n        private: bool = False,\n    ):\n        \"\"\"Upload model to HuggingFace Hub.\n\n        Args:\n            repo_id: Repository ID (e.g., \"username/model-name\")\n            commit_message: Commit message\n            private: Make repository private\n\n        Returns:\n            URL to uploaded model\n        \"\"\"\n        # Create repository\n        repo_url = create_repo(\n            repo_id=repo_id,\n            exist_ok=True,\n            private=private,\n        )\n\n        # Save model locally\n        save_dir = Path(f\"./tmp/{self.model_name}\")\n        save_dir.mkdir(parents=True, exist_ok=True)\n\n        # Export model parameters\n        self._save_model(save_dir)\n\n        # Create model card\n        self._create_model_card(save_dir, repo_id)\n\n        # Upload to Hub\n        self.api.upload_folder(\n            folder_path=str(save_dir),\n            repo_id=repo_id,\n            commit_message=commit_message,\n        )\n\n        print(f\"Model uploaded to: {repo_url}\")\n        return repo_url\n\n    def _save_model(self, save_dir: Path):\n        \"\"\"Save model parameters and config.\"\"\"\n        # Extract and save parameters\n        state = nnx.state(self.model)\n\n        # Convert to serializable format\n        serialized_state = {\n            k: v.tolist() if isinstance(v, jnp.ndarray) else v\n            for k, v in state.items()\n        }\n\n        # Save parameters\n        with open(save_dir / \"model_params.json\", \"w\") as f:\n            json.dump(serialized_state, f)\n\n        # Save configuration\n        config = {\n            \"model_type\": self.model.__class__.__name__,\n            \"latent_dim\": getattr(self.model, \"latent_dim\", None),\n            \"image_shape\": getattr(self.model, \"image_shape\", None),\n            \"framework\": \"artifex\",\n            \"backend\": \"flax-nnx\",\n        }\n\n        with open(save_dir / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=2)\n\n    def _create_model_card(self, save_dir: Path, repo_id: str):\n        \"\"\"Create README.md model card.\"\"\"\n        model_card = self._generate_model_card_content(repo_id)\n\n        # Write model card to file\n        with open(save_dir / \"README.md\", \"w\") as f:\n            f.write(model_card)\n\n    def _generate_model_card_content(self, repo_id: str) -&gt; str:\n        \"\"\"Generate model card markdown content.\"\"\"\n        return f\"\"\"---\nlibrary_name: artifex\ntags: [generative-models, jax, flax]\nlicense: apache-2.0\n---\n\n# {repo_id}\n\nArtifex generative model trained with JAX/Flax NNX.\n\n## Model Details\n- **Type**: {self.model.__class__.__name__}\n- **Framework**: Artifex (JAX/Flax NNX)\n\n## Usage\nSee [Model Cards section](#model-cards-and-metadata) for complete example.\n\"\"\"\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#downloading-models-from-hub","title":"Downloading Models from Hub","text":"<p>Load pretrained models shared by others.</p> <pre><code>from huggingface_hub import hf_hub_download\nimport json\n\nclass ModelDownloader:\n    \"\"\"Download Artifex models from HuggingFace Hub.\"\"\"\n\n    @staticmethod\n    def download_model(\n        repo_id: str,\n        revision: str = \"main\",\n    ):\n        \"\"\"Download model from HuggingFace Hub.\n\n        Args:\n            repo_id: Repository ID\n            revision: Git revision (branch, tag, or commit)\n\n        Returns:\n            Loaded model\n        \"\"\"\n        # Download config\n        config_path = hf_hub_download(\n            repo_id=repo_id,\n            filename=\"config.json\",\n            revision=revision,\n        )\n\n        with open(config_path) as f:\n            config = json.load(f)\n\n        # Download parameters\n        params_path = hf_hub_download(\n            repo_id=repo_id,\n            filename=\"model_params.json\",\n            revision=revision,\n        )\n\n        with open(params_path) as f:\n            params = json.load(f)\n\n        # Reconstruct model\n        model = ModelDownloader._build_model(config, params)\n\n        return model\n\n    @staticmethod\n    def _build_model(config: dict, params: dict):\n        \"\"\"Build model from config and parameters.\"\"\"\n        from artifex.generative_models.models.vae import VAE\n        from artifex.generative_models.models.gan import GAN\n\n        model_type = config[\"model_type\"]\n\n        if model_type == \"VAE\":\n            model = VAE(\n                latent_dim=config[\"latent_dim\"],\n                image_shape=config[\"image_shape\"],\n                rngs=nnx.Rngs(0),\n            )\n        elif model_type == \"GAN\":\n            model = GAN(\n                latent_dim=config[\"latent_dim\"],\n                image_shape=config[\"image_shape\"],\n                rngs=nnx.Rngs(0),\n            )\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        # Load parameters\n        # (Simplified - actual implementation needs proper parameter loading)\n\n        return model\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#datasets-integration","title":"Datasets Integration","text":""},{"location":"user-guide/integrations/huggingface/#loading-huggingface-datasets","title":"Loading HuggingFace Datasets","text":"<p>Access thousands of datasets for training.</p> <pre><code>from datasets import load_dataset\nimport jax.numpy as jnp\nimport numpy as np\n\nclass HFDatasetLoader:\n    \"\"\"Load HuggingFace datasets for Artifex.\"\"\"\n\n    @staticmethod\n    def load_image_dataset(\n        dataset_name: str,\n        split: str = \"train\",\n        image_key: str = \"image\",\n    ):\n        \"\"\"Load image dataset from HuggingFace.\n\n        Args:\n            dataset_name: Dataset name (e.g., \"mnist\", \"cifar10\")\n            split: Dataset split\n            image_key: Key for image column\n\n        Returns:\n            JAX-compatible dataset\n        \"\"\"\n        # Load dataset\n        dataset = load_dataset(dataset_name, split=split)\n\n        # Convert to JAX arrays\n        def process_example(example):\n            image = np.array(example[image_key])\n\n            # Normalize to [-1, 1]\n            image = (image.astype(np.float32) / 255.0) * 2 - 1\n\n            return {\"image\": image}\n\n        # Process dataset\n        dataset = dataset.map(process_example)\n        dataset.set_format(type=\"numpy\")\n\n        return dataset\n\n    @staticmethod\n    def load_text_dataset(\n        dataset_name: str,\n        split: str = \"train\",\n        text_key: str = \"text\",\n        max_length: int = 512,\n    ):\n        \"\"\"Load text dataset.\n\n        Args:\n            dataset_name: Dataset name\n            split: Dataset split\n            text_key: Key for text column\n            max_length: Maximum sequence length\n\n        Returns:\n            Processed text dataset\n        \"\"\"\n        from transformers import AutoTokenizer\n\n        # Load dataset\n        dataset = load_dataset(dataset_name, split=split)\n\n        # Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n        def tokenize(example):\n            tokens = tokenizer(\n                example[text_key],\n                truncation=True,\n                max_length=max_length,\n                padding=\"max_length\",\n                return_tensors=\"np\",\n            )\n            return {\"input_ids\": tokens[\"input_ids\"][0]}\n\n        dataset = dataset.map(tokenize)\n        dataset.set_format(type=\"numpy\")\n\n        return dataset\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#custom-dataset-creation","title":"Custom Dataset Creation","text":"<p>Create and upload your own datasets.</p> <pre><code>from datasets import Dataset, Features, Image, Value\nfrom pathlib import Path\n\nclass CustomDatasetCreator:\n    \"\"\"Create custom HuggingFace datasets.\"\"\"\n\n    @staticmethod\n    def create_image_dataset(\n        image_dir: Path,\n        save_path: str = \"my_dataset\",\n    ):\n        \"\"\"Create dataset from image directory.\n\n        Args:\n            image_dir: Directory containing images\n            save_path: Path to save dataset\n\n        Returns:\n            Created dataset\n        \"\"\"\n        # Collect image paths\n        image_paths = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n\n        # Create dataset dictionary\n        data = {\n            \"image\": [str(p) for p in image_paths],\n            \"filename\": [p.name for p in image_paths],\n        }\n\n        # Define features\n        features = Features({\n            \"image\": Image(),\n            \"filename\": Value(\"string\"),\n        })\n\n        # Create dataset\n        dataset = Dataset.from_dict(data, features=features)\n\n        # Save dataset\n        dataset.save_to_disk(save_path)\n\n        return dataset\n\n    @staticmethod\n    def upload_dataset(\n        dataset_path: str,\n        repo_id: str,\n        private: bool = False,\n    ):\n        \"\"\"Upload dataset to HuggingFace Hub.\n\n        Args:\n            dataset_path: Path to saved dataset\n            repo_id: Repository ID\n            private: Make repository private\n        \"\"\"\n        from datasets import load_from_disk\n\n        # Load dataset\n        dataset = load_from_disk(dataset_path)\n\n        # Upload to Hub\n        dataset.push_to_hub(\n            repo_id=repo_id,\n            private=private,\n        )\n\n        print(f\"Dataset uploaded to: https://huggingface.co/datasets/{repo_id}\")\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#streaming-large-datasets","title":"Streaming Large Datasets","text":"<p>Efficiently handle datasets too large for memory.</p> <pre><code>from datasets import load_dataset\n\ndef stream_large_dataset(\n    dataset_name: str,\n    split: str = \"train\",\n    batch_size: int = 32,\n):\n    \"\"\"Stream large dataset in batches.\n\n    Args:\n        dataset_name: Dataset name\n        split: Dataset split\n        batch_size: Batch size for streaming\n\n    Yields:\n        Batches of data\n    \"\"\"\n    # Load in streaming mode\n    dataset = load_dataset(\n        dataset_name,\n        split=split,\n        streaming=True,\n    )\n\n    # Batch and iterate\n    batch = []\n    for example in dataset:\n        batch.append(example)\n\n        if len(batch) &gt;= batch_size:\n            yield jnp.array([ex[\"image\"] for ex in batch])\n            batch = []\n\n    # Yield final batch\n    if batch:\n        yield jnp.array([ex[\"image\"] for ex in batch])\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#spaces-integration","title":"Spaces Integration","text":""},{"location":"user-guide/integrations/huggingface/#creating-gradio-demos","title":"Creating Gradio Demos","text":"<p>Deploy interactive demos with Gradio.</p> <pre><code>import gradio as gr\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass GradioDemo:\n    \"\"\"Create Gradio demo for generative model.\"\"\"\n\n    def __init__(self, model):\n        self.model = model\n\n    def create_interface(self):\n        \"\"\"Create Gradio interface.\n\n        Returns:\n            Gradio interface\n        \"\"\"\n        def generate(\n            num_samples: int,\n            temperature: float,\n            seed: int,\n        ):\n            \"\"\"Generate samples.\"\"\"\n            rngs = nnx.Rngs(seed)\n\n            # Sample latent codes\n            z = jax.random.normal(\n                rngs.sample(),\n                (num_samples, self.model.latent_dim)\n            ) * temperature\n\n            # Generate images\n            images = self.model.decode(z)\n\n            # Convert to numpy and denormalize\n            images = np.array(images)\n            images = ((images + 1) / 2 * 255).astype(np.uint8)\n\n            return images\n\n        # Create interface\n        interface = gr.Interface(\n            fn=generate,\n            inputs=[\n                gr.Slider(1, 16, value=4, step=1, label=\"Number of Samples\"),\n                gr.Slider(0.1, 2.0, value=1.0, step=0.1, label=\"Temperature\"),\n                gr.Slider(0, 1000, value=42, step=1, label=\"Random Seed\"),\n            ],\n            outputs=gr.Gallery(label=\"Generated Images\"),\n            title=\"Artifex Generative Model\",\n            description=\"Generate images using a trained generative model\",\n        )\n\n        return interface\n\n    def launch(self, share: bool = True):\n        \"\"\"Launch demo.\n\n        Args:\n            share: Create public link\n        \"\"\"\n        interface = self.create_interface()\n        interface.launch(share=share)\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#deploying-to-spaces","title":"Deploying to Spaces","text":"<p>Deploy your demo to HuggingFace Spaces.</p> <pre><code># app.py - Main Gradio app file\nfrom huggingface_hub import hf_hub_download\nfrom artifex.generative_models.models import load_model\nimport gradio as gr\n\n# Download model\nmodel_path = hf_hub_download(\n    repo_id=\"username/my-model\",\n    filename=\"model_params.json\"\n)\n\n# Load model\nmodel = load_model(model_path)\n\n# Create demo\ndemo = GradioDemo(model)\ninterface = demo.create_interface()\n\n# Launch\nif __name__ == \"__main__\":\n    interface.launch()\n</code></pre> <p>Create <code>requirements.txt</code>:</p> <pre><code>artifex\ngradio\njax[cpu]\nflax\nhuggingface_hub\n</code></pre> <p>Deploy to Spaces:</p> <pre><code># Create Space\nhuggingface-cli repo create my-demo --type space --space_sdk gradio\n\n# Clone and setup\ngit clone https://huggingface.co/spaces/username/my-demo\ncd my-demo\n\n# Add files\ncp app.py .\ncp requirements.txt .\n\n# Commit and push\ngit add .\ngit commit -m \"Initial demo\"\ngit push\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#advanced-gradio-features","title":"Advanced Gradio Features","text":"<p>Create more interactive demos.</p> <pre><code>class AdvancedGradioDemo:\n    \"\"\"Advanced Gradio demo with multiple features.\"\"\"\n\n    def __init__(self, model):\n        self.model = model\n\n    def create_interface(self):\n        \"\"\"Create advanced interface.\"\"\"\n\n        with gr.Blocks() as demo:\n            gr.Markdown(\"# Artifex Generative Model Demo\")\n\n            with gr.Tab(\"Generate\"):\n                with gr.Row():\n                    with gr.Column():\n                        num_samples = gr.Slider(1, 16, value=4, label=\"Samples\")\n                        temperature = gr.Slider(0.1, 2.0, value=1.0, label=\"Temperature\")\n                        seed = gr.Number(value=42, label=\"Seed\")\n                        generate_btn = gr.Button(\"Generate\")\n\n                    with gr.Column():\n                        output_gallery = gr.Gallery(label=\"Generated Images\")\n\n                generate_btn.click(\n                    fn=self.generate,\n                    inputs=[num_samples, temperature, seed],\n                    outputs=output_gallery,\n                )\n\n            with gr.Tab(\"Interpolate\"):\n                with gr.Row():\n                    seed1 = gr.Number(value=42, label=\"Start Seed\")\n                    seed2 = gr.Number(value=123, label=\"End Seed\")\n                    steps = gr.Slider(5, 20, value=10, label=\"Steps\")\n                    interpolate_btn = gr.Button(\"Interpolate\")\n\n                interpolation_output = gr.Gallery(label=\"Interpolation\")\n\n                interpolate_btn.click(\n                    fn=self.interpolate,\n                    inputs=[seed1, seed2, steps],\n                    outputs=interpolation_output,\n                )\n\n        return demo\n\n    def generate(self, num_samples, temperature, seed):\n        \"\"\"Generate samples.\"\"\"\n        # Implementation from previous example\n        pass\n\n    def interpolate(self, seed1, seed2, steps):\n        \"\"\"Interpolate between two latent codes.\"\"\"\n        rngs1 = nnx.Rngs(int(seed1))\n        rngs2 = nnx.Rngs(int(seed2))\n\n        z1 = jax.random.normal(rngs1.sample(), (1, self.model.latent_dim))\n        z2 = jax.random.normal(rngs2.sample(), (1, self.model.latent_dim))\n\n        alphas = jnp.linspace(0, 1, steps)[:, None]\n        z_interp = z1 * (1 - alphas) + z2 * alphas\n\n        images = self.model.decode(z_interp)\n        images = np.array(images)\n        images = ((images + 1) / 2 * 255).astype(np.uint8)\n\n        return images\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#model-cards-and-metadata","title":"Model Cards and Metadata","text":""},{"location":"user-guide/integrations/huggingface/#complete-model-card-template","title":"Complete Model Card Template","text":"<pre><code>---\nlibrary_name: artifex\ntags:\n  - generative-models\n  - vae\n  - image-generation\n  - jax\n  - flax\ndatasets:\n  - mnist\nmetrics:\n  - fid\n  - inception_score\nlicense: apache-2.0\n---\n\n# VAE for MNIST Generation\n\nVariational Autoencoder trained on MNIST for digit generation.\n\n## Model Details\n\n- **Model Type**: Variational Autoencoder (VAE)\n- **Architecture**: Convolutional encoder/decoder\n- **Latent Dimension**: 128\n- **Framework**: Artifex (JAX/Flax NNX)\n- **Training Data**: MNIST (60,000 images)\n- **Parameters**: 2.4M\n\n## Intended Use\n\nGenerate realistic handwritten digits or encode images to latent space.\n\n## Training Details\n\n- **Optimizer**: Adam (lr=1e-4)\n- **Batch Size**: 128\n- **Training Steps**: 50,000\n- **Hardware**: NVIDIA A100 (1x)\n- **Training Time**: 2 hours\n\n## Evaluation\n\n| Metric | Value |\n|--------|-------|\n| FID | 12.3 |\n| Reconstruction MSE | 0.012 |\n| KL Divergence | 8.5 |\n\n## Usage\n\n```python\nfrom huggingface_hub import hf_hub_download\nfrom artifex.generative_models.models import load_model\n\nmodel = load_model(\"username/vae-mnist\")\nsamples = model.sample(num_samples=16)\n</code></pre>"},{"location":"user-guide/integrations/huggingface/#limitations","title":"Limitations","text":"<ul> <li>Trained only on MNIST (28\u00d728 grayscale)</li> <li>May not generalize to other digit styles</li> <li>Limited to digit generation (0-9)</li> </ul>"},{"location":"user-guide/integrations/huggingface/#citation","title":"Citation","text":"<pre><code>@software{vae_mnist,\n  author = {Your Name},\n  title = {VAE for MNIST},\n  year = {2025},\n  url = {https://huggingface.co/username/vae-mnist}\n}\n</code></pre> <p>```</p>"},{"location":"user-guide/integrations/huggingface/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/integrations/huggingface/#do","title":"DO","text":"<p>Recommended Practices</p> <p>\u2705 Create detailed model cards with usage examples</p> <p>\u2705 Version your models with Git tags</p> <p>\u2705 Include evaluation metrics (FID, IS, etc.)</p> <p>\u2705 Provide example outputs in model card</p> <p>\u2705 Document limitations and intended use</p> <p>\u2705 Use meaningful tags for discoverability</p>"},{"location":"user-guide/integrations/huggingface/#dont","title":"DON'T","text":"<p>Avoid These Mistakes</p> <p>\u274c Don't upload without model card (required for Hub)</p> <p>\u274c Don't hardcode API tokens (use environment variables)</p> <p>\u274c Don't upload large files directly (use Git LFS)</p> <p>\u274c Don't skip dataset licensing information</p> <p>\u274c Don't ignore versioning (use semantic versioning)</p>"},{"location":"user-guide/integrations/huggingface/#summary","title":"Summary","text":"<p>HuggingFace integration enables:</p> <ul> <li>Model Sharing: Upload and discover pretrained models</li> <li>Dataset Access: Leverage 50,000+ datasets</li> <li>Demo Deployment: Create interactive Gradio demos</li> <li>Community: Connect with researchers worldwide</li> </ul> <p>Start sharing your models and building demos today!</p>"},{"location":"user-guide/integrations/huggingface/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Weights &amp; Biases</p> <p>Track experiments and hyperparameter sweeps</p> <p> W&amp;B Integration</p> </li> <li> <p> TensorBoard</p> <p>Visualize training metrics and samples</p> <p> TensorBoard Guide</p> </li> <li> <p> Deployment</p> <p>Deploy models to production servers</p> <p> Deployment Guide</p> </li> <li> <p> API Reference</p> <p>Explore complete API documentation</p> <p> API Docs</p> </li> </ul>"},{"location":"user-guide/integrations/tensorboard/","title":"TensorBoard Integration","text":"<p>Visualize training metrics, generated samples, and model architecture using TensorBoard with Artifex.</p>"},{"location":"user-guide/integrations/tensorboard/#overview","title":"Overview","text":"<p>TensorBoard provides powerful visualization tools for machine learning experiments. Artifex integrates with TensorBoard to log metrics, visualize generated samples, and track training progress.</p> <p>TensorBoard Benefits</p> <ul> <li>Real-time Monitoring: Watch training progress live</li> <li>Visualization: Interactive charts and image galleries</li> <li>Lightweight: No external services required</li> <li>Integration: Works seamlessly with JAX/Flax</li> </ul> <ul> <li> <p> Metrics Logging</p> <p>Track scalars, histograms, and custom metrics</p> <p> Logging Guide</p> </li> <li> <p> Visualization</p> <p>Visualize samples, latent spaces, and attention</p> <p> Visualization Guide</p> </li> <li> <p> Training Integration</p> <p>Integrate TensorBoard with Artifex training</p> <p> Integration Guide</p> </li> </ul>"},{"location":"user-guide/integrations/tensorboard/#quick-start-with-built-in-callback","title":"Quick Start with Built-in Callback","text":"<p>For most use cases, use the built-in <code>TensorBoardLoggerCallback</code>:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    TensorBoardLoggerCallback,\n    TensorBoardLoggerConfig,\n)\n\ncallback = TensorBoardLoggerCallback(TensorBoardLoggerConfig(\n    log_dir=\"logs/experiment-1\",\n    flush_secs=60,\n    log_every_n_steps=10,\n))\n\ntrainer.fit(callbacks=[callback])\n\n# View with: tensorboard --logdir logs\n</code></pre> <p>See Logging Callbacks for full documentation.</p>"},{"location":"user-guide/integrations/tensorboard/#profiling-traces","title":"Profiling Traces","text":"<p>Use <code>JAXProfiler</code> to capture performance traces viewable in TensorBoard's Profile tab:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    JAXProfiler,\n    ProfilingConfig,\n)\n\nprofiler = JAXProfiler(ProfilingConfig(\n    log_dir=\"logs/profiles\",\n    start_step=10,  # Skip JIT warmup\n    end_step=20,    # Profile 10 steps\n))\n\ntrainer.fit(callbacks=[profiler])\n\n# View traces: tensorboard --logdir logs/profiles\n</code></pre> <p>The Profile tab shows:</p> <ul> <li>XLA compilation times</li> <li>Device (GPU/TPU) execution times</li> <li>Memory allocation patterns</li> <li>Kernel execution traces</li> </ul> <p>See Profiling Callbacks for complete documentation.</p> <p>The sections below cover advanced TensorBoard features not available through the callback.</p>"},{"location":"user-guide/integrations/tensorboard/#prerequisites","title":"Prerequisites","text":"<pre><code># Install TensorBoard\npip install tensorboard tensorboardX\n\n# Or using uv\nuv pip install tensorboard tensorboardX\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#logging-patterns","title":"Logging Patterns","text":""},{"location":"user-guide/integrations/tensorboard/#basic-scalar-logging","title":"Basic Scalar Logging","text":"<p>Track training metrics over time.</p> <pre><code>from torch.utils.tensorboard import SummaryWriter\nimport jax.numpy as jnp\nimport numpy as np\n\nclass TensorBoardLogger:\n    \"\"\"TensorBoard logging for Artifex models.\"\"\"\n\n    def __init__(self, log_dir: str = \"./runs/experiment\"):\n        self.writer = SummaryWriter(log_dir)\n        self.step = 0\n\n    def log_scalars(self, metrics: dict, step: int = None):\n        \"\"\"Log scalar metrics.\n\n        Args:\n            metrics: Dictionary of metric names and values\n            step: Global step (uses internal counter if None)\n        \"\"\"\n        step = step if step is not None else self.step\n\n        for name, value in metrics.items():\n            # Convert JAX arrays to Python scalars\n            if isinstance(value, jnp.ndarray):\n                value = float(value)\n\n            self.writer.add_scalar(name, value, step)\n\n        if step is None:\n            self.step += 1\n\n    def log_training_step(\n        self,\n        loss: float,\n        learning_rate: float,\n        step: int,\n    ):\n        \"\"\"Log training step metrics.\"\"\"\n        self.log_scalars({\n            \"train/loss\": loss,\n            \"train/learning_rate\": learning_rate,\n        }, step=step)\n\n    def log_validation(\n        self,\n        val_loss: float,\n        metrics: dict,\n        epoch: int,\n    ):\n        \"\"\"Log validation metrics.\"\"\"\n        self.log_scalars({\n            \"val/loss\": val_loss,\n            **{f\"val/{k}\": v for k, v in metrics.items()}\n        }, step=epoch)\n\n    def close(self):\n        \"\"\"Close the writer.\"\"\"\n        self.writer.close()\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#image-logging","title":"Image Logging","text":"<p>Visualize generated samples.</p> <pre><code>class ImageLogger:\n    \"\"\"Log images to TensorBoard.\"\"\"\n\n    def __init__(self, writer: SummaryWriter):\n        self.writer = writer\n\n    def log_images(\n        self,\n        images: jax.Array,\n        tag: str,\n        step: int,\n        max_images: int = 16,\n    ):\n        \"\"\"Log image batch.\n\n        Args:\n            images: Image batch (B, H, W, C) or (B, C, H, W)\n            tag: Tag for the images\n            step: Global step\n            max_images: Maximum number of images to log\n        \"\"\"\n        # Convert to numpy and limit number\n        images_np = np.array(images[:max_images])\n\n        # Denormalize from [-1, 1] to [0, 1]\n        images_np = (images_np + 1) / 2\n        images_np = np.clip(images_np, 0, 1)\n\n        # Ensure channel-first format (C, H, W)\n        if images_np.shape[-1] in [1, 3]:  # Channel-last\n            images_np = np.transpose(images_np, (0, 3, 1, 2))\n\n        # Log as image grid\n        self.writer.add_images(tag, images_np, step)\n\n    def log_image_comparison(\n        self,\n        real_images: jax.Array,\n        generated_images: jax.Array,\n        step: int,\n    ):\n        \"\"\"Log real vs generated comparison.\"\"\"\n        self.log_images(real_images, \"comparison/real\", step)\n        self.log_images(generated_images, \"comparison/generated\", step)\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#histogram-logging","title":"Histogram Logging","text":"<p>Track parameter distributions.</p> <pre><code>from flax import nnx\n\nclass HistogramLogger:\n    \"\"\"Log parameter histograms.\"\"\"\n\n    def __init__(self, writer: SummaryWriter):\n        self.writer = writer\n\n    def log_model_parameters(\n        self,\n        model,\n        step: int,\n    ):\n        \"\"\"Log all model parameter histograms.\n\n        Args:\n            model: Flax NNX model\n            step: Global step\n        \"\"\"\n        state = nnx.state(model)\n\n        for name, param in state.items():\n            if isinstance(param, jnp.ndarray):\n                # Convert to numpy\n                param_np = np.array(param)\n\n                # Log histogram\n                self.writer.add_histogram(\n                    f\"parameters/{name}\",\n                    param_np,\n                    step\n                )\n\n    def log_gradients(\n        self,\n        grads: dict,\n        step: int,\n    ):\n        \"\"\"Log gradient histograms.\"\"\"\n        for name, grad in grads.items():\n            if isinstance(grad, jnp.ndarray):\n                grad_np = np.array(grad)\n                self.writer.add_histogram(\n                    f\"gradients/{name}\",\n                    grad_np,\n                    step\n                )\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#visualization","title":"Visualization","text":""},{"location":"user-guide/integrations/tensorboard/#training-curves","title":"Training Curves","text":"<p>Monitor loss and metrics over time.</p> <pre><code>class TrainingVisualizer:\n    \"\"\"Visualize training progress.\"\"\"\n\n    def __init__(self, log_dir: str):\n        self.writer = SummaryWriter(log_dir)\n\n    def log_loss_components(\n        self,\n        total_loss: float,\n        reconstruction_loss: float,\n        kl_loss: float,\n        step: int,\n    ):\n        \"\"\"Log VAE loss components.\"\"\"\n        self.writer.add_scalars(\"loss_components\", {\n            \"total\": total_loss,\n            \"reconstruction\": reconstruction_loss,\n            \"kl_divergence\": kl_loss,\n        }, step)\n\n    def log_gan_losses(\n        self,\n        g_loss: float,\n        d_loss: float,\n        d_real: float,\n        d_fake: float,\n        step: int,\n    ):\n        \"\"\"Log GAN training metrics.\"\"\"\n        self.writer.add_scalars(\"gan/losses\", {\n            \"generator\": g_loss,\n            \"discriminator\": d_loss,\n        }, step)\n\n        self.writer.add_scalars(\"gan/discriminator\", {\n            \"real_score\": d_real,\n            \"fake_score\": d_fake,\n        }, step)\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#sample-galleries","title":"Sample Galleries","text":"<p>Create grids of generated samples.</p> <pre><code>import matplotlib.pyplot as plt\nfrom matplotlib.backends.backend_agg import FigureCanvasAgg\n\ndef create_sample_grid(images: np.ndarray, nrow: int = 8) -&gt; np.ndarray:\n    \"\"\"Create image grid for visualization.\n\n    Args:\n        images: Batch of images (B, H, W, C)\n        nrow: Number of images per row\n\n    Returns:\n        Grid image as numpy array\n    \"\"\"\n    batch_size, h, w, c = images.shape\n    nrow = min(nrow, batch_size)\n    ncol = (batch_size + nrow - 1) // nrow\n\n    # Create figure\n    fig, axes = plt.subplots(ncol, nrow, figsize=(nrow * 2, ncol * 2))\n    axes = axes.flatten() if batch_size &gt; 1 else [axes]\n\n    for idx, (ax, img) in enumerate(zip(axes, images)):\n        if c == 1:  # Grayscale\n            ax.imshow(img.squeeze(), cmap='gray')\n        else:  # RGB\n            ax.imshow(img)\n        ax.axis('off')\n\n    # Hide extra subplots\n    for idx in range(batch_size, len(axes)):\n        axes[idx].axis('off')\n\n    plt.tight_layout()\n\n    # Convert to numpy array\n    canvas = FigureCanvasAgg(fig)\n    canvas.draw()\n    grid = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n    grid = grid.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n\n    plt.close(fig)\n\n    return grid\n\n\ndef log_sample_gallery(\n    logger: TensorBoardLogger,\n    model,\n    num_samples: int,\n    step: int,\n    rngs,\n):\n    \"\"\"Log gallery of generated samples.\"\"\"\n    # Generate samples\n    samples = model.sample(num_samples=num_samples, rngs=rngs)\n\n    # Convert and denormalize\n    samples_np = np.array(samples)\n    samples_np = ((samples_np + 1) / 2 * 255).astype(np.uint8)\n\n    # Create grid\n    grid = create_sample_grid(samples_np)\n\n    # Log to TensorBoard\n    logger.writer.add_image(\n        \"samples/generated\",\n        grid,\n        step,\n        dataformats='HWC'\n    )\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#latent-space-visualization","title":"Latent Space Visualization","text":"<p>Visualize learned latent representations.</p> <pre><code>def log_latent_space(\n    logger: TensorBoardLogger,\n    model,\n    test_data: jax.Array,\n    labels: jax.Array,\n    step: int,\n):\n    \"\"\"Log latent space embedding.\n\n    Args:\n        logger: TensorBoard logger\n        model: Trained model with encoder\n        test_data: Test images\n        labels: Image labels\n        step: Global step\n    \"\"\"\n    # Encode to latent space\n    latents, _ = model.encode(test_data)\n    latents_np = np.array(latents)\n    labels_np = np.array(labels)\n\n    # Log embedding\n    logger.writer.add_embedding(\n        latents_np,\n        metadata=labels_np.tolist(),\n        label_img=test_data,\n        global_step=step,\n        tag=\"latent_space\"\n    )\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#integration-with-training","title":"Integration with Training","text":""},{"location":"user-guide/integrations/tensorboard/#tensorboard-callback","title":"TensorBoard Callback","text":"<p>Integrate with Artifex training loop.</p> <pre><code>from artifex.generative_models.training import Trainer\n\nclass TensorBoardTrainer(Trainer):\n    \"\"\"Trainer with TensorBoard logging.\"\"\"\n\n    def __init__(\n        self,\n        model,\n        config: dict,\n        log_dir: str = \"./runs/experiment\",\n        **kwargs\n    ):\n        super().__init__(model, config, **kwargs)\n\n        # Initialize TensorBoard\n        self.tb_logger = TensorBoardLogger(log_dir)\n        self.log_frequency = config.get(\"tb_log_frequency\", 100)\n\n    def on_train_step_end(self, step: int, loss: float, metrics: dict):\n        \"\"\"Log after each training step.\"\"\"\n        if step % self.log_frequency == 0:\n            self.tb_logger.log_scalars({\n                \"train/loss\": loss,\n                **{f\"train/{k}\": v for k, v in metrics.items()}\n            }, step=step)\n\n    def on_validation_end(self, epoch: int, val_metrics: dict):\n        \"\"\"Log after validation.\"\"\"\n        self.tb_logger.log_scalars({\n            f\"val/{k}\": v for k, v in val_metrics.items()\n        }, step=epoch)\n\n        # Log generated samples\n        samples = self.model.sample(num_samples=16, rngs=self.rngs)\n        image_logger = ImageLogger(self.tb_logger.writer)\n        image_logger.log_images(samples, \"samples/generated\", epoch)\n\n    def on_training_end(self):\n        \"\"\"Close TensorBoard on training end.\"\"\"\n        self.tb_logger.close()\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#complete-example","title":"Complete Example","text":"<p>Full training example with TensorBoard.</p> <pre><code>from flax import nnx\nimport jax\nfrom artifex.generative_models.models.vae import VAE\nfrom artifex.generative_models.training import Trainer\n\n# Create model\nmodel = VAE(\n    latent_dim=128,\n    image_shape=(28, 28, 1),\n    rngs=nnx.Rngs(0),\n)\n\n# Training configuration\nconfig = {\n    \"learning_rate\": 1e-4,\n    \"batch_size\": 128,\n    \"num_epochs\": 50,\n    \"tb_log_frequency\": 100,\n}\n\n# Create trainer with TensorBoard\ntrainer = TensorBoardTrainer(\n    model=model,\n    config=config,\n    log_dir=\"./runs/vae_experiment\",\n)\n\n# Train\ntrainer.train(train_data, val_data)\n\n# View results\nprint(\"To view TensorBoard, run:\")\nprint(\"tensorboard --logdir=./runs\")\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#launching-tensorboard","title":"Launching TensorBoard","text":""},{"location":"user-guide/integrations/tensorboard/#basic-launch","title":"Basic Launch","text":"<pre><code># Launch TensorBoard\ntensorboard --logdir=./runs\n\n# Custom port\ntensorboard --logdir=./runs --port=6007\n\n# Multiple experiments\ntensorboard --logdir=./runs --reload_interval=5\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#comparing-experiments","title":"Comparing Experiments","text":"<pre><code># Compare multiple experiments\ntensorboard --logdir_spec=baseline:./runs/baseline,improved:./runs/improved\n</code></pre>"},{"location":"user-guide/integrations/tensorboard/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/integrations/tensorboard/#do","title":"DO","text":"<p>Recommended Practices</p> <p>\u2705 Organize logs by experiment in separate directories</p> <p>\u2705 Log periodically (every 100-1000 steps)</p> <p>\u2705 Use meaningful tags for metrics and images</p> <p>\u2705 Log validation samples to track generation quality</p> <p>\u2705 Close writer when training completes</p>"},{"location":"user-guide/integrations/tensorboard/#dont","title":"DON'T","text":"<p>Avoid These Mistakes</p> <p>\u274c Don't log every step (creates huge files)</p> <p>\u274c Don't log high-res images frequently (use max_images)</p> <p>\u274c Don't forget to flush the writer periodically</p> <p>\u274c Don't reuse log directories without clearing</p>"},{"location":"user-guide/integrations/tensorboard/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution TensorBoard not showing data Data not flushed Call <code>writer.flush()</code> or close writer Large log files Logging too frequently Reduce logging frequency Images not appearing Wrong format Ensure channel-first format (C, H, W) Port already in use TensorBoard running Use different port with <code>--port</code> Slow performance Too many logs Reduce log frequency or clear old runs"},{"location":"user-guide/integrations/tensorboard/#summary","title":"Summary","text":"<p>TensorBoard provides essential visualization:</p> <ul> <li>Real-time Monitoring: Track training progress live</li> <li>Scalar Metrics: Loss curves and validation metrics</li> <li>Image Galleries: Visualize generated samples</li> <li>Histograms: Monitor parameter distributions</li> <li>Embeddings: Explore latent spaces</li> <li>Profiling Traces: Analyze XLA compilation and device execution</li> </ul> <p>Start visualizing your training today!</p>"},{"location":"user-guide/integrations/tensorboard/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Weights &amp; Biases</p> <p>Advanced experiment tracking and sweeps</p> <p> W&amp;B Integration</p> </li> <li> <p> HuggingFace Hub</p> <p>Share models with the community</p> <p> HuggingFace Integration</p> </li> <li> <p> Training Guide</p> <p>Master the training system</p> <p> Training Guide</p> </li> <li> <p> Benchmarking</p> <p>Evaluate models comprehensively</p> <p> Evaluation Framework</p> </li> </ul>"},{"location":"user-guide/integrations/wandb/","title":"Weights &amp; Biases Integration","text":"<p>Complete guide to experiment tracking, hyperparameter sweeps, and artifact management using Weights &amp; Biases with Artifex.</p>"},{"location":"user-guide/integrations/wandb/#overview","title":"Overview","text":"<p>Weights &amp; Biases (W&amp;B) is a powerful platform for tracking machine learning experiments. Artifex integrates seamlessly with W&amp;B to log metrics, visualize training progress, run hyperparameter sweeps, and manage model artifacts.</p> <p>W&amp;B Benefits</p> <ul> <li>Experiment Tracking: Log metrics, hyperparameters, and system info</li> <li>Visualizations: Interactive charts and sample galleries</li> <li>Hyperparameter Sweeps: Automated optimization</li> <li>Artifact Management: Version models and datasets</li> <li>Team Collaboration: Share results with teammates</li> </ul> <ul> <li> <p> Experiment Tracking</p> <p>Log metrics, losses, and visualizations</p> <p> Tracking Guide</p> </li> <li> <p> Hyperparameter Sweeps</p> <p>Automate hyperparameter optimization</p> <p> Sweeps Guide</p> </li> <li> <p> Artifact Management</p> <p>Version and track models and datasets</p> <p> Artifacts Guide</p> </li> <li> <p> Reports</p> <p>Create shareable experiment reports</p> <p> Reports Guide</p> </li> </ul>"},{"location":"user-guide/integrations/wandb/#quick-start-with-built-in-callback","title":"Quick Start with Built-in Callback","text":"<p>For most use cases, use the built-in <code>WandbLoggerCallback</code>:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n)\n\ncallback = WandbLoggerCallback(WandbLoggerConfig(\n    project=\"my-project\",\n    name=\"experiment-1\",\n    tags=[\"vae\", \"baseline\"],\n    config={\"learning_rate\": 1e-3},\n))\n\ntrainer.fit(callbacks=[callback])\n</code></pre> <p>See Logging Callbacks for full documentation.</p> <p>The sections below cover advanced W&amp;B features not available through the callback.</p>"},{"location":"user-guide/integrations/wandb/#prerequisites","title":"Prerequisites","text":"<p>Install and configure W&amp;B:</p> <pre><code># Install wandb\npip install wandb\n\n# Or using uv\nuv pip install wandb\n\n# Login\nwandb login\n</code></pre>"},{"location":"user-guide/integrations/wandb/#experiment-tracking","title":"Experiment Tracking","text":""},{"location":"user-guide/integrations/wandb/#basic-logging","title":"Basic Logging","text":"<p>Track metrics during training.</p> <pre><code>import wandb\nfrom flax import nnx\nimport jax.numpy as jnp\n\nclass WandBTrainer:\n    \"\"\"Trainer with W&amp;B logging.\"\"\"\n\n    def __init__(\n        self,\n        model,\n        config: dict,\n        project_name: str = \"artifex-experiments\",\n    ):\n        self.model = model\n        self.config = config\n\n        # Initialize W&amp;B\n        wandb.init(\n            project=project_name,\n            config=config,\n            name=f\"{config['model_type']}-{config.get('experiment_name', 'default')}\",\n        )\n\n        # Log model architecture\n        wandb.watch(self.model, log_freq=100)\n\n    def train_step(self, batch, step: int):\n        \"\"\"Single training step with logging.\"\"\"\n        # Compute loss (simplified)\n        loss, metrics = self.compute_loss(batch)\n\n        # Log metrics\n        wandb.log({\n            \"train/loss\": float(loss),\n            \"train/step\": step,\n            **{f\"train/{k}\": float(v) for k, v in metrics.items()}\n        }, step=step)\n\n        return loss, metrics\n\n    def validation_step(self, val_data, step: int):\n        \"\"\"Validation with logging.\"\"\"\n        val_loss, val_metrics = self.evaluate(val_data)\n\n        # Log validation metrics\n        wandb.log({\n            \"val/loss\": float(val_loss),\n            **{f\"val/{k}\": float(v) for k, v in val_metrics.items()}\n        }, step=step)\n\n        return val_loss, val_metrics\n\n    def log_images(self, images, step: int, key: str = \"samples\"):\n        \"\"\"Log generated images.\"\"\"\n        # Convert to numpy and denormalize\n        images_np = np.array(images)\n        images_np = ((images_np + 1) / 2 * 255).astype(np.uint8)\n\n        # Log to W&amp;B\n        wandb.log({\n            key: [wandb.Image(img) for img in images_np]\n        }, step=step)\n\n    def finish(self):\n        \"\"\"Finish W&amp;B run.\"\"\"\n        wandb.finish()\n</code></pre>"},{"location":"user-guide/integrations/wandb/#advanced-metrics-logging","title":"Advanced Metrics Logging","text":"<p>Track custom metrics and visualizations.</p> <pre><code>class AdvancedWandBLogger:\n    \"\"\"Advanced W&amp;B logging with custom metrics.\"\"\"\n\n    def __init__(self, project: str, config: dict):\n        wandb.init(project=project, config=config)\n        self.step = 0\n\n    def log_training_metrics(\n        self,\n        loss: float,\n        reconstruction_loss: float,\n        kl_loss: float,\n        learning_rate: float,\n    ):\n        \"\"\"Log detailed training metrics.\"\"\"\n        wandb.log({\n            # Loss components\n            \"loss/total\": loss,\n            \"loss/reconstruction\": reconstruction_loss,\n            \"loss/kl_divergence\": kl_loss,\n\n            # Optimization\n            \"optimization/learning_rate\": learning_rate,\n            \"optimization/step\": self.step,\n\n            # Loss ratios\n            \"analysis/recon_kl_ratio\": reconstruction_loss / (kl_loss + 1e-8),\n        }, step=self.step)\n\n        self.step += 1\n\n    def log_histograms(self, model):\n        \"\"\"Log parameter histograms.\"\"\"\n        state = nnx.state(model)\n\n        for name, param in state.items():\n            if isinstance(param, jnp.ndarray):\n                wandb.log({\n                    f\"histograms/{name}\": wandb.Histogram(np.array(param))\n                }, step=self.step)\n\n    def log_latent_space(\n        self,\n        latent_codes: jax.Array,\n        labels: jax.Array = None,\n    ):\n        \"\"\"Log latent space visualization.\"\"\"\n        # Convert to numpy\n        latent_np = np.array(latent_codes)\n\n        # Log scatter plot\n        if labels is not None:\n            labels_np = np.array(labels)\n            data = [[x, y, label] for (x, y), label in zip(latent_np[:, :2], labels_np)]\n            table = wandb.Table(data=data, columns=[\"x\", \"y\", \"label\"])\n            wandb.log({\n                \"latent_space\": wandb.plot.scatter(\n                    table, \"x\", \"y\", \"label\",\n                    title=\"Latent Space Visualization\"\n                )\n            }, step=self.step)\n        else:\n            wandb.log({\n                \"latent_space\": wandb.Histogram(latent_np)\n            }, step=self.step)\n\n    def log_generation_quality(\n        self,\n        real_images: jax.Array,\n        fake_images: jax.Array,\n    ):\n        \"\"\"Log generation quality metrics.\"\"\"\n        from artifex.benchmarks.metrics import compute_fid, compute_inception_score\n\n        # Compute metrics\n        fid = compute_fid(real_images, fake_images)\n        inception_score, _ = compute_inception_score(fake_images)\n\n        wandb.log({\n            \"quality/fid\": fid,\n            \"quality/inception_score\": inception_score,\n        }, step=self.step)\n</code></pre>"},{"location":"user-guide/integrations/wandb/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":""},{"location":"user-guide/integrations/wandb/#sweep-configuration","title":"Sweep Configuration","text":"<p>Define hyperparameter search space.</p> <pre><code># sweep_config.yaml or Python dict\nsweep_config = {\n    \"method\": \"bayes\",  # bayes, grid, or random\n    \"metric\": {\n        \"name\": \"val/loss\",\n        \"goal\": \"minimize\",\n    },\n    \"parameters\": {\n        \"learning_rate\": {\n            \"distribution\": \"log_uniform_values\",\n            \"min\": 1e-5,\n            \"max\": 1e-2,\n        },\n        \"latent_dim\": {\n            \"values\": [64, 128, 256, 512],\n        },\n        \"beta\": {\n            \"distribution\": \"uniform\",\n            \"min\": 0.1,\n            \"max\": 10.0,\n        },\n        \"batch_size\": {\n            \"values\": [32, 64, 128],\n        },\n        \"architecture\": {\n            \"values\": [\"conv\", \"resnet\", \"transformer\"],\n        },\n    },\n}\n\n# Create sweep\nsweep_id = wandb.sweep(\n    sweep=sweep_config,\n    project=\"artifex-sweeps\",\n)\n</code></pre>"},{"location":"user-guide/integrations/wandb/#running-sweeps","title":"Running Sweeps","text":"<p>Execute hyperparameter sweep.</p> <pre><code>def train_with_sweep():\n    \"\"\"Training function for W&amp;B sweep.\"\"\"\n    # Initialize W&amp;B run\n    run = wandb.init()\n\n    # Get hyperparameters from sweep\n    config = wandb.config\n\n    # Build model with sweep config\n    model = build_model(\n        latent_dim=config.latent_dim,\n        architecture=config.architecture,\n    )\n\n    # Train model\n    for epoch in range(config.num_epochs):\n        # Training loop\n        train_loss = train_epoch(\n            model,\n            learning_rate=config.learning_rate,\n            batch_size=config.batch_size,\n            beta=config.beta,\n        )\n\n        # Validation\n        val_loss = validate(model)\n\n        # Log metrics\n        wandb.log({\n            \"train/loss\": train_loss,\n            \"val/loss\": val_loss,\n            \"epoch\": epoch,\n        })\n\n    # Finish run\n    wandb.finish()\n\n\n# Run sweep agent\nwandb.agent(\n    sweep_id=sweep_id,\n    function=train_with_sweep,\n    count=50,  # Number of runs\n)\n</code></pre>"},{"location":"user-guide/integrations/wandb/#multi-objective-optimization","title":"Multi-Objective Optimization","text":"<p>Optimize for multiple metrics simultaneously.</p> <pre><code>sweep_config_multi = {\n    \"method\": \"bayes\",\n    \"metric\": {\n        \"name\": \"combined_score\",\n        \"goal\": \"maximize\",\n    },\n    \"parameters\": {\n        # ... same as before\n    },\n}\n\ndef train_with_multi_objective():\n    \"\"\"Train with multiple objectives.\"\"\"\n    run = wandb.init()\n    config = wandb.config\n\n    # Training...\n    val_loss = validate(model)\n    fid_score = compute_fid(model)\n    inference_time = benchmark_inference(model)\n\n    # Combine objectives\n    # Lower is better for loss and FID, lower is better for time\n    combined_score = -val_loss - fid_score / 100 - inference_time\n\n    wandb.log({\n        \"val/loss\": val_loss,\n        \"quality/fid\": fid_score,\n        \"performance/inference_time\": inference_time,\n        \"combined_score\": combined_score,\n    })\n\n    wandb.finish()\n</code></pre>"},{"location":"user-guide/integrations/wandb/#artifact-management","title":"Artifact Management","text":""},{"location":"user-guide/integrations/wandb/#model-artifacts","title":"Model Artifacts","text":"<p>Version and track trained models.</p> <pre><code>class ArtifactManager:\n    \"\"\"Manage W&amp;B artifacts for models and datasets.\"\"\"\n\n    def __init__(self, project: str):\n        self.project = project\n\n    def save_model_artifact(\n        self,\n        model,\n        artifact_name: str,\n        metadata: dict = None,\n    ):\n        \"\"\"Save model as W&amp;B artifact.\n\n        Args:\n            model: Trained model\n            artifact_name: Artifact name (e.g., \"vae-mnist\")\n            metadata: Additional metadata\n        \"\"\"\n        # Create artifact\n        artifact = wandb.Artifact(\n            name=artifact_name,\n            type=\"model\",\n            metadata=metadata or {},\n        )\n\n        # Save model to temporary directory\n        import tempfile\n        with tempfile.TemporaryDirectory() as tmpdir:\n            model_path = f\"{tmpdir}/model\"\n\n            # Export model\n            state = nnx.state(model)\n            with open(f\"{model_path}/params.pkl\", \"wb\") as f:\n                import pickle\n                pickle.dump(state, f)\n\n            # Add to artifact\n            artifact.add_dir(model_path)\n\n        # Log artifact\n        wandb.log_artifact(artifact)\n\n        print(f\"Model saved as artifact: {artifact_name}\")\n\n    def load_model_artifact(\n        self,\n        artifact_name: str,\n        version: str = \"latest\",\n    ):\n        \"\"\"Load model from artifact.\n\n        Args:\n            artifact_name: Artifact name\n            version: Artifact version (\"latest\" or \"v0\", \"v1\", etc.)\n\n        Returns:\n            Loaded model\n        \"\"\"\n        # Download artifact\n        artifact = wandb.use_artifact(\n            f\"{artifact_name}:{version}\",\n            type=\"model\",\n        )\n        artifact_dir = artifact.download()\n\n        # Load model\n        import pickle\n        with open(f\"{artifact_dir}/params.pkl\", \"rb\") as f:\n            state = pickle.load(f)\n\n        # Reconstruct model\n        # (Simplified - actual implementation needs proper model reconstruction)\n        model = reconstruct_model(state)\n\n        return model\n\n    def save_dataset_artifact(\n        self,\n        data: jax.Array,\n        artifact_name: str,\n        description: str = \"\",\n    ):\n        \"\"\"Save dataset as artifact.\n\n        Args:\n            data: Dataset array\n            artifact_name: Artifact name\n            description: Dataset description\n        \"\"\"\n        artifact = wandb.Artifact(\n            name=artifact_name,\n            type=\"dataset\",\n            description=description,\n        )\n\n        # Save as numpy array\n        import tempfile\n        with tempfile.TemporaryDirectory() as tmpdir:\n            data_path = f\"{tmpdir}/data.npy\"\n            np.save(data_path, np.array(data))\n            artifact.add_file(data_path)\n\n        wandb.log_artifact(artifact)\n\n    def link_artifacts(\n        self,\n        model_artifact: str,\n        dataset_artifact: str,\n    ):\n        \"\"\"Link model to training dataset.\n\n        Args:\n            model_artifact: Model artifact name\n            dataset_artifact: Dataset artifact name\n        \"\"\"\n        # Get artifacts\n        model = wandb.use_artifact(f\"{model_artifact}:latest\")\n        dataset = wandb.use_artifact(f\"{dataset_artifact}:latest\")\n\n        # Link them\n        model.link(f\"trained_on_{dataset_artifact}\")\n        wandb.log_artifact(model)\n</code></pre>"},{"location":"user-guide/integrations/wandb/#artifact-lineage","title":"Artifact Lineage","text":"<p>Track relationships between artifacts.</p> <pre><code>def create_artifact_lineage():\n    \"\"\"Create artifact lineage for full experiment tracking.\"\"\"\n    run = wandb.init(project=\"artifex-lineage\")\n\n    # Log dataset\n    dataset_artifact = wandb.Artifact(\"mnist-train\", type=\"dataset\")\n    dataset_artifact.add_file(\"data/mnist_train.npy\")\n    wandb.log_artifact(dataset_artifact)\n\n    # Use dataset in training\n    dataset = wandb.use_artifact(\"mnist-train:latest\")\n    dataset_dir = dataset.download()\n\n    # Train model...\n    model = train_model(dataset_dir)\n\n    # Log trained model (automatically linked to dataset)\n    model_artifact = wandb.Artifact(\"vae-model\", type=\"model\")\n    model_artifact.add_dir(\"models/vae\")\n    wandb.log_artifact(model_artifact)\n\n    # Log evaluation results\n    eval_artifact = wandb.Artifact(\"vae-evaluation\", type=\"evaluation\")\n    eval_artifact.add_file(\"results/metrics.json\")\n    wandb.log_artifact(eval_artifact)\n\n    wandb.finish()\n</code></pre>"},{"location":"user-guide/integrations/wandb/#report-generation","title":"Report Generation","text":""},{"location":"user-guide/integrations/wandb/#creating-reports","title":"Creating Reports","text":"<p>Generate shareable experiment reports.</p> <pre><code>class ReportGenerator:\n    \"\"\"Generate W&amp;B reports.\"\"\"\n\n    @staticmethod\n    def create_experiment_report(\n        project: str,\n        runs: list[str],\n        title: str,\n    ) -&gt; str:\n        \"\"\"Create comparison report.\n\n        Args:\n            project: W&amp;B project name\n            runs: List of run IDs to compare\n            title: Report title\n\n        Returns:\n            Report URL\n        \"\"\"\n        import wandb\n\n        # Create report\n        report = wandb.apis.reports.Report(\n            project=project,\n            title=title,\n            description=\"Experiment comparison report\",\n        )\n\n        # Add run comparison\n        report.blocks = [\n            wandb.apis.reports.RunComparer(\n                diff_only=False,\n                runsets=[\n                    wandb.apis.reports.Runset(\n                        project=project,\n                        filters={\"$or\": [{\"name\": run_id} for run_id in runs]},\n                    )\n                ],\n            ),\n            wandb.apis.reports.LinePlot(\n                title=\"Training Loss Comparison\",\n                x=\"step\",\n                y=[\"train/loss\"],\n                runsets=[\n                    wandb.apis.reports.Runset(\n                        project=project,\n                        filters={\"$or\": [{\"name\": run_id} for run_id in runs]},\n                    )\n                ],\n            ),\n        ]\n\n        # Save and get URL\n        report.save()\n        return report.url\n</code></pre>"},{"location":"user-guide/integrations/wandb/#custom-visualizations","title":"Custom Visualizations","text":"<p>Create custom plots for reports.</p> <pre><code>def log_custom_visualizations(model, test_data):\n    \"\"\"Log custom visualizations to W&amp;B.\"\"\"\n\n    # 1. Sample Grid\n    samples = model.sample(num_samples=64)\n    wandb.log({\n        \"visualizations/sample_grid\": wandb.Image(\n            create_image_grid(samples, nrow=8)\n        )\n    })\n\n    # 2. Reconstruction Comparison\n    reconstructions = model.reconstruct(test_data[:8])\n    comparison = np.concatenate([test_data[:8], reconstructions], axis=0)\n    wandb.log({\n        \"visualizations/reconstruction\": wandb.Image(\n            create_image_grid(comparison, nrow=8)\n        )\n    })\n\n    # 3. Latent Space 2D Projection\n    from sklearn.manifold import TSNE\n\n    latents, labels = encode_dataset(model, test_data)\n    tsne = TSNE(n_components=2)\n    latents_2d = tsne.fit_transform(np.array(latents))\n\n    # Create scatter plot\n    data = [[x, y, int(label)] for (x, y), label in zip(latents_2d, labels)]\n    table = wandb.Table(data=data, columns=[\"x\", \"y\", \"label\"])\n    wandb.log({\n        \"visualizations/latent_tsne\": wandb.plot.scatter(\n            table, \"x\", \"y\", \"label\",\n            title=\"Latent Space t-SNE\"\n        )\n    })\n\n    # 4. Interpolation Video\n    interpolation_frames = create_interpolation(model, num_frames=60)\n    wandb.log({\n        \"visualizations/interpolation\": wandb.Video(\n            interpolation_frames, fps=30, format=\"gif\"\n        )\n    })\n</code></pre>"},{"location":"user-guide/integrations/wandb/#integration-with-artifex-training","title":"Integration with Artifex Training","text":""},{"location":"user-guide/integrations/wandb/#complete-training-example","title":"Complete Training Example","text":"<p>Full integration example.</p> <pre><code>from artifex.generative_models.training import Trainer\nfrom artifex.generative_models.core import DeviceManager\n\nclass ArtifexWandBTrainer(Trainer):\n    \"\"\"Artifex Trainer with W&amp;B integration.\"\"\"\n\n    def __init__(\n        self,\n        model,\n        config: dict,\n        wandb_project: str = \"artifex\",\n        **kwargs\n    ):\n        super().__init__(model, config, **kwargs)\n\n        # Initialize W&amp;B\n        wandb.init(\n            project=wandb_project,\n            config=config,\n            name=config.get(\"experiment_name\"),\n        )\n\n        self.wandb_log_frequency = config.get(\"wandb_log_frequency\", 100)\n\n    def on_train_step_end(self, step: int, loss: float, metrics: dict):\n        \"\"\"Called after each training step.\"\"\"\n        if step % self.wandb_log_frequency == 0:\n            wandb.log({\n                \"train/loss\": float(loss),\n                \"train/step\": step,\n                **{f\"train/{k}\": float(v) for k, v in metrics.items()}\n            }, step=step)\n\n    def on_validation_end(self, epoch: int, val_metrics: dict):\n        \"\"\"Called after validation.\"\"\"\n        wandb.log({\n            \"val/epoch\": epoch,\n            **{f\"val/{k}\": float(v) for k, v in val_metrics.items()}\n        }, step=epoch)\n\n        # Log sample images\n        samples = self.model.sample(num_samples=16, rngs=self.rngs)\n        wandb.log({\n            \"samples\": [wandb.Image(img) for img in np.array(samples)]\n        }, step=epoch)\n\n    def on_training_end(self):\n        \"\"\"Called when training completes.\"\"\"\n        # Save final model as artifact\n        artifact = wandb.Artifact(\"final_model\", type=\"model\")\n        artifact.add_dir(self.checkpoint_dir)\n        wandb.log_artifact(artifact)\n\n        wandb.finish()\n\n\n# Usage\nconfig = {\n    \"model_type\": \"vae\",\n    \"latent_dim\": 128,\n    \"learning_rate\": 1e-4,\n    \"batch_size\": 128,\n    \"num_epochs\": 100,\n    \"experiment_name\": \"vae-mnist-baseline\",\n}\n\ntrainer = ArtifexWandBTrainer(\n    model=model,\n    config=config,\n    wandb_project=\"artifex-experiments\",\n)\n\ntrainer.train(train_data, val_data)\n</code></pre>"},{"location":"user-guide/integrations/wandb/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/integrations/wandb/#do","title":"DO","text":"<p>Recommended Practices</p> <p>\u2705 Log hyperparameters at the start of each run</p> <p>\u2705 Use meaningful run names for easy identification</p> <p>\u2705 Tag runs with experiment type (baseline, ablation, etc.)</p> <p>\u2705 Save artifacts for reproducibility</p> <p>\u2705 Create reports for team sharing</p> <p>\u2705 Use sweeps for systematic hyperparameter search</p>"},{"location":"user-guide/integrations/wandb/#dont","title":"DON'T","text":"<p>Avoid These Mistakes</p> <p>\u274c Don't log too frequently (causes overhead)</p> <p>\u274c Don't hardcode API keys (use environment variables)</p> <p>\u274c Don't skip run names (default names are hard to track)</p> <p>\u274c Don't log high-resolution images every step (use subsampling)</p> <p>\u274c Don't forget to call <code>wandb.finish()</code></p> <p>\u274c Don't create too many artifacts (version strategically)</p>"},{"location":"user-guide/integrations/wandb/#troubleshooting","title":"Troubleshooting","text":"Issue Cause Solution Slow logging Logging too frequently Reduce log frequency to every 100-1000 steps Missing metrics Not calling <code>wandb.log()</code> Ensure metrics are logged in training loop Artifact upload fails Large file size Use compression or split artifacts Sweep not starting Invalid config Validate sweep config with W&amp;B docs Run not appearing Network issues Check internet connection, retry Memory leak Not finishing runs Always call <code>wandb.finish()</code>"},{"location":"user-guide/integrations/wandb/#summary","title":"Summary","text":"<p>W&amp;B integration provides powerful experiment tracking:</p> <ul> <li>Metrics Logging: Track training progress in real-time</li> <li>Hyperparameter Sweeps: Automate optimization</li> <li>Artifacts: Version models and datasets</li> <li>Reports: Share results with teammates</li> <li>Visualization: Interactive charts and galleries</li> </ul> <p>Start tracking your experiments systematically!</p>"},{"location":"user-guide/integrations/wandb/#next-steps","title":"Next Steps","text":"<ul> <li> <p> TensorBoard</p> <p>Alternative visualization with TensorBoard</p> <p> TensorBoard Guide</p> </li> <li> <p> HuggingFace Hub</p> <p>Share models with the community</p> <p> HuggingFace Integration</p> </li> <li> <p> Benchmarking</p> <p>Evaluate models with comprehensive metrics</p> <p> Evaluation Framework</p> </li> <li> <p> Training Guide</p> <p>Learn advanced training techniques</p> <p> Training Guide</p> </li> </ul>"},{"location":"user-guide/modalities/audio/","title":"Audio Modality Guide","text":"<p>This guide covers working with audio data in Artifex, including audio representations, waveform processing, spectrograms, and best practices for audio-based generative models.</p>"},{"location":"user-guide/modalities/audio/#overview","title":"Overview","text":"<p>Artifex's audio modality provides a unified interface for working with audio data in different representations: raw waveforms, mel-spectrograms, and STFTs (Short-Time Fourier Transforms).</p> <ul> <li> <p> Multiple Representations</p> <p>Support for raw waveforms, mel-spectrograms, and STFT representations</p> </li> <li> <p> Sample Rate Control</p> <p>Work with any sample rate from 8kHz to 48kHz and beyond</p> </li> <li> <p> Spectrogram Processing</p> <p>Built-in mel-spectrogram and STFT computation with configurable parameters</p> </li> <li> <p> Synthetic Datasets</p> <p>Ready-to-use synthetic audio datasets (sine waves, noise, chirps)</p> </li> <li> <p> Augmentation</p> <p>Time-domain and frequency-domain audio augmentation techniques</p> </li> <li> <p> JAX-Native</p> <p>Full JAX compatibility with JIT compilation and GPU acceleration</p> </li> </ul>"},{"location":"user-guide/modalities/audio/#audio-representations","title":"Audio Representations","text":""},{"location":"user-guide/modalities/audio/#supported-formats","title":"Supported Formats","text":"<p>Artifex supports three audio representations:</p> <pre><code>from artifex.generative_models.modalities.audio.base import AudioRepresentation\n\n# Raw waveform (time-domain signal)\nAudioRepresentation.RAW_WAVEFORM\n\n# Mel-spectrogram (perceptually-scaled frequency representation)\nAudioRepresentation.MEL_SPECTROGRAM\n\n# STFT (Short-Time Fourier Transform)\nAudioRepresentation.STFT\n</code></pre>"},{"location":"user-guide/modalities/audio/#configuring-audio-modality","title":"Configuring Audio Modality","text":"<pre><code>from artifex.generative_models.modalities.audio import (\n    AudioModality,\n    AudioModalityConfig,\n    AudioRepresentation\n)\nfrom flax import nnx\n\n# Initialize RNG\nrngs = nnx.Rngs(0)\n\n# Raw waveform configuration\nwaveform_config = AudioModalityConfig(\n    representation=AudioRepresentation.RAW_WAVEFORM,\n    sample_rate=16000,  # 16 kHz\n    duration=2.0,       # 2 seconds\n    normalize=True\n)\n\nwaveform_modality = AudioModality(config=waveform_config, rngs=rngs)\n\n# Mel-spectrogram configuration\nmel_config = AudioModalityConfig(\n    representation=AudioRepresentation.MEL_SPECTROGRAM,\n    sample_rate=16000,\n    n_mel_channels=80,   # Number of mel bins\n    hop_length=256,      # Hop size for STFT\n    n_fft=1024,          # FFT size\n    duration=2.0,\n    normalize=True\n)\n\nmel_modality = AudioModality(config=mel_config, rngs=rngs)\n\n# STFT configuration\nstft_config = AudioModalityConfig(\n    representation=AudioRepresentation.STFT,\n    sample_rate=22050,   # CD-quality / 2\n    n_fft=2048,\n    hop_length=512,\n    duration=3.0,\n    normalize=True\n)\n\nstft_modality = AudioModality(config=stft_config, rngs=rngs)\n</code></pre>"},{"location":"user-guide/modalities/audio/#audio-shape-properties","title":"Audio Shape Properties","text":"<pre><code># Raw waveform\nprint(f\"Time steps: {waveform_modality.n_time_steps}\")  # 32000 (16000 * 2.0)\nprint(f\"Output shape: {waveform_modality.output_shape}\")  # (32000,)\n\n# Mel-spectrogram\nprint(f\"Time frames: {mel_modality.n_time_frames}\")  # 125 (32000 // 256)\nprint(f\"Output shape: {mel_modality.output_shape}\")  # (80, 125)\n\n# STFT\nprint(f\"Frequency bins: {stft_config.n_fft // 2 + 1}\")  # 1025\nprint(f\"Output shape: {stft_modality.output_shape}\")  # (1025, time_frames)\n</code></pre>"},{"location":"user-guide/modalities/audio/#audio-datasets","title":"Audio Datasets","text":""},{"location":"user-guide/modalities/audio/#synthetic-audio-datasets","title":"Synthetic Audio Datasets","text":"<p>Artifex provides synthetic audio datasets for testing and development:</p> <pre><code>from artifex.generative_models.modalities.audio.datasets import (\n    SyntheticAudioDataset,\n    create_audio_dataset\n)\n\n# Configure audio\naudio_config = AudioModalityConfig(\n    representation=AudioRepresentation.RAW_WAVEFORM,\n    sample_rate=16000,\n    duration=1.0,\n    normalize=True\n)\n\n# Create synthetic dataset\naudio_dataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=1000,\n    audio_types=[\"sine\", \"noise\", \"chirp\"],\n    name=\"SyntheticAudio\"\n)\n\n# Access sample\nsample = audio_dataset[0]\nprint(sample[\"audio\"].shape)  # (16000,) - 1 second at 16kHz\nprint(sample[\"audio_type\"])   # \"sine\" or \"noise\" or \"chirp\"\nprint(sample[\"sample_rate\"])  # 16000\nprint(sample[\"duration\"])     # 1.0\n</code></pre>"},{"location":"user-guide/modalities/audio/#audio-types","title":"Audio Types","text":"<p>Sine Waves - Pure tones at random frequencies:</p> <pre><code>audio_dataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=1000,\n    audio_types=[\"sine\"]\n)\n\n# Sine waves have:\n# - Random frequencies (200-800 Hz)\n# - Fixed amplitude (0.5)\n# - Clean sinusoidal waveform\n</code></pre> <p>White Noise - Random Gaussian noise:</p> <pre><code>audio_dataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=1000,\n    audio_types=[\"noise\"]\n)\n\n# White noise has:\n# - Gaussian distribution\n# - Fixed amplitude (0.3)\n# - Flat frequency spectrum\n</code></pre> <p>Chirp Signals - Linear frequency sweeps:</p> <pre><code>audio_dataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=1000,\n    audio_types=[\"chirp\"]\n)\n\n# Chirps have:\n# - Linear frequency sweep (200-800 Hz)\n# - Smooth transitions\n# - Useful for testing time-frequency representations\n</code></pre>"},{"location":"user-guide/modalities/audio/#mixed-audio-types","title":"Mixed Audio Types","text":"<pre><code># Mix multiple audio types\nmixed_dataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=3000,\n    audio_types=[\"sine\", \"noise\", \"chirp\"]\n)\n\n# Dataset will cycle through types\n# Sample 0: sine\n# Sample 1: noise\n# Sample 2: chirp\n# Sample 3: sine (cycles back)\n</code></pre>"},{"location":"user-guide/modalities/audio/#batching-audio-data","title":"Batching Audio Data","text":"<pre><code># Get batch using collate function\nbatch = audio_dataset.collate_fn([\n    audio_dataset[0],\n    audio_dataset[1],\n    audio_dataset[2],\n    audio_dataset[3]\n])\n\nprint(batch[\"audio\"].shape)  # (4, 16000)\nprint(len(batch[\"audio_type\"]))  # 4\nprint(batch[\"sample_rate\"])  # [16000, 16000, 16000, 16000]\n\n# Iterate through dataset\nfor i, sample in enumerate(audio_dataset):\n    if i &gt;= 5:\n        break\n    print(f\"Sample {i}: {sample['audio_type']}, shape: {sample['audio'].shape}\")\n</code></pre>"},{"location":"user-guide/modalities/audio/#factory-function","title":"Factory Function","text":"<pre><code># Create dataset using factory\ndataset = create_audio_dataset(\n    dataset_type=\"synthetic\",\n    config=audio_config,\n    n_samples=5000,\n    audio_types=[\"sine\", \"noise\"]\n)\n\n# With custom parameters\ncustom_dataset = create_audio_dataset(\n    dataset_type=\"synthetic\",\n    config=None,  # Will use defaults\n    n_samples=1000,\n    audio_types=[\"chirp\"]\n)\n</code></pre>"},{"location":"user-guide/modalities/audio/#audio-preprocessing","title":"Audio Preprocessing","text":""},{"location":"user-guide/modalities/audio/#normalization","title":"Normalization","text":"<pre><code>import jax.numpy as jnp\n\ndef normalize_audio(audio: jax.Array) -&gt; jax.Array:\n    \"\"\"Normalize audio to [-1, 1] range.\n\n    Args:\n        audio: Input audio waveform\n\n    Returns:\n        Normalized audio\n    \"\"\"\n    max_val = jnp.max(jnp.abs(audio))\n    return jnp.where(max_val &gt; 0, audio / max_val, audio)\n\ndef rms_normalize(audio: jax.Array, target_rms: float = 0.1) -&gt; jax.Array:\n    \"\"\"Normalize audio by RMS (root mean square) energy.\n\n    Args:\n        audio: Input audio waveform\n        target_rms: Target RMS value\n\n    Returns:\n        RMS-normalized audio\n    \"\"\"\n    rms = jnp.sqrt(jnp.mean(audio ** 2))\n    return audio * (target_rms / (rms + 1e-8))\n\n# Usage\naudio = jnp.array([...])  # Raw audio\nnormalized = normalize_audio(audio)\nrms_normalized = rms_normalize(audio, target_rms=0.1)\n</code></pre>"},{"location":"user-guide/modalities/audio/#resampling","title":"Resampling","text":"<pre><code>import jax\nimport jax.numpy as jnp\n\ndef resample_audio(\n    audio: jax.Array,\n    orig_sr: int,\n    target_sr: int\n) -&gt; jax.Array:\n    \"\"\"Resample audio to different sample rate.\n\n    Args:\n        audio: Input audio waveform\n        orig_sr: Original sample rate\n        target_sr: Target sample rate\n\n    Returns:\n        Resampled audio\n    \"\"\"\n    # Simple linear interpolation resampling\n    orig_length = len(audio)\n    target_length = int(orig_length * target_sr / orig_sr)\n\n    # Create interpolation indices\n    orig_indices = jnp.linspace(0, orig_length - 1, target_length)\n\n    # Interpolate\n    resampled = jnp.interp(orig_indices, jnp.arange(orig_length), audio)\n\n    return resampled\n\n# Usage\naudio = jnp.array([...])  # Audio at 16kHz\naudio_22k = resample_audio(audio, orig_sr=16000, target_sr=22050)\naudio_8k = resample_audio(audio, orig_sr=16000, target_sr=8000)\n</code></pre>"},{"location":"user-guide/modalities/audio/#duration-adjustment","title":"Duration Adjustment","text":"<pre><code>def trim_or_pad(\n    audio: jax.Array,\n    target_length: int,\n    pad_value: float = 0.0\n) -&gt; jax.Array:\n    \"\"\"Trim or pad audio to target length.\n\n    Args:\n        audio: Input audio waveform\n        target_length: Target number of samples\n        pad_value: Value to use for padding\n\n    Returns:\n        Trimmed or padded audio\n    \"\"\"\n    current_length = len(audio)\n\n    if current_length &gt;= target_length:\n        # Trim\n        return audio[:target_length]\n    else:\n        # Pad\n        padding = jnp.full((target_length - current_length,), pad_value)\n        return jnp.concatenate([audio, padding])\n\n# Usage\naudio = jnp.array([...])  # Variable length audio\nfixed_length = trim_or_pad(audio, target_length=16000)\nprint(fixed_length.shape)  # (16000,)\n</code></pre>"},{"location":"user-guide/modalities/audio/#spectrogram-processing","title":"Spectrogram Processing","text":""},{"location":"user-guide/modalities/audio/#jit-compatible-stft-recommended","title":"JIT-Compatible STFT (Recommended)","text":"<p>For production use, the <code>SpectralAnalysis</code> extension provides a fully JIT-compatible STFT implementation using <code>jax.scipy.signal.stft</code>:</p> <pre><code>from artifex.generative_models.extensions.audio_processing.spectral import SpectralAnalysis\nfrom artifex.generative_models.core.configuration import ExtensionConfig\nfrom flax import nnx\nimport jax.numpy as jnp\n\nrngs = nnx.Rngs(0)\n\n# Configure spectral analysis\nconfig = ExtensionConfig(\n    weight=1.0,\n    enabled=True,\n    extensions={\n        \"spectral\": {\n            \"sample_rate\": 22050,\n            \"n_fft\": 2048,\n            \"hop_length\": 512,\n            \"window_type\": \"hann\",  # \"hann\", \"hamming\", \"blackman\"\n            \"n_mels\": 128\n        }\n    }\n)\n\n# Create spectral analysis module\nspectral = SpectralAnalysis(config=config, rngs=rngs)\n\n# Generate test audio\naudio = jnp.sin(2 * jnp.pi * 440 * jnp.linspace(0, 1, 22050))\n\n# Compute STFT (JIT-compatible, GPU-accelerated)\nstft_magnitude = spectral.compute_stft(audio)\nprint(f\"STFT shape: {stft_magnitude.shape}\")\n\n# Compute various spectrograms\npower_spec = spectral.compute_spectrogram(audio)\nmel_spec = spectral.compute_mel_spectrogram(audio)\nlog_mel_spec = spectral.compute_log_mel_spectrogram(audio)\nmfcc = spectral.compute_mfcc(audio, n_mfcc=13)\n\n# Spectral features\ncentroid = spectral.compute_spectral_centroid(audio)\nbandwidth = spectral.compute_spectral_bandwidth(audio)\nrolloff = spectral.compute_spectral_rolloff(audio)\n\n# Extract all features at once\nfeatures = spectral.extract_spectral_features(audio)\n</code></pre> <p>Key benefits:</p> <ul> <li>Fully JIT-compatible using <code>jax.scipy.signal.stft</code></li> <li>GPU-accelerated computation</li> <li>Supports batch processing via <code>jax.vmap</code></li> <li>Comprehensive spectral feature extraction</li> </ul>"},{"location":"user-guide/modalities/audio/#audio-augmentation","title":"Audio Augmentation","text":""},{"location":"user-guide/modalities/audio/#time-domain-augmentations","title":"Time-Domain Augmentations","text":"<pre><code>import jax\nimport jax.numpy as jnp\n\ndef time_shift(audio: jax.Array, key, max_shift: int = 1600):\n    \"\"\"Randomly shift audio in time.\n\n    Args:\n        audio: Input audio waveform\n        key: Random key\n        max_shift: Maximum shift in samples\n\n    Returns:\n        Time-shifted audio\n    \"\"\"\n    shift = jax.random.randint(key, (), -max_shift, max_shift)\n    return jnp.roll(audio, int(shift))\n\ndef time_stretch(audio: jax.Array, key, rate_range=(0.8, 1.2)):\n    \"\"\"Randomly stretch or compress audio in time.\n\n    Args:\n        audio: Input audio waveform\n        key: Random key\n        rate_range: (min_rate, max_rate) for stretching\n\n    Returns:\n        Time-stretched audio\n    \"\"\"\n    rate = jax.random.uniform(key, minval=rate_range[0], maxval=rate_range[1])\n\n    # Simple resampling for time stretching\n    orig_length = len(audio)\n    new_length = int(orig_length / rate)\n\n    indices = jnp.linspace(0, orig_length - 1, new_length)\n    stretched = jnp.interp(indices, jnp.arange(orig_length), audio)\n\n    # Pad or trim to original length\n    if len(stretched) &lt; orig_length:\n        padding = jnp.zeros(orig_length - len(stretched))\n        stretched = jnp.concatenate([stretched, padding])\n    else:\n        stretched = stretched[:orig_length]\n\n    return stretched\n\ndef add_gaussian_noise(audio: jax.Array, key, noise_level: float = 0.005):\n    \"\"\"Add Gaussian noise to audio.\n\n    Args:\n        audio: Input audio waveform\n        key: Random key\n        noise_level: Standard deviation of noise\n\n    Returns:\n        Noisy audio\n    \"\"\"\n    noise = noise_level * jax.random.normal(key, audio.shape)\n    return audio + noise\n\ndef random_gain(audio: jax.Array, key, gain_range=(0.7, 1.3)):\n    \"\"\"Apply random gain (amplitude scaling).\n\n    Args:\n        audio: Input audio waveform\n        key: Random key\n        gain_range: (min_gain, max_gain)\n\n    Returns:\n        Gain-adjusted audio\n    \"\"\"\n    gain = jax.random.uniform(key, minval=gain_range[0], maxval=gain_range[1])\n    return audio * gain\n\n# Usage\naudio = jnp.array([...])  # Audio waveform\nkey = jax.random.key(0)\nkeys = jax.random.split(key, 4)\n\nshifted = time_shift(audio, keys[0], max_shift=1600)\nstretched = time_stretch(audio, keys[1], rate_range=(0.9, 1.1))\nnoisy = add_gaussian_noise(audio, keys[2], noise_level=0.005)\ngained = random_gain(audio, keys[3], gain_range=(0.8, 1.2))\n</code></pre>"},{"location":"user-guide/modalities/audio/#frequency-domain-augmentations","title":"Frequency-Domain Augmentations","text":"<pre><code>def frequency_mask(\n    spectrogram: jax.Array,\n    key,\n    num_masks: int = 1,\n    mask_param: int = 10\n):\n    \"\"\"Apply frequency masking to spectrogram (SpecAugment).\n\n    Args:\n        spectrogram: Input spectrogram (freq_bins, time_frames)\n        key: Random key\n        num_masks: Number of masks to apply\n        mask_param: Maximum width of mask\n\n    Returns:\n        Masked spectrogram\n    \"\"\"\n    masked_spec = spectrogram.copy()\n    n_freq_bins = spectrogram.shape[0]\n\n    keys = jax.random.split(key, num_masks * 2)\n\n    for i in range(num_masks):\n        # Random mask width\n        mask_width = jax.random.randint(keys[2*i], (), 0, mask_param)\n\n        # Random starting frequency\n        start_freq = jax.random.randint(\n            keys[2*i + 1], (), 0, n_freq_bins - mask_width\n        )\n\n        # Apply mask\n        masked_spec = masked_spec.at[start_freq:start_freq+mask_width, :].set(0)\n\n    return masked_spec\n\ndef time_mask(\n    spectrogram: jax.Array,\n    key,\n    num_masks: int = 1,\n    mask_param: int = 10\n):\n    \"\"\"Apply time masking to spectrogram (SpecAugment).\n\n    Args:\n        spectrogram: Input spectrogram (freq_bins, time_frames)\n        key: Random key\n        num_masks: Number of masks to apply\n        mask_param: Maximum width of mask\n\n    Returns:\n        Masked spectrogram\n    \"\"\"\n    masked_spec = spectrogram.copy()\n    n_time_frames = spectrogram.shape[1]\n\n    keys = jax.random.split(key, num_masks * 2)\n\n    for i in range(num_masks):\n        # Random mask width\n        mask_width = jax.random.randint(keys[2*i], (), 0, mask_param)\n\n        # Random starting time\n        start_time = jax.random.randint(\n            keys[2*i + 1], (), 0, n_time_frames - mask_width\n        )\n\n        # Apply mask\n        masked_spec = masked_spec.at[:, start_time:start_time+mask_width].set(0)\n\n    return masked_spec\n\ndef spec_augment(\n    spectrogram: jax.Array,\n    key,\n    freq_masks: int = 2,\n    time_masks: int = 2,\n    freq_mask_param: int = 15,\n    time_mask_param: int = 20\n):\n    \"\"\"Apply SpecAugment (frequency + time masking).\n\n    Args:\n        spectrogram: Input spectrogram\n        key: Random key\n        freq_masks: Number of frequency masks\n        time_masks: Number of time masks\n        freq_mask_param: Max frequency mask width\n        time_mask_param: Max time mask width\n\n    Returns:\n        Augmented spectrogram\n    \"\"\"\n    keys = jax.random.split(key, 2)\n\n    # Apply frequency masking\n    spec = frequency_mask(spectrogram, keys[0], freq_masks, freq_mask_param)\n\n    # Apply time masking\n    spec = time_mask(spec, keys[1], time_masks, time_mask_param)\n\n    return spec\n\n# Usage (using SpectralAnalysis from above)\nmel_spec = spectral.compute_mel_spectrogram(audio)\nkey = jax.random.key(0)\n\naugmented_spec = spec_augment(\n    mel_spec,\n    key,\n    freq_masks=2,\n    time_masks=2,\n    freq_mask_param=15,\n    time_mask_param=20\n)\n</code></pre>"},{"location":"user-guide/modalities/audio/#complete-augmentation-pipeline","title":"Complete Augmentation Pipeline","text":"<pre><code>@jax.jit\ndef augment_audio(audio: jax.Array, key):\n    \"\"\"Apply comprehensive audio augmentation pipeline.\n\n    Args:\n        audio: Input audio waveform\n        key: Random key\n\n    Returns:\n        Augmented audio\n    \"\"\"\n    keys = jax.random.split(key, 5)\n\n    # Time-domain augmentations\n    audio = time_shift(audio, keys[0], max_shift=800)\n    audio = add_gaussian_noise(audio, keys[1], noise_level=0.005)\n    audio = random_gain(audio, keys[2], gain_range=(0.8, 1.2))\n    audio = time_stretch(audio, keys[3], rate_range=(0.95, 1.05))\n\n    # Normalize after augmentation\n    audio = normalize_audio(audio)\n\n    return audio\n\ndef augment_audio_batch(audio_batch: jax.Array, key):\n    \"\"\"Augment batch of audio waveforms.\n\n    Args:\n        audio_batch: Batch of audio (N, n_samples)\n        key: Random key\n\n    Returns:\n        Augmented batch\n    \"\"\"\n    batch_size = audio_batch.shape[0]\n    keys = jax.random.split(key, batch_size)\n\n    # Vectorize over batch\n    augmented = jax.vmap(augment_audio)(audio_batch, keys)\n\n    return augmented\n\n# Usage in training\nkey = jax.random.key(0)\nfor batch in data_loader:\n    key, subkey = jax.random.split(key)\n    augmented_audio = augment_audio_batch(batch[\"audio\"], subkey)\n    # Use augmented_audio for training\n</code></pre>"},{"location":"user-guide/modalities/audio/#working-with-different-sample-rates","title":"Working with Different Sample Rates","text":""},{"location":"user-guide/modalities/audio/#common-sample-rates","title":"Common Sample Rates","text":"<pre><code># Telephone quality (8 kHz)\ntelephone_config = AudioModalityConfig(\n    representation=AudioRepresentation.RAW_WAVEFORM,\n    sample_rate=8000,\n    duration=2.0\n)\n\n# Standard speech (16 kHz)\nspeech_config = AudioModalityConfig(\n    representation=AudioRepresentation.RAW_WAVEFORM,\n    sample_rate=16000,\n    duration=2.0\n)\n\n# High-quality speech (22.05 kHz)\nhq_speech_config = AudioModalityConfig(\n    representation=AudioRepresentation.RAW_WAVEFORM,\n    sample_rate=22050,\n    duration=2.0\n)\n\n# CD quality (44.1 kHz)\ncd_config = AudioModalityConfig(\n    representation=AudioRepresentation.RAW_WAVEFORM,\n    sample_rate=44100,\n    duration=2.0\n)\n\n# Studio quality (48 kHz)\nstudio_config = AudioModalityConfig(\n    representation=AudioRepresentation.RAW_WAVEFORM,\n    sample_rate=48000,\n    duration=2.0\n)\n</code></pre>"},{"location":"user-guide/modalities/audio/#sample-rate-conversion","title":"Sample Rate Conversion","text":"<pre><code>def convert_sample_rate(\n    audio: jax.Array,\n    orig_sr: int,\n    target_sr: int\n) -&gt; jax.Array:\n    \"\"\"Convert audio to different sample rate.\n\n    Args:\n        audio: Input audio\n        orig_sr: Original sample rate\n        target_sr: Target sample rate\n\n    Returns:\n        Resampled audio\n    \"\"\"\n    return resample_audio(audio, orig_sr, target_sr)\n\n# Usage\naudio_16k = jnp.array([...])  # Audio at 16 kHz\n\n# Upsample to 22.05 kHz\naudio_22k = convert_sample_rate(audio_16k, 16000, 22050)\n\n# Downsample to 8 kHz\naudio_8k = convert_sample_rate(audio_16k, 16000, 8000)\n\nprint(f\"16 kHz: {len(audio_16k)} samples\")\nprint(f\"22.05 kHz: {len(audio_22k)} samples\")\nprint(f\"8 kHz: {len(audio_8k)} samples\")\n</code></pre>"},{"location":"user-guide/modalities/audio/#complete-examples","title":"Complete Examples","text":""},{"location":"user-guide/modalities/audio/#example-1-audio-generation-dataset","title":"Example 1: Audio Generation Dataset","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.modalities.audio import (\n    AudioModality,\n    AudioModalityConfig,\n    AudioRepresentation\n)\nfrom artifex.generative_models.modalities.audio.datasets import (\n    SyntheticAudioDataset\n)\n\n# Setup\nrngs = nnx.Rngs(0)\n\n# Configure\naudio_config = AudioModalityConfig(\n    representation=AudioRepresentation.RAW_WAVEFORM,\n    sample_rate=16000,\n    duration=1.0,\n    normalize=True\n)\n\nmodality = AudioModality(config=audio_config, rngs=rngs)\n\n# Create datasets\ntrain_dataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=10000,\n    audio_types=[\"sine\", \"noise\", \"chirp\"]\n)\n\nval_dataset = SyntheticAudioDataset(\n    config=audio_config,\n    n_samples=1000,\n    audio_types=[\"sine\", \"noise\", \"chirp\"]\n)\n\n# Training loop\nbatch_size = 32\nnum_epochs = 10\nkey = jax.random.key(42)\n\nfor epoch in range(num_epochs):\n    num_batches = len(train_dataset) // batch_size\n\n    for i in range(num_batches):\n        # Get batch\n        batch_samples = [train_dataset[j] for j in range(i*batch_size, (i+1)*batch_size)]\n        batch = train_dataset.collate_fn(batch_samples)\n        audio_batch = batch[\"audio\"]\n\n        # Apply augmentation\n        key, subkey = jax.random.split(key)\n        augmented = augment_audio_batch(audio_batch, subkey)\n\n        # Training step\n        # loss = train_step(model, augmented)\n\n    # Validation (no augmentation)\n    val_batches = len(val_dataset) // batch_size\n    for i in range(val_batches):\n        val_samples = [val_dataset[j] for j in range(i*batch_size, (i+1)*batch_size)]\n        val_batch = val_dataset.collate_fn(val_samples)\n        # val_loss = validate_step(model, val_batch[\"audio\"])\n\n    print(f\"Epoch {epoch + 1}/{num_epochs} complete\")\n</code></pre>"},{"location":"user-guide/modalities/audio/#example-2-mel-spectrogram-training","title":"Example 2: Mel-Spectrogram Training","text":"<pre><code># Configure for mel-spectrograms\nmel_config = AudioModalityConfig(\n    representation=AudioRepresentation.MEL_SPECTROGRAM,\n    sample_rate=16000,\n    n_mel_channels=80,\n    hop_length=256,\n    n_fft=1024,\n    duration=2.0,\n    normalize=True\n)\n\nmel_modality = AudioModality(config=mel_config, rngs=rngs)\n\n# Create dataset\nmel_dataset = SyntheticAudioDataset(\n    config=mel_config,\n    n_samples=5000,\n    audio_types=[\"sine\", \"chirp\"]\n)\n\n# Create SpectralAnalysis for mel-spectrogram computation\nspectral_config = ExtensionConfig(\n    weight=1.0,\n    enabled=True,\n    extensions={\n        \"spectral\": {\n            \"sample_rate\": 16000,\n            \"n_fft\": 1024,\n            \"hop_length\": 256,\n            \"n_mels\": 80\n        }\n    }\n)\nspectral = SpectralAnalysis(config=spectral_config, rngs=rngs)\n\n# Training with spectrograms\nfor epoch in range(num_epochs):\n    for i in range(len(mel_dataset) // batch_size):\n        # Get audio batch\n        batch_samples = [mel_dataset[j] for j in range(i*batch_size, (i+1)*batch_size)]\n        batch = mel_dataset.collate_fn(batch_samples)\n        audio_batch = batch[\"audio\"]\n\n        # Compute mel-spectrograms (batch processing via vmap)\n        mel_batch = jax.vmap(spectral.compute_mel_spectrogram)(audio_batch)\n\n        # Apply SpecAugment\n        key, subkey = jax.random.split(key)\n        augmented_specs = jax.vmap(lambda spec, k: spec_augment(spec, k))(\n            mel_batch,\n            jax.random.split(subkey, batch_size)\n        )\n\n        # Training step\n        # loss = train_step(model, augmented_specs)\n</code></pre>"},{"location":"user-guide/modalities/audio/#example-3-custom-audio-dataset","title":"Example 3: Custom Audio Dataset","text":"<pre><code>from typing import Iterator\nfrom artifex.generative_models.modalities.base import BaseDataset\n\nclass CustomAudioDataset(BaseDataset):\n    \"\"\"Custom audio dataset loading from files.\"\"\"\n\n    def __init__(\n        self,\n        config: AudioModalityConfig,\n        audio_paths: list[str],\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(config, split, rngs=rngs)\n        self.audio_paths = audio_paths\n        self.audio_samples = self._load_audio_files()\n\n    def _load_audio_files(self):\n        \"\"\"Load audio files.\"\"\"\n        samples = []\n        for path in self.audio_paths:\n            # In practice: use librosa, soundfile, etc.\n            # For demo, generate synthetic\n            audio = jax.random.uniform(\n                jax.random.key(hash(path)),\n                (16000,)\n            )\n            samples.append(audio)\n        return samples\n\n    def __len__(self) -&gt; int:\n        return len(self.audio_samples)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        for i, audio in enumerate(self.audio_samples):\n            yield {\n                \"audio\": audio,\n                \"index\": jnp.array(i),\n                \"path\": self.audio_paths[i]\n            }\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        key = self.rngs.sample() if \"sample\" in self.rngs else jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self))\n\n        batch_audio = [self.audio_samples[int(idx)] for idx in indices]\n        batch_paths = [self.audio_paths[int(idx)] for idx in indices]\n\n        return {\n            \"audio\": jnp.stack(batch_audio),\n            \"indices\": indices,\n            \"paths\": batch_paths\n        }\n\n# Usage\naudio_paths = [\"/path/to/audio1.wav\", \"/path/to/audio2.wav\", ...]\n\ncustom_dataset = CustomAudioDataset(\n    config=audio_config,\n    audio_paths=audio_paths,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/modalities/audio/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/modalities/audio/#do","title":"DO","text":"<p>Audio Loading</p> <ul> <li>Use appropriate sample rate for your task</li> <li>Normalize audio to consistent amplitude</li> <li>Trim or pad to consistent duration for batching</li> <li>Validate audio shapes before training</li> <li>Cache processed audio when possible</li> <li>Use synthetic datasets for testing pipelines</li> </ul> <p>Spectrograms</p> <ul> <li>Choose appropriate n_fft and hop_length for task</li> <li>Use mel-spectrograms for perceptual tasks</li> <li>Apply log scaling to spectrograms</li> <li>Consider phase information for reconstruction tasks</li> <li>Use appropriate number of mel bins (typically 40-128)</li> </ul> <p>Augmentation</p> <ul> <li>Apply augmentation only during training</li> <li>Balance augmentation strength with audio quality</li> <li>Use SpecAugment for spectrogram-based models</li> <li>Test augmentations by listening to samples</li> <li>Use JIT compilation for performance</li> </ul>"},{"location":"user-guide/modalities/audio/#dont","title":"DON'T","text":"<p>Common Mistakes</p> <ul> <li>Mix different sample rates in same batch</li> <li>Forget to normalize audio amplitudes</li> <li>Apply augmentation during validation</li> <li>Use non-JAX operations in data pipeline</li> <li>Load full audio files if working with clips</li> <li>Ignore clipping (values &gt; 1.0 or &lt; -1.0)</li> </ul> <p>Performance Issues</p> <ul> <li>Load audio from disk during training</li> <li>Compute spectrograms on-the-fly every epoch</li> <li>Use Python loops for audio processing</li> <li>Keep multiple copies of audio in memory</li> <li>Use very long audio clips on limited GPU memory</li> </ul> <p>Quality Issues</p> <ul> <li>Over-augment audio (too much distortion)</li> <li>Use inappropriate sample rates</li> <li>Mix time-domain and frequency-domain representations</li> <li>Ignore audio phase for waveform reconstruction</li> <li>Apply excessive noise</li> </ul>"},{"location":"user-guide/modalities/audio/#summary","title":"Summary","text":"<p>This guide covered:</p> <ul> <li>Audio representations - Waveforms, mel-spectrograms, STFT</li> <li>Audio datasets - Synthetic datasets with various audio types</li> <li>Preprocessing - Normalization, resampling, duration adjustment</li> <li>Spectrograms - JIT-compatible STFT, magnitude, power, and mel-spectrograms</li> <li>SpectralAnalysis extension - Production-ready spectral processing with <code>jax.scipy.signal.stft</code></li> <li>Augmentation - Time-domain and frequency-domain techniques</li> <li>Sample rates - Working with different sample rates</li> <li>Complete examples - Training pipelines and custom datasets</li> <li>Best practices - DOs and DON'Ts for audio data</li> </ul>"},{"location":"user-guide/modalities/audio/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Multi-modal Guide</p> <p>Working with multiple modalities and aligned multi-modal datasets</p> </li> <li> <p> Image Modality Guide</p> <p>Deep dive into image datasets, preprocessing, and augmentation</p> </li> <li> <p> Text Modality Guide</p> <p>Learn about text tokenization, vocabulary management, and sequences</p> </li> <li> <p> Data API Reference</p> <p>Complete API documentation for all dataset classes and functions</p> </li> </ul>"},{"location":"user-guide/modalities/image/","title":"Image Modality Guide","text":"<p>This guide covers working with image data in Artifex, including image representations, datasets, preprocessing, and best practices for image-based generative models.</p>"},{"location":"user-guide/modalities/image/#overview","title":"Overview","text":"<p>Artifex's image modality provides a unified interface for working with different image formats and resolutions. It supports RGB, RGBA, and grayscale images with configurable preprocessing and augmentation.</p> <ul> <li> <p> Multiple Representations</p> <p>Support for RGB, RGBA, and grayscale images with automatic channel handling</p> </li> <li> <p> Flexible Resolutions</p> <p>Work with any image size from 28x28 to 512x512 and beyond</p> </li> <li> <p> Preprocessing Pipeline</p> <p>Built-in normalization, resizing, and validation</p> </li> <li> <p> Synthetic Datasets</p> <p>Ready-to-use synthetic datasets for testing and development</p> </li> <li> <p> Augmentation</p> <p>Common image augmentation techniques (flip, rotate, brightness, contrast)</p> </li> <li> <p> JAX-Native</p> <p>Full JAX compatibility with JIT compilation and GPU acceleration</p> </li> </ul>"},{"location":"user-guide/modalities/image/#image-representations","title":"Image Representations","text":""},{"location":"user-guide/modalities/image/#supported-formats","title":"Supported Formats","text":"<p>Artifex supports three image representations:</p> <pre><code>from artifex.generative_models.modalities.image.base import ImageRepresentation\n\n# RGB images (3 channels)\nImageRepresentation.RGB\n\n# RGBA images (4 channels with alpha)\nImageRepresentation.RGBA\n\n# Grayscale images (1 channel)\nImageRepresentation.GRAYSCALE\n</code></pre>"},{"location":"user-guide/modalities/image/#configuring-image-modality","title":"Configuring Image Modality","text":"<pre><code>from artifex.generative_models.modalities import ImageModality\nfrom artifex.generative_models.modalities.image.base import (\n    ImageModalityConfig,\n    ImageRepresentation\n)\nfrom flax import nnx\n\n# Initialize RNG\nrngs = nnx.Rngs(0)\n\n# RGB configuration (64x64)\nrgb_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=64,\n    width=64,\n    channels=3,  # Auto-determined from representation if None\n    normalize=True,  # Normalize to [0, 1]\n    augmentation=False,\n    resize_method=\"bilinear\"\n)\n\n# Create modality\nrgb_modality = ImageModality(config=rgb_config, rngs=rngs)\n\n# RGBA configuration\nrgba_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGBA,\n    height=128,\n    width=128,\n    channels=4  # Alpha channel included\n)\n\nrgba_modality = ImageModality(config=rgba_config, rngs=rngs)\n\n# Grayscale configuration\ngrayscale_config = ImageModalityConfig(\n    representation=ImageRepresentation.GRAYSCALE,\n    height=28,\n    width=28,\n    channels=1\n)\n\ngrayscale_modality = ImageModality(config=grayscale_config, rngs=rngs)\n</code></pre>"},{"location":"user-guide/modalities/image/#image-shape-properties","title":"Image Shape Properties","text":"<pre><code># Access image dimensions\nprint(f\"Image shape: {rgb_modality.image_shape}\")  # (64, 64, 3)\nprint(f\"Output shape: {rgb_modality.output_shape}\")  # (64, 64, 3)\n\n# For MNIST-like\nprint(f\"Grayscale shape: {grayscale_modality.image_shape}\")  # (28, 28, 1)\n</code></pre>"},{"location":"user-guide/modalities/image/#image-datasets","title":"Image Datasets","text":""},{"location":"user-guide/modalities/image/#synthetic-image-datasets","title":"Synthetic Image Datasets","text":"<p>Artifex provides several synthetic dataset types for testing and development:</p>"},{"location":"user-guide/modalities/image/#random-patterns","title":"Random Patterns","text":"<pre><code>from artifex.generative_models.modalities.image.datasets import SyntheticImageDataset\n\n# Random noise patterns\nrandom_dataset = SyntheticImageDataset(\n    config=rgb_config,\n    dataset_size=10000,\n    pattern_type=\"random\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Get batch\nbatch = random_dataset.get_batch(batch_size=32)\nprint(batch[\"images\"].shape)  # (32, 64, 64, 3)\n\n# Each image is filled with uniform random noise\n</code></pre>"},{"location":"user-guide/modalities/image/#gradient-patterns","title":"Gradient Patterns","text":"<pre><code># Linear gradients with varying directions\ngradient_dataset = SyntheticImageDataset(\n    config=rgb_config,\n    dataset_size=10000,\n    pattern_type=\"gradient\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Gradients have:\n# - Random directions\n# - Smooth color transitions (for RGB)\n# - Sinusoidal variations for visual interest\n</code></pre>"},{"location":"user-guide/modalities/image/#checkerboard-patterns","title":"Checkerboard Patterns","text":"<pre><code># Checkerboard patterns with random sizes\ncheckerboard_dataset = SyntheticImageDataset(\n    config=rgb_config,\n    dataset_size=10000,\n    pattern_type=\"checkerboard\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Checkerboards have:\n# - Random tile sizes (4-16 pixels)\n# - Binary black/white pattern\n# - Repeated across color channels\n</code></pre>"},{"location":"user-guide/modalities/image/#circular-patterns","title":"Circular Patterns","text":"<pre><code># Circular patterns with random positions and radii\ncircles_dataset = SyntheticImageDataset(\n    config=rgb_config,\n    dataset_size=10000,\n    pattern_type=\"circles\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Circles have:\n# - Random center positions\n# - Random radii\n# - Gaussian noise for variation\n</code></pre>"},{"location":"user-guide/modalities/image/#mnist-like-datasets","title":"MNIST-Like Datasets","text":"<p>For digit-like pattern recognition:</p> <pre><code>from artifex.generative_models.modalities.image.datasets import MNISTLikeDataset\n\n# Configure for MNIST-like images (28x28 grayscale)\nmnist_config = ImageModalityConfig(\n    representation=ImageRepresentation.GRAYSCALE,\n    height=28,\n    width=28,\n    channels=1,\n    normalize=True\n)\n\n# Create MNIST-like dataset\nmnist_dataset = MNISTLikeDataset(\n    config=mnist_config,\n    dataset_size=60000,\n    num_classes=10,\n    split=\"train\",\n    rngs=rngs\n)\n\n# Get labeled batch\nbatch = mnist_dataset.get_batch(batch_size=128)\nprint(batch[\"images\"].shape)  # (128, 28, 28, 1)\nprint(batch[\"labels\"].shape)  # (128,)\n\n# Iterate with labels\nfor sample in mnist_dataset:\n    image = sample[\"images\"]  # (28, 28, 1)\n    label = sample[\"labels\"]  # Scalar label\n    print(f\"Label: {label}, Image shape: {image.shape}\")\n    break\n</code></pre> <p>Generated patterns:</p> <ul> <li>Class 0: Circle (hollow)</li> <li>Class 1: Vertical line</li> <li>Class 2: Horizontal line</li> <li>Additional classes follow similar geometric patterns</li> </ul>"},{"location":"user-guide/modalities/image/#factory-function","title":"Factory Function","text":"<pre><code>from artifex.generative_models.modalities.image.datasets import create_image_dataset\n\n# Create dataset using factory\ndataset = create_image_dataset(\n    dataset_type=\"synthetic\",  # or \"mnist_like\"\n    config=rgb_config,\n    pattern_type=\"gradient\",\n    dataset_size=5000,\n    rngs=rngs\n)\n\n# MNIST-like via factory\nmnist = create_image_dataset(\n    dataset_type=\"mnist_like\",\n    config=mnist_config,\n    dataset_size=60000,\n    num_classes=10,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/modalities/image/#image-preprocessing","title":"Image Preprocessing","text":""},{"location":"user-guide/modalities/image/#normalization","title":"Normalization","text":"<pre><code>import jax.numpy as jnp\n\n# Images in [0, 255] \u2192 [0, 1]\ndef normalize_uint8_images(images):\n    \"\"\"Normalize uint8 images to [0, 1].\"\"\"\n    return images.astype(jnp.float32) / 255.0\n\n# Images in [0, 1] \u2192 [-1, 1]\ndef normalize_to_symmetric(images):\n    \"\"\"Normalize to [-1, 1] range.\"\"\"\n    return images * 2.0 - 1.0\n\n# Standardization (mean=0, std=1)\ndef standardize_images(images):\n    \"\"\"Standardize images to zero mean, unit variance.\"\"\"\n    mean = jnp.mean(images, axis=(1, 2, 3), keepdims=True)\n    std = jnp.std(images, axis=(1, 2, 3), keepdims=True)\n    return (images - mean) / (std + 1e-8)\n\n# Usage\nraw_images = jnp.array([...])  # Raw pixel values\nnormalized = normalize_uint8_images(raw_images)\nstandardized = standardize_images(normalized)\n</code></pre>"},{"location":"user-guide/modalities/image/#resizing","title":"Resizing","text":"<pre><code>import jax\nfrom jax import image as jax_image\n\ndef resize_images(images, target_height, target_width, method=\"bilinear\"):\n    \"\"\"Resize images to target dimensions.\n\n    Args:\n        images: Input images (N, H, W, C)\n        target_height: Target height\n        target_width: Target width\n        method: Resize method (\"bilinear\" or \"nearest\")\n\n    Returns:\n        Resized images (N, target_height, target_width, C)\n    \"\"\"\n    batch_size = images.shape[0]\n    channels = images.shape[3]\n\n    if method == \"bilinear\":\n        # Use JAX's resize function\n        resized = jax_image.resize(\n            images,\n            shape=(batch_size, target_height, target_width, channels),\n            method=\"bilinear\"\n        )\n    elif method == \"nearest\":\n        resized = jax_image.resize(\n            images,\n            shape=(batch_size, target_height, target_width, channels),\n            method=\"nearest\"\n        )\n    else:\n        raise ValueError(f\"Unknown resize method: {method}\")\n\n    return resized\n\n# Usage\nimages = jnp.array([...])  # (N, 32, 32, 3)\nresized = resize_images(images, 64, 64, method=\"bilinear\")\nprint(resized.shape)  # (N, 64, 64, 3)\n</code></pre>"},{"location":"user-guide/modalities/image/#using-modality-processor","title":"Using Modality Processor","text":"<pre><code># Create modality with preprocessing\nconfig = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=64,\n    width=64,\n    normalize=True\n)\n\nmodality = ImageModality(config=config, rngs=rngs)\n\n# Process raw images\nraw_images = jnp.array([...])  # Any shape\nprocessed = modality.process(raw_images)\n\n# Processed images:\n# - Resized to (64, 64)\n# - Normalized to [0, 1]\n# - Batch dimension handled automatically\n</code></pre>"},{"location":"user-guide/modalities/image/#image-augmentation","title":"Image Augmentation","text":""},{"location":"user-guide/modalities/image/#basic-augmentations","title":"Basic Augmentations","text":"<pre><code>import jax\nimport jax.numpy as jnp\n\ndef random_horizontal_flip(image, key, prob=0.5):\n    \"\"\"Randomly flip image horizontally.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n        prob: Probability of flipping\n\n    Returns:\n        Flipped or original image\n    \"\"\"\n    flip = jax.random.bernoulli(key, prob)\n    return jax.lax.cond(\n        flip,\n        lambda img: jnp.flip(img, axis=1),\n        lambda img: img,\n        image\n    )\n\ndef random_vertical_flip(image, key, prob=0.5):\n    \"\"\"Randomly flip image vertically.\"\"\"\n    flip = jax.random.bernoulli(key, prob)\n    return jax.lax.cond(\n        flip,\n        lambda img: jnp.flip(img, axis=0),\n        lambda img: img,\n        image\n    )\n\ndef random_rotation(image, key):\n    \"\"\"Randomly rotate image by 0, 90, 180, or 270 degrees.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n\n    Returns:\n        Rotated image\n    \"\"\"\n    k = jax.random.randint(key, (), 0, 4)\n    return jnp.rot90(image, k=int(k), axes=(0, 1))\n\n# Usage\nkey = jax.random.key(0)\nkeys = jax.random.split(key, 3)\n\nimage = jnp.array([...])  # (H, W, C)\nimage = random_horizontal_flip(image, keys[0])\nimage = random_vertical_flip(image, keys[1])\nimage = random_rotation(image, keys[2])\n</code></pre>"},{"location":"user-guide/modalities/image/#color-augmentations","title":"Color Augmentations","text":"<pre><code>def random_brightness(image, key, delta=0.2):\n    \"\"\"Randomly adjust brightness.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n        delta: Maximum brightness change\n\n    Returns:\n        Brightness-adjusted image\n    \"\"\"\n    factor = jax.random.uniform(key, minval=1-delta, maxval=1+delta)\n    return jnp.clip(image * factor, 0, 1)\n\ndef random_contrast(image, key, delta=0.2):\n    \"\"\"Randomly adjust contrast.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n        delta: Maximum contrast change\n\n    Returns:\n        Contrast-adjusted image\n    \"\"\"\n    factor = jax.random.uniform(key, minval=1-delta, maxval=1+delta)\n    mean = jnp.mean(image)\n    return jnp.clip((image - mean) * factor + mean, 0, 1)\n\ndef random_saturation(image, key, delta=0.2):\n    \"\"\"Randomly adjust saturation (RGB only).\n\n    Args:\n        image: Input RGB image (H, W, 3)\n        key: Random key\n        delta: Maximum saturation change\n\n    Returns:\n        Saturation-adjusted image\n    \"\"\"\n    factor = jax.random.uniform(key, minval=1-delta, maxval=1+delta)\n\n    # Convert to grayscale\n    gray = jnp.mean(image, axis=-1, keepdims=True)\n\n    # Interpolate between gray and original\n    adjusted = gray + factor * (image - gray)\n\n    return jnp.clip(adjusted, 0, 1)\n\ndef random_hue(image, key, delta=0.1):\n    \"\"\"Randomly adjust hue (RGB only).\n\n    Args:\n        image: Input RGB image (H, W, 3)\n        key: Random key\n        delta: Maximum hue change\n\n    Returns:\n        Hue-adjusted image\n    \"\"\"\n    factor = jax.random.uniform(key, minval=-delta, maxval=delta)\n\n    # Simple hue rotation by channel shifting\n    r, g, b = image[..., 0], image[..., 1], image[..., 2]\n\n    # Rotate through channels\n    shifted = jnp.stack([\n        r + factor * (g - r),\n        g + factor * (b - g),\n        b + factor * (r - b)\n    ], axis=-1)\n\n    return jnp.clip(shifted, 0, 1)\n\n# Usage\nkey = jax.random.key(0)\nkeys = jax.random.split(key, 4)\n\nrgb_image = jnp.array([...])  # (H, W, 3)\nrgb_image = random_brightness(rgb_image, keys[0])\nrgb_image = random_contrast(rgb_image, keys[1])\nrgb_image = random_saturation(rgb_image, keys[2])\nrgb_image = random_hue(rgb_image, keys[3])\n</code></pre>"},{"location":"user-guide/modalities/image/#noise-augmentations","title":"Noise Augmentations","text":"<pre><code>def add_gaussian_noise(image, key, std=0.05):\n    \"\"\"Add Gaussian noise to image.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n        std: Standard deviation of noise\n\n    Returns:\n        Noisy image\n    \"\"\"\n    noise = std * jax.random.normal(key, image.shape)\n    return jnp.clip(image + noise, 0, 1)\n\ndef add_salt_pepper_noise(image, key, prob=0.01):\n    \"\"\"Add salt and pepper noise.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n        prob: Probability of noise per pixel\n\n    Returns:\n        Noisy image\n    \"\"\"\n    keys = jax.random.split(key, 2)\n\n    # Salt (white pixels)\n    salt_mask = jax.random.bernoulli(keys[0], prob, image.shape)\n    image = jnp.where(salt_mask, 1.0, image)\n\n    # Pepper (black pixels)\n    pepper_mask = jax.random.bernoulli(keys[1], prob, image.shape)\n    image = jnp.where(pepper_mask, 0.0, image)\n\n    return image\n\ndef add_speckle_noise(image, key, std=0.1):\n    \"\"\"Add multiplicative speckle noise.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n        std: Standard deviation of noise\n\n    Returns:\n        Noisy image\n    \"\"\"\n    noise = 1 + std * jax.random.normal(key, image.shape)\n    return jnp.clip(image * noise, 0, 1)\n\n# Usage\nkey = jax.random.key(0)\nkeys = jax.random.split(key, 3)\n\nimage = jnp.array([...])  # (H, W, C)\nnoisy1 = add_gaussian_noise(image, keys[0], std=0.05)\nnoisy2 = add_salt_pepper_noise(image, keys[1], prob=0.01)\nnoisy3 = add_speckle_noise(image, keys[2], std=0.1)\n</code></pre>"},{"location":"user-guide/modalities/image/#geometric-augmentations","title":"Geometric Augmentations","text":"<pre><code>def random_crop(image, key, crop_height, crop_width):\n    \"\"\"Randomly crop image.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n        crop_height: Height of crop\n        crop_width: Width of crop\n\n    Returns:\n        Cropped image\n    \"\"\"\n    h, w = image.shape[:2]\n\n    # Random starting position\n    top = jax.random.randint(key, (), 0, h - crop_height + 1)\n    left = jax.random.randint(jax.random.fold_in(key, 1), (), 0, w - crop_width + 1)\n\n    return image[top:top+crop_height, left:left+crop_width]\n\ndef center_crop(image, crop_height, crop_width):\n    \"\"\"Center crop image.\n\n    Args:\n        image: Input image (H, W, C)\n        crop_height: Height of crop\n        crop_width: Width of crop\n\n    Returns:\n        Center-cropped image\n    \"\"\"\n    h, w = image.shape[:2]\n\n    top = (h - crop_height) // 2\n    left = (w - crop_width) // 2\n\n    return image[top:top+crop_height, left:left+crop_width]\n\ndef random_zoom(image, key, zoom_range=(0.8, 1.2)):\n    \"\"\"Randomly zoom image.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n        zoom_range: (min_zoom, max_zoom)\n\n    Returns:\n        Zoomed image\n    \"\"\"\n    h, w, c = image.shape\n    zoom_factor = jax.random.uniform(key, minval=zoom_range[0], maxval=zoom_range[1])\n\n    # Calculate new size\n    new_h = int(h * zoom_factor)\n    new_w = int(w * zoom_factor)\n\n    # Resize\n    from jax import image as jax_image\n    zoomed = jax_image.resize(\n        image[jnp.newaxis, ...],\n        shape=(1, new_h, new_w, c),\n        method=\"bilinear\"\n    )[0]\n\n    # Crop or pad to original size\n    if zoom_factor &gt; 1.0:\n        # Crop\n        zoomed = center_crop(zoomed, h, w)\n    else:\n        # Pad\n        pad_h = (h - new_h) // 2\n        pad_w = (w - new_w) // 2\n        zoomed = jnp.pad(\n            zoomed,\n            ((pad_h, h - new_h - pad_h), (pad_w, w - new_w - pad_w), (0, 0)),\n            mode='constant'\n        )\n\n    return zoomed\n\n# Usage\nkey = jax.random.key(0)\nkeys = jax.random.split(key, 2)\n\nimage = jnp.array([...])  # (64, 64, 3)\ncropped = random_crop(image, keys[0], 48, 48)\nzoomed = random_zoom(image, keys[1], zoom_range=(0.9, 1.1))\n</code></pre>"},{"location":"user-guide/modalities/image/#complete-augmentation-pipeline","title":"Complete Augmentation Pipeline","text":"<pre><code>@jax.jit\ndef augment_image(image, key):\n    \"\"\"Apply comprehensive augmentation pipeline.\n\n    Args:\n        image: Input image (H, W, C)\n        key: Random key\n\n    Returns:\n        Augmented image\n    \"\"\"\n    keys = jax.random.split(key, 8)\n\n    # Geometric augmentations\n    image = random_horizontal_flip(image, keys[0], prob=0.5)\n    image = random_rotation(image, keys[1])\n\n    # Color augmentations\n    image = random_brightness(image, keys[2], delta=0.2)\n    image = random_contrast(image, keys[3], delta=0.2)\n\n    # RGB-specific\n    if image.shape[-1] == 3:\n        image = random_saturation(image, keys[4], delta=0.2)\n        image = random_hue(image, keys[5], delta=0.1)\n\n    # Noise\n    image = add_gaussian_noise(image, keys[6], std=0.02)\n\n    return image\n\n# Batch augmentation\ndef augment_batch(images, key):\n    \"\"\"Augment batch of images.\n\n    Args:\n        images: Batch of images (N, H, W, C)\n        key: Random key\n\n    Returns:\n        Augmented batch\n    \"\"\"\n    batch_size = images.shape[0]\n    keys = jax.random.split(key, batch_size)\n\n    # Vectorize over batch\n    augmented = jax.vmap(augment_image)(images, keys)\n\n    return augmented\n\n# Usage in training\nkey = jax.random.key(0)\nfor batch in data_loader:\n    key, subkey = jax.random.split(key)\n    augmented_batch = augment_batch(batch[\"images\"], subkey)\n    # Use augmented_batch for training\n</code></pre>"},{"location":"user-guide/modalities/image/#working-with-different-image-sizes","title":"Working with Different Image Sizes","text":""},{"location":"user-guide/modalities/image/#common-image-sizes","title":"Common Image Sizes","text":"<pre><code># MNIST-like (28x28 grayscale)\nmnist_config = ImageModalityConfig(\n    representation=ImageRepresentation.GRAYSCALE,\n    height=28,\n    width=28\n)\n\n# CIFAR-like (32x32 RGB)\ncifar_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=32,\n    width=32\n)\n\n# Standard (64x64 RGB)\nstandard_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=64,\n    width=64\n)\n\n# High-res (128x128 RGB)\nhighres_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=128,\n    width=128\n)\n\n# Very high-res (256x256 RGB)\nveryhighres_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=256,\n    width=256\n)\n</code></pre>"},{"location":"user-guide/modalities/image/#handling-non-square-images","title":"Handling Non-Square Images","text":"<pre><code># Wide images (16:9 aspect ratio)\nwide_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=360,\n    width=640\n)\n\n# Portrait images (9:16 aspect ratio)\nportrait_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=640,\n    width=360\n)\n\n# Custom aspect ratio\ncustom_config = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=224,\n    width=448  # 2:1 aspect ratio\n)\n</code></pre>"},{"location":"user-guide/modalities/image/#complete-examples","title":"Complete Examples","text":""},{"location":"user-guide/modalities/image/#example-1-training-with-augmentation","title":"Example 1: Training with Augmentation","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.modalities import ImageModality\nfrom artifex.generative_models.modalities.image import (\n    ImageModalityConfig,\n    ImageRepresentation,\n    SyntheticImageDataset\n)\n\n# Setup\nrngs = nnx.Rngs(0)\n\nconfig = ImageModalityConfig(\n    representation=ImageRepresentation.RGB,\n    height=64,\n    width=64,\n    normalize=True\n)\n\nmodality = ImageModality(config=config, rngs=rngs)\n\n# Create datasets\ntrain_dataset = SyntheticImageDataset(\n    config=config,\n    dataset_size=10000,\n    pattern_type=\"gradient\",\n    split=\"train\",\n    rngs=rngs\n)\n\nval_dataset = SyntheticImageDataset(\n    config=config,\n    dataset_size=1000,\n    pattern_type=\"gradient\",\n    split=\"val\",\n    rngs=rngs\n)\n\n# Training loop with augmentation\nbatch_size = 128\nnum_epochs = 10\nkey = jax.random.key(42)\n\nfor epoch in range(num_epochs):\n    # Training\n    num_batches = len(train_dataset) // batch_size\n\n    for i in range(num_batches):\n        # Get batch\n        batch = train_dataset.get_batch(batch_size)\n\n        # Apply augmentation\n        key, subkey = jax.random.split(key)\n        augmented = augment_batch(batch[\"images\"], subkey)\n\n        # Training step (placeholder)\n        # loss = train_step(model, augmented)\n\n    # Validation (no augmentation)\n    val_batches = len(val_dataset) // batch_size\n    for i in range(val_batches):\n        val_batch = val_dataset.get_batch(batch_size)\n        # Validation step\n        # val_loss = validate_step(model, val_batch[\"images\"])\n\n    print(f\"Epoch {epoch + 1}/{num_epochs} complete\")\n</code></pre>"},{"location":"user-guide/modalities/image/#example-2-multi-resolution-training","title":"Example 2: Multi-Resolution Training","text":"<pre><code># Create datasets at multiple resolutions\nresolutions = [32, 64, 128]\ndatasets = {}\n\nfor res in resolutions:\n    config = ImageModalityConfig(\n        representation=ImageRepresentation.RGB,\n        height=res,\n        width=res\n    )\n\n    datasets[res] = SyntheticImageDataset(\n        config=config,\n        dataset_size=5000,\n        pattern_type=\"random\",\n        rngs=rngs\n    )\n\n# Progressive training\nfor resolution in resolutions:\n    print(f\"Training at {resolution}x{resolution}\")\n\n    dataset = datasets[resolution]\n\n    for epoch in range(5):\n        for i in range(len(dataset) // 32):\n            batch = dataset.get_batch(32)\n            # Train at this resolution\n            # loss = train_step(model, batch[\"images\"], resolution)\n\n        print(f\"  Epoch {epoch + 1}/5 at {resolution}x{resolution}\")\n</code></pre>"},{"location":"user-guide/modalities/image/#example-3-custom-image-dataset","title":"Example 3: Custom Image Dataset","text":"<pre><code>from typing import Iterator\nfrom artifex.generative_models.modalities.base import BaseDataset\n\nclass CustomImageDataset(BaseDataset):\n    \"\"\"Custom dataset loading images from file paths.\"\"\"\n\n    def __init__(\n        self,\n        config: ImageModalityConfig,\n        image_paths: list[str],\n        labels: list[int] = None,\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(config, split, rngs=rngs)\n        self.image_paths = image_paths\n        self.labels = labels\n\n        # Load and preprocess images\n        self.images = self._load_images()\n\n    def _load_images(self):\n        \"\"\"Load images from paths.\"\"\"\n        images = []\n        for path in self.image_paths:\n            # In practice, use PIL, OpenCV, etc.\n            # For demo, generate synthetic\n            img = jax.random.uniform(\n                jax.random.key(hash(path)),\n                (self.config.height, self.config.width, self.config.channels)\n            )\n            images.append(img)\n        return images\n\n    def __len__(self) -&gt; int:\n        return len(self.images)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        for i, image in enumerate(self.images):\n            sample = {\"images\": image, \"index\": jnp.array(i)}\n            if self.labels:\n                sample[\"labels\"] = jnp.array(self.labels[i])\n            yield sample\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        key = self.rngs.sample() if \"sample\" in self.rngs else jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self))\n\n        batch_images = [self.images[int(idx)] for idx in indices]\n        batch = {\"images\": jnp.stack(batch_images), \"indices\": indices}\n\n        if self.labels:\n            batch_labels = [self.labels[int(idx)] for idx in indices]\n            batch[\"labels\"] = jnp.array(batch_labels)\n\n        return batch\n\n# Usage\nimage_paths = [\"/path/to/img1.jpg\", \"/path/to/img2.jpg\", ...]\nlabels = [0, 1, 0, 2, ...]  # Optional labels\n\ncustom_dataset = CustomImageDataset(\n    config=config,\n    image_paths=image_paths,\n    labels=labels,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/modalities/image/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/modalities/image/#do","title":"DO","text":"<p>Image Loading</p> <ul> <li>Use appropriate image resolution for your task</li> <li>Normalize images to [0, 1] or [-1, 1] consistently</li> <li>Choose representation that matches your data (RGB vs grayscale)</li> <li>Validate image shapes before training</li> <li>Cache preprocessed images when possible</li> <li>Use synthetic datasets for testing pipelines</li> </ul> <p>Augmentation</p> <ul> <li>Apply augmentation only during training, not validation</li> <li>Use JIT compilation for augmentation pipelines</li> <li>Balance augmentation strength with training stability</li> <li>Apply geometric augmentations before color augmentations</li> <li>Use vectorized operations for batch augmentation</li> <li>Test augmentations visually before training</li> </ul> <p>Performance</p> <ul> <li>Resize images to target resolution once</li> <li>Use JAX's native image operations for GPU acceleration</li> <li>Batch operations when possible</li> <li>Clear image cache periodically for long runs</li> <li>Profile image loading to identify bottlenecks</li> <li>Consider mixed precision (float16) for memory savings</li> </ul>"},{"location":"user-guide/modalities/image/#dont","title":"DON'T","text":"<p>Common Mistakes</p> <ul> <li>Mix different image resolutions in same batch</li> <li>Forget to normalize images</li> <li>Apply augmentation during validation/testing</li> <li>Use non-JAX operations in data pipeline</li> <li>Load full-resolution images if working with downscaled versions</li> <li>Ignore color space (RGB vs BGR)</li> <li>Use excessive augmentation that destroys image structure</li> </ul> <p>Performance Issues</p> <ul> <li>Load images from disk in training loop</li> <li>Use Python loops for image processing</li> <li>Apply expensive augmentations without JIT</li> <li>Keep multiple copies of images in memory</li> <li>Use very large batch sizes on limited GPU memory</li> </ul> <p>Quality Issues</p> <ul> <li>Over-augment images (too much distortion)</li> <li>Use inappropriate resize methods (nearest for photos)</li> <li>Mix normalized and unnormalized images</li> <li>Ignore aspect ratio when resizing</li> <li>Apply same augmentation to all images in batch</li> </ul>"},{"location":"user-guide/modalities/image/#summary","title":"Summary","text":"<p>This guide covered:</p> <ul> <li>Image representations - RGB, RGBA, and grayscale configurations</li> <li>Image datasets - Synthetic datasets with various patterns</li> <li>Preprocessing - Normalization, resizing, and validation</li> <li>Augmentation - Geometric, color, and noise augmentations</li> <li>Different sizes - Working with various image resolutions</li> <li>Complete examples - Training with augmentation, multi-resolution, custom datasets</li> <li>Best practices - DOs and DON'Ts for image data</li> </ul>"},{"location":"user-guide/modalities/image/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Text Modality Guide</p> <p>Learn about text tokenization, vocabulary management, and sequence handling</p> </li> <li> <p> Audio Modality Guide</p> <p>Audio waveform processing, spectrograms, and audio augmentation</p> </li> <li> <p> Multi-modal Guide</p> <p>Working with multiple modalities and aligned multi-modal datasets</p> </li> <li> <p> Data API Reference</p> <p>Complete API documentation for dataset classes and functions</p> </li> </ul>"},{"location":"user-guide/modalities/multimodal/","title":"Multi-Modal Guide","text":"<p>This guide covers working with multi-modal data in Artifex, including aligned datasets, modality fusion, cross-modal generation, and best practices for multi-modal generative models.</p>"},{"location":"user-guide/modalities/multimodal/#overview","title":"Overview","text":"<p>Artifex's multi-modal system enables working with multiple data modalities (image, text, audio) simultaneously, supporting alignment, fusion, and cross-modal generation tasks.</p> <ul> <li> <p> Modality Alignment</p> <p>Create aligned multi-modal datasets with shared latent representations</p> </li> <li> <p> Paired Datasets</p> <p>Handle explicitly paired multi-modal data with alignment scores</p> </li> <li> <p> Modality Fusion</p> <p>Combine information from multiple modalities for joint representations</p> </li> <li> <p> Cross-Modal Generation</p> <p>Generate one modality from another (e.g., image from text)</p> </li> <li> <p> Shared Latent Space</p> <p>Learn unified representations across modalities</p> </li> <li> <p> JAX-Native</p> <p>Full JAX compatibility with efficient batch processing</p> </li> </ul>"},{"location":"user-guide/modalities/multimodal/#multi-modal-datasets","title":"Multi-Modal Datasets","text":""},{"location":"user-guide/modalities/multimodal/#aligned-multi-modal-dataset","title":"Aligned Multi-Modal Dataset","text":"<p>Create datasets with aligned samples across modalities:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.modalities.multi_modal.datasets import (\n    create_synthetic_multi_modal_dataset\n)\n\n# Initialize RNG\nrngs = nnx.Rngs(0)\n\n# Create aligned multi-modal dataset\ndataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\", \"audio\"],\n    num_samples=1000,\n    alignment_strength=0.8,  # 0.0 = random, 1.0 = perfectly aligned\n    image_shape=(32, 32, 3),\n    text_vocab_size=1000,\n    text_sequence_length=50,\n    audio_sample_rate=16000,\n    audio_duration=1.0,\n    rngs=rngs\n)\n\n# Access multi-modal sample\nsample = dataset[0]\nprint(sample.keys())\n# dict_keys(['image', 'text', 'audio', 'alignment_score', 'latent'])\n\nprint(f\"Image shape: {sample['image'].shape}\")  # (32, 32, 3)\nprint(f\"Text shape: {sample['text'].shape}\")  # (50,)\nprint(f\"Audio shape: {sample['audio'].shape}\")  # (16000,)\nprint(f\"Alignment: {sample['alignment_score']}\")  # 0.8\nprint(f\"Shared latent: {sample['latent'].shape}\")  # (32,)\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#understanding-alignment-strength","title":"Understanding Alignment Strength","text":"<p>The alignment strength controls how strongly modalities are correlated:</p> <pre><code># Weakly aligned (more random)\nweak_dataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\"],\n    num_samples=1000,\n    alignment_strength=0.3,  # 30% alignment\n    rngs=rngs\n)\n\n# Moderately aligned\nmoderate_dataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\"],\n    num_samples=1000,\n    alignment_strength=0.6,  # 60% alignment\n    rngs=rngs\n)\n\n# Strongly aligned\nstrong_dataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\"],\n    num_samples=1000,\n    alignment_strength=0.9,  # 90% alignment\n    rngs=rngs\n)\n\n# Perfect alignment\nperfect_dataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\"],\n    num_samples=1000,\n    alignment_strength=1.0,  # 100% alignment\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#paired-multi-modal-dataset","title":"Paired Multi-Modal Dataset","text":"<p>For explicitly paired data:</p> <pre><code>from artifex.generative_models.modalities.multi_modal.datasets import (\n    MultiModalPairedDataset\n)\n\n# Prepare paired data\nimage_data = jnp.array([...])  # (N, H, W, C)\ntext_data = jnp.array([...])  # (N, max_length)\naudio_data = jnp.array([...])  # (N, n_samples)\n\n# Define modality pairs\npairs = [\n    (\"image\", \"text\"),\n    (\"image\", \"audio\"),\n    (\"text\", \"audio\")\n]\n\n# Optional alignment scores for each pair\nalignments = jnp.ones((len(image_data),))  # All perfectly aligned\n\n# Create paired dataset\npaired_dataset = MultiModalPairedDataset(\n    pairs=pairs,\n    data={\n        \"image\": image_data,\n        \"text\": text_data,\n        \"audio\": audio_data\n    },\n    alignments=alignments\n)\n\n# Access paired sample\nsample = paired_dataset[0]\nprint(sample[\"image\"].shape)  # (H, W, C)\nprint(sample[\"text\"].shape)  # (max_length,)\nprint(sample[\"audio\"].shape)  # (n_samples,)\nprint(sample[\"alignment_scores\"])  # 1.0\nprint(sample[\"pairs\"])  # [('image', 'text'), ('image', 'audio'), ('text', 'audio')]\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#batching-multi-modal-data","title":"Batching Multi-Modal Data","text":"<pre><code># Get batch from aligned dataset\nbatch = dataset.get_batch(batch_size=32)\n\nprint(batch[\"image\"].shape)  # (32, 32, 32, 3)\nprint(batch[\"text\"].shape)  # (32, 50)\nprint(batch[\"audio\"].shape)  # (32, 16000)\nprint(batch[\"latent\"].shape)  # (32, 32)\n\n# Iterate over paired dataset\nfor i, sample in enumerate(paired_dataset):\n    if i &gt;= 3:\n        break\n    print(f\"Sample {i}:\")\n    for modality in [\"image\", \"text\", \"audio\"]:\n        print(f\"  {modality}: {sample[modality].shape}\")\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#how-alignment-works","title":"How Alignment Works","text":"<p>The synthetic multi-modal dataset creates aligned data through a shared latent representation:</p> <pre><code># Simplified alignment process\ndef generate_aligned_sample(latent, alignment_strength):\n    \"\"\"Generate aligned multi-modal sample.\n\n    Args:\n        latent: Shared latent vector (32,)\n        alignment_strength: Strength of alignment (0-1)\n\n    Returns:\n        Dictionary with aligned modalities\n    \"\"\"\n    # Generate image from latent\n    image = generate_image_from_latent(latent, alignment_strength)\n\n    # Generate text from latent\n    text = generate_text_from_latent(latent, alignment_strength)\n\n    # Generate audio from latent\n    audio = generate_audio_from_latent(latent, alignment_strength)\n\n    return {\n        \"image\": image,\n        \"text\": text,\n        \"audio\": audio,\n        \"latent\": latent,\n        \"alignment_score\": alignment_strength\n    }\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#image-generation-from-latent","title":"Image Generation from Latent","text":"<pre><code>def generate_image_from_latent(latent, alignment_strength, image_shape=(32, 32, 3)):\n    \"\"\"Generate image from latent representation.\n\n    The latent vector modulates spatial patterns:\n    - Higher alignment \u2192 stronger influence from latent\n    - Lower alignment \u2192 more random noise\n\n    Args:\n        latent: Shared latent (32,)\n        alignment_strength: Alignment factor\n        image_shape: Target image shape\n\n    Returns:\n        Generated image\n    \"\"\"\n    h, w, c = image_shape\n\n    # Create spatial coordinate grids\n    x = jnp.linspace(-1, 1, w)\n    y = jnp.linspace(-1, 1, h)\n    xx, yy = jnp.meshgrid(x, y)\n\n    # Create patterns from latent\n    pattern = jnp.zeros((h, w))\n    for i in range(min(len(latent), 8)):\n        freq = 2 + i\n        phase = latent[i] * jnp.pi\n        amplitude = jnp.abs(latent[i])\n        pattern += amplitude * jnp.sin(freq * xx + phase) * jnp.cos(freq * yy + phase)\n\n    # Normalize pattern\n    pattern = (pattern - pattern.min()) / (pattern.max() - pattern.min() + 1e-8)\n    pattern = jnp.stack([pattern] * c, axis=-1)\n\n    # Mix with random noise based on alignment\n    key = jax.random.key(0)\n    noise = jax.random.normal(key, (h, w, c))\n\n    image = alignment_strength * pattern + (1 - alignment_strength) * noise\n\n    # Normalize to [0, 1]\n    image = (image - image.min()) / (image.max() - image.min() + 1e-8)\n\n    return image\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#text-generation-from-latent","title":"Text Generation from Latent","text":"<pre><code>def generate_text_from_latent(\n    latent,\n    alignment_strength,\n    vocab_size=1000,\n    seq_length=50\n):\n    \"\"\"Generate text from latent representation.\n\n    The latent vector biases token selection:\n    - Higher alignment \u2192 stronger bias from latent\n    - Lower alignment \u2192 more random tokens\n\n    Args:\n        latent: Shared latent (32,)\n        alignment_strength: Alignment factor\n        vocab_size: Vocabulary size\n        seq_length: Sequence length\n\n    Returns:\n        Generated token sequence\n    \"\"\"\n    # Expand latent to vocab size\n    latent_expanded = jnp.tile(latent, (vocab_size // len(latent) + 1))\n    latent_expanded = latent_expanded[:vocab_size]\n\n    # Create token probabilities from latent\n    token_logits = latent_expanded * alignment_strength\n    token_probs = jax.nn.softmax(token_logits)\n\n    # Sample tokens\n    key = jax.random.key(0)\n    tokens = []\n    for i in range(seq_length):\n        token_key = jax.random.fold_in(key, i)\n        token = jax.random.choice(token_key, vocab_size, p=token_probs)\n        tokens.append(token)\n\n    return jnp.array(tokens)\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#audio-generation-from-latent","title":"Audio Generation from Latent","text":"<pre><code>def generate_audio_from_latent(\n    latent,\n    alignment_strength,\n    sample_rate=16000,\n    duration=1.0\n):\n    \"\"\"Generate audio from latent representation.\n\n    The latent vector controls frequency content:\n    - Higher alignment \u2192 stronger latent influence\n    - Lower alignment \u2192 more random noise\n\n    Args:\n        latent: Shared latent (32,)\n        alignment_strength: Alignment factor\n        sample_rate: Sample rate in Hz\n        duration: Duration in seconds\n\n    Returns:\n        Generated audio waveform\n    \"\"\"\n    num_samples = int(sample_rate * duration)\n    t = jnp.linspace(0, duration, num_samples)\n\n    # Create audio as sum of sinusoids from latent\n    waveform = jnp.zeros(num_samples)\n\n    for i in range(min(len(latent), 10)):\n        # Frequency from latent (100-2000 Hz)\n        freq = 100 + 1900 * (jnp.abs(latent[i]) % 1)\n        phase = latent[i] * 2 * jnp.pi\n        amplitude = jnp.abs(latent[i]) * 0.1\n\n        waveform += amplitude * jnp.sin(2 * jnp.pi * freq * t + phase)\n\n    # Add noise based on alignment\n    key = jax.random.key(0)\n    noise = jax.random.normal(key, (num_samples,)) * 0.1\n\n    waveform = alignment_strength * waveform + (1 - alignment_strength) * noise\n\n    # Normalize\n    waveform = waveform / (jnp.max(jnp.abs(waveform)) + 1e-8)\n\n    return waveform\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#modality-fusion","title":"Modality Fusion","text":"<p>Combine information from multiple modalities:</p>"},{"location":"user-guide/modalities/multimodal/#early-fusion","title":"Early Fusion","text":"<p>Concatenate raw features:</p> <pre><code>def early_fusion(image, text, audio):\n    \"\"\"Concatenate features from all modalities.\n\n    Args:\n        image: Image features (H, W, C)\n        text: Text features (seq_length,)\n        audio: Audio features (n_samples,)\n\n    Returns:\n        Fused features\n    \"\"\"\n    # Flatten each modality\n    image_flat = image.reshape(-1)\n    text_flat = text.reshape(-1)\n    audio_flat = audio.reshape(-1)\n\n    # Concatenate\n    fused = jnp.concatenate([image_flat, text_flat, audio_flat])\n\n    return fused\n\n# Usage\nsample = dataset[0]\nfused_features = early_fusion(\n    sample[\"image\"],\n    sample[\"text\"],\n    sample[\"audio\"]\n)\nprint(f\"Fused features shape: {fused_features.shape}\")\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#late-fusion","title":"Late Fusion","text":"<p>Combine high-level representations:</p> <pre><code>def late_fusion(image_embedding, text_embedding, audio_embedding):\n    \"\"\"Combine embeddings from separate encoders.\n\n    Args:\n        image_embedding: Image encoder output (d_model,)\n        text_embedding: Text encoder output (d_model,)\n        audio_embedding: Audio encoder output (d_model,)\n\n    Returns:\n        Fused embedding\n    \"\"\"\n    # Option 1: Concatenation\n    fused_concat = jnp.concatenate([\n        image_embedding,\n        text_embedding,\n        audio_embedding\n    ])\n\n    # Option 2: Average pooling\n    fused_avg = (image_embedding + text_embedding + audio_embedding) / 3\n\n    # Option 3: Weighted sum\n    weights = jnp.array([0.4, 0.4, 0.2])  # image, text, audio\n    fused_weighted = (\n        weights[0] * image_embedding +\n        weights[1] * text_embedding +\n        weights[2] * audio_embedding\n    )\n\n    return fused_weighted\n\n# Usage\n# Assuming we have encoders for each modality\n# image_emb = image_encoder(sample[\"image\"])\n# text_emb = text_encoder(sample[\"text\"])\n# audio_emb = audio_encoder(sample[\"audio\"])\n# fused = late_fusion(image_emb, text_emb, audio_emb)\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#attention-based-fusion","title":"Attention-Based Fusion","text":"<p>Use attention to weight modalities:</p> <pre><code>import jax.numpy as jnp\nfrom flax import nnx\n\nclass MultiModalAttentionFusion(nnx.Module):\n    \"\"\"Attention-based multi-modal fusion.\"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        num_heads: int = 4,\n        *,\n        rngs: nnx.Rngs\n    ):\n        \"\"\"Initialize attention fusion.\n\n        Args:\n            d_model: Model dimension\n            num_heads: Number of attention heads\n            rngs: Random number generators\n        \"\"\"\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n\n        # Projection layers\n        self.query_proj = nnx.Linear(d_model, d_model, rngs=rngs)\n        self.key_proj = nnx.Linear(d_model, d_model, rngs=rngs)\n        self.value_proj = nnx.Linear(d_model, d_model, rngs=rngs)\n        self.out_proj = nnx.Linear(d_model, d_model, rngs=rngs)\n\n    def __call__(\n        self,\n        image_emb: jax.Array,\n        text_emb: jax.Array,\n        audio_emb: jax.Array,\n        *,\n        deterministic: bool = False\n    ) -&gt; jax.Array:\n        \"\"\"Fuse modality embeddings with attention.\n\n        Args:\n            image_emb: Image embedding (d_model,)\n            text_emb: Text embedding (d_model,)\n            audio_emb: Audio embedding (d_model,)\n            deterministic: Whether in eval mode\n\n        Returns:\n            Fused embedding (d_model,)\n        \"\"\"\n        # Stack embeddings (3, d_model)\n        embeddings = jnp.stack([image_emb, text_emb, audio_emb])\n\n        # Project to Q, K, V\n        queries = self.query_proj(embeddings)\n        keys = self.key_proj(embeddings)\n        values = self.value_proj(embeddings)\n\n        # Compute attention scores\n        scores = jnp.matmul(queries, keys.T) / jnp.sqrt(self.d_model)\n        attention_weights = jax.nn.softmax(scores, axis=-1)\n\n        # Apply attention\n        attended = jnp.matmul(attention_weights, values)\n\n        # Pool across modalities (average)\n        fused = jnp.mean(attended, axis=0)\n\n        # Final projection\n        output = self.out_proj(fused)\n\n        return output\n\n# Usage\nrngs = nnx.Rngs(0)\nfusion = MultiModalAttentionFusion(d_model=256, num_heads=4, rngs=rngs)\n\n# image_emb = image_encoder(sample[\"image\"])  # (256,)\n# text_emb = text_encoder(sample[\"text\"])  # (256,)\n# audio_emb = audio_encoder(sample[\"audio\"])  # (256,)\n\n# fused = fusion(image_emb, text_emb, audio_emb)\n# print(f\"Fused embedding: {fused.shape}\")  # (256,)\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#cross-modal-generation","title":"Cross-Modal Generation","text":"<p>Generate one modality from another:</p>"},{"location":"user-guide/modalities/multimodal/#image-from-text-text-to-image","title":"Image from Text (Text-to-Image)","text":"<pre><code>from flax import nnx\nimport jax.numpy as jnp\n\nclass TextToImageGenerator(nnx.Module):\n    \"\"\"Generate images from text.\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        embed_dim: int,\n        image_shape: tuple,\n        *,\n        rngs: nnx.Rngs\n    ):\n        \"\"\"Initialize text-to-image generator.\n\n        Args:\n            vocab_size: Text vocabulary size\n            embed_dim: Embedding dimension\n            image_shape: Target image shape (H, W, C)\n            rngs: Random number generators\n        \"\"\"\n        super().__init__()\n        self.image_shape = image_shape\n\n        # Text encoder\n        self.text_embed = nnx.Embed(vocab_size, embed_dim, rngs=rngs)\n        self.encoder = nnx.Linear(embed_dim, 512, rngs=rngs)\n\n        # Image decoder\n        self.decoder = nnx.Sequential(\n            nnx.Linear(512, 1024, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(1024, int(jnp.prod(jnp.array(image_shape))), rngs=rngs),\n            nnx.sigmoid\n        )\n\n    def __call__(self, text_tokens: jax.Array) -&gt; jax.Array:\n        \"\"\"Generate image from text.\n\n        Args:\n            text_tokens: Text token sequence (seq_length,)\n\n        Returns:\n            Generated image (H, W, C)\n        \"\"\"\n        # Encode text\n        text_emb = self.text_embed(text_tokens)  # (seq_length, embed_dim)\n        text_feat = jnp.mean(text_emb, axis=0)  # Pool: (embed_dim,)\n        encoded = self.encoder(text_feat)  # (512,)\n\n        # Decode to image\n        image_flat = self.decoder(encoded)\n        image = image_flat.reshape(self.image_shape)\n\n        return image\n\n# Usage\n# generator = TextToImageGenerator(\n#     vocab_size=10000,\n#     embed_dim=256,\n#     image_shape=(32, 32, 3),\n#     rngs=rngs\n# )\n# generated_image = generator(sample[\"text\"])\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#text-from-image-image-to-text","title":"Text from Image (Image-to-Text)","text":"<pre><code>class ImageToTextGenerator(nnx.Module):\n    \"\"\"Generate text from images.\"\"\"\n\n    def __init__(\n        self,\n        image_shape: tuple,\n        vocab_size: int,\n        max_length: int,\n        hidden_dim: int = 512,\n        *,\n        rngs: nnx.Rngs\n    ):\n        \"\"\"Initialize image-to-text generator.\n\n        Args:\n            image_shape: Input image shape (H, W, C)\n            vocab_size: Text vocabulary size\n            max_length: Maximum text length\n            hidden_dim: Hidden dimension\n            rngs: Random number generators\n        \"\"\"\n        super().__init__()\n        self.max_length = max_length\n        self.vocab_size = vocab_size\n\n        # Image encoder\n        image_size = int(jnp.prod(jnp.array(image_shape)))\n        self.encoder = nnx.Sequential(\n            nnx.Linear(image_size, hidden_dim, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(hidden_dim, hidden_dim, rngs=rngs)\n        )\n\n        # Text decoder\n        self.decoder = nnx.Linear(hidden_dim, vocab_size, rngs=rngs)\n\n    def __call__(self, image: jax.Array, *, rngs: nnx.Rngs | None = None) -&gt; jax.Array:\n        \"\"\"Generate text from image.\n\n        Args:\n            image: Input image (H, W, C)\n            rngs: Random number generators\n\n        Returns:\n            Generated text tokens (max_length,)\n        \"\"\"\n        # Encode image\n        image_flat = image.reshape(-1)\n        encoded = self.encoder(image_flat)  # (hidden_dim,)\n\n        # Decode to text (simplified - sample tokens)\n        tokens = []\n        for i in range(self.max_length):\n            logits = self.decoder(encoded)  # (vocab_size,)\n\n            # Sample token\n            if rngs and \"sample\" in rngs:\n                key = rngs.sample()\n                token = jax.random.categorical(key, logits)\n            else:\n                token = jnp.argmax(logits)\n\n            tokens.append(token)\n\n        return jnp.array(tokens)\n\n# Usage\n# generator = ImageToTextGenerator(\n#     image_shape=(32, 32, 3),\n#     vocab_size=10000,\n#     max_length=50,\n#     rngs=rngs\n# )\n# generated_text = generator(sample[\"image\"], rngs=rngs)\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#audio-from-text-text-to-speech","title":"Audio from Text (Text-to-Speech)","text":"<pre><code>class TextToAudioGenerator(nnx.Module):\n    \"\"\"Generate audio from text.\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        embed_dim: int,\n        audio_length: int,\n        *,\n        rngs: nnx.Rngs\n    ):\n        \"\"\"Initialize text-to-audio generator.\n\n        Args:\n            vocab_size: Text vocabulary size\n            embed_dim: Embedding dimension\n            audio_length: Target audio length in samples\n            rngs: Random number generators\n        \"\"\"\n        super().__init__()\n        self.audio_length = audio_length\n\n        # Text encoder\n        self.text_embed = nnx.Embed(vocab_size, embed_dim, rngs=rngs)\n        self.encoder = nnx.Linear(embed_dim, 512, rngs=rngs)\n\n        # Audio decoder\n        self.decoder = nnx.Sequential(\n            nnx.Linear(512, 1024, rngs=rngs),\n            nnx.relu,\n            nnx.Linear(1024, audio_length, rngs=rngs),\n            nnx.tanh  # Audio in [-1, 1]\n        )\n\n    def __call__(self, text_tokens: jax.Array) -&gt; jax.Array:\n        \"\"\"Generate audio from text.\n\n        Args:\n            text_tokens: Text token sequence (seq_length,)\n\n        Returns:\n            Generated audio waveform (audio_length,)\n        \"\"\"\n        # Encode text\n        text_emb = self.text_embed(text_tokens)  # (seq_length, embed_dim)\n        text_feat = jnp.mean(text_emb, axis=0)  # Pool\n        encoded = self.encoder(text_feat)  # (512,)\n\n        # Decode to audio\n        audio = self.decoder(encoded)  # (audio_length,)\n\n        return audio\n\n# Usage\n# generator = TextToAudioGenerator(\n#     vocab_size=10000,\n#     embed_dim=256,\n#     audio_length=16000,\n#     rngs=rngs\n# )\n# generated_audio = generator(sample[\"text\"])\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#complete-multi-modal-training-example","title":"Complete Multi-Modal Training Example","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\n# Setup\nrngs = nnx.Rngs(0)\n\n# Create multi-modal dataset\ndataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\", \"audio\"],\n    num_samples=10000,\n    alignment_strength=0.8,\n    image_shape=(32, 32, 3),\n    text_vocab_size=1000,\n    text_sequence_length=50,\n    audio_sample_rate=16000,\n    audio_duration=1.0,\n    rngs=rngs\n)\n\n# Create validation dataset\nval_dataset = create_synthetic_multi_modal_dataset(\n    modalities=[\"image\", \"text\", \"audio\"],\n    num_samples=1000,\n    alignment_strength=0.8,\n    image_shape=(32, 32, 3),\n    text_vocab_size=1000,\n    text_sequence_length=50,\n    audio_sample_rate=16000,\n    audio_duration=1.0,\n    rngs=rngs\n)\n\n# Training loop\nbatch_size = 32\nnum_epochs = 10\nkey = jax.random.key(42)\n\nfor epoch in range(num_epochs):\n    num_batches = len(dataset) // batch_size\n\n    for i in range(num_batches):\n        # Get batch\n        batch = dataset.get_batch(batch_size)\n\n        # Extract modalities\n        images = batch[\"image\"]\n        texts = batch[\"text\"]\n        audios = batch[\"audio\"]\n        latents = batch[\"latent\"]\n\n        # Training step (placeholder)\n        # 1. Encode each modality\n        # image_emb = image_encoder(images)\n        # text_emb = text_encoder(texts)\n        # audio_emb = audio_encoder(audios)\n\n        # 2. Compute alignment loss\n        # alignment_loss = contrastive_loss(image_emb, text_emb, audio_emb)\n\n        # 3. Compute reconstruction losses\n        # recon_loss_img = reconstruction_loss(images, reconstructed_images)\n        # recon_loss_text = reconstruction_loss(texts, reconstructed_texts)\n        # recon_loss_audio = reconstruction_loss(audios, reconstructed_audios)\n\n        # 4. Total loss\n        # loss = alignment_loss + recon_loss_img + recon_loss_text + recon_loss_audio\n\n        # 5. Update parameters\n        # params = optimizer.update(grads, params)\n\n    # Validation\n    val_batches = len(val_dataset) // batch_size\n    for i in range(val_batches):\n        val_batch = val_dataset.get_batch(batch_size)\n        # Validation step\n        # val_loss = validate_step(model, val_batch)\n\n    print(f\"Epoch {epoch + 1}/{num_epochs} complete\")\n</code></pre>"},{"location":"user-guide/modalities/multimodal/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/modalities/multimodal/#do","title":"DO","text":"<p>Multi-Modal Design</p> <ul> <li>Use shared latent representations for alignment</li> <li>Balance modality contributions in fusion</li> <li>Normalize features before fusion</li> <li>Use attention for dynamic modality weighting</li> <li>Test alignment quality visually/qualitatively</li> <li>Cache aligned datasets when possible</li> </ul> <p>Cross-Modal Generation</p> <ul> <li>Use separate encoders for each modality</li> <li>Implement residual connections in decoders</li> <li>Use appropriate loss functions per modality</li> <li>Test generation quality separately per modality</li> <li>Consider cycle consistency for bidirectional generation</li> </ul> <p>Training</p> <ul> <li>Use contrastive losses for alignment</li> <li>Balance reconstruction and alignment losses</li> <li>Apply modality-specific augmentation</li> <li>Monitor per-modality metrics</li> <li>Use curriculum learning for complex tasks</li> </ul>"},{"location":"user-guide/modalities/multimodal/#dont","title":"DON'T","text":"<p>Common Mistakes</p> <ul> <li>Mix different alignment strengths in same batch</li> <li>Ignore modality-specific preprocessing</li> <li>Use same architecture for all modalities</li> <li>Apply same augmentation to all modalities</li> <li>Forget to normalize embeddings before fusion</li> <li>Ignore computational cost of attention</li> </ul> <p>Alignment Issues</p> <ul> <li>Use too low alignment strength for supervised tasks</li> <li>Mix aligned and unaligned samples</li> <li>Ignore alignment scores during training</li> <li>Use mismatched modality sizes</li> <li>Forget to validate alignment quality</li> </ul> <p>Performance</p> <ul> <li>Concatenate raw features from all modalities</li> <li>Use very deep fusion networks</li> <li>Process all modalities even when not needed</li> <li>Ignore modality-specific batch sizes</li> <li>Use attention for simple fusion tasks</li> </ul>"},{"location":"user-guide/modalities/multimodal/#summary","title":"Summary","text":"<p>This guide covered:</p> <ul> <li>Multi-modal datasets - Aligned and paired datasets</li> <li>Alignment - Shared latent representations and alignment strength</li> <li>Modality fusion - Early, late, and attention-based fusion</li> <li>Cross-modal generation - Image\u2194Text, Text\u2194Audio</li> <li>Complete example - Multi-modal training pipeline</li> <li>Best practices - DOs and DON'Ts for multi-modal learning</li> </ul>"},{"location":"user-guide/modalities/multimodal/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Image Modality Guide</p> <p>Deep dive into image datasets, preprocessing, and augmentation</p> </li> <li> <p> Text Modality Guide</p> <p>Learn about text tokenization, vocabulary management, and sequences</p> </li> <li> <p> Audio Modality Guide</p> <p>Audio waveform processing, spectrograms, and audio augmentation</p> </li> <li> <p> Data API Reference</p> <p>Complete API documentation for all dataset classes and functions</p> </li> </ul>"},{"location":"user-guide/modalities/text/","title":"Text Modality Guide","text":"<p>This guide covers working with text data in Artifex, including tokenization, vocabulary management, text datasets, and best practices for text-based generative models.</p>"},{"location":"user-guide/modalities/text/#overview","title":"Overview","text":"<p>Artifex's text modality provides a unified interface for processing text data, handling tokenization, vocabulary management, and sequence processing for generative models.</p> <ul> <li> <p> Tokenization</p> <p>Built-in tokenization with special token handling (BOS, EOS, PAD, UNK)</p> </li> <li> <p> Vocabulary Management</p> <p>Configurable vocabulary size and token mapping</p> </li> <li> <p> Sequence Handling</p> <p>Padding, truncation, and sequence length management</p> </li> <li> <p> Synthetic Datasets</p> <p>Ready-to-use synthetic text datasets for testing</p> </li> <li> <p> Text Augmentation</p> <p>Token masking, replacement, and sequence augmentation</p> </li> <li> <p> JAX-Native</p> <p>Full JAX compatibility with efficient batch processing</p> </li> </ul>"},{"location":"user-guide/modalities/text/#text-configuration","title":"Text Configuration","text":""},{"location":"user-guide/modalities/text/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from artifex.generative_models.core.configuration import ModalityConfiguration\nfrom artifex.generative_models.modalities import TextModality\nfrom flax import nnx\n\n# Initialize RNG\nrngs = nnx.Rngs(0)\n\n# Configure text modality\ntext_config = ModalityConfiguration(\n    name=\"text\",\n    modality_type=\"text\",\n    metadata={\n        \"text_params\": {\n            \"vocab_size\": 10000,\n            \"max_length\": 512,\n            \"pad_token_id\": 0,\n            \"unk_token_id\": 1,\n            \"bos_token_id\": 2,\n            \"eos_token_id\": 3,\n            \"case_sensitive\": False\n        }\n    }\n)\n\n# Create modality\ntext_modality = TextModality(config=text_config, rngs=rngs)\n\n# Access configuration\nprint(f\"Vocab size: {text_modality.vocab_size}\")  # 10000\nprint(f\"Max length: {text_modality.max_length}\")  # 512\n</code></pre>"},{"location":"user-guide/modalities/text/#special-tokens","title":"Special Tokens","text":"<p>Artifex uses standard special tokens for sequence processing:</p> Token ID Purpose PAD 0 Padding token for variable-length sequences UNK 1 Unknown token for out-of-vocabulary words BOS 2 Beginning-of-sequence marker EOS 3 End-of-sequence marker <pre><code># Special token configuration\ntext_config = ModalityConfiguration(\n    name=\"text\",\n    modality_type=\"text\",\n    metadata={\n        \"text_params\": {\n            \"vocab_size\": 50000,\n            \"max_length\": 1024,\n            \"pad_token_id\": 0,    # Padding\n            \"unk_token_id\": 1,    # Unknown\n            \"bos_token_id\": 2,    # Beginning\n            \"eos_token_id\": 3,    # End\n            \"case_sensitive\": True  # Preserve case\n        }\n    }\n)\n</code></pre>"},{"location":"user-guide/modalities/text/#text-datasets","title":"Text Datasets","text":""},{"location":"user-guide/modalities/text/#synthetic-text-datasets","title":"Synthetic Text Datasets","text":"<p>Artifex provides several synthetic text dataset types:</p>"},{"location":"user-guide/modalities/text/#random-sentences","title":"Random Sentences","text":"<pre><code>from artifex.generative_models.modalities.text.datasets import SyntheticTextDataset\n\n# Create dataset with random sentences\nrandom_text = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=5000,\n    pattern_type=\"random_sentences\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Get sample\nsample = next(iter(random_text))\nprint(sample[\"text\"])  # \"the cat runs quickly\"\nprint(sample[\"text_tokens\"].shape)  # (512,) - padded to max_length\n\n# Get batch\nbatch = random_text.get_batch(batch_size=32)\nprint(batch[\"text_tokens\"].shape)  # (32, 512)\nprint(len(batch[\"texts\"]))  # 32 - list of strings\n</code></pre> <p>Generated patterns:</p> <ul> <li>Simple subject-verb-adverb sentences</li> <li>Random selection from vocabulary</li> <li>Natural-looking structure</li> </ul>"},{"location":"user-guide/modalities/text/#repeated-phrases","title":"Repeated Phrases","text":"<pre><code># Dataset with repeated phrases\nrepeated_text = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=5000,\n    pattern_type=\"repeated_phrases\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Example output: \"hello world hello world hello world\"\nsample = next(iter(repeated_text))\nprint(sample[\"text\"])\n</code></pre> <p>Useful for:</p> <ul> <li>Testing sequence models</li> <li>Pattern recognition</li> <li>Repetition detection</li> </ul>"},{"location":"user-guide/modalities/text/#numerical-sequences","title":"Numerical Sequences","text":"<pre><code># Dataset with numerical sequences\nsequences = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=5000,\n    pattern_type=\"sequences\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Example output: \"0 1 2 3 4\"\nsample = next(iter(sequences))\nprint(sample[\"text\"])\n</code></pre> <p>Useful for:</p> <ul> <li>Sequence learning tasks</li> <li>Arithmetic operations</li> <li>Ordering and counting</li> </ul>"},{"location":"user-guide/modalities/text/#palindromes","title":"Palindromes","text":"<pre><code># Dataset with palindromic patterns\npalindromes = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=5000,\n    pattern_type=\"palindromes\",\n    split=\"train\",\n    rngs=rngs\n)\n\n# Example output: \"racecar is a palindrome racecar\"\nsample = next(iter(palindromes))\nprint(sample[\"text\"])\n</code></pre> <p>Useful for:</p> <ul> <li>Reversibility testing</li> <li>Symmetry detection</li> <li>Pattern recognition</li> </ul>"},{"location":"user-guide/modalities/text/#simple-text-datasets","title":"Simple Text Datasets","text":"<p>For custom text data:</p> <pre><code>from artifex.generative_models.modalities.text.datasets import SimpleTextDataset\n\n# Your text data\ntexts = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Machine learning is a subset of artificial intelligence\",\n    \"Deep learning uses neural networks with multiple layers\",\n    \"Natural language processing enables text understanding\",\n    \"Transformers revolutionized NLP with attention mechanisms\"\n]\n\n# Create dataset\ntext_dataset = SimpleTextDataset(\n    config=text_config,\n    texts=texts,\n    split=\"train\",\n    rngs=rngs\n)\n\n# Iterate over samples\nfor sample in text_dataset:\n    print(f\"Text: {sample['text']}\")\n    print(f\"Tokens: {sample['text_tokens'].shape}\")\n    print(f\"Index: {sample['index']}\")\n    break\n\n# Get batch\nbatch = text_dataset.get_batch(batch_size=3)\nprint(batch[\"text_tokens\"].shape)  # (3, 512)\nprint(batch[\"texts\"])  # List of 3 strings\n</code></pre>"},{"location":"user-guide/modalities/text/#factory-function","title":"Factory Function","text":"<pre><code>from artifex.generative_models.modalities.text.datasets import create_text_dataset\n\n# Create synthetic dataset\ndataset = create_text_dataset(\n    config=text_config,\n    dataset_type=\"synthetic\",\n    split=\"train\",\n    pattern_type=\"random_sentences\",\n    dataset_size=10000,\n    rngs=rngs\n)\n\n# Create simple dataset\ncustom_dataset = create_text_dataset(\n    config=text_config,\n    dataset_type=\"simple\",\n    split=\"train\",\n    texts=[\"text 1\", \"text 2\", ...],\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/modalities/text/#tokenization","title":"Tokenization","text":""},{"location":"user-guide/modalities/text/#basic-tokenization","title":"Basic Tokenization","text":"<p>Artifex's text datasets use simple hash-based tokenization:</p> <pre><code>def tokenize_text(text: str, config) -&gt; jax.Array:\n    \"\"\"Tokenize text to token IDs.\n\n    Args:\n        text: Input text string\n        config: Text configuration\n\n    Returns:\n        Token sequence (max_length,)\n    \"\"\"\n    # Get parameters from config\n    text_params = config.metadata.get(\"text_params\", {})\n    vocab_size = text_params.get(\"vocab_size\", 10000)\n    max_length = text_params.get(\"max_length\", 512)\n    pad_token_id = text_params.get(\"pad_token_id\", 0)\n    bos_token_id = text_params.get(\"bos_token_id\", 2)\n    eos_token_id = text_params.get(\"eos_token_id\", 3)\n    case_sensitive = text_params.get(\"case_sensitive\", False)\n\n    # Normalize case\n    if not case_sensitive:\n        text = text.lower()\n\n    # Split into words\n    words = text.strip().split()\n\n    # Convert to tokens\n    tokens = [bos_token_id]  # Add BOS\n\n    for word in words:\n        # Simple hash-based token ID\n        token_id = hash(word) % (vocab_size - 4) + 4\n        tokens.append(token_id)\n\n    tokens.append(eos_token_id)  # Add EOS\n\n    # Pad or truncate\n    if len(tokens) &gt; max_length:\n        tokens = tokens[:max_length]\n    else:\n        tokens.extend([pad_token_id] * (max_length - len(tokens)))\n\n    return jnp.array(tokens, dtype=jnp.int32)\n\n# Usage\ntext = \"Hello world, this is a test\"\ntokens = tokenize_text(text, text_config)\nprint(tokens.shape)  # (512,)\nprint(tokens[:10])  # [2, 5234, 8761, 1234, 9876, 4321, 6543, 3, 0, 0]\n</code></pre>"},{"location":"user-guide/modalities/text/#detokenization","title":"Detokenization","text":"<pre><code>def detokenize_tokens(tokens: jax.Array, config) -&gt; str:\n    \"\"\"Convert tokens back to text.\n\n    Args:\n        tokens: Token sequence\n        config: Text configuration\n\n    Returns:\n        Detokenized text string\n    \"\"\"\n    text_params = config.metadata.get(\"text_params\", {})\n    pad_token_id = text_params.get(\"pad_token_id\", 0)\n    bos_token_id = text_params.get(\"bos_token_id\", 2)\n    eos_token_id = text_params.get(\"eos_token_id\", 3)\n\n    # Convert to list\n    token_list = tokens.tolist()\n\n    # Remove special tokens\n    filtered_tokens = []\n    for token in token_list:\n        if token in [pad_token_id, bos_token_id, eos_token_id]:\n            if token == eos_token_id:\n                break  # Stop at EOS\n            continue\n        filtered_tokens.append(token)\n\n    # Convert back to words (placeholder - in practice use vocabulary)\n    words = [f\"token_{token}\" for token in filtered_tokens]\n\n    return \" \".join(words)\n\n# Usage\ntext = \"Hello world\"\ntokens = tokenize_text(text, text_config)\nrecovered = detokenize_tokens(tokens, text_config)\nprint(recovered)\n</code></pre>"},{"location":"user-guide/modalities/text/#custom-tokenizers","title":"Custom Tokenizers","text":"<p>For production use, integrate real tokenizers:</p> <pre><code>class CustomTextDataset(BaseDataset):\n    \"\"\"Dataset with custom tokenizer.\"\"\"\n\n    def __init__(\n        self,\n        config: ModalityConfiguration,\n        texts: list[str],\n        tokenizer,  # Your tokenizer (e.g., from HuggingFace)\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(config, split, rngs=rngs)\n        self.texts = texts\n        self.tokenizer = tokenizer\n\n        # Tokenize all texts\n        self.tokens = self._tokenize_all()\n\n    def _tokenize_all(self):\n        \"\"\"Tokenize all texts using custom tokenizer.\"\"\"\n        tokens = []\n        for text in self.texts:\n            # Use your tokenizer\n            # encoded = self.tokenizer.encode(text)\n            # For demo:\n            encoded = jnp.array([2, 100, 200, 300, 3])  # BOS ... EOS\n            tokens.append(encoded)\n        return tokens\n\n    def __len__(self) -&gt; int:\n        return len(self.texts)\n\n    def __iter__(self):\n        for i, (text, tokens) in enumerate(zip(self.texts, self.tokens)):\n            yield {\n                \"text\": text,\n                \"text_tokens\": tokens,\n                \"index\": jnp.array(i)\n            }\n\n    def get_batch(self, batch_size: int):\n        key = self.rngs.sample() if \"sample\" in self.rngs else jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self))\n\n        batch_tokens = [self.tokens[int(idx)] for idx in indices]\n        batch_texts = [self.texts[int(idx)] for idx in indices]\n\n        return {\n            \"text_tokens\": jnp.stack(batch_tokens),\n            \"texts\": batch_texts,\n            \"indices\": indices\n        }\n</code></pre>"},{"location":"user-guide/modalities/text/#text-preprocessing","title":"Text Preprocessing","text":""},{"location":"user-guide/modalities/text/#padding-and-truncation","title":"Padding and Truncation","text":"<pre><code>import jax.numpy as jnp\n\ndef pad_sequence(tokens: jax.Array, max_length: int, pad_token_id: int = 0):\n    \"\"\"Pad token sequence to max_length.\n\n    Args:\n        tokens: Token sequence\n        max_length: Target length\n        pad_token_id: Padding token ID\n\n    Returns:\n        Padded sequence\n    \"\"\"\n    current_length = len(tokens)\n\n    if current_length &gt;= max_length:\n        return tokens[:max_length]\n\n    padding = jnp.full((max_length - current_length,), pad_token_id, dtype=tokens.dtype)\n    return jnp.concatenate([tokens, padding])\n\ndef truncate_sequence(tokens: jax.Array, max_length: int, eos_token_id: int = 3):\n    \"\"\"Truncate sequence and add EOS token.\n\n    Args:\n        tokens: Token sequence\n        max_length: Maximum length\n        eos_token_id: EOS token ID\n\n    Returns:\n        Truncated sequence with EOS\n    \"\"\"\n    if len(tokens) &lt;= max_length:\n        return tokens\n\n    # Truncate and add EOS\n    truncated = tokens[:max_length-1]\n    return jnp.concatenate([truncated, jnp.array([eos_token_id])])\n\n# Usage\ntokens = jnp.array([2, 100, 200, 300, 400, 3])  # BOS ... EOS\n\n# Pad to 10\npadded = pad_sequence(tokens, max_length=10, pad_token_id=0)\nprint(padded)  # [2, 100, 200, 300, 400, 3, 0, 0, 0, 0]\n\n# Truncate to 5\ntruncated = truncate_sequence(tokens, max_length=5, eos_token_id=3)\nprint(truncated)  # [2, 100, 200, 300, 3]\n</code></pre>"},{"location":"user-guide/modalities/text/#batch-padding","title":"Batch Padding","text":"<pre><code>def pad_batch(token_sequences: list[jax.Array], pad_token_id: int = 0):\n    \"\"\"Pad batch of sequences to same length.\n\n    Args:\n        token_sequences: List of token sequences\n        pad_token_id: Padding token ID\n\n    Returns:\n        Padded batch (batch_size, max_length)\n    \"\"\"\n    # Find maximum length\n    max_length = max(len(seq) for seq in token_sequences)\n\n    # Pad all sequences\n    padded = []\n    for seq in token_sequences:\n        padded_seq = pad_sequence(seq, max_length, pad_token_id)\n        padded.append(padded_seq)\n\n    return jnp.stack(padded)\n\n# Usage\nsequences = [\n    jnp.array([2, 100, 200, 3]),\n    jnp.array([2, 300, 400, 500, 600, 3]),\n    jnp.array([2, 700, 3])\n]\n\nbatch = pad_batch(sequences, pad_token_id=0)\nprint(batch.shape)  # (3, 6) - padded to longest sequence\nprint(batch)\n# [[  2 100 200   3   0   0]\n#  [  2 300 400 500 600   3]\n#  [  2 700   3   0   0   0]]\n</code></pre>"},{"location":"user-guide/modalities/text/#attention-masks","title":"Attention Masks","text":"<pre><code>def create_attention_mask(tokens: jax.Array, pad_token_id: int = 0):\n    \"\"\"Create attention mask for padded sequences.\n\n    Args:\n        tokens: Token sequence with padding\n        pad_token_id: Padding token ID\n\n    Returns:\n        Attention mask (1 for real tokens, 0 for padding)\n    \"\"\"\n    return (tokens != pad_token_id).astype(jnp.int32)\n\ndef create_causal_mask(seq_length: int):\n    \"\"\"Create causal mask for autoregressive generation.\n\n    Args:\n        seq_length: Sequence length\n\n    Returns:\n        Causal mask (seq_length, seq_length)\n    \"\"\"\n    mask = jnp.tril(jnp.ones((seq_length, seq_length)))\n    return mask\n\n# Usage\ntokens = jnp.array([2, 100, 200, 300, 3, 0, 0, 0])\n\n# Padding mask\npad_mask = create_attention_mask(tokens, pad_token_id=0)\nprint(pad_mask)  # [1 1 1 1 1 0 0 0]\n\n# Causal mask for generation\ncausal_mask = create_causal_mask(8)\nprint(causal_mask)\n# [[1 0 0 0 0 0 0 0]\n#  [1 1 0 0 0 0 0 0]\n#  [1 1 1 0 0 0 0 0]\n#  ...\n#  [1 1 1 1 1 1 1 1]]\n</code></pre>"},{"location":"user-guide/modalities/text/#positional-embeddings","title":"Positional Embeddings","text":"<p>Artifex provides multiple positional encoding methods for transformer-based models.</p>"},{"location":"user-guide/modalities/text/#learned-position-embeddings","title":"Learned Position Embeddings","text":"<p>The default approach using learnable position embeddings:</p> <pre><code>from artifex.generative_models.extensions.nlp.embeddings import TextEmbeddings\nfrom artifex.generative_models.core.configuration import ExtensionConfig\nfrom flax import nnx\n\nrngs = nnx.Rngs(0)\n\n# Configure embeddings\nconfig = ExtensionConfig(\n    weight=1.0,\n    enabled=True,\n    extensions={\n        \"embeddings\": {\n            \"embedding_dim\": 512,\n            \"vocab_size\": 50000,\n            \"max_position_embeddings\": 1024,\n            \"dropout_rate\": 0.1,\n            \"use_position_embeddings\": True\n        }\n    }\n)\n\n# Create embedding module\nembeddings = TextEmbeddings(config=config, rngs=rngs)\n\n# Embed tokens with learned positions\ntokens = jnp.array([[2, 100, 200, 300, 3]])  # [batch, seq_len]\nembedded = embeddings.embed(tokens, deterministic=True)\nprint(embedded.shape)  # (1, 5, 512)\n</code></pre>"},{"location":"user-guide/modalities/text/#rotary-position-embeddings-rope","title":"Rotary Position Embeddings (RoPE)","text":"<p>RoPE is the state-of-the-art positional encoding used in modern LLMs like Llama 2. It encodes position through rotation of embedding vectors:</p> <pre><code># Embed with RoPE (Rotary Position Embeddings)\nembedded_rope = embeddings.embed_with_rope(\n    tokens,\n    deterministic=True,\n    base=10000.0  # RoPE base frequency\n)\nprint(embedded_rope.shape)  # (1, 5, 512)\n\n# Apply RoPE to existing embeddings\nraw_embeddings = embeddings.get_token_embeddings(tokens[0])\nrotated = embeddings.apply_rope_embeddings(raw_embeddings[None], base=10000.0)\n</code></pre> <p>Key benefits of RoPE:</p> <ul> <li>Enables relative position attention patterns</li> <li>Better length generalization</li> <li>No learned parameters for positions</li> <li>Used in Llama 2, PaLM, and other modern LLMs</li> </ul>"},{"location":"user-guide/modalities/text/#sinusoidal-position-embeddings","title":"Sinusoidal Position Embeddings","text":"<p>Fixed positional encodings from the original Transformer paper \"Attention is All You Need\":</p> <pre><code># Embed with sinusoidal positions\nembedded_sin = embeddings.embed_with_sinusoidal_positions(\n    tokens,\n    deterministic=True,\n    base=10000.0\n)\nprint(embedded_sin.shape)  # (1, 5, 512)\n\n# Get raw sinusoidal encodings\nsin_encodings = embeddings.get_sinusoidal_embeddings(\n    seq_len=100,\n    dim=512,\n    base=10000.0\n)\nprint(sin_encodings.shape)  # (100, 512)\n</code></pre> <p>Formula:</p> <pre><code>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n</code></pre>"},{"location":"user-guide/modalities/text/#standalone-rope-functions","title":"Standalone RoPE Functions","text":"<p>For custom implementations, use the standalone utility functions:</p> <pre><code>from artifex.generative_models.extensions.nlp.embeddings import (\n    precompute_rope_freqs,\n    apply_rope,\n    create_sinusoidal_positions\n)\n\n# Precompute RoPE frequencies\nfreqs_sin, freqs_cos = precompute_rope_freqs(\n    dim=64,          # Must be even\n    max_seq_len=512,\n    base=10000.0\n)\n\n# Apply to query/key tensors in attention\nq = jnp.ones((2, 8, 128, 64))  # [batch, heads, seq, dim]\nk = jnp.ones((2, 8, 128, 64))\n\nq_rotated = apply_rope(q, freqs_sin, freqs_cos)\nk_rotated = apply_rope(k, freqs_sin, freqs_cos)\n\n# Create standalone sinusoidal positions\npositions = create_sinusoidal_positions(\n    max_seq_len=1024,\n    dim=512\n)\n</code></pre>"},{"location":"user-guide/modalities/text/#text-augmentation","title":"Text Augmentation","text":""},{"location":"user-guide/modalities/text/#token-masking","title":"Token Masking","text":"<pre><code>import jax\nimport jax.numpy as jnp\n\ndef mask_tokens(tokens: jax.Array, key, mask_prob: float = 0.15, mask_token_id: int = 1):\n    \"\"\"Randomly mask tokens (BERT-style).\n\n    Args:\n        tokens: Token sequence\n        key: Random key\n        mask_prob: Probability of masking\n        mask_token_id: Token ID for masked positions\n\n    Returns:\n        Masked tokens, original tokens\n    \"\"\"\n    # Create mask (don't mask special tokens)\n    special_tokens = jnp.array([0, 1, 2, 3])  # PAD, UNK, BOS, EOS\n    is_special = jnp.isin(tokens, special_tokens)\n\n    # Random mask\n    mask = jax.random.bernoulli(key, mask_prob, tokens.shape)\n    mask = mask &amp; (~is_special)  # Don't mask special tokens\n\n    # Apply mask\n    masked_tokens = jnp.where(mask, mask_token_id, tokens)\n\n    return masked_tokens, tokens\n\n# Usage\ntokens = jnp.array([2, 100, 200, 300, 400, 3, 0, 0])\nkey = jax.random.key(0)\n\nmasked, original = mask_tokens(tokens, key, mask_prob=0.15)\nprint(\"Original:\", original)\nprint(\"Masked:  \", masked)\n</code></pre>"},{"location":"user-guide/modalities/text/#token-replacement","title":"Token Replacement","text":"<pre><code>def replace_tokens(\n    tokens: jax.Array,\n    key,\n    replace_prob: float = 0.1,\n    vocab_size: int = 10000\n):\n    \"\"\"Randomly replace tokens with random tokens.\n\n    Args:\n        tokens: Token sequence\n        key: Random key\n        replace_prob: Probability of replacement\n        vocab_size: Vocabulary size\n\n    Returns:\n        Augmented tokens\n    \"\"\"\n    keys = jax.random.split(key, 2)\n\n    # Create replacement mask (don't replace special tokens)\n    special_tokens = jnp.array([0, 1, 2, 3])\n    is_special = jnp.isin(tokens, special_tokens)\n\n    mask = jax.random.bernoulli(keys[0], replace_prob, tokens.shape)\n    mask = mask &amp; (~is_special)\n\n    # Generate random tokens (from vocab, excluding special tokens)\n    random_tokens = jax.random.randint(keys[1], tokens.shape, 4, vocab_size)\n\n    # Apply replacement\n    augmented = jnp.where(mask, random_tokens, tokens)\n\n    return augmented\n\n# Usage\ntokens = jnp.array([2, 100, 200, 300, 400, 3, 0, 0])\nkey = jax.random.key(0)\n\naugmented = replace_tokens(tokens, key, replace_prob=0.1, vocab_size=10000)\nprint(\"Original:  \", tokens)\nprint(\"Augmented: \", augmented)\n</code></pre>"},{"location":"user-guide/modalities/text/#sequence-shuffling","title":"Sequence Shuffling","text":"<pre><code>def shuffle_tokens(\n    tokens: jax.Array,\n    key,\n    shuffle_prob: float = 0.1\n):\n    \"\"\"Randomly shuffle tokens within a window.\n\n    Args:\n        tokens: Token sequence\n        key: Random key\n        shuffle_prob: Probability of shuffling each position\n\n    Returns:\n        Shuffled tokens\n    \"\"\"\n    # Don't shuffle special tokens\n    special_tokens = jnp.array([0, 1, 2, 3])\n    is_special = jnp.isin(tokens, special_tokens)\n\n    # For simplicity, shuffle entire sequence\n    should_shuffle = jax.random.bernoulli(key, shuffle_prob)\n\n    def do_shuffle(t):\n        # Extract non-special tokens\n        non_special_mask = ~is_special\n        non_special_tokens = t[non_special_mask]\n\n        # Shuffle\n        shuffled_key = jax.random.key(0)\n        shuffled = jax.random.permutation(shuffled_key, non_special_tokens)\n\n        # Put back\n        result = t.copy()\n        result = jnp.where(non_special_mask, shuffled, t)\n        return result\n\n    result = jax.lax.cond(\n        should_shuffle,\n        do_shuffle,\n        lambda t: t,\n        tokens\n    )\n\n    return result\n\n# Usage\ntokens = jnp.array([2, 100, 200, 300, 400, 3, 0, 0])\nkey = jax.random.key(0)\n\nshuffled = shuffle_tokens(tokens, key, shuffle_prob=0.5)\nprint(\"Original: \", tokens)\nprint(\"Shuffled: \", shuffled)\n</code></pre>"},{"location":"user-guide/modalities/text/#complete-augmentation-pipeline","title":"Complete Augmentation Pipeline","text":"<pre><code>@jax.jit\ndef augment_text(tokens: jax.Array, key, vocab_size: int = 10000):\n    \"\"\"Apply comprehensive text augmentation.\n\n    Args:\n        tokens: Token sequence\n        key: Random key\n        vocab_size: Vocabulary size\n\n    Returns:\n        Augmented tokens\n    \"\"\"\n    keys = jax.random.split(key, 3)\n\n    # Token masking (15%)\n    tokens, _ = mask_tokens(tokens, keys[0], mask_prob=0.15)\n\n    # Token replacement (5%)\n    tokens = replace_tokens(tokens, keys[1], replace_prob=0.05, vocab_size=vocab_size)\n\n    # Note: Shuffling typically not used with masking\n    # tokens = shuffle_tokens(tokens, keys[2], shuffle_prob=0.05)\n\n    return tokens\n\n# Batch augmentation\ndef augment_text_batch(token_batch: jax.Array, key, vocab_size: int = 10000):\n    \"\"\"Augment batch of text sequences.\n\n    Args:\n        token_batch: Batch of token sequences (N, max_length)\n        key: Random key\n        vocab_size: Vocabulary size\n\n    Returns:\n        Augmented batch\n    \"\"\"\n    batch_size = token_batch.shape[0]\n    keys = jax.random.split(key, batch_size)\n\n    # Vectorize over batch\n    augmented = jax.vmap(lambda t, k: augment_text(t, k, vocab_size))(\n        token_batch, keys\n    )\n\n    return augmented\n\n# Usage in training\nkey = jax.random.key(0)\nfor batch in data_loader:\n    key, subkey = jax.random.split(key)\n    augmented_tokens = augment_text_batch(\n        batch[\"text_tokens\"],\n        subkey,\n        vocab_size=10000\n    )\n    # Use augmented_tokens for training\n</code></pre>"},{"location":"user-guide/modalities/text/#vocabulary-statistics","title":"Vocabulary Statistics","text":""},{"location":"user-guide/modalities/text/#computing-statistics","title":"Computing Statistics","text":"<pre><code>def compute_vocab_stats(dataset):\n    \"\"\"Compute vocabulary statistics for dataset.\n\n    Args:\n        dataset: Text dataset\n\n    Returns:\n        Dictionary of statistics\n    \"\"\"\n    all_tokens = set()\n    sequence_lengths = []\n    token_frequencies = {}\n\n    for sample in dataset:\n        tokens = sample[\"text_tokens\"]\n\n        # Collect unique tokens\n        all_tokens.update(tokens.tolist())\n\n        # Sequence length (excluding padding)\n        pad_token_id = 0\n        length = jnp.sum(tokens != pad_token_id)\n        sequence_lengths.append(int(length))\n\n        # Token frequencies\n        for token in tokens:\n            token = int(token)\n            if token != pad_token_id:\n                token_frequencies[token] = token_frequencies.get(token, 0) + 1\n\n    return {\n        \"unique_tokens\": len(all_tokens),\n        \"vocab_coverage\": len(all_tokens) / dataset.vocab_size,\n        \"avg_sequence_length\": jnp.mean(jnp.array(sequence_lengths)),\n        \"max_sequence_length\": max(sequence_lengths),\n        \"min_sequence_length\": min(sequence_lengths),\n        \"total_tokens\": sum(token_frequencies.values()),\n        \"most_common\": sorted(token_frequencies.items(), key=lambda x: x[1], reverse=True)[:10]\n    }\n\n# Usage\nstats = compute_vocab_stats(text_dataset)\nprint(f\"Unique tokens: {stats['unique_tokens']}\")\nprint(f\"Vocab coverage: {stats['vocab_coverage']:.2%}\")\nprint(f\"Avg sequence length: {stats['avg_sequence_length']:.1f}\")\nprint(f\"Most common tokens: {stats['most_common']}\")\n</code></pre>"},{"location":"user-guide/modalities/text/#complete-examples","title":"Complete Examples","text":""},{"location":"user-guide/modalities/text/#example-1-text-generation-dataset","title":"Example 1: Text Generation Dataset","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.core.configuration import ModalityConfiguration\nfrom artifex.generative_models.modalities.text.datasets import SyntheticTextDataset\n\n# Setup\nrngs = nnx.Rngs(0)\n\n# Configure\ntext_config = ModalityConfiguration(\n    name=\"text\",\n    modality_type=\"text\",\n    metadata={\n        \"text_params\": {\n            \"vocab_size\": 50000,\n            \"max_length\": 256,\n            \"pad_token_id\": 0,\n            \"unk_token_id\": 1,\n            \"bos_token_id\": 2,\n            \"eos_token_id\": 3,\n            \"case_sensitive\": False\n        }\n    }\n)\n\n# Create datasets\ntrain_dataset = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=100000,\n    pattern_type=\"random_sentences\",\n    split=\"train\",\n    rngs=rngs\n)\n\nval_dataset = SyntheticTextDataset(\n    config=text_config,\n    dataset_size=10000,\n    pattern_type=\"random_sentences\",\n    split=\"val\",\n    rngs=rngs\n)\n\n# Training loop\nbatch_size = 64\nnum_epochs = 10\nkey = jax.random.key(42)\n\nfor epoch in range(num_epochs):\n    num_batches = len(train_dataset) // batch_size\n\n    for i in range(num_batches):\n        # Get batch\n        batch = train_dataset.get_batch(batch_size)\n        tokens = batch[\"text_tokens\"]\n\n        # Apply augmentation during training\n        key, subkey = jax.random.split(key)\n        augmented_tokens = augment_text_batch(tokens, subkey, vocab_size=50000)\n\n        # Training step\n        # loss = train_step(model, augmented_tokens)\n\n    # Validation (no augmentation)\n    val_batches = len(val_dataset) // batch_size\n    for i in range(val_batches):\n        val_batch = val_dataset.get_batch(batch_size)\n        # val_loss = validate_step(model, val_batch[\"text_tokens\"])\n\n    print(f\"Epoch {epoch + 1}/{num_epochs} complete\")\n</code></pre>"},{"location":"user-guide/modalities/text/#example-2-custom-text-dataset","title":"Example 2: Custom Text Dataset","text":"<pre><code>from typing import Iterator\nfrom artifex.generative_models.modalities.base import BaseDataset\n\nclass CustomTextDataset(BaseDataset):\n    \"\"\"Custom text dataset from file.\"\"\"\n\n    def __init__(\n        self,\n        config: ModalityConfiguration,\n        text_file: str,\n        split: str = \"train\",\n        *,\n        rngs: nnx.Rngs,\n    ):\n        super().__init__(config, split, rngs=rngs)\n        self.text_file = text_file\n\n        # Get text parameters\n        text_params = config.metadata.get(\"text_params\", {})\n        self.vocab_size = text_params.get(\"vocab_size\", 10000)\n        self.max_length = text_params.get(\"max_length\", 512)\n\n        # Load texts\n        self.texts = self._load_texts()\n        self.tokens = self._tokenize_all()\n\n    def _load_texts(self):\n        \"\"\"Load texts from file.\"\"\"\n        texts = []\n        # In practice: with open(self.text_file) as f: ...\n        # For demo:\n        texts = [\n            \"Sample text 1\",\n            \"Sample text 2\",\n            \"Sample text 3\"\n        ]\n        return texts\n\n    def _tokenize_all(self):\n        \"\"\"Tokenize all texts.\"\"\"\n        tokens = []\n        for text in self.texts:\n            # Use tokenization function\n            token_seq = tokenize_text(text, self.config)\n            tokens.append(token_seq)\n        return tokens\n\n    def __len__(self) -&gt; int:\n        return len(self.texts)\n\n    def __iter__(self) -&gt; Iterator[dict[str, jax.Array]]:\n        for i, (text, tokens) in enumerate(zip(self.texts, self.tokens)):\n            yield {\n                \"text\": text,\n                \"text_tokens\": tokens,\n                \"index\": jnp.array(i)\n            }\n\n    def get_batch(self, batch_size: int) -&gt; dict[str, jax.Array]:\n        key = self.rngs.sample() if \"sample\" in self.rngs else jax.random.key(0)\n        indices = jax.random.randint(key, (batch_size,), 0, len(self))\n\n        batch_tokens = [self.tokens[int(idx)] for idx in indices]\n        batch_texts = [self.texts[int(idx)] for idx in indices]\n\n        return {\n            \"text_tokens\": jnp.stack(batch_tokens),\n            \"texts\": batch_texts,\n            \"indices\": indices\n        }\n\n# Usage\ncustom_dataset = CustomTextDataset(\n    config=text_config,\n    text_file=\"data/texts.txt\",\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/modalities/text/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/modalities/text/#do","title":"DO","text":"<p>Tokenization</p> <ul> <li>Use consistent tokenization across train/val/test splits</li> <li>Handle special tokens properly (BOS, EOS, PAD, UNK)</li> <li>Choose appropriate vocabulary size for your task</li> <li>Preserve case if semantically important</li> <li>Validate tokenized sequences</li> <li>Cache tokenized data when possible</li> </ul> <p>Sequence Handling</p> <ul> <li>Pad sequences to consistent length for batching</li> <li>Use attention masks to handle padding</li> <li>Truncate long sequences appropriately</li> <li>Add BOS/EOS tokens for generation tasks</li> <li>Handle variable-length sequences efficiently</li> </ul> <p>Augmentation</p> <ul> <li>Apply augmentation only during training</li> <li>Don't mask special tokens</li> <li>Balance augmentation strength</li> <li>Use JIT compilation for speed</li> <li>Validate augmented sequences</li> </ul>"},{"location":"user-guide/modalities/text/#dont","title":"DON'T","text":"<p>Common Mistakes</p> <ul> <li>Mix different tokenization schemes</li> <li>Forget to add special tokens</li> <li>Ignore padding in loss computation</li> <li>Apply augmentation during validation</li> <li>Use case-sensitive when not needed</li> <li>Exceed vocabulary size with token IDs</li> </ul> <p>Performance Issues</p> <ul> <li>Tokenize on-the-fly during training</li> <li>Use Python loops for token processing</li> <li>Load entire corpus into memory</li> <li>Recompute masks every forward pass</li> </ul> <p>Data Quality</p> <ul> <li>Skip sequence validation</li> <li>Mix different sequence lengths without padding</li> <li>Use inconsistent special token IDs</li> <li>Ignore out-of-vocabulary tokens</li> </ul>"},{"location":"user-guide/modalities/text/#summary","title":"Summary","text":"<p>This guide covered:</p> <ul> <li>Text configuration - Vocabulary, sequence length, special tokens</li> <li>Text datasets - Synthetic and custom text datasets</li> <li>Tokenization - Token mapping, padding, truncation</li> <li>Preprocessing - Attention masks, batch padding</li> <li>Positional embeddings - Learned, RoPE, and sinusoidal encoding methods</li> <li>Augmentation - Token masking, replacement, shuffling</li> <li>Vocabulary stats - Computing coverage and frequency</li> <li>Complete examples - Training pipelines and custom datasets</li> <li>Best practices - DOs and DON'Ts for text data</li> </ul>"},{"location":"user-guide/modalities/text/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Audio Modality Guide</p> <p>Learn about audio processing, spectrograms, and audio augmentation</p> </li> <li> <p> Multi-modal Guide</p> <p>Working with multiple modalities and aligned multi-modal datasets</p> </li> <li> <p> Image Modality Guide</p> <p>Deep dive into image datasets, preprocessing, and augmentation</p> </li> <li> <p> Data API Reference</p> <p>Complete API documentation for all dataset classes and functions</p> </li> </ul>"},{"location":"user-guide/models/autoregressive-guide/","title":"Autoregressive Models User Guide","text":"<p>Complete guide to building, training, and using Autoregressive Models with Artifex.</p>"},{"location":"user-guide/models/autoregressive-guide/#overview","title":"Overview","text":"<p>This guide covers practical usage of autoregressive models in Artifex, from basic setup to advanced generation techniques. You'll learn how to:</p> <ul> <li> <p> Configure AR Models</p> <p>Set up PixelCNN, WaveNet, and Transformer architectures</p> </li> <li> <p> Train Models</p> <p>Train with teacher forcing and monitor perplexity</p> </li> <li> <p> Generate Samples</p> <p>Sequential generation with various sampling strategies</p> </li> <li> <p> Optimize &amp; Sample</p> <p>Tune generation quality with temperature, top-k, and nucleus sampling</p> </li> </ul>"},{"location":"user-guide/models/autoregressive-guide/#quick-start","title":"Quick Start","text":""},{"location":"user-guide/models/autoregressive-guide/#basic-transformer-example","title":"Basic Transformer Example","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration.autoregressive_config import (\n    TransformerConfig,\n    TransformerNetworkConfig,\n)\nfrom artifex.generative_models.models.autoregressive import TransformerAutoregressiveModel\n\n# Initialize RNGs\nrngs = nnx.Rngs(params=0, dropout=1, sample=2)\n\n# Configure the transformer network\nnetwork_config = TransformerNetworkConfig(\n    name=\"transformer_network\",\n    hidden_dims=(512, 512),  # Hidden layer dimensions (required)\n    activation=\"gelu\",        # Activation function (required)\n    embed_dim=512,\n    num_heads=8,\n    mlp_ratio=4.0,\n    positional_encoding=\"sinusoidal\",\n    dropout_rate=0.1,\n)\n\n# Configure the transformer model\nconfig = TransformerConfig(\n    name=\"text_transformer\",\n    vocab_size=10000,\n    sequence_length=512,\n    network=network_config,\n    num_layers=6,\n    use_cache=True,\n)\n\n# Create model\nmodel = TransformerAutoregressiveModel(config, rngs=rngs)\n\n# Training: forward pass\nsequences = jnp.array([[1, 2, 3, 4, 5]])  # [batch, seq_len]\noutputs = model(sequences)\nlogits = outputs[\"logits\"]  # [batch, seq_len, vocab_size]\n\nprint(f\"Logits shape: {logits.shape}\")\n\n# Generation\nmodel.eval()\nsamples = model.generate(n_samples=4, max_length=128, temperature=0.8)\nprint(f\"Generated samples shape: {samples.shape}\")\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#creating-autoregressive-models","title":"Creating Autoregressive Models","text":""},{"location":"user-guide/models/autoregressive-guide/#1-pixelcnn-image-generation","title":"1. PixelCNN (Image Generation)","text":"<p>For autoregressive image generation with masked convolutions:</p> <pre><code>from artifex.generative_models.core.configuration.autoregressive_config import PixelCNNConfig\nfrom artifex.generative_models.models.autoregressive import PixelCNN\n\n# Configure PixelCNN for MNIST (28\u00d728 grayscale)\nconfig = PixelCNNConfig(\n    name=\"pixelcnn_mnist\",\n    image_shape=(28, 28, 1),\n    hidden_channels=128,\n    num_layers=7,\n    num_residual_blocks=5,\n    kernel_size=3,\n    dropout_rate=0.1,\n)\n\n# Create model\nmodel = PixelCNN(config, rngs=rngs)\n\n# Training\nimages = jnp.zeros((16, 28, 28, 1), dtype=jnp.int32)  # Values in [0, 255]\noutputs = model(images, rngs=rngs, training=True)\n\n# Loss\nbatch = {\"images\": images}\nloss_dict = model.loss_fn(batch, outputs, rngs=rngs)\nprint(f\"Loss: {loss_dict['loss']:.4f}\")\nprint(f\"Bits per dim: {loss_dict['bits_per_dim']:.4f}\")\n\n# Generation (pixel by pixel)\ngenerated = model.generate(\n    n_samples=16,\n    temperature=1.0,\n    rngs=rngs\n)\n</code></pre> <p>Key Parameters:</p> Parameter Default Description <code>image_shape</code> - (height, width, channels) <code>num_layers</code> 7 Number of masked conv layers <code>hidden_channels</code> 128 Hidden layer channels <code>num_residual_blocks</code> 5 Residual block count <p>Use Cases:</p> <ul> <li>Density estimation on images</li> <li>Lossless image compression</li> <li>Inpainting with spatial conditioning</li> </ul>"},{"location":"user-guide/models/autoregressive-guide/#2-wavenet-audio-generation","title":"2. WaveNet (Audio Generation)","text":"<p>For raw audio waveform modeling:</p> <pre><code>from artifex.generative_models.core.configuration.autoregressive_config import WaveNetConfig\nfrom artifex.generative_models.models.autoregressive import WaveNet\n\n# Configure WaveNet\nconfig = WaveNetConfig(\n    name=\"wavenet_audio\",\n    vocab_size=256,            # 8-bit mu-law encoding\n    sequence_length=16000,     # 1 second at 16kHz\n    residual_channels=128,\n    skip_channels=512,\n    num_blocks=3,              # Number of dilation stacks\n    layers_per_block=10,       # Layers per stack\n    kernel_size=2,\n    dilation_base=2,\n    use_gated_activation=True,\n)\n\n# Create WaveNet\nmodel = WaveNet(config, rngs=rngs)\n\n# Training\nwaveform = jnp.zeros((4, 16000), dtype=jnp.int32)  # 1 second at 16kHz\noutputs = model(waveform, rngs=rngs)\n\n# Loss\nbatch = {\"waveform\": waveform}\nloss_dict = model.loss_fn(batch, outputs, rngs=rngs)\n\n# Generation\ngenerated_audio = model.generate(\n    n_samples=1,\n    max_length=16000,  # 1 second\n    temperature=0.9,\n    rngs=rngs\n)\n</code></pre> <p>Key Parameters:</p> Parameter Default Description <code>num_layers</code> 30 Total dilated conv layers <code>num_stacks</code> 3 Number of dilation stacks <code>residual_channels</code> 128 Residual connection channels <code>dilation_channels</code> 256 Dilated conv channels <p>Dilation Pattern:</p> <pre><code># WaveNet uses exponentially increasing dilations\n# Stack 1: dilations = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n# Stack 2: dilations = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n# Stack 3: dilations = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n# Receptive field = 1024 time steps\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#3-transformer-sequence-modeling","title":"3. Transformer (Sequence Modeling)","text":"<p>For text, code, and general sequences:</p> <pre><code>from artifex.generative_models.core.configuration.autoregressive_config import (\n    TransformerConfig,\n    TransformerNetworkConfig,\n)\nfrom artifex.generative_models.models.autoregressive import TransformerAutoregressiveModel\n\n# Configure the transformer network\nnetwork_config = TransformerNetworkConfig(\n    name=\"transformer_network\",\n    hidden_dims=(768, 768),\n    activation=\"gelu\",\n    embed_dim=768,\n    num_heads=12,\n    mlp_ratio=4.0,\n    positional_encoding=\"sinusoidal\",\n    dropout_rate=0.1,\n)\n\n# Configure the transformer model\nconfig = TransformerConfig(\n    name=\"text_transformer\",\n    vocab_size=50000,\n    sequence_length=1024,\n    network=network_config,\n    num_layers=12,\n    dropout_rate=0.1,\n    use_cache=True,\n)\n\n# Create model\nmodel = TransformerAutoregressiveModel(config, rngs=rngs)\n\n# Training\nsequences = jnp.zeros((8, 512), dtype=jnp.int32)  # [batch, seq_len]\noutputs = model(sequences, rngs=rngs, training=True)\nlogits = outputs['logits']\n\n# Compute loss\nbatch = {\"sequences\": sequences}\nloss_dict = model.loss_fn(batch, outputs, rngs=rngs)\nprint(f\"NLL Loss: {loss_dict['nll_loss']:.4f}\")\nprint(f\"Perplexity: {loss_dict['perplexity']:.2f}\")\nprint(f\"Accuracy: {loss_dict['accuracy']:.4f}\")\n</code></pre> <p>Architecture Scaling:</p> <pre><code>from artifex.generative_models.core.configuration.autoregressive_config import (\n    TransformerConfig,\n    TransformerNetworkConfig,\n)\n\n# Small (for experiments)\nsmall_config = TransformerConfig(\n    name=\"small_transformer\",\n    vocab_size=50000,\n    sequence_length=512,\n    network=TransformerNetworkConfig(\n        name=\"small_net\",\n        hidden_dims=(256, 256),\n        activation=\"gelu\",\n        embed_dim=256,\n        num_heads=4,\n        mlp_ratio=4.0,\n    ),\n    num_layers=4,\n)\n\n# Medium (GPT-2 small)\nmedium_config = TransformerConfig(\n    name=\"medium_transformer\",\n    vocab_size=50000,\n    sequence_length=1024,\n    network=TransformerNetworkConfig(\n        name=\"medium_net\",\n        hidden_dims=(768, 768),\n        activation=\"gelu\",\n        embed_dim=768,\n        num_heads=12,\n        mlp_ratio=4.0,\n    ),\n    num_layers=12,\n)\n\n# Large (GPT-2 medium)\nlarge_config = TransformerConfig(\n    name=\"large_transformer\",\n    vocab_size=50000,\n    sequence_length=1024,\n    network=TransformerNetworkConfig(\n        name=\"large_net\",\n        hidden_dims=(1024, 1024),\n        activation=\"gelu\",\n        embed_dim=1024,\n        num_heads=16,\n        mlp_ratio=4.0,\n    ),\n    num_layers=24,\n)\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#training-autoregressive-models","title":"Training Autoregressive Models","text":""},{"location":"user-guide/models/autoregressive-guide/#teacher-forcing-training","title":"Teacher Forcing Training","text":"<p>Standard training uses ground truth previous tokens:</p> <pre><code>def train_step(model, batch, optimizer_state):\n    \"\"\"Standard teacher forcing training step.\"\"\"\n\n    def loss_fn(model):\n        # Forward pass with ground truth input\n        outputs = model(batch['sequences'], training=True, rngs=rngs)\n\n        # Compute loss\n        loss_dict = model.loss_fn(batch, outputs, rngs=rngs)\n\n        return loss_dict['loss'], loss_dict\n\n    # Compute gradients\n    (loss, metrics), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n\n    # Update parameters\n    optimizer_state = optimizer.update(grads, optimizer_state)\n\n    return loss, metrics, optimizer_state\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        loss, metrics, optimizer_state = train_step(\n            model, batch, optimizer_state\n        )\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#monitoring-training","title":"Monitoring Training","text":"<p>Track key metrics:</p> <pre><code>def train_with_monitoring(model, train_loader, val_loader, num_epochs):\n    \"\"\"Training with detailed monitoring.\"\"\"\n\n    for epoch in range(num_epochs):\n        # Training\n        train_losses = []\n        train_perplexities = []\n\n        for step, batch in enumerate(train_loader):\n            outputs = model(batch['sequences'], training=True, rngs=rngs)\n            loss_dict = model.loss_fn(batch, outputs, rngs=rngs)\n\n            train_losses.append(loss_dict['loss'])\n            train_perplexities.append(loss_dict['perplexity'])\n\n            if step % 100 == 0:\n                print(f\"Epoch {epoch}, Step {step}:\")\n                print(f\"  Loss: {loss_dict['loss']:.4f}\")\n                print(f\"  Perplexity: {loss_dict['perplexity']:.2f}\")\n                print(f\"  Accuracy: {loss_dict['accuracy']:.4f}\")\n\n        # Validation\n        if val_loader is not None:\n            val_loss, val_ppl = evaluate(model, val_loader)\n            print(f\"\\nEpoch {epoch} Validation:\")\n            print(f\"  Loss: {val_loss:.4f}\")\n            print(f\"  Perplexity: {val_ppl:.2f}\")\n\ndef evaluate(model, val_loader):\n    \"\"\"Evaluate on validation set.\"\"\"\n    total_loss = 0\n    total_tokens = 0\n\n    for batch in val_loader:\n        outputs = model(batch['sequences'], training=False, rngs=rngs)\n        loss_dict = model.loss_fn(batch, outputs, rngs=rngs)\n\n        batch_size, seq_len = batch['sequences'].shape\n        total_loss += loss_dict['loss'] * batch_size * seq_len\n        total_tokens += batch_size * seq_len\n\n    avg_loss = total_loss / total_tokens\n    perplexity = jnp.exp(avg_loss)\n\n    return avg_loss, perplexity\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Transformers benefit from learning rate warmup:</p> <pre><code>def transformer_lr_schedule(step, warmup_steps=4000, d_model=512):\n    \"\"\"Transformer learning rate schedule with warmup.\"\"\"\n    step = jnp.maximum(step, 1)\n    arg1 = step ** -0.5\n    arg2 = step * (warmup_steps ** -1.5)\n    return (d_model ** -0.5) * jnp.minimum(arg1, arg2)\n\n# Apply schedule\nlr = transformer_lr_schedule(current_step, warmup_steps=4000, d_model=768)\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#generation-and-sampling","title":"Generation and Sampling","text":""},{"location":"user-guide/models/autoregressive-guide/#1-greedy-decoding","title":"1. Greedy Decoding","text":"<p>Most likely token at each step:</p> <pre><code># Greedy generation\nsamples = model.generate(\n    n_samples=4,\n    max_length=128,\n    temperature=1.0,  # No effect with greedy (argmax)\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#2-temperature-sampling","title":"2. Temperature Sampling","text":"<p>Control randomness:</p> <pre><code># Low temperature (more deterministic)\ndeterministic_samples = model.generate(\n    n_samples=4,\n    max_length=128,\n    temperature=0.5,  # More peaked distribution\n    rngs=rngs\n)\n\n# High temperature (more random)\nrandom_samples = model.generate(\n    n_samples=4,\n    max_length=128,\n    temperature=1.5,  # Flatter distribution\n    rngs=rngs\n)\n</code></pre> <p>Temperature Guidelines:</p> <ul> <li><code>0.5</code>: Very deterministic, repetitive</li> <li><code>0.7</code>: Slightly creative, coherent</li> <li><code>1.0</code>: Sample from true model distribution</li> <li><code>1.2</code>: More diverse, less coherent</li> <li><code>1.5+</code>: Very random, often incoherent</li> </ul>"},{"location":"user-guide/models/autoregressive-guide/#3-top-k-sampling","title":"3. Top-k Sampling","text":"<p>Sample from k most likely tokens:</p> <pre><code># Top-k sampling\nsamples = model.generate(\n    n_samples=4,\n    max_length=128,\n    temperature=1.0,\n    top_k=40,  # Only consider top 40 tokens\n    rngs=rngs\n)\n</code></pre> <p>Top-k Values:</p> <ul> <li><code>k=1</code>: Greedy (deterministic)</li> <li><code>k=10</code>: Very focused</li> <li><code>k=40</code>: Balanced (common for text)</li> <li><code>k=100</code>: More diverse</li> </ul>"},{"location":"user-guide/models/autoregressive-guide/#4-top-p-nucleus-sampling","title":"4. Top-p (Nucleus) Sampling","text":"<p>Sample from smallest set with cumulative probability \u2265 p:</p> <pre><code># Top-p (nucleus) sampling\nsamples = model.generate(\n    n_samples=4,\n    max_length=128,\n    temperature=1.0,\n    top_p=0.9,  # Nucleus with 90% probability mass\n    rngs=rngs\n)\n</code></pre> <p>Top-p Values:</p> <ul> <li><code>p=0.5</code>: Very focused</li> <li><code>p=0.7</code>: Focused but creative</li> <li><code>p=0.9</code>: Balanced (recommended)</li> <li><code>p=0.95</code>: More diverse</li> <li><code>p=1.0</code>: No filtering</li> </ul>"},{"location":"user-guide/models/autoregressive-guide/#5-beam-search","title":"5. Beam Search","text":"<p>Maintain multiple hypotheses:</p> <pre><code># Beam search (returns tuple of sequences and scores)\nprompt = jnp.array([1, 45, 23, 89])  # Token IDs\nsequences, scores = model.beam_search(\n    prompt=prompt,\n    beam_size=5,\n    max_length=128,\n    rngs=rngs\n)\nprint(f\"Top sequence shape: {sequences.shape}\")  # [beam_size, max_length]\nprint(f\"Scores shape: {scores.shape}\")            # [beam_size]\n</code></pre> <p>Beam Search Use Cases:</p> <ul> <li>Machine translation</li> <li>Summarization</li> <li>When likelihood is more important than diversity</li> </ul>"},{"location":"user-guide/models/autoregressive-guide/#6-combined-strategies","title":"6. Combined Strategies","text":"<p>Combine multiple techniques:</p> <pre><code># Recommended: temperature + top-p\nsamples = model.generate(\n    n_samples=4,\n    max_length=128,\n    temperature=0.8,   # Slight sharpening\n    top_p=0.9,         # Nucleus sampling\n    rngs=rngs\n)\n\n# Alternative: temperature + top-k\nsamples = model.generate(\n    n_samples=4,\n    max_length=128,\n    temperature=0.7,\n    top_k=50,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#conditional-generation","title":"Conditional Generation","text":""},{"location":"user-guide/models/autoregressive-guide/#1-prompt-based-generation","title":"1. Prompt-Based Generation","text":"<p>Generate from a prefix:</p> <pre><code># Start with a prompt\nprompt = jnp.array([[1, 45, 23, 89]])  # Token IDs\n\n# Continue from prompt\ncontinuation = model.sample_with_conditioning(\n    conditioning=prompt,\n    n_samples=4,  # 4 completions for same prompt\n    temperature=0.8,\n    rngs=rngs\n)\n\nprint(f\"Prompt length: {prompt.shape[1]}\")\nprint(f\"Continuation shape: {continuation.shape}\")\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#2-class-conditional-generation-pixelcnn","title":"2. Class-Conditional Generation (PixelCNN)","text":"<p>For class-conditional image generation:</p> <pre><code># Add class conditioning to PixelCNN\nclass ConditionalPixelCNN(PixelCNN):\n    def __init__(self, *args, num_classes=10, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_classes = num_classes\n        self.class_embedding = nnx.Embed(\n            num_classes, self.hidden_channels, rngs=kwargs['rngs']\n        )\n\n# Generate specific class\nclass_label = 7\nconditional_images = model.generate_conditional(\n    class_label=class_label,\n    n_samples=16,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#3-inpainting-pixelcnn","title":"3. Inpainting (PixelCNN)","text":"<p>Spatial conditioning for inpainting:</p> <pre><code># Conditioning image with mask\nconditioning = jnp.zeros((28, 28, 1), dtype=jnp.int32)\nmask = jnp.zeros((28, 28))  # 0 = generate, 1 = keep\n\n# Set known pixels\nconditioning = conditioning.at[0:10, 0:10, :].set(known_values)\nmask = mask.at[0:10, 0:10].set(1)\n\n# Inpaint\ninpainted = model.inpaint(\n    conditioning=conditioning,\n    mask=mask,\n    n_samples=4,\n    temperature=1.0,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"user-guide/models/autoregressive-guide/#1-caching-for-faster-generation","title":"1. Caching for Faster Generation","text":"<p>Cache key-value pairs for Transformers:</p> <pre><code># TransformerAutoregressiveModel has a built-in generate_with_cache() method\n# that handles KV caching automatically.\n\n# Basic cached generation\nsamples = model.generate_with_cache(\n    n_samples=4,\n    max_length=128,\n    temperature=0.8,\n    rngs=rngs,\n)\n\n# Cached generation with prompt and top-p sampling\nprompt = jnp.array([1, 45, 23, 89])  # Token IDs\nsamples = model.generate_with_cache(\n    n_samples=4,\n    prompt=prompt,\n    max_length=128,\n    temperature=0.8,\n    top_p=0.9,\n    rngs=rngs,\n)\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#2-speculative-sampling","title":"2. Speculative Sampling","text":"<p>Speed up generation with a draft model:</p> <pre><code>def speculative_sampling(target_model, draft_model, n_samples, max_length):\n    \"\"\"Faster sampling using a smaller draft model.\"\"\"\n    sequences = jnp.zeros((n_samples, max_length), dtype=jnp.int32)\n\n    pos = 0\n    while pos &lt; max_length:\n        # Draft model generates k tokens quickly\n        k = 5\n        draft_tokens = draft_model.generate(\n            conditioning=sequences[:, :pos],\n            n_samples=k,\n            rngs=rngs\n        )\n\n        # Target model verifies\n        target_outputs = target_model(draft_tokens, rngs=rngs)\n        target_probs = nnx.softmax(target_outputs['logits'], axis=-1)\n\n        # Accept or reject based on probability ratios\n        # ... acceptance logic ...\n\n        pos += accepted_tokens\n\n    return sequences\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#3-prefix-tuning-for-adaptation","title":"3. Prefix Tuning for Adaptation","text":"<p>Adapt to new tasks with prefix tuning:</p> <pre><code>class PrefixTunedTransformer(TransformerAutoregressiveModel):\n    \"\"\"Transformer with learnable prefix for task adaptation.\"\"\"\n\n    def __init__(self, *args, prefix_length=10, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.prefix_length = prefix_length\n\n        # Learnable prefix embeddings\n        self.prefix_embeddings = nnx.Param(\n            jax.random.normal(\n                kwargs['rngs'].params(),\n                (prefix_length, self.embed_dim)\n            )\n        )\n\n    def forward_with_prefix(self, x, rngs):\n        \"\"\"Forward pass with prefix prepended.\"\"\"\n        batch_size = x.shape[0]\n\n        # Expand prefix for batch\n        prefix = jnp.tile(self.prefix_embeddings[None], (batch_size, 1, 1))\n\n        # Embed input\n        x_embedded = self.embedding(x)\n\n        # Concatenate prefix and input\n        x_with_prefix = jnp.concatenate([prefix, x_embedded], axis=1)\n\n        # Forward through Transformer\n        outputs = self.transformer(x_with_prefix, rngs=rngs)\n\n        return outputs\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/models/autoregressive-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li> <p> High Perplexity</p> <p>Symptoms: Perplexity stays high, poor generation</p> <p>Solutions:   - Increase model capacity   - More training epochs   - Better data preprocessing   - Check for label smoothing</p> <pre><code># Increase model size by creating a larger config\nnetwork_config = TransformerNetworkConfig(\n    name=\"larger_net\",\n    hidden_dims=(768, 768),\n    activation=\"gelu\",\n    embed_dim=768,  # from 512\n    num_heads=12,\n    mlp_ratio=4.0,\n)\nconfig = TransformerConfig(\n    name=\"larger_transformer\",\n    vocab_size=50000,\n    sequence_length=1024,\n    network=network_config,\n    num_layers=12,  # from 6\n)\n</code></pre> </li> <li> <p> Slow Generation</p> <p>Symptoms: Sequential generation takes too long</p> <p>Solutions:   - Use KV caching (Transformers)   - Reduce sequence length   - Use smaller model for drafting   - JIT compile generation</p> <pre><code>@jax.jit\ndef fast_generate(model, n_samples, max_length):\n    return model.generate(n_samples, max_length, rngs=rngs)\n</code></pre> </li> <li> <p> Repetitive Output</p> <p>Symptoms: Model generates same tokens repeatedly</p> <p>Solutions:   - Increase temperature   - Use nucleus (top-p) sampling   - Add repetition penalty   - More diverse training data</p> <pre><code># More diverse sampling\nsamples = model.generate(\n    temperature=1.2,  # Higher than 1.0\n    top_p=0.95,       # Wider nucleus\n    rngs=rngs\n)\n</code></pre> </li> <li> <p> Training Instability</p> <p>Symptoms: Loss spikes, NaN gradients</p> <p>Solutions:   - Lower learning rate   - Add gradient clipping   - Use warmup schedule   - Check data preprocessing</p> <pre><code># Gradient clipping\ngrads = jax.tree_map(\n    lambda g: jnp.clip(g, -1.0, 1.0), grads\n)\n</code></pre> </li> </ul>"},{"location":"user-guide/models/autoregressive-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/models/autoregressive-guide/#1-data-preprocessing","title":"1. Data Preprocessing","text":"<pre><code>def preprocess_text(text, tokenizer):\n    \"\"\"Proper text preprocessing.\"\"\"\n    # Tokenize\n    tokens = tokenizer.encode(text)\n\n    # Add special tokens\n    tokens = [tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]\n\n    # Pad/truncate to fixed length\n    max_length = 512\n    if len(tokens) &lt; max_length:\n        tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))\n    else:\n        tokens = tokens[:max_length]\n\n    return jnp.array(tokens)\n\ndef preprocess_image(image):\n    \"\"\"Proper image preprocessing for PixelCNN.\"\"\"\n    # Ensure uint8 values [0, 255]\n    image = jnp.clip(image, 0, 255).astype(jnp.uint8)\n    return image\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#2-start-with-small-models","title":"2. Start with Small Models","text":"<pre><code># Quick iteration with small model\nsmall_network = TransformerNetworkConfig(\n    name=\"small_net\",\n    hidden_dims=(256, 256),\n    activation=\"gelu\",\n    embed_dim=256,\n    num_heads=4,\n    mlp_ratio=4.0,\n)\n\nsmall_config = TransformerConfig(\n    name=\"small_transformer\",\n    vocab_size=10000,\n    sequence_length=128,  # Short sequences\n    network=small_network,\n    num_layers=4,         # Few layers\n)\n\nsmall_model = TransformerAutoregressiveModel(small_config, rngs=rngs)\n\n# Train quickly, verify everything works\n# Then scale up\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#3-monitor-generation-quality","title":"3. Monitor Generation Quality","text":"<pre><code>def monitor_generation_quality(model, val_prompts, epoch):\n    \"\"\"Regularly check generation quality.\"\"\"\n    print(f\"\\nEpoch {epoch} - Generation Samples:\")\n\n    for i, prompt in enumerate(val_prompts[:3]):\n        # Generate\n        completion = model.sample_with_conditioning(\n            conditioning=prompt,\n            temperature=0.8,\n            rngs=rngs\n        )\n\n        # Decode and display\n        text = tokenizer.decode(completion[0])\n        print(f\"\\nPrompt {i+1}: {tokenizer.decode(prompt[0])}\")\n        print(f\"Completion: {text}\")\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#4-use-appropriate-metrics","title":"4. Use Appropriate Metrics","text":"<pre><code># Track multiple metrics\nmetrics = {\n    \"nll_loss\": [],     # Negative log-likelihood\n    \"perplexity\": [],   # exp(nll_loss)\n    \"accuracy\": [],     # Token-level accuracy\n    \"bpd\": [],          # Bits per dimension (for images)\n}\n\n# For text generation\ndef evaluate_text_generation(model, generated_samples):\n    \"\"\"Evaluate generation quality.\"\"\"\n    return {\n        \"diversity\": compute_diversity(generated_samples),\n        \"coherence\": compute_coherence(generated_samples),\n        \"fluency\": compute_fluency(generated_samples),\n    }\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#example-complete-text-generation","title":"Example: Complete Text Generation","text":"<pre><code>from artifex.generative_models.core.configuration.autoregressive_config import (\n    TransformerConfig,\n    TransformerNetworkConfig,\n)\nfrom artifex.generative_models.models.autoregressive import TransformerAutoregressiveModel\nimport tensorflow_datasets as tfds\n\n# Load dataset (e.g., WikiText)\ntrain_ds = tfds.load('wiki40b/en', split='train')\n\n# Configure and create model\nnetwork_config = TransformerNetworkConfig(\n    name=\"transformer_network\",\n    hidden_dims=(768, 768),\n    activation=\"gelu\",\n    embed_dim=768,\n    num_heads=12,\n    mlp_ratio=4.0,\n    positional_encoding=\"sinusoidal\",\n    dropout_rate=0.1,\n)\n\nconfig = TransformerConfig(\n    name=\"text_transformer\",\n    vocab_size=50000,\n    sequence_length=512,\n    network=network_config,\n    num_layers=12,\n    dropout_rate=0.1,\n    use_cache=True,\n)\n\nmodel = TransformerAutoregressiveModel(config, rngs=rngs)\n\n# Training configuration\nlearning_rate = 1e-4\nnum_epochs = 10\nbatch_size = 32\n\n# Training loop\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    for step, batch in enumerate(train_ds.batch(batch_size)):\n        # Preprocess\n        sequences = preprocess_batch(batch)\n\n        # Forward pass\n        outputs = model(sequences, training=True, rngs=rngs)\n\n        # Compute loss\n        loss_dict = model.loss_fn(\n            {\"sequences\": sequences}, outputs, rngs=rngs\n        )\n\n        # Backward pass (via optimizer)\n        # ... update parameters ...\n\n        if step % 100 == 0:\n            print(f\"  Step {step}: Loss={loss_dict['nll_loss']:.4f}, \"\n                  f\"PPL={loss_dict['perplexity']:.2f}\")\n\n    # Generate samples\n    prompt = \"The quick brown fox\"\n    prompt_tokens = tokenizer.encode(prompt)\n    completion = model.sample_with_conditioning(\n        conditioning=jnp.array([prompt_tokens]),\n        temperature=0.8,\n        rngs=rngs\n    )\n    print(f\"\\nGeneration: {tokenizer.decode(completion[0])}\")\n\nprint(\"Training complete!\")\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/models/autoregressive-guide/#gpu-utilization","title":"GPU Utilization","text":"<pre><code># Move to GPU\nfrom artifex.generative_models.core.device_manager import DeviceManager\n\ndevice_manager = DeviceManager()\ndevice = device_manager.get_device()\n\n# Move model and data to GPU\nmodel = jax.device_put(model, device)\nbatch = jax.device_put(batch, device)\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#batch-size-tuning","title":"Batch Size Tuning","text":"<pre><code># Larger batches for better GPU utilization\n# But: limited by memory\n\n# PixelCNN (memory intensive)\npixelcnn_batch_size = 32\n\n# Transformer (depends on sequence length)\ntransformer_batch_sizes = {\n    128: 256,   # Short sequences\n    512: 64,    # Medium sequences\n    1024: 16,   # Long sequences\n}\n\n# WaveNet (very memory intensive)\nwavenet_batch_size = 4\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code># Use bfloat16 for faster training\nfrom jax import config\nconfig.update(\"jax_enable_x64\", False)\n\n# Model automatically uses bfloat16 on TPU\n</code></pre>"},{"location":"user-guide/models/autoregressive-guide/#further-reading","title":"Further Reading","text":"<ul> <li>Autoregressive Explained - Theoretical foundations</li> <li>AR API Reference - Complete API documentation</li> <li>Training Guide - General training workflows</li> <li>Examples - More AR examples</li> </ul>"},{"location":"user-guide/models/autoregressive-guide/#summary","title":"Summary","text":"<p>Key Takeaways:</p> <ul> <li>Autoregressive models factorize probability via chain rule: \\(p(x) = \\prod_i p(x_i | x_{&lt;i})\\)</li> <li>Training uses teacher forcing with cross-entropy loss</li> <li>Generation is sequential, one token at a time</li> <li>Sampling strategies (temperature, top-k, top-p) control diversity vs quality</li> <li>PixelCNN for images, WaveNet for audio, Transformers for text</li> </ul> <p>Recommended Workflow:</p> <ol> <li>Choose architecture based on data type (PixelCNN/WaveNet/Transformer)</li> <li>Start with small model for quick iteration</li> <li>Train with teacher forcing, monitor perplexity</li> <li>Generate samples with temperature=0.8, top_p=0.9</li> <li>Scale up model size for better quality</li> <li>Use caching and JIT for faster inference</li> </ol> <p>For theoretical understanding, see the Autoregressive Explained guide.</p>"},{"location":"user-guide/models/diffusion-guide/","title":"Diffusion Models User Guide","text":"<p>This guide covers practical usage of diffusion models in Artifex, from basic DDPM to advanced techniques like latent diffusion and guidance.</p>"},{"location":"user-guide/models/diffusion-guide/#quick-start","title":"Quick Start","text":"<p>Here's a minimal example to get started with diffusion models:</p> <pre><code>import jax.numpy as jnp\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration.backbone_config import UNetBackboneConfig\nfrom artifex.generative_models.core.configuration.diffusion_config import (\n    DDPMConfig,\n    NoiseScheduleConfig,\n)\nfrom artifex.generative_models.models.diffusion.ddpm import DDPMModel\n\n# Initialize RNGs with required streams\nrngs = nnx.Rngs(params=0, noise=1, sample=2, dropout=3)\n\n# Configure the backbone network (UNet)\nbackbone_config = UNetBackboneConfig(\n    name=\"unet_backbone\",\n    hidden_dims=(64, 128, 256),\n    activation=\"gelu\",\n    in_channels=1,\n    out_channels=1,\n    time_embedding_dim=128,\n    attention_resolutions=(16, 8),\n    num_res_blocks=2,\n    channel_mult=(1, 2, 4),\n)\n\n# Configure the noise schedule\nnoise_schedule_config = NoiseScheduleConfig(\n    name=\"linear_schedule\",\n    schedule_type=\"linear\",\n    num_timesteps=1000,\n    beta_start=1e-4,\n    beta_end=2e-2,\n)\n\n# Configure the DDPM model\nconfig = DDPMConfig(\n    name=\"ddpm_mnist\",\n    backbone=backbone_config,\n    noise_schedule=noise_schedule_config,\n    input_shape=(28, 28, 1),\n    loss_type=\"mse\",\n    clip_denoised=True,\n)\n\n# Create model\nmodel = DDPMModel(config, rngs=rngs)\n\n# Generate samples\nmodel.eval()  # Set to evaluation mode\nsamples = model.generate(n_samples=16)\nprint(f\"Generated samples shape: {samples.shape}\")  # (16, 28, 28, 1)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#creating-diffusion-models","title":"Creating Diffusion Models","text":""},{"location":"user-guide/models/diffusion-guide/#ddpm-denoising-diffusion-probabilistic-models","title":"DDPM (Denoising Diffusion Probabilistic Models)","text":"<p>DDPM is the foundational diffusion model with stable training and excellent quality.</p> <pre><code>from artifex.generative_models.core.configuration.backbone_config import UNetBackboneConfig\nfrom artifex.generative_models.core.configuration.diffusion_config import (\n    DDPMConfig,\n    NoiseScheduleConfig,\n)\nfrom artifex.generative_models.models.diffusion.ddpm import DDPMModel\n\n# Configure backbone\nbackbone_config = UNetBackboneConfig(\n    name=\"unet_32x32\",\n    hidden_dims=(64, 128, 256),\n    in_channels=3,\n    out_channels=3,\n    time_embedding_dim=128,\n)\n\n# Configure noise schedule\nnoise_config = NoiseScheduleConfig(\n    name=\"linear\",\n    schedule_type=\"linear\",       # or \"cosine\"\n    num_timesteps=1000,           # Number of diffusion steps\n    beta_start=1e-4,              # Starting noise level\n    beta_end=0.02,                # Ending noise level\n)\n\n# Configure DDPM\nconfig = DDPMConfig(\n    name=\"ddpm_model\",\n    backbone=backbone_config,\n    noise_schedule=noise_config,\n    input_shape=(32, 32, 3),\n    loss_type=\"mse\",\n    clip_denoised=True,\n)\n\n# Create model\nmodel = DDPMModel(config, rngs=rngs)\n\n# Forward diffusion (add noise)\nx_clean = jnp.ones((4, 32, 32, 3))\nt = jnp.array([100, 200, 300, 400])  # Different timesteps\nx_noisy, noise = model.forward_diffusion(x_clean, t)\n\nprint(f\"Clean shape: {x_clean.shape}\")\nprint(f\"Noisy shape: {x_noisy.shape}\")\n</code></pre> <p>Key Configuration Objects:</p> Config Description <code>UNetBackboneConfig</code> Neural network architecture for noise prediction <code>NoiseScheduleConfig</code> How noise is added across timesteps <code>DDPMConfig</code> Complete DDPM model configuration"},{"location":"user-guide/models/diffusion-guide/#ddim-faster-sampling","title":"DDIM (Faster Sampling)","text":"<p>DDIM enables much faster sampling with fewer steps while maintaining quality.</p> <pre><code>from artifex.generative_models.core.configuration.diffusion_config import DDIMConfig\n\n# Note: DDIMModel is not yet exported from the diffusion package __init__.py,\n# so import directly from the submodule:\nfrom artifex.generative_models.models.diffusion.ddim import DDIMModel\n\n# Configure DDIM\nconfig = DDIMConfig(\n    name=\"ddim_model\",\n    backbone=backbone_config,\n    noise_schedule=noise_config,\n    input_shape=(32, 32, 3),\n    num_inference_steps=50,       # Sampling steps (much fewer than 1000!)\n    eta=0.0,                      # 0 = deterministic, 1 = stochastic\n)\n\n# Create DDIM model\nmodel = DDIMModel(config, rngs=rngs)\n\n# Fast sampling - steps configured via num_inference_steps in config\nmodel.eval()\nsamples = model.generate(n_samples=16)\n\nprint(f\"Generated {samples.shape[0]} samples in only 50 steps!\")\n</code></pre> <p>DDIM vs DDPM:</p> Aspect DDPM DDIM Sampling Steps 1000 50-100 Speed Slow 10-20x faster Stochasticity Stochastic Deterministic (\u03b7=0) Quality Excellent Very good Use Case Training, quality Inference, speed"},{"location":"user-guide/models/diffusion-guide/#ddim-inversion-image-editing","title":"DDIM Inversion (Image Editing)","text":"<p>DDIM's deterministic nature enables image editing through inversion:</p> <pre><code># Encode a real image to noise\nreal_image = load_image(\"path/to/image.png\")  # Shape: (1, 32, 32, 3)\n\n# DDIM reverse (image \u2192 noise)\nnoise_code = model.ddim_reverse(\n    real_image,\n    ddim_steps=50,\n)\n\n# Now you can edit the noise and decode back\nedited_noise = noise_code + 0.1 * modification_vector\n\n# DDIM forward (noise \u2192 image)\nedited_image = model.ddim_sample(\n    n_samples=1,\n    steps=50,\n)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#score-based-diffusion-models","title":"Score-Based Diffusion Models","text":"<p>Score-based models predict the score function (gradient of log-likelihood) using continuous-time SDEs.</p> <pre><code>from artifex.generative_models.core.configuration.diffusion_config import (\n    ScoreDiffusionConfig,\n    NoiseScheduleConfig,\n)\nfrom artifex.generative_models.core.configuration.backbone_config import UNetBackboneConfig\nfrom artifex.generative_models.models.diffusion import ScoreDiffusionModel\n\n# Score-based configuration uses ScoreDiffusionConfig with nested configs\nconfig = ScoreDiffusionConfig(\n    name=\"score_model\",\n    backbone=UNetBackboneConfig(\n        name=\"unet_score\",\n        hidden_dims=(64, 128, 256),\n        in_channels=3,\n        out_channels=3,\n    ),\n    noise_schedule=NoiseScheduleConfig(\n        name=\"linear\",\n        schedule_type=\"linear\",\n        num_timesteps=1000,\n    ),\n    input_shape=(32, 32, 3),\n    sigma_min=0.01,              # Minimum noise level\n    sigma_max=50.0,              # Maximum noise level\n    score_scaling=1.0,           # Score scaling factor\n)\n\n# Create model\nmodel = ScoreDiffusionModel(config=config, rngs=rngs)\n\n# Generate samples using reverse SDE\nsamples = model.sample(\n    num_samples=16,\n    num_steps=1000,\n    return_trajectory=False,\n)\n</code></pre> <p>Score-Based Features:</p> <ul> <li>Continuous-time formulation</li> <li>Flexible noise schedules</li> <li>Connection to score matching theory</li> <li>Can use various SDE solvers</li> </ul>"},{"location":"user-guide/models/diffusion-guide/#latent-diffusion-models-efficient-high-res","title":"Latent Diffusion Models (Efficient High-Res)","text":"<p>Latent diffusion applies diffusion in a compressed latent space for efficiency.</p> <pre><code>from artifex.generative_models.core.configuration.diffusion_config import (\n    LatentDiffusionConfig,\n    NoiseScheduleConfig,\n)\nfrom artifex.generative_models.core.configuration.backbone_config import UNetBackboneConfig\nfrom artifex.generative_models.core.configuration.network_configs import (\n    EncoderConfig,\n    DecoderConfig,\n)\nfrom artifex.generative_models.models.diffusion import LDMModel\n\n# Latent diffusion uses LatentDiffusionConfig with nested encoder/decoder configs\nconfig = LatentDiffusionConfig(\n    name=\"ldm_model\",\n    backbone=UNetBackboneConfig(\n        name=\"unet_latent\",\n        hidden_dims=(64, 128),\n        in_channels=1,       # Must match latent spatial channels\n        out_channels=1,\n    ),\n    noise_schedule=NoiseScheduleConfig(\n        name=\"linear\",\n        schedule_type=\"linear\",\n        num_timesteps=1000,\n    ),\n    input_shape=(64, 64, 3),       # High resolution input\n    encoder=EncoderConfig(\n        name=\"encoder\",\n        input_shape=(64, 64, 3),\n        latent_dim=16,\n        hidden_dims=(64, 128),\n    ),\n    decoder=DecoderConfig(\n        name=\"decoder\",\n        latent_dim=16,\n        output_shape=(64, 64, 3),\n        hidden_dims=(128, 64),\n    ),\n    latent_scale_factor=0.18215,   # Latent scaling\n)\n\n# Create latent diffusion model\nmodel = LDMModel(config=config, rngs=rngs)\n\n# The model automatically encodes to latent space\n# Training happens in latent space (much faster!)\nsamples = model.sample(num_samples=16)\n# Samples are automatically decoded to pixel space\nprint(f\"High-res samples: {samples.shape}\")  # (16, 64, 64, 3)\n</code></pre> <p>LDM Advantages:</p> <ul> <li>8x faster training than pixel-space diffusion</li> <li>Lower memory requirements</li> <li>Enables high-resolution generation</li> <li>Foundation of Stable Diffusion</li> </ul>"},{"location":"user-guide/models/diffusion-guide/#diffusion-transformers-dit","title":"Diffusion Transformers (DiT)","text":"<p>DiT uses a Vision Transformer backbone for better scalability.</p> <pre><code>from artifex.generative_models.core.configuration.diffusion_config import (\n    DiTConfig,\n    NoiseScheduleConfig,\n)\nfrom artifex.generative_models.models.diffusion import DiTModel\n\n# DiT uses DiTConfig with transformer-specific parameters\nconfig = DiTConfig(\n    name=\"dit_model\",\n    noise_schedule=NoiseScheduleConfig(\n        name=\"linear\",\n        schedule_type=\"linear\",\n        num_timesteps=1000,\n    ),\n    input_shape=(3, 32, 32),         # (C, H, W) format for DiT\n    patch_size=4,                     # Patch size (32/4 = 8 patches per side)\n    hidden_size=512,                  # Transformer hidden dimension\n    depth=12,                         # Number of transformer layers\n    num_heads=8,                      # Number of attention heads\n    mlp_ratio=4.0,                    # MLP expansion ratio\n    num_classes=10,                   # For conditional generation\n    learn_sigma=False,                # Learn variance\n    cfg_scale=2.0,                    # Classifier-free guidance scale\n)\n\n# Create DiT model\nmodel = DiTModel(config, rngs=rngs)\n\n# Generate with class conditioning\nclass_labels = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  # One of each class\n\nsamples = model.generate(\n    n_samples=10,\n    rngs=rngs,            # rngs is required for DiT.generate()\n    y=class_labels,\n    cfg_scale=2.0,        # Classifier-free guidance\n    num_steps=1000,\n)\n</code></pre> <p>DiT Architecture:</p> <pre><code>graph TD\n    Input[Image 32\u00d732\u00d73] --&gt; Patch[Patchify&lt;br/&gt;8\u00d78 patches]\n    Patch --&gt; Embed[Linear Projection]\n    Embed --&gt; PE[+ Position Embedding]\n\n    Time[Timestep t] --&gt; TEmb[Time MLP]\n    Class[Class y] --&gt; CEmb[Class Embedding]\n\n    PE --&gt; T1[Transformer&lt;br/&gt;Block 1]\n    TEmb --&gt; T1\n    CEmb --&gt; T1\n\n    T1 --&gt; T2[...]\n    T2 --&gt; T12[Transformer&lt;br/&gt;Block 12]\n\n    T12 --&gt; Final[Final Layer Norm]\n    Final --&gt; Linear[Linear&lt;br/&gt;Projection]\n    Linear --&gt; Reshape[Reshape to Image]\n    Reshape --&gt; Output[Predicted Noise]\n\n    style T1 fill:#9C27B0\n    style T12 fill:#9C27B0</code></pre>"},{"location":"user-guide/models/diffusion-guide/#training-diffusion-models","title":"Training Diffusion Models","text":""},{"location":"user-guide/models/diffusion-guide/#basic-training-loop-with-diffusiontrainer","title":"Basic Training Loop with DiffusionTrainer","text":"<p>The recommended approach uses the JIT-compatible <code>DiffusionTrainer</code> which handles timestep sampling, noise generation, loss computation, and EMA updates:</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport optax\nfrom datarax import from_source\nfrom datarax.core.config import ElementOperatorConfig\nfrom datarax.dag.nodes import OperatorNode\nfrom datarax.operators import ElementOperator\nfrom datarax.sources import TfdsDataSourceConfig, TFDSSource\nfrom flax import nnx\n\nfrom artifex.generative_models.models.diffusion import DDPMModel\nfrom artifex.generative_models.core.configuration import (\n    DDPMConfig, UNetBackboneConfig, NoiseScheduleConfig,\n)\nfrom artifex.generative_models.training.trainers import (\n    DiffusionTrainer, DiffusionTrainingConfig,\n)\n\n# 1. Load Fashion-MNIST with datarax\ndef normalize(element, _key):\n    \"\"\"Normalize images to [-1, 1] for diffusion models.\"\"\"\n    image = element.data[\"image\"].astype(jnp.float32) / 127.5 - 1.0\n    return element.replace(data={**element.data, \"image\": image})\n\nsource = TFDSSource(\n    TfdsDataSourceConfig(name=\"fashion_mnist\", split=\"train\", shuffle=True),\n    rngs=nnx.Rngs(0),\n)\nnormalize_op = ElementOperator(\n    ElementOperatorConfig(stochastic=False), fn=normalize, rngs=nnx.Rngs(1)\n)\npipeline = from_source(source, batch_size=64) &gt;&gt; OperatorNode(normalize_op)\n\n# 2. Create DDPM model\nconfig = DDPMConfig(\n    name=\"fashion_ddpm\",\n    input_shape=(28, 28, 1),\n    backbone=UNetBackboneConfig(\n        name=\"unet\", in_channels=1, out_channels=1,\n        hidden_dims=(32, 64, 128), channel_mult=(1, 2, 4), activation=\"silu\",\n    ),\n    noise_schedule=NoiseScheduleConfig(\n        name=\"cosine\", schedule_type=\"cosine\", num_timesteps=1000,\n    ),\n)\nmodel = DDPMModel(config, rngs=nnx.Rngs(42))\noptimizer = nnx.Optimizer(model, optax.adamw(1e-4), wrt=nnx.Param)\n\n# 3. Configure trainer with SOTA techniques\ntrainer = DiffusionTrainer(\n    noise_schedule=model.noise_schedule,\n    config=DiffusionTrainingConfig(\n        prediction_type=\"epsilon\",      # Predict noise (standard DDPM)\n        timestep_sampling=\"uniform\",    # Uniform timestep sampling\n        loss_weighting=\"min_snr\",       # Min-SNR weighting for faster convergence\n        snr_gamma=5.0,\n        ema_decay=0.9999,               # EMA for stable generation\n        ema_update_every=10,\n    ),\n)\n\n# JIT-compile for performance\njit_train_step = nnx.jit(trainer.train_step)\n\n# 4. Training loop\nrng = jax.random.PRNGKey(0)\nnum_epochs = 5\nstep = 0\n\nfor epoch in range(num_epochs):\n    source.reset()  # Reset data source for new epoch\n\n    for batch in pipeline:\n        rng, step_rng = jax.random.split(rng)\n\n        # JIT-compiled train_step handles:\n        # - Timestep sampling (uniform/logit-normal/mode)\n        # - Noise generation and forward diffusion\n        # - Loss computation with weighting (uniform/SNR/min-SNR/EDM)\n        # - Gradient computation and optimizer update\n        _, metrics = jit_train_step(model, optimizer, {\"image\": batch[\"image\"]}, step_rng)\n\n        # EMA updates called separately (outside JIT boundary)\n        trainer.update_ema(model)\n\n        if step % 100 == 0:\n            print(f\"Epoch {epoch}, Step {step}, Loss: {metrics['loss']:.4f}\")\n        step += 1\n</code></pre> <p>DiffusionTrainingConfig Options:</p> Parameter Options Description <code>prediction_type</code> <code>\"epsilon\"</code>, <code>\"v_prediction\"</code>, <code>\"sample\"</code> What the model predicts <code>timestep_sampling</code> <code>\"uniform\"</code>, <code>\"logit_normal\"</code>, <code>\"mode\"</code> How timesteps are sampled <code>loss_weighting</code> <code>\"uniform\"</code>, <code>\"snr\"</code>, <code>\"min_snr\"</code>, <code>\"edm\"</code> Loss weighting scheme <code>snr_gamma</code> float (default 5.0) Gamma for min-SNR weighting <code>ema_decay</code> float (default 0.9999) EMA decay rate"},{"location":"user-guide/models/diffusion-guide/#training-with-ema-exponential-moving-average","title":"Training with EMA (Exponential Moving Average)","text":"<p>EMA improves sample quality by maintaining a moving average of parameters:</p> <pre><code># Note: EMA is not yet provided as a standalone class in Artifex.\n# The recommended approach is to use DiffusionTrainer which includes\n# built-in EMA support (see DiffusionTrainingConfig.ema_decay above).\n#\n# Alternatively, you can implement EMA manually with standard NNX patterns:\nimport jax\n\nmodel = DDPMModel(config, rngs=rngs)\n\n# Store a copy of initial parameters as EMA state\nema_params = jax.tree.map(lambda p: p.copy(), nnx.state(model, nnx.Param))\nema_decay = 0.9999\n\ndef update_ema(model, ema_params, decay=0.9999):\n    \"\"\"Update EMA parameters.\"\"\"\n    model_params = nnx.state(model, nnx.Param)\n    new_ema = jax.tree.map(\n        lambda ema, p: decay * ema + (1 - decay) * p,\n        ema_params, model_params,\n    )\n    return new_ema\n\n# After each training step:\n# ema_params = update_ema(model, ema_params)\n\n# To sample with EMA parameters, temporarily swap them in:\n# original_params = nnx.state(model, nnx.Param)\n# nnx.update(model, ema_params)\n# samples = model.generate(n_samples=16)\n# nnx.update(model, original_params)  # Restore original\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Use mixed precision to speed up training and reduce memory:</p> <pre><code># Mixed precision is handled at the JAX/computation level, not in model config.\n# Create a standard DDPM model and use half-precision during computation.\nmodel = DDPMModel(config, rngs=rngs)\n\n# Use dynamic loss scaling\nloss_scale = 2 ** 15\n\n@nnx.jit\ndef train_step_fp16(model, optimizer, batch, rngs):\n    def loss_fn(model):\n        # ... compute loss ...\n        return loss * loss_scale  # Scale loss\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n\n    # Unscale gradients\n    grads = jax.tree_map(lambda g: g / loss_scale, grads)\n\n    optimizer.update(model, grads)  # NNX 0.11.0+ API\n\n    return {\"loss\": loss / loss_scale}\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#sampling-strategies","title":"Sampling Strategies","text":""},{"location":"user-guide/models/diffusion-guide/#ddpm-sampling-high-quality","title":"DDPM Sampling (High Quality)","text":"<p>Standard DDPM sampling with all 1000 steps:</p> <pre><code># Generate with full DDPM sampling\n# RNGs are stored internally from __init__, not passed to generate()\nsamples = model.generate(\n    n_samples=16,\n    shape=(32, 32, 3),\n    clip_denoised=True,  # Clip to [-1, 1]\n)\n\n# This takes all 1000 steps - highest quality but slow\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#ddim-sampling-fast","title":"DDIM Sampling (Fast)","text":"<p>Use DDIM for 10-20x faster sampling:</p> <pre><code># Generate with DDIM (50 steps instead of 1000)\n# DDPMModel.sample() uses stored RNGs internally\nsamples = model.sample(\n    n_samples_or_shape=16,\n    scheduler=\"ddim\",\n    steps=50,  # Only 50 steps!\n)\n\n# Quality vs Speed tradeoff:\n# - 20 steps: Fast but lower quality\n# - 50 steps: Good balance\n# - 100 steps: High quality, still 10x faster than DDPM\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#progressive-sampling-visualize-process","title":"Progressive Sampling (Visualize Process)","text":"<p>Visualize the denoising process:</p> <pre><code>def progressive_sampling(model, n_samples, save_every=100):\n    \"\"\"Generate samples and save intermediate steps.\"\"\"\n    trajectory = []\n\n    # Start from noise (model uses its stored RNGs internally)\n    shape = model._get_sample_shape()\n    x = jax.random.normal(model.rngs.sample(), (n_samples, *shape))\n\n    # Denoise step by step\n    for t in range(model.noise_steps - 1, -1, -1):\n        t_batch = jnp.full((n_samples,), t, dtype=jnp.int32)\n\n        # Model prediction (no rngs parameter needed)\n        outputs = model(x, t_batch)\n        predicted_noise = outputs[\"predicted_noise\"]\n\n        # Denoising step (no rngs parameter needed)\n        x = model.p_sample(predicted_noise, x, t_batch)\n\n        # Save intermediate results\n        if t % save_every == 0 or t == 0:\n            trajectory.append(x.copy())\n            print(f\"Step {1000-t}/{1000}\")\n\n    return jnp.stack(trajectory)\n\n# Generate and visualize\ntrajectory = progressive_sampling(model, n_samples=4, save_every=100)\n# trajectory shape: (11, 4, 32, 32, 3) - 11 snapshots of 4 images\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#conditional-sampling-with-guidance","title":"Conditional Sampling with Guidance","text":""},{"location":"user-guide/models/diffusion-guide/#classifier-free-guidance","title":"Classifier-Free Guidance","text":"<pre><code>from artifex.generative_models.models.diffusion.guidance import ClassifierFreeGuidance\n\n# Create guidance\ncfg = ClassifierFreeGuidance(\n    guidance_scale=7.5,  # Higher = stronger conditioning\n    unconditional_conditioning=None  # Null token\n)\n\n# Sample with guidance\ndef sample_with_cfg(model, class_labels, guidance_scale=7.5):\n    \"\"\"Generate samples with classifier-free guidance.\"\"\"\n\n    n_samples = len(class_labels)\n    shape = model._get_sample_shape()\n\n    # Start from noise (uses model's stored RNGs)\n    x = jax.random.normal(model.rngs.sample(), (n_samples, *shape))\n\n    # Denoise with guidance\n    # Note: DiffusionModel.__call__ accepts conditioning as keyword arg\n    for t in range(model.noise_steps - 1, -1, -1):\n        t_batch = jnp.full((n_samples,), t)\n\n        # Get conditional prediction (no rngs needed)\n        cond_output = model(x, t_batch, conditioning=class_labels)\n        cond_noise = cond_output[\"predicted_noise\"]\n\n        # Get unconditional prediction (no rngs needed)\n        uncond_output = model(x, t_batch, conditioning=None)\n        uncond_noise = uncond_output[\"predicted_noise\"]\n\n        # Apply guidance\n        guided_noise = uncond_noise + guidance_scale * (cond_noise - uncond_noise)\n\n        # Denoising step with guided noise (no rngs needed)\n        x = model.p_sample(guided_noise, x, t_batch)\n\n    return x\n\n# Generate class-conditional samples\nclass_labels = jnp.array([0, 1, 2, 3])  # Classes to generate\nsamples = sample_with_cfg(model, class_labels, guidance_scale=7.5)\n</code></pre> <p>Guidance Scale Effects:</p> Scale Effect <code>w = 1.0</code> No guidance (unconditional) <code>w = 2.0</code> Mild conditioning <code>w = 7.5</code> Strong conditioning (common default) <code>w = 15.0</code> Very strong, may reduce diversity"},{"location":"user-guide/models/diffusion-guide/#classifier-guidance","title":"Classifier Guidance","text":"<pre><code>from artifex.generative_models.models.diffusion.guidance import ClassifierGuidance\n\n# Assuming you have a trained classifier\nclassifier = load_pretrained_classifier()\n\n# Create classifier guidance\ncg = ClassifierGuidance(\n    classifier=classifier,\n    guidance_scale=1.0,\n    class_label=5  # Generate class 5\n)\n\n# Sample with classifier guidance\nguided_samples = cg(\n    model=model,\n    x=initial_noise,\n    t=timesteps,\n)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#temperature-sampling","title":"Temperature Sampling","text":"<p>Control sample diversity with temperature:</p> <pre><code>def sample_with_temperature(model, n_samples, temperature=1.0):\n    \"\"\"Sample with temperature control.\n\n    Args:\n        temperature: Higher = more diverse, Lower = more conservative\n    \"\"\"\n    shape = model._get_sample_shape()\n    x = jax.random.normal(model.rngs.sample(), (n_samples, *shape))\n\n    for t in range(model.noise_steps - 1, -1, -1):\n        t_batch = jnp.full((n_samples,), t)\n\n        # Model prediction (no rngs needed)\n        outputs = model(x, t_batch)\n        predicted_noise = outputs[\"predicted_noise\"]\n\n        # Get mean and variance\n        out = model.p_mean_variance(predicted_noise, x, t_batch)\n\n        # Sample with temperature-scaled variance\n        if t &gt; 0:\n            noise = jax.random.normal(model.rngs.noise(), x.shape)\n            scaled_std = jnp.exp(0.5 * out[\"log_variance\"]) * temperature\n            x = out[\"mean\"] + scaled_std * noise\n        else:\n            x = out[\"mean\"]\n\n    return x\n\n# Different temperatures\nconservative = sample_with_temperature(model, 16, temperature=0.8)\ndiverse = sample_with_temperature(model, 16, temperature=1.2)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/models/diffusion-guide/#pattern-1-custom-noise-schedules","title":"Pattern 1: Custom Noise Schedules","text":"<p>Implement a custom noise schedule:</p> <pre><code># Noise schedules are configured via NoiseScheduleConfig, not by overriding methods.\n# The built-in schedule types are: \"linear\", \"cosine\", \"quadratic\", \"sqrt\".\nfrom artifex.generative_models.core.configuration.diffusion_config import (\n    DDPMConfig,\n    NoiseScheduleConfig,\n)\n\n# Use the built-in cosine schedule:\ncosine_schedule = NoiseScheduleConfig(\n    name=\"cosine_schedule\",\n    schedule_type=\"cosine\",   # Uses the Improved DDPM cosine schedule\n    num_timesteps=1000,\n    beta_start=1e-4,          # Not used by cosine, but required by validation\n    beta_end=2e-2,            # Not used by cosine, but required by validation\n)\n\nconfig = DDPMConfig(\n    name=\"ddpm_cosine\",\n    backbone=backbone_config,\n    noise_schedule=cosine_schedule,\n    input_shape=(32, 32, 3),\n)\nmodel = DDPMModel(config, rngs=rngs)\n\n# Other available schedule types:\n# \"linear\"    - Linear beta schedule (original DDPM)\n# \"cosine\"    - Cosine schedule (Improved DDPM)\n# \"quadratic\" - Quadratic beta schedule\n# \"sqrt\"      - Square root beta schedule\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#pattern-2-multi-scale-diffusion","title":"Pattern 2: Multi-Scale Diffusion","text":"<p>Apply diffusion at multiple resolutions:</p> <pre><code>from artifex.generative_models.core.configuration.diffusion_config import (\n    DDPMConfig,\n    NoiseScheduleConfig,\n)\nfrom artifex.generative_models.core.configuration.backbone_config import UNetBackboneConfig\n\nclass MultiScaleDiffusion:\n    \"\"\"Diffusion at multiple resolutions for better quality.\"\"\"\n\n    def __init__(self, scales=[1.0, 0.5, 0.25], rngs=None):\n        self.models = {}\n        for scale in scales:\n            size = int(32 * scale)\n            config = DDPMConfig(\n                name=f\"ddpm_{size}x{size}\",\n                backbone=UNetBackboneConfig(\n                    name=f\"unet_{size}\",\n                    hidden_dims=(32, 64),\n                    in_channels=3,\n                    out_channels=3,\n                ),\n                noise_schedule=NoiseScheduleConfig(\n                    name=\"linear\",\n                    schedule_type=\"linear\",\n                    num_timesteps=1000,\n                ),\n                input_shape=(size, size, 3),\n            )\n            self.models[scale] = DDPMModel(config, rngs=rngs)\n\n    def generate(self, n_samples):\n        \"\"\"Generate using coarse-to-fine approach.\"\"\"\n        # Generate at coarsest scale\n        x = self.models[0.25].generate(n_samples)\n\n        # Upsample and refine at each scale\n        for scale in [0.5, 1.0]:\n            # Upsample\n            x = jax.image.resize(x, (n_samples, int(32*scale), int(32*scale), 3), \"bilinear\")\n\n            # Refine with diffusion at this scale\n            # Add noise and denoise for refinement\n            t = jnp.full((n_samples,), 200)  # Partial noise\n            x_noisy = self.models[scale].q_sample(x, t)  # noise auto-generated internally\n\n            # Denoise (no rngs needed - stored internally)\n            for step in range(200, 0, -1):\n                t = jnp.full((n_samples,), step)\n                outputs = self.models[scale](x_noisy, t)\n                x_noisy = self.models[scale].p_sample(\n                    outputs[\"predicted_noise\"], x_noisy, t\n                )\n\n            x = x_noisy\n\n        return x\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#pattern-3-inpainting","title":"Pattern 3: Inpainting","text":"<p>Use diffusion for image inpainting:</p> <pre><code>def inpaint(model, image, mask):\n    \"\"\"Inpaint masked regions using diffusion.\n\n    Args:\n        image: Original image (1, H, W, C)\n        mask: Binary mask (1, H, W, 1), 1 = inpaint, 0 = keep\n\n    Returns:\n        Inpainted image\n    \"\"\"\n    # Start from noise (uses model's stored RNGs)\n    x = jax.random.normal(model.rngs.sample(), image.shape)\n\n    # Denoise with guidance from known pixels\n    for t in range(model.noise_steps - 1, -1, -1):\n        t_batch = jnp.full((1,), t)\n\n        # Predict noise (no rngs needed)\n        outputs = model(x, t_batch)\n        predicted_noise = outputs[\"predicted_noise\"]\n\n        # Denoising step (no rngs needed)\n        x = model.p_sample(predicted_noise, x, t_batch)\n\n        # Replace known regions with noisy version of original\n        # q_sample auto-generates noise internally when not provided\n        x_noisy_orig = model.q_sample(image, t_batch)\n        x = mask * x + (1 - mask) * x_noisy_orig\n\n    return x\n\n# Usage\nimage = load_image(\"photo.png\")\nmask = create_mask(image, region=\"center\")  # Mask out center\ninpainted = inpaint(model, image, mask)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#pattern-4-image-interpolation","title":"Pattern 4: Image Interpolation","text":"<p>Interpolate between images in latent space:</p> <pre><code>def interpolate_images(model, img1, img2, steps=10):\n    \"\"\"Interpolate between two images using DDIM inversion.\n\n    Args:\n        img1, img2: Images to interpolate (1, H, W, C)\n        steps: Number of interpolation steps\n\n    Returns:\n        Interpolated images (steps, H, W, C)\n    \"\"\"\n    # Encode both images to noise using DDIM (no rngs needed)\n    noise1 = model.ddim_reverse(img1, ddim_steps=50)\n    noise2 = model.ddim_reverse(img2, ddim_steps=50)\n\n    # Interpolate in noise space\n    alphas = jnp.linspace(0, 1, steps)\n    interpolated = []\n\n    for alpha in alphas:\n        # Spherical interpolation (better than linear)\n        noise_interp = slerp(noise1, noise2, alpha)\n\n        # Decode back to image (no rngs needed)\n        img = model.ddim_sample(n_samples=1, steps=50)\n        interpolated.append(img[0])\n\n    return jnp.stack(interpolated)\n\ndef slerp(v1, v2, alpha):\n    \"\"\"Spherical linear interpolation.\"\"\"\n    v1_norm = v1 / jnp.linalg.norm(v1)\n    v2_norm = v2 / jnp.linalg.norm(v2)\n\n    dot = jnp.sum(v1_norm * v2_norm)\n    theta = jnp.arccos(jnp.clip(dot, -1.0, 1.0))\n\n    if theta &lt; 1e-6:\n        return (1 - alpha) * v1 + alpha * v2\n\n    sin_theta = jnp.sin(theta)\n    w1 = jnp.sin((1 - alpha) * theta) / sin_theta\n    w2 = jnp.sin(alpha * theta) / sin_theta\n\n    return w1 * v1 + w2 * v2\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/models/diffusion-guide/#issue-1-blurry-samples","title":"Issue 1: Blurry Samples","text":"<p>Symptoms:</p> <ul> <li>Generated images lack detail</li> <li>Samples are smooth but not sharp</li> </ul> <p>Solutions:</p> <pre><code># Configs are frozen dataclasses - create new configs with desired parameters.\n\n# Solution 1: Increase model capacity (use larger hidden_dims in backbone)\nbackbone_config = UNetBackboneConfig(\n    name=\"unet_large\",\n    hidden_dims=(128, 256, 512),  # Larger network\n    in_channels=3,\n    out_channels=3,\n)\n\n# Solution 2: Use cosine schedule\nnoise_config = NoiseScheduleConfig(\n    name=\"cosine\",\n    schedule_type=\"cosine\",       # Cosine schedule for better quality\n    num_timesteps=1000,\n)\n\n# Solution 3: Train longer\nnum_epochs = 500  # More training\n\n# Solution 4: Use more timesteps\nnoise_config = NoiseScheduleConfig(\n    name=\"long_schedule\",\n    schedule_type=\"linear\",\n    num_timesteps=2000,           # More steps\n)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#issue-2-training-instability","title":"Issue 2: Training Instability","text":"<p>Symptoms:</p> <ul> <li>Loss spikes or diverges</li> <li>NaN values in gradients</li> </ul> <p>Solutions:</p> <pre><code># Solution 1: Lower learning rate\noptimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-5), wrt=nnx.Param)\n\n# Solution 2: Gradient clipping (wrt=nnx.Param required in NNX 0.11.0+)\noptimizer = nnx.Optimizer(\n    model,\n    optax.chain(\n        optax.clip_by_global_norm(1.0),  # Clip gradients\n        optax.adam(1e-4),\n    ),\n    wrt=nnx.Param\n)\n\n# Solution 3: Warmup learning rate\nschedule = optax.warmup_cosine_decay_schedule(\n    init_value=1e-6,\n    peak_value=1e-4,\n    warmup_steps=1000,\n    decay_steps=100000,\n)\noptimizer = nnx.Optimizer(model, optax.adam(schedule), wrt=nnx.Param)\n\n# Solution 4: Mixed precision with loss scaling\n# (See mixed precision training section above)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#issue-3-slow-sampling","title":"Issue 3: Slow Sampling","text":"<p>Symptoms:</p> <ul> <li>Generating samples takes too long</li> <li>Inference is impractical for real-time use</li> </ul> <p>Solutions:</p> <pre><code># Solution 1: Use DDIM sampling (no rngs needed - stored internally)\nsamples = model.sample(16, scheduler=\"ddim\", steps=50)  # 20x faster\n\n# Solution 2: Use fewer sampling steps\nsamples = model.sample(16, scheduler=\"ddim\", steps=20)  # Even faster\n\n# Solution 3: Use Latent Diffusion\nldm = LDMModel(ldm_config, rngs=rngs)  # Operates in compressed space\n\n# Solution 4: Distillation (train student model)\n# Train a model to match DDPM in fewer steps\n# (Advanced technique, requires separate training)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#issue-4-mode-collapse-repetitive-samples","title":"Issue 4: Mode Collapse (Repetitive Samples)","text":"<p>Symptoms:</p> <ul> <li>Generated samples look too similar</li> <li>Lack of diversity</li> </ul> <p>Solutions:</p> <pre><code># Solution 1: Increase temperature\nsamples = sample_with_temperature(model, 16, temperature=1.2)\n\n# Solution 2: Decrease guidance scale (for DiT models)\nsamples = model.generate(16, rngs=rngs, cfg_scale=2.0)  # Lower than 7.5\n\n# Solution 3: More training data\n# Ensure diverse training set\n\n# Solution 4: Data augmentation\n# Apply augmentations during training\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#issue-5-out-of-memory","title":"Issue 5: Out of Memory","text":"<p>Symptoms:</p> <ul> <li>GPU/TPU runs out of memory during training or sampling</li> </ul> <p>Solutions:</p> <pre><code># Solution 1: Reduce batch size\nbatch_size = 32  # Instead of 128\n\n# Solution 2: Use gradient accumulation\nfor i in range(accumulation_steps):\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    accumulated_grads = jax.tree.map(lambda a, b: a + b, accumulated_grads, grads)\n\naccumulated_grads = jax.tree.map(lambda g: g / accumulation_steps, accumulated_grads)\noptimizer.update(model, accumulated_grads)  # NNX 0.11.0+ API\n\n# Solution 3: Use Latent Diffusion\n# Operate in compressed latent space (8x less memory)\n\n# Solution 4: Use half precision via JAX dtype control\n# Pass dtype=jnp.float16 or jnp.bfloat16 when creating backbone config\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/models/diffusion-guide/#dos","title":"Do's \u2705","text":"<ol> <li>Use EMA for sampling: Exponential moving average improves quality</li> <li>Start with DDPM: Master the basics before advanced techniques</li> <li>Try DDIM for speed: 10-20x faster with minimal quality loss</li> <li>Use cosine schedule for high-res: Better than linear for large images</li> <li>Implement proper data preprocessing: Scale to [-1, 1] range</li> <li>Monitor sample quality: Generate samples during training</li> <li>Use classifier-free guidance: Better than classifier guidance usually</li> <li>Save checkpoints frequently: Long training requires safety nets</li> </ol>"},{"location":"user-guide/models/diffusion-guide/#donts","title":"Don'ts \u274c","text":"<ol> <li>Don't skip EMA: Samples will be lower quality</li> <li>Don't use too few steps: DDIM needs at least 20-50 steps</li> <li>Don't forget to clip outputs: Keeps samples in valid range</li> <li>Don't train without augmentation: Especially for small datasets</li> <li>Don't use batch size 1: Larger batches stabilize training</li> <li>Don't ignore timestep sampling: Uniform works well</li> <li>Don't use same RNG for everything: Separate RNGs for different operations</li> <li>Don't expect instant results: Diffusion training takes time</li> </ol>"},{"location":"user-guide/models/diffusion-guide/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":"Parameter Typical Range Notes Learning Rate 1e-5 to 1e-4 Lower for large models Batch Size 64-512 Larger is better (if memory allows) Noise Steps 1000-2000 1000 is standard DDIM Steps 20-100 50 is good balance EMA Decay 0.999-0.9999 Higher for slower updates Guidance Scale 1.0-15.0 7.5 is common default Beta Start 1e-5 to 1e-4 1e-4 is standard Beta End 0.02-0.05 0.02 is standard"},{"location":"user-guide/models/diffusion-guide/#summary","title":"Summary","text":"<p>This guide covered practical usage of diffusion models:</p> <p>Key Takeaways:</p> <ol> <li>DDPM: Foundation model, excellent quality, slow sampling</li> <li>DDIM: Fast sampling (50 steps), deterministic, enables editing</li> <li>Score-Based: Continuous-time formulation, flexible schedules</li> <li>Latent Diffusion: Efficient high-resolution generation</li> <li>DiT: Transformer backbone, better scalability</li> <li>Guidance: Classifier-free guidance for conditional generation</li> <li>Training: Use EMA, proper preprocessing, and patience</li> <li>Sampling: DDIM for speed, temperature for diversity</li> </ol> <p>Quick Reference:</p> <pre><code># Standard training\nmodel = DDPMModel(config, rngs=rngs)\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n# ... train ...\n\n# Fast inference (RNGs stored internally)\nsamples = model.sample(16, scheduler=\"ddim\", steps=50)\n\n# Full DDPM generation\nsamples = model.generate(n_samples=16)\n</code></pre>"},{"location":"user-guide/models/diffusion-guide/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Concepts</p> <p>Understand the theory behind diffusion models</p> </li> <li> <p> API Reference</p> <p>Complete API documentation for all classes</p> </li> <li> <p> MNIST Tutorial</p> <p>Hands-on tutorial with complete working example</p> </li> <li> <p> Advanced Topics</p> <p>Explore distillation, super-resolution, and more</p> </li> </ul>"},{"location":"user-guide/models/ebm-guide/","title":"Energy-Based Models User Guide","text":"<p>Complete guide to building, training, and using Energy-Based Models with Artifex.</p>"},{"location":"user-guide/models/ebm-guide/#overview","title":"Overview","text":"<p>This guide covers practical usage of EBMs in Artifex, from basic setup to advanced techniques. You'll learn how to:</p> <ul> <li> <p> Configure EBMs</p> <p>Set up energy functions and MCMC sampling parameters</p> </li> <li> <p> Train Models</p> <p>Train with persistent contrastive divergence and monitor stability</p> </li> <li> <p> Generate Samples</p> <p>Sample using Langevin dynamics and MCMC methods</p> </li> <li> <p> Tune &amp; Debug</p> <p>Optimize hyperparameters and troubleshoot common issues</p> </li> </ul>"},{"location":"user-guide/models/ebm-guide/#quick-start","title":"Quick Start","text":""},{"location":"user-guide/models/ebm-guide/#basic-ebm-example","title":"Basic EBM Example","text":"<p>Artifex provides factory functions for common use cases:</p> <pre><code>import jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.models.energy import create_mnist_ebm\n\n# Initialize RNGs\nrngs = nnx.Rngs(0)\n\n# Create EBM optimized for MNIST-like data\nmodel = create_mnist_ebm(rngs=rngs)\n\n# Compute energy for a batch of images\nbatch = jnp.ones((4, 28, 28, 1))\noutput = model(batch)\n\nprint(f\"Energy values: {output['energy']}\")\nprint(f\"Score shape: {output['score'].shape}\")\n\n# Generate samples using MCMC\nmodel.eval()  # Set to evaluation mode\nsamples = model.generate(n_samples=4, shape=(28, 28, 1))\nprint(f\"Generated samples shape: {samples.shape}\")\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#configuration-based-approach","title":"Configuration-Based Approach","text":"<p>For more control, use the config-based API:</p> <pre><code>from artifex.generative_models.core.configuration.energy_config import (\n    EBMConfig,\n    EnergyNetworkConfig,\n    MCMCConfig,\n    SampleBufferConfig,\n)\nfrom artifex.generative_models.models.energy import EBM\n\n# Configure energy network\nenergy_config = EnergyNetworkConfig(\n    name=\"energy_net\",\n    hidden_dims=(128, 256, 128),\n    activation=\"silu\",\n    network_type=\"mlp\",\n)\n\n# Configure MCMC sampling\nmcmc_config = MCMCConfig(\n    name=\"langevin\",\n    n_steps=60,           # Number of MCMC steps\n    step_size=0.01,\n    noise_scale=0.005,\n)\n\n# Configure sample buffer (replay buffer)\nbuffer_config = SampleBufferConfig(\n    name=\"buffer\",\n    capacity=8192,        # Maximum samples to store\n    reinit_prob=0.05,     # Probability to reinitialize samples\n)\n\n# Create EBM config\nconfig = EBMConfig(\n    name=\"mnist_ebm\",\n    input_dim=784,  # Flattened MNIST\n    energy_network=energy_config,\n    mcmc=mcmc_config,\n    sample_buffer=buffer_config,\n    alpha=0.01,  # Regularization\n)\n\n# Create model\nrngs = nnx.Rngs(params=0, noise=1, sample=2)\nmodel = EBM(config, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#creating-ebm-models","title":"Creating EBM Models","text":""},{"location":"user-guide/models/ebm-guide/#1-standard-ebm-mlp-energy-function","title":"1. Standard EBM (MLP Energy Function)","text":"<p>For tabular or low-dimensional data:</p> <pre><code>from artifex.generative_models.core.configuration.energy_config import (\n    EBMConfig,\n    EnergyNetworkConfig,\n    MCMCConfig,\n    SampleBufferConfig,\n)\nfrom artifex.generative_models.models.energy import EBM\n\n# MLP energy function configuration\nenergy_config = EnergyNetworkConfig(\n    name=\"tabular_energy\",\n    hidden_dims=(256, 256, 128),\n    activation=\"gelu\",\n    dropout_rate=0.1,\n    network_type=\"mlp\",\n)\n\n# MCMC sampling configuration\nmcmc_config = MCMCConfig(\n    name=\"mcmc\",\n    n_steps=60,\n    step_size=0.01,\n    noise_scale=0.005,\n)\n\n# Sample buffer configuration\nbuffer_config = SampleBufferConfig(\n    name=\"buffer\",\n    capacity=4096,\n)\n\n# Create EBM config\nconfig = EBMConfig(\n    name=\"tabular_ebm\",\n    input_dim=784,  # Flattened input\n    energy_network=energy_config,\n    mcmc=mcmc_config,\n    sample_buffer=buffer_config,\n    alpha=0.01,\n)\n\nrngs = nnx.Rngs(params=0, noise=1, sample=2)\nmodel = EBM(config, rngs=rngs)\n</code></pre> <p>Key Parameters:</p> Config Class Parameter Default Description <code>EnergyNetworkConfig</code> <code>network_type</code> \"mlp\" Energy function architecture (\"mlp\" or \"cnn\") <code>EnergyNetworkConfig</code> <code>hidden_dims</code> (128, 128) Hidden layer dimensions <code>EnergyNetworkConfig</code> <code>activation</code> \"gelu\" Activation function name <code>MCMCConfig</code> <code>n_steps</code> 60 Number of Langevin dynamics steps <code>MCMCConfig</code> <code>step_size</code> 0.01 Step size for gradient descent <code>MCMCConfig</code> <code>noise_scale</code> 0.005 Noise scale for exploration <code>SampleBufferConfig</code> <code>capacity</code> 8192 Maximum samples in replay buffer <code>SampleBufferConfig</code> <code>reinit_prob</code> 0.05 Probability to reinitialize from scratch <code>EBMConfig</code> <code>alpha</code> 0.01 Regularization strength"},{"location":"user-guide/models/ebm-guide/#2-cnn-energy-function-for-images","title":"2. CNN Energy Function (for Images)","text":"<p>For image data:</p> <pre><code>from artifex.generative_models.core.configuration.energy_config import (\n    DeepEBMConfig,\n    EnergyNetworkConfig,\n    MCMCConfig,\n    SampleBufferConfig,\n)\nfrom artifex.generative_models.models.energy import DeepEBM\n\n# CNN energy network for images\nenergy_config = EnergyNetworkConfig(\n    name=\"image_energy\",\n    hidden_dims=(64, 128, 256),\n    activation=\"silu\",\n    network_type=\"cnn\",\n)\n\nmcmc_config = MCMCConfig(\n    name=\"mcmc\",\n    n_steps=100,\n    step_size=0.005,\n    noise_scale=0.001,\n)\n\nbuffer_config = SampleBufferConfig(\n    name=\"buffer\",\n    capacity=8192,\n    reinit_prob=0.05,\n)\n\nconfig = DeepEBMConfig(\n    name=\"image_ebm\",\n    input_shape=(32, 32, 3),  # CIFAR-10 dimensions (H, W, C)\n    energy_network=energy_config,\n    mcmc=mcmc_config,\n    sample_buffer=buffer_config,\n    alpha=0.001,\n)\n\nrngs = nnx.Rngs(params=0, noise=1, sample=2)\nmodel = DeepEBM(config, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#3-deep-ebm-complex-data","title":"3. Deep EBM (Complex Data)","text":"<p>For complex datasets requiring deeper architectures:</p> <pre><code>from artifex.generative_models.core.configuration.energy_config import (\n    DeepEBMConfig,\n    EnergyNetworkConfig,\n    MCMCConfig,\n    SampleBufferConfig,\n)\nfrom artifex.generative_models.models.energy import DeepEBM\n\nenergy_config = EnergyNetworkConfig(\n    name=\"deep_energy\",\n    hidden_dims=(32, 64, 128, 256),\n    activation=\"silu\",\n    network_type=\"cnn\",\n    use_residual=True,\n    use_spectral_norm=True,\n)\n\nmcmc_config = MCMCConfig(\n    name=\"mcmc\",\n    n_steps=100,\n    step_size=0.005,\n    noise_scale=0.001,\n)\n\nbuffer_config = SampleBufferConfig(\n    name=\"buffer\",\n    capacity=8192,\n)\n\nconfig = DeepEBMConfig(\n    name=\"deep_ebm\",\n    input_shape=(32, 32, 3),\n    energy_network=energy_config,\n    mcmc=mcmc_config,\n    sample_buffer=buffer_config,\n    alpha=0.001,\n)\n\nrngs = nnx.Rngs(params=0, noise=1, sample=2)\nmodel = DeepEBM(config, rngs=rngs)\n</code></pre> <p>Deep EBM Features:</p> <ul> <li>Residual connections: Enable deeper networks (10+ layers)</li> <li>Spectral normalization: Stabilizes training</li> <li>GroupNorm: Better than BatchNorm for MCMC sampling</li> </ul>"},{"location":"user-guide/models/ebm-guide/#training-ebms","title":"Training EBMs","text":""},{"location":"user-guide/models/ebm-guide/#basic-training-loop","title":"Basic Training Loop","text":"<pre><code>import optax\nfrom flax import nnx\nfrom artifex.generative_models.training.trainers import (\n    EnergyTrainer,\n    EnergyTrainingConfig,\n)\n\n# Training configuration\ntrain_config = EnergyTrainingConfig(\n    training_method=\"pcd\",       # Persistent Contrastive Divergence\n    mcmc_sampler=\"langevin\",\n    mcmc_steps=20,\n    step_size=0.01,\n    noise_scale=0.005,\n    replay_buffer_size=10000,\n    replay_buffer_init_prob=0.95,\n)\n\n# Create trainer, optimizer, and RNG key\ntrainer = EnergyTrainer(train_config)\noptimizer = nnx.Optimizer(model, optax.adam(1e-4))\nrng = jax.random.key(42)\n\n# Training loop\nfor epoch in range(100):\n    for batch in train_loader:\n        rng, step_rng = jax.random.split(rng)\n        loss, metrics = trainer.train_step(model, optimizer, batch, step_rng)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#training-with-monitoring","title":"Training with Monitoring","text":"<p>Monitor key metrics during training:</p> <pre><code>def train_step_with_monitoring(model, batch):\n    \"\"\"Training step with detailed monitoring using model.train_step().\"\"\"\n    loss_dict = model.train_step(batch)\n\n    # Log metrics (keys from contrastive_divergence_loss)\n    print(f\"Step metrics:\")\n    print(f\"  Loss: {loss_dict['loss']:.4f}\")\n    print(f\"  Real energy: {loss_dict['real_energy_mean']:.4f}\")\n    print(f\"  Fake energy: {loss_dict['fake_energy_mean']:.4f}\")\n\n    # Compute energy gap\n    energy_gap = float(loss_dict['fake_energy_mean'] - loss_dict['real_energy_mean'])\n    print(f\"  Energy gap: {energy_gap:.4f}\")\n\n    # Check for issues\n    if energy_gap &lt; 0:\n        print(\"WARNING: Negative energy gap - real data has higher energy!\")\n\n    if abs(float(loss_dict['real_energy_mean'])) &gt; 100:\n        print(\"WARNING: Energy explosion detected!\")\n\n    return loss_dict\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        loss_dict = train_step_with_monitoring(model, batch)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":"<p>MCMC Sampling:</p> <pre><code># Quick sampling (less accurate)\nquick_config = {\n    \"mcmc_steps\": 20,\n    \"mcmc_step_size\": 0.02,\n    \"mcmc_noise_scale\": 0.01,\n}\n\n# Standard sampling (balanced)\nstandard_config = {\n    \"mcmc_steps\": 60,\n    \"mcmc_step_size\": 0.01,\n    \"mcmc_noise_scale\": 0.005,\n}\n\n# High-quality sampling (slower)\nquality_config = {\n    \"mcmc_steps\": 200,\n    \"mcmc_step_size\": 0.005,\n    \"mcmc_noise_scale\": 0.001,\n}\n</code></pre> <p>Learning Rates:</p> <pre><code># EBMs typically need lower learning rates than supervised models\nlearning_rates = {\n    \"small_model\": 1e-4,\n    \"medium_model\": 5e-5,\n    \"large_model\": 1e-5,\n}\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#generating-samples","title":"Generating Samples","text":""},{"location":"user-guide/models/ebm-guide/#sampling-from-the-model","title":"Sampling from the Model","text":"<pre><code># Generate samples using Langevin dynamics MCMC\nn_samples = 16\nmodel.eval()  # Set to evaluation mode for sampling\nsamples = model.generate(\n    n_samples=n_samples,\n    shape=(784,),         # Shape of each sample (must match input_dim for MLP)\n    n_steps=100,          # More steps = better quality\n    step_size=0.01,\n    noise_scale=0.005,\n)\n\nprint(f\"Generated samples shape: {samples.shape}\")\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#sampling-with-different-configurations","title":"Sampling with Different Configurations","text":"<pre><code># Quick sampling (fewer steps, larger step size)\nquick_samples = model.generate(\n    n_samples=16,\n    shape=(784,),\n    n_steps=30,\n    step_size=0.02,\n    noise_scale=0.01,\n)\n\n# High-quality sampling (more steps, smaller step size)\nhq_samples = model.generate(\n    n_samples=16,\n    shape=(784,),\n    n_steps=200,\n    step_size=0.005,\n    noise_scale=0.001,\n)\n\n# Sample from the replay buffer (returns buffered MCMC samples)\nbuffer_samples = model.sample_from_buffer(n_samples=16)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#using-the-energytrainer-for-generation","title":"Using the EnergyTrainer for Generation","text":"<p>The <code>EnergyTrainer</code> also provides a <code>generate_samples</code> method for MCMC-based generation:</p> <pre><code>from artifex.generative_models.training.trainers import (\n    EnergyTrainer,\n    EnergyTrainingConfig,\n)\n\ntrainer = EnergyTrainer(EnergyTrainingConfig(mcmc_steps=20, step_size=0.01))\nrng = jax.random.key(0)\n\n# Generate samples via the trainer's MCMC chain\nsamples = trainer.generate_samples(\n    model=model,\n    batch_size=16,\n    key=rng,\n    shape=(784,),       # Shape of each sample\n    num_steps=200,      # Defaults to 10x config mcmc_steps if None\n)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"user-guide/models/ebm-guide/#1-sample-buffer-management","title":"1. Sample Buffer Management","text":"<p>The sample buffer is critical for stable training:</p> <pre><code># Access buffer statistics\nbuffer_size = len(model.sample_buffer.buffer)\nprint(f\"Buffer contains {buffer_size} samples\")\n\n# Manually populate buffer by running train steps\n# The train_step method automatically updates the sample buffer\nfor batch in train_loader:\n    loss_dict = model.train_step(batch)\n    # Samples are automatically added to the buffer during training\n\n# Clear buffer (for reinitialization)\nmodel.sample_buffer.buffer = []\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#2-energy-landscape-visualization","title":"2. Energy Landscape Visualization","text":"<p>Visualize the energy landscape:</p> <pre><code>import matplotlib.pyplot as plt\n\ndef visualize_energy_landscape(model, data_range=(-3, 3), resolution=100):\n    \"\"\"Visualize 2D energy landscape.\"\"\"\n    x = jnp.linspace(data_range[0], data_range[1], resolution)\n    y = jnp.linspace(data_range[0], data_range[1], resolution)\n    X, Y = jnp.meshgrid(x, y)\n\n    # Compute energy for each point\n    points = jnp.stack([X.ravel(), Y.ravel()], axis=1)\n    energies = model.energy(points)\n    energies = energies.reshape(resolution, resolution)\n\n    # Plot\n    plt.figure(figsize=(10, 8))\n    plt.contourf(X, Y, energies, levels=50, cmap='viridis')\n    plt.colorbar(label='Energy')\n    plt.title('Energy Landscape')\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.show()\n\n# For 2D data\nvisualize_energy_landscape(model)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#3-annealed-importance-sampling","title":"3. Annealed Importance Sampling","text":"<p>For better sampling quality:</p> <pre><code>def annealed_sampling(model, n_samples, sample_shape, n_steps=1000, rng_key=None):\n    \"\"\"Annealed importance sampling for high-quality samples.\"\"\"\n    if rng_key is None:\n        rng_key = jax.random.key(0)\n\n    # Start with high temperature\n    temperatures = jnp.linspace(10.0, 1.0, n_steps)\n\n    # Initialize samples\n    rng_key, init_key = jax.random.split(rng_key)\n    samples = jax.random.normal(init_key, (n_samples, *sample_shape))\n\n    for i, temp in enumerate(temperatures):\n        rng_key, step_key = jax.random.split(rng_key)\n\n        # Compute energy gradient\n        energy_grad = jax.grad(lambda x: jnp.sum(model.energy(x)))(samples)\n\n        # Langevin step with temperature\n        step_size = 0.01 * temp\n        noise_scale = jnp.sqrt(2 * step_size * temp)\n\n        samples = samples - step_size * energy_grad\n        samples = samples + noise_scale * jax.random.normal(\n            step_key, samples.shape\n        )\n\n    return samples\n\n# Use annealed sampling (pass sample shape explicitly)\nhigh_quality_samples = annealed_sampling(\n    model, n_samples=16, sample_shape=(784,), rng_key=jax.random.key(0)\n)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/models/ebm-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ul> <li> <p> Energy Explosion</p> <p>Symptoms: Energy values grow unbounded, NaN losses</p> <p>Solutions:   - Reduce learning rate (try 1e-5)   - Add/increase regularization (alpha=0.01 to 0.1)   - Use spectral normalization   - Clip gradients: <code>max_grad_norm=1.0</code></p> <pre><code># Use a higher alpha when creating the config\nconfig = EBMConfig(..., alpha=0.1)\n</code></pre> </li> <li> <p> Poor Sample Quality</p> <p>Symptoms: Samples look like noise or blurry</p> <p>Solutions:   - Increase MCMC steps (60 \u2192 100+)   - Better step size tuning   - Larger buffer capacity   - Deeper energy function</p> <pre><code># Increase MCMC steps and buffer capacity in config\nmcmc_config = MCMCConfig(name=\"mcmc\", n_steps=100, ...)\nbuffer_config = SampleBufferConfig(name=\"buffer\", capacity=16384, ...)\n</code></pre> </li> <li> <p> Mode Collapse</p> <p>Symptoms: All samples look similar</p> <p>Solutions:   - Increase buffer reinit probability   - Use data augmentation   - Longer MCMC chains   - Larger buffer</p> <pre><code># Increase reinit probability in the sample buffer config\nbuffer_config = SampleBufferConfig(name=\"buffer\", capacity=8192, reinit_prob=0.1)\n</code></pre> </li> <li> <p> Training Instability</p> <p>Symptoms: Oscillating losses, sudden divergence</p> <p>Solutions:   - Lower learning rate   - Use persistent buffer   - Add gradient clipping   - Monitor energy gap</p> <pre><code># Use PCD training method for persistent buffer\ntrain_config = EnergyTrainingConfig(training_method=\"pcd\", ...)\ntrainer = EnergyTrainer(train_config)\n</code></pre> </li> </ul>"},{"location":"user-guide/models/ebm-guide/#debugging-checklist","title":"Debugging Checklist","text":"<pre><code>def diagnose_ebm(model, batch, sample_shape):\n    \"\"\"Diagnostic checks for EBM training.\n\n    Args:\n        model: Trained EBM or DeepEBM instance\n        batch: A batch dict with 'image' or 'data' key\n        sample_shape: Shape of each sample (e.g. (784,) or (28, 28, 1))\n    \"\"\"\n    # Extract data from batch\n    real_data = batch.get('image', batch.get('data'))\n\n    # 1. Check energy values\n    real_energy = model.energy(real_data).mean()\n    print(f\"Real data energy: {real_energy:.3f}\")\n\n    # Generate samples\n    model.eval()\n    fake_samples = model.generate(n_samples=16, shape=sample_shape, n_steps=100)\n    fake_energy = model.energy(fake_samples).mean()\n    print(f\"Generated samples energy: {fake_energy:.3f}\")\n\n    # Energy gap should be positive\n    gap = fake_energy - real_energy\n    print(f\"Energy gap: {gap:.3f}\")\n\n    # 2. Check MCMC convergence\n    rng_key = jax.random.key(0)\n    init_samples = jax.random.uniform(rng_key, (16, *sample_shape), minval=-1.0, maxval=1.0)\n    init_energy = model.energy(init_samples).mean()\n\n    # Use generate() to run MCMC from random init\n    final_samples = model.generate(\n        n_samples=16,\n        shape=sample_shape,\n        n_steps=100,\n        step_size=0.01,\n        noise_scale=0.005,\n    )\n    final_energy = model.energy(final_samples).mean()\n\n    energy_decrease = init_energy - final_energy\n    print(f\"MCMC energy decrease: {energy_decrease:.3f}\")\n\n    # 3. Check buffer health\n    buffer_size = len(model.sample_buffer.buffer)\n    print(f\"Buffer size: {buffer_size}/{model.sample_buffer.capacity}\")\n\n    # 4. Check sample validity\n    sample_min, sample_max = float(fake_samples.min()), float(fake_samples.max())\n    print(f\"Sample range: [{sample_min:.3f}, {sample_max:.3f}]\")\n\n    return {\n        \"real_energy\": real_energy,\n        \"fake_energy\": fake_energy,\n        \"energy_gap\": gap,\n        \"mcmc_decrease\": energy_decrease,\n        \"buffer_usage\": buffer_size / model.sample_buffer.capacity,\n    }\n\n# Run diagnostics\ndiagnostics = diagnose_ebm(model, batch, sample_shape=(784,))\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/models/ebm-guide/#1-start-simple","title":"1. Start Simple","text":"<pre><code># Begin with a small model and simple data using the factory function\nfrom artifex.generative_models.models.energy import create_simple_ebm\n\nrngs = nnx.Rngs(params=0, noise=1, sample=2)\nmodel = create_simple_ebm(\n    input_dim=2,  # 2D for visualization\n    rngs=rngs,\n    hidden_dims=(64, 64),\n    activation=\"relu\",\n    mcmc_steps=30,\n    step_size=0.02,\n    sample_buffer_capacity=1024,\n)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#2-gradually-increase-complexity","title":"2. Gradually Increase Complexity","text":"<pre><code>from artifex.generative_models.models.energy import create_mnist_ebm, create_cifar_ebm\n\n# Once stable, increase capacity (MNIST-like data)\nmedium_model = create_mnist_ebm(\n    rngs=rngs,\n    hidden_dims=(128, 256),\n    mcmc_steps=60,\n    sample_buffer_capacity=4096,\n)\n\n# For complex data (CIFAR-like images with residual + spectral norm)\ncomplex_model = create_cifar_ebm(\n    rngs=rngs,\n    hidden_dims=(64, 128, 256, 512),\n    use_residual=True,\n    use_spectral_norm=True,\n    mcmc_steps=100,\n    sample_buffer_capacity=8192,\n)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#3-monitor-training-carefully","title":"3. Monitor Training Carefully","text":"<pre><code># Log detailed metrics\ndef detailed_training_step(model, batch, step, sample_shape):\n    loss_dict = model.train_step(batch)\n\n    if step % 100 == 0:\n        # Detailed logging (keys from contrastive_divergence_loss)\n        print(f\"\\nStep {step}:\")\n        print(f\"  Loss: {loss_dict['loss']:.4f}\")\n        print(f\"  Real energy: {loss_dict['real_energy_mean']:.4f}\")\n        print(f\"  Fake energy: {loss_dict['fake_energy_mean']:.4f}\")\n        gap = float(loss_dict['fake_energy_mean'] - loss_dict['real_energy_mean'])\n        print(f\"  Gap: {gap:.4f}\")\n\n        # Generate samples for visual inspection\n        if step % 1000 == 0:\n            model.eval()\n            samples = model.generate(n_samples=64, shape=sample_shape)\n            model.train()\n            visualize_samples(samples, f\"step_{step}.png\")\n\n    return loss_dict\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#4-use-proper-preprocessing","title":"4. Use Proper Preprocessing","text":"<pre><code>def preprocess_for_ebm(images):\n    \"\"\"Proper preprocessing for image EBMs.\"\"\"\n    # Normalize to [-1, 1]\n    images = (images - 127.5) / 127.5\n\n    # Add small noise during training\n    if training:\n        noise = jax.random.normal(rng_key, images.shape) * 0.005\n        images = images + noise\n        images = jnp.clip(images, -1.0, 1.0)\n\n    return images\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/models/ebm-guide/#gpu-acceleration","title":"GPU Acceleration","text":"<pre><code># EBMs benefit significantly from GPU\nfrom artifex.generative_models.core.device_manager import DeviceManager\n\ndevice_manager = DeviceManager()\ndevice = device_manager.get_device()\nprint(f\"Using device: {device}\")\n\n# Move data to GPU\nbatch_gpu = jax.device_put(batch, device)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#batch-size-tuning","title":"Batch Size Tuning","text":"<pre><code># Larger batches = more stable gradients\n# But: limited by GPU memory\n\nbatch_sizes = {\n    \"small_model\": 256,\n    \"medium_model\": 128,\n    \"large_model\": 64,\n}\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#jit-compilation","title":"JIT Compilation","text":"<pre><code># Use nnx.jit with the EnergyTrainer for JIT-compiled training\nfrom artifex.generative_models.training.trainers import EnergyTrainer, EnergyTrainingConfig\n\ntrainer = EnergyTrainer(EnergyTrainingConfig(training_method=\"pcd\"))\njit_step = nnx.jit(trainer.train_step)\n\n# Much faster after first call\nrng = jax.random.key(0)\nrng, step_rng = jax.random.split(rng)\nloss, metrics = jit_step(model, optimizer, batch, step_rng)\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#example-complete-mnist-training","title":"Example: Complete MNIST Training","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.models.energy import create_mnist_ebm\n\n# Create model using factory function\nrngs = nnx.Rngs(params=0, noise=1, sample=2)\nmodel = create_mnist_ebm(\n    rngs=rngs,\n    hidden_dims=(128, 256, 512),\n    activation=\"silu\",\n    mcmc_steps=60,\n    step_size=0.01,\n    noise_scale=0.005,\n    sample_buffer_capacity=8192,\n    alpha=0.01,\n)\n\n# Preprocessing: normalize images to [-1, 1]\ndef preprocess(images):\n    images = jnp.array(images, dtype=jnp.float32) / 255.0\n    images = (images - 0.5) / 0.5  # Normalize to [-1, 1]\n    return {\"image\": images}\n\n# Training loop using model.train_step()\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n    for step, batch in enumerate(train_loader):\n        batch = preprocess(batch)\n        loss_dict = model.train_step(batch)\n\n        if step % 100 == 0:\n            gap = float(loss_dict['fake_energy_mean'] - loss_dict['real_energy_mean'])\n            print(f\"  Step {step}: Loss={loss_dict['loss']:.4f}, \"\n                  f\"Gap={gap:.4f}\")\n\n    # Generate samples\n    if (epoch + 1) % 10 == 0:\n        model.eval()\n        samples = model.generate(n_samples=64, shape=(28, 28, 1))\n        model.train()\n        save_image_grid(samples, f\"epoch_{epoch+1}.png\")\n\nprint(\"Training complete!\")\n</code></pre>"},{"location":"user-guide/models/ebm-guide/#further-reading","title":"Further Reading","text":"<ul> <li>EBM Explained - Theoretical foundations</li> <li>EBM API Reference - Complete API documentation</li> <li>Training Guide - General training workflows</li> <li>Examples - More EBM examples</li> </ul>"},{"location":"user-guide/models/ebm-guide/#summary","title":"Summary","text":"<p>Key Takeaways:</p> <ul> <li>EBMs learn by assigning low energy to data, high energy to non-data</li> <li>Persistent Contrastive Divergence (PCD) with MCMC sampling is the standard training method</li> <li>Sample buffer management is critical for stable training</li> <li>Monitor energy gap: fake_energy should be &gt; real_energy</li> <li>Start simple, increase complexity gradually</li> <li>Use spectral normalization and regularization for stability</li> </ul> <p>Recommended Workflow:</p> <ol> <li>Start with simple 2D data to verify training works</li> <li>Use MLP energy for tabular, CNN for images</li> <li>Monitor energy gap and buffer health</li> <li>Tune MCMC steps and step size for your data</li> <li>Use DeepEBM for complex distributions</li> <li>Visualize samples frequently during training</li> </ol> <p>For theoretical understanding, see the EBM Explained guide.</p>"},{"location":"user-guide/models/flow-guide/","title":"Flow Models: Practical User Guide","text":"<p>This guide provides practical instructions for working with normalizing flow models in Artifex. We cover creating, training, and using various flow architectures for density estimation and generation.</p>"},{"location":"user-guide/models/flow-guide/#quick-start","title":"Quick Start","text":"<p>Here's a minimal example to get started with RealNVP:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration.flow_config import (\n    RealNVPConfig,\n    CouplingNetworkConfig,\n)\nfrom artifex.generative_models.models.flow import RealNVP\n\n# Create RNG streams\nrngs = nnx.Rngs(params=0, noise=1, sample=2, dropout=3)\n\n# Configure the coupling network\ncoupling_config = CouplingNetworkConfig(\n    name=\"coupling_mlp\",\n    hidden_dims=(256, 256),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\n# Configure RealNVP\nconfig = RealNVPConfig(\n    name=\"realnvp_mnist\",\n    coupling_network=coupling_config,\n    input_dim=784,              # MNIST flattened\n    base_distribution=\"normal\",\n    num_coupling_layers=6,\n    mask_type=\"checkerboard\",\n)\n\n# Create model\nmodel = RealNVP(config, rngs=rngs)\n\n# Forward pass (data \u2192 latent)\nbatch = jax.random.normal(jax.random.key(0), (32, 784))\nz, log_det = model.forward(batch)\n\n# Compute log probability\nlog_prob = model.log_prob(batch)\nloss = -jnp.mean(log_prob)\n\n# Generate samples (latent \u2192 data)\nmodel.eval()\nsamples = model.generate(n_samples=16)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#creating-flow-models","title":"Creating Flow Models","text":"<p>Artifex provides multiple flow architectures. Choose based on your needs (see Flow Concepts for detailed comparison).</p>"},{"location":"user-guide/models/flow-guide/#realnvp-recommended-for-most-tasks","title":"RealNVP (Recommended for Most Tasks)","text":"<p>RealNVP offers a good balance between performance and computational cost.</p> <pre><code>from artifex.generative_models.core.configuration.flow_config import (\n    RealNVPConfig,\n    CouplingNetworkConfig,\n)\nfrom artifex.generative_models.models.flow import RealNVP\nfrom flax import nnx\nimport jax\nimport jax.numpy as jnp\n\n# Create RNGs\nrngs = nnx.Rngs(params=0, noise=1, sample=2, dropout=3)\n\n# Configure coupling network\ncoupling_config = CouplingNetworkConfig(\n    name=\"coupling_mlp\",\n    hidden_dims=(256, 256),      # Coupling network hidden layers\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\n# Configure RealNVP\nconfig = RealNVPConfig(\n    name=\"realnvp_flow\",\n    coupling_network=coupling_config,\n    input_dim=64,                # Feature dimension\n    base_distribution=\"normal\",\n    num_coupling_layers=8,       # Number of coupling transformations\n    mask_type=\"checkerboard\",    # or \"channel-wise\" for images\n)\n\n# Create model\nmodel = RealNVP(config, rngs=rngs)\n\n# Forward pass (data to latent)\nx = jax.random.normal(jax.random.key(0), (32, 64))\nz, log_det = model.forward(x)\n\n# Inverse pass (latent to data)\nsamples, _ = model.inverse(z)\n\n# Compute log probability\nlog_prob = model.log_prob(x)\nprint(f\"Log probability: {jnp.mean(log_prob):.3f}\")\n</code></pre> <p>Mask Types:</p> <ul> <li><code>\"checkerboard\"</code>: Alternates dimensions (good for tabular data)</li> <li><code>\"channel-wise\"</code>: Splits along channels (better for images)</li> </ul> <pre><code># For image data (flattened), use larger coupling network\ncoupling_config_image = CouplingNetworkConfig(\n    name=\"coupling_image\",\n    hidden_dims=(512, 512),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\nconfig_image = RealNVPConfig(\n    name=\"realnvp_image\",\n    coupling_network=coupling_config_image,\n    input_dim=784,                  # MNIST flattened (28*28*1)\n    base_distribution=\"normal\",\n    num_coupling_layers=12,\n    mask_type=\"checkerboard\",\n)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#glow-high-quality-image-generation","title":"Glow (High-Quality Image Generation)","text":"<p>Glow uses a multi-scale architecture with ActNorm, invertible 1\u00d71 convolutions, and coupling layers.</p> <pre><code>from artifex.generative_models.core.configuration.flow_config import (\n    GlowConfig,\n    CouplingNetworkConfig,\n)\nfrom artifex.generative_models.models.flow import Glow\n\n# Configure coupling network for Glow\ncoupling_config = CouplingNetworkConfig(\n    name=\"glow_coupling\",\n    hidden_dims=(512, 512),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\n# Configure Glow\nconfig = GlowConfig(\n    name=\"glow_model\",\n    coupling_network=coupling_config,\n    input_dim=3072,                  # 32*32*3 flattened\n    base_distribution=\"normal\",\n    image_shape=(32, 32, 3),         # Image shape for multi-scale\n    num_scales=3,                    # Multi-scale architecture\n    blocks_per_scale=6,              # Flow steps per scale\n)\n\n# Create Glow model\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = Glow(config, rngs=rngs)\n\n# Training\nimages = jax.random.normal(rngs.sample(), (16, 32, 32, 3))\noutputs = model(images, rngs=rngs)\nloss = -jnp.mean(outputs[\"log_prob\"])\n\n# Generate high-quality samples\nsamples = model.generate(n_samples=16, rngs=rngs)\n</code></pre> <p>Glow Architecture Parameters:</p> <ul> <li><code>num_scales</code>: Number of multi-scale levels (typically 2-4)</li> <li><code>blocks_per_scale</code>: Flow steps at each scale (typically 4-8)</li> <li>Higher values = more expressive but slower</li> </ul>"},{"location":"user-guide/models/flow-guide/#maf-fast-density-estimation","title":"MAF (Fast Density Estimation)","text":"<p>MAF (Masked Autoregressive Flow) excels at density estimation but has slow sampling.</p> <pre><code>from artifex.generative_models.core.configuration.flow_config import (\n    MAFConfig,\n    CouplingNetworkConfig,\n)\nfrom artifex.generative_models.models.flow import MAF\n\n# Configure coupling network for MAF\ncoupling_config = CouplingNetworkConfig(\n    name=\"maf_coupling\",\n    hidden_dims=(512,),              # MADE hidden dimensions\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\n# Configure MAF\nconfig = MAFConfig(\n    name=\"maf_model\",\n    coupling_network=coupling_config,\n    input_dim=64,\n    base_distribution=\"normal\",\n    num_layers=5,                    # Number of MAF layers\n    reverse_ordering=True,           # Alternate variable ordering\n)\n\n# Create MAF model\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = MAF(config, rngs=rngs)\n\n# Fast forward pass (density estimation)\nx = jax.random.normal(rngs.sample(), (100, 64))\nlog_prob = model.log_prob(x, rngs=rngs)\nprint(f\"Mean log-likelihood: {jnp.mean(log_prob):.3f}\")\n\n# Slow inverse pass (sampling)\nsamples = model.sample(n_samples=10, rngs=rngs)  # Sequential, slower\n</code></pre> <p>When to Use MAF:</p> <ul> <li>Primary goal is density estimation or anomaly detection</li> <li>Sampling speed is not critical</li> <li>Working with tabular or low-dimensional data</li> <li>Need high-quality likelihood estimates</li> </ul>"},{"location":"user-guide/models/flow-guide/#iaf-fast-sampling","title":"IAF (Fast Sampling)","text":"<p>IAF (Inverse Autoregressive Flow) provides fast sampling at the cost of slow density estimation.</p> <pre><code>from artifex.generative_models.core.configuration.flow_config import (\n    IAFConfig,\n    CouplingNetworkConfig,\n)\nfrom artifex.generative_models.models.flow import IAF\n\n# Configure coupling network for IAF\ncoupling_config = CouplingNetworkConfig(\n    name=\"iaf_coupling\",\n    hidden_dims=(512,),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\n# Configure IAF\nconfig = IAFConfig(\n    name=\"iaf_model\",\n    coupling_network=coupling_config,\n    input_dim=64,\n    base_distribution=\"normal\",\n    num_layers=5,\n    reverse_ordering=True,\n)\n\n# Create IAF model\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = IAF(config, rngs=rngs)\n\n# Fast sampling (parallel computation)\nsamples = model.sample(n_samples=100, rngs=rngs)  # Fast!\n\n# Slow density estimation (sequential)\nlog_prob = model.log_prob(samples, rngs=rngs)  # Slower\n</code></pre> <p>When to Use IAF:</p> <ul> <li>Fast sampling is critical (real-time generation)</li> <li>Often used as variational posterior in VAEs</li> <li>Density estimation is secondary</li> <li>Generation frequency &gt;&gt; inference frequency</li> </ul>"},{"location":"user-guide/models/flow-guide/#neural-spline-flows-most-expressive","title":"Neural Spline Flows (Most Expressive)","text":"<p>Neural Spline Flows use rational quadratic splines for highly expressive transformations.</p> <pre><code>from artifex.generative_models.core.configuration.flow_config import (\n    NeuralSplineConfig,\n    CouplingNetworkConfig,\n)\nfrom artifex.generative_models.models.flow import NeuralSplineFlow\n\n# Configure coupling network for spline flow\ncoupling_config = CouplingNetworkConfig(\n    name=\"spline_coupling\",\n    hidden_dims=(128, 128),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\n# Configure Neural Spline Flow\nconfig = NeuralSplineConfig(\n    name=\"spline_flow\",\n    coupling_network=coupling_config,\n    input_dim=64,\n    base_distribution=\"normal\",\n    num_layers=8,\n    num_bins=8,                      # Number of spline segments\n    tail_bound=3.0,                  # Spline domain bounds\n)\n\n# Create Neural Spline Flow\nrngs = nnx.Rngs(params=0, sample=1)\nmodel = NeuralSplineFlow(config, rngs=rngs)\n\n# More expressive transformations\nx = jax.random.normal(rngs.sample(), (32, 64))\nlog_prob = model.log_prob(x, rngs=rngs)\n\n# Generate samples\nsamples = model.generate(n_samples=16, rngs=rngs)\n</code></pre> <p>Spline Parameters:</p> <ul> <li><code>num_bins</code>: Number of spline segments (8-16 typical)</li> <li><code>tail_bound</code>: Domain bounds for spline (3.0-5.0 typical)</li> <li>More bins = more expressive but higher memory cost</li> </ul>"},{"location":"user-guide/models/flow-guide/#training-flow-models","title":"Training Flow Models","text":""},{"location":"user-guide/models/flow-guide/#basic-training-loop","title":"Basic Training Loop","text":"<p>Flow models are trained using maximum likelihood estimation.</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.models.flow import RealNVP\n\n# Initialize model and optimizer\nrngs = nnx.Rngs(params=0, dropout=1, sample=2)\nmodel = RealNVP(config, rngs=rngs)\noptimizer = nnx.Optimizer(model, optax.adam(learning_rate=1e-4), wrt=nnx.Param)\n\n# Training step function\n@nnx.jit\ndef train_step(model, optimizer, batch, rngs):\n    \"\"\"Single training step.\"\"\"\n    def loss_fn(model):\n        # Forward pass through flow\n        outputs = model(batch, rngs=rngs, training=True)\n\n        # Negative log-likelihood loss\n        log_prob = outputs[\"log_prob\"]\n        loss = -jnp.mean(log_prob)\n\n        return loss, {\"nll\": loss, \"mean_log_prob\": jnp.mean(log_prob)}\n\n    # Compute loss and gradients\n    (loss, metrics), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n\n    # Update parameters (NNX 0.11.0+ API)\n    optimizer.update(model, grads)\n\n    return metrics\n\n# Training loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    epoch_metrics = []\n\n    for batch in train_dataloader:\n        # Preprocess: add uniform noise for dequantization\n        batch = batch + jax.random.uniform(rngs.sample(), batch.shape) / 256.0\n\n        # Scale to appropriate range\n        batch = (batch - 0.5) / 0.5  # Scale to [-1, 1]\n\n        # Training step\n        metrics = train_step(model, optimizer, batch, rngs)\n        epoch_metrics.append(metrics)\n\n    # Log epoch statistics\n    avg_nll = jnp.mean(jnp.array([m[\"nll\"] for m in epoch_metrics]))\n    print(f\"Epoch {epoch+1}/{num_epochs}, NLL: {avg_nll:.3f}\")\n</code></pre>"},{"location":"user-guide/models/flow-guide/#training-with-gradient-clipping","title":"Training with Gradient Clipping","text":"<p>Gradient clipping helps stabilize flow training:</p> <pre><code>import optax\n\n# Create optimizer with gradient clipping\noptimizer_chain = optax.chain(\n    optax.clip_by_global_norm(1.0),  # Clip gradients\n    optax.adam(learning_rate=1e-4),\n)\n\noptimizer = nnx.Optimizer(model, optimizer_chain, wrt=nnx.Param)\n\n# Training step with clipping\n@nnx.jit\ndef train_step_clipped(model, optimizer, batch, rngs):\n    def loss_fn(model):\n        outputs = model(batch, rngs=rngs, training=True)\n        loss = -jnp.mean(outputs[\"log_prob\"])\n        return loss\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n\n    # Optimizer applies gradient clipping automatically (NNX 0.11.0+ API)\n    optimizer.update(model, grads)\n\n    return {\"loss\": loss}\n</code></pre>"},{"location":"user-guide/models/flow-guide/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Use learning rate warmup and decay for better convergence:</p> <pre><code>import optax\n\n# Learning rate schedule\nwarmup_steps = 1000\ntotal_steps = 50000\n\nschedule = optax.warmup_cosine_decay_schedule(\n    init_value=1e-7,\n    peak_value=1e-4,\n    warmup_steps=warmup_steps,\n    decay_steps=total_steps - warmup_steps,\n    end_value=1e-6,\n)\n\n# Create optimizer with schedule (wrt=nnx.Param required in NNX 0.11.0+)\noptimizer = nnx.Optimizer(\n    model,\n    optax.adam(learning_rate=schedule),\n    wrt=nnx.Param\n)\n\n# Track global step\nglobal_step = 0\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        metrics = train_step(model, optimizer, batch, rngs)\n        global_step += 1\n\n        # Learning rate automatically updated by optax\n</code></pre>"},{"location":"user-guide/models/flow-guide/#monitoring-training","title":"Monitoring Training","text":"<p>Track important metrics during training:</p> <pre><code># Training with metrics tracking\n@nnx.jit\ndef train_step_with_metrics(model, optimizer, batch, rngs):\n    def loss_fn(model):\n        # Forward pass\n        z, log_det = model.forward(batch, rngs=rngs)\n\n        # Base distribution log prob\n        log_p_z = -0.5 * jnp.sum(z**2, axis=-1) - 0.5 * z.shape[-1] * jnp.log(2 * jnp.pi)\n\n        # Total log probability\n        log_prob = log_p_z + log_det\n\n        # Loss\n        loss = -jnp.mean(log_prob)\n\n        # Additional metrics\n        metrics = {\n            \"loss\": loss,\n            \"log_p_z\": jnp.mean(log_p_z),\n            \"log_det\": jnp.mean(log_det),\n            \"log_prob\": jnp.mean(log_prob),\n            \"z_norm\": jnp.mean(jnp.linalg.norm(z, axis=-1)),\n        }\n\n        return loss, metrics\n\n    (loss, metrics), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n    optimizer.update(model, grads)\n\n    return metrics\n\n# Training loop with logging\nfor epoch in range(num_epochs):\n    metrics_list = []\n\n    for batch in train_dataloader:\n        batch = preprocess(batch)\n        metrics = train_step_with_metrics(model, optimizer, batch, rngs)\n        metrics_list.append(metrics)\n\n    # Aggregate epoch metrics\n    epoch_metrics = {\n        k: jnp.mean(jnp.array([m[k] for m in metrics_list]))\n        for k in metrics_list[0].keys()\n    }\n\n    print(f\"Epoch {epoch+1}: {epoch_metrics}\")\n</code></pre> <p>Important Metrics to Monitor:</p> <ul> <li><code>loss</code>: Negative log-likelihood (should decrease)</li> <li><code>log_p_z</code>: Base distribution log-prob (should be near 0 for standard Gaussian)</li> <li><code>log_det</code>: Jacobian log-determinant (tracks transformation magnitude)</li> <li><code>z_norm</code>: Latent norm (should be near \u221ad for d-dimensional Gaussian)</li> </ul>"},{"location":"user-guide/models/flow-guide/#data-preprocessing-for-flows","title":"Data Preprocessing for Flows","text":"<p>Proper preprocessing is crucial for flow models:</p>"},{"location":"user-guide/models/flow-guide/#dequantization","title":"Dequantization","text":"<p>Images are discrete (0-255), but flows need continuous values:</p> <pre><code>def dequantize(images, rngs):\n    \"\"\"Add uniform noise to dequantize discrete images.\"\"\"\n    # images should be in [0, 1] range\n    noise = jax.random.uniform(rngs.sample(), images.shape)\n    return images + noise / 256.0\n\n# Apply during training\nbatch = dequantize(batch, rngs)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#logit-transform","title":"Logit Transform","text":"<p>Map bounded data to unbounded space:</p> <pre><code>def logit_transform(x, alpha=0.05):\n    \"\"\"Apply logit transform with boundary handling.\"\"\"\n    # Squeeze to (alpha, 1-alpha) to avoid infinities\n    x = alpha + (1 - 2*alpha) * x\n\n    # Apply logit\n    return jnp.log(x) - jnp.log(1 - x)\n\ndef inverse_logit_transform(y, alpha=0.05):\n    \"\"\"Inverse of logit transform.\"\"\"\n    x = jax.nn.sigmoid(y)\n    return (x - alpha) / (1 - 2*alpha)\n\n# Apply during training\nbatch = logit_transform(batch)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#normalization","title":"Normalization","text":"<p>Standardize data to zero mean and unit variance:</p> <pre><code># Compute statistics on training data\ntrain_mean = jnp.mean(train_data, axis=0, keepdims=True)\ntrain_std = jnp.std(train_data, axis=0, keepdims=True)\n\n# Normalize\nbatch = (batch - train_mean) / (train_std + 1e-6)\n\n# Remember to denormalize samples\nsamples = model.generate(n_samples=16, rngs=rngs)\nsamples = samples * train_std + train_mean\n</code></pre>"},{"location":"user-guide/models/flow-guide/#sampling-and-generation","title":"Sampling and Generation","text":""},{"location":"user-guide/models/flow-guide/#basic-sampling","title":"Basic Sampling","text":"<p>Generate samples from a trained flow model:</p> <pre><code># Generate samples\nn_samples = 16\nsamples = model.generate(n_samples=n_samples, rngs=rngs)\n\n# For image data, reshape from flattened to spatial dimensions\n# input_dim is always an int (e.g., 784 for MNIST)\nH, W, C = 28, 28, 1\nimages = samples.reshape(n_samples, H, W, C)\n\n# Denormalize for visualization\nimages = (images * 0.5) + 0.5  # From [-1, 1] to [0, 1]\nimages = jnp.clip(images, 0, 1)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#temperature-sampling","title":"Temperature Sampling","text":"<p>Control sample diversity with temperature:</p> <pre><code>def sample_with_temperature(model, n_samples, temperature, rngs):\n    \"\"\"Sample with temperature scaling.\n\n    temperature &gt; 1: More diverse samples\n    temperature &lt; 1: More conservative samples\n    temperature = 1: Standard sampling\n    \"\"\"\n    # Sample from base distribution\n    z = jax.random.normal(rngs.sample(), (n_samples, model.latent_dim))\n\n    # Scale by temperature\n    z = z * temperature\n\n    # Transform to data space\n    samples, _ = model.inverse(z, rngs=rngs)\n\n    return samples\n\n# Conservative samples (sharper, less diverse)\nconservative = sample_with_temperature(model, 16, temperature=0.7, rngs=rngs)\n\n# Diverse samples (more variety, less sharp)\ndiverse = sample_with_temperature(model, 16, temperature=1.3, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#conditional-sampling","title":"Conditional Sampling","text":"<p>Some flow architectures support conditional generation:</p> <pre><code># For conditional flows\nfrom artifex.generative_models.core.configuration.flow_config import (\n    RealNVPConfig,\n    CouplingNetworkConfig,\n)\nfrom artifex.generative_models.models.flow import ConditionalRealNVP\n\n# Configure coupling network\ncoupling_config = CouplingNetworkConfig(\n    name=\"cond_coupling\",\n    hidden_dims=(512, 512),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\n# Create conditional model using RealNVPConfig\n# ConditionalRealNVP extracts condition_dim via getattr with default=10\nconfig = RealNVPConfig(\n    name=\"conditional_realnvp\",\n    coupling_network=coupling_config,\n    input_dim=784,\n    base_distribution=\"normal\",\n    num_coupling_layers=8,\n    mask_type=\"checkerboard\",\n)\n\nmodel = ConditionalRealNVP(config, rngs=rngs)\n\n# Sample conditioned on class labels\nclass_labels = jax.nn.one_hot(jnp.array([0, 1, 2]), 10)  # 3 classes\nconditional_samples = model.generate(\n    n_samples=3,\n    condition=class_labels,\n    rngs=rngs\n)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#interpolation-in-latent-space","title":"Interpolation in Latent Space","text":"<p>Interpolate between two data points:</p> <pre><code>def interpolate(model, x1, x2, num_steps, rngs):\n    \"\"\"Interpolate between two data points in latent space.\"\"\"\n    # Encode to latent space\n    z1, _ = model.forward(x1[None, ...], rngs=rngs)\n    z2, _ = model.forward(x2[None, ...], rngs=rngs)\n\n    # Linear interpolation in latent space\n    alphas = jnp.linspace(0, 1, num_steps)\n    z_interp = jnp.array([\n        (1 - alpha) * z1 + alpha * z2\n        for alpha in alphas\n    ]).squeeze(1)\n\n    # Decode to data space\n    x_interp, _ = model.inverse(z_interp, rngs=rngs)\n\n    return x_interp\n\n# Interpolate between two images\nx1 = train_data[0]  # First image\nx2 = train_data[1]  # Second image\ninterpolations = interpolate(model, x1, x2, num_steps=10, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#density-estimation-and-evaluation","title":"Density Estimation and Evaluation","text":""},{"location":"user-guide/models/flow-guide/#computing-log-likelihood","title":"Computing Log-Likelihood","text":"<p>Flow models provide exact log-likelihood:</p> <pre><code># Compute log-likelihood for test data\ntest_data = ...  # Your test dataset\n\nlog_likelihoods = []\nfor batch in test_dataloader:\n    # Preprocess same as training\n    batch = dequantize(batch, rngs)\n    batch = (batch - 0.5) / 0.5\n\n    # Compute log probability\n    log_prob = model.log_prob(batch, rngs=rngs)\n    log_likelihoods.append(log_prob)\n\n# Average log-likelihood\nall_log_probs = jnp.concatenate(log_likelihoods)\navg_log_likelihood = jnp.mean(all_log_probs)\nprint(f\"Test log-likelihood: {avg_log_likelihood:.3f}\")\n\n# Bits per dimension (common metric)\ninput_dim = config.input_dim  # int, e.g., 784 for MNIST\nbits_per_dim = -avg_log_likelihood / (input_dim * jnp.log(2))\nprint(f\"Bits per dimension: {bits_per_dim:.3f}\")\n</code></pre>"},{"location":"user-guide/models/flow-guide/#anomaly-detection","title":"Anomaly Detection","text":"<p>Use log-likelihood for anomaly detection:</p> <pre><code>def detect_anomalies(model, data, threshold, rngs):\n    \"\"\"Detect anomalies using log-likelihood threshold.\"\"\"\n    # Compute log probabilities\n    log_probs = model.log_prob(data, rngs=rngs)\n\n    # Flag samples below threshold as anomalies\n    is_anomaly = log_probs &lt; threshold\n\n    return is_anomaly, log_probs\n\n# Set threshold (e.g., 5th percentile of training data)\ntrain_log_probs = model.log_prob(train_data, rngs=rngs)\nthreshold = jnp.percentile(train_log_probs, 5)\n\n# Detect anomalies in test data\nanomalies, test_log_probs = detect_anomalies(\n    model, test_data, threshold, rngs\n)\n\nprint(f\"Detected {jnp.sum(anomalies)} anomalies out of {len(test_data)} samples\")\n</code></pre>"},{"location":"user-guide/models/flow-guide/#model-comparison","title":"Model Comparison","text":"<p>Compare different flow architectures using likelihood:</p> <pre><code># Train multiple models\nmodels = {\n    \"RealNVP\": realnvp_model,\n    \"Glow\": glow_model,\n    \"MAF\": maf_model,\n    \"Spline\": spline_model,\n}\n\n# Evaluate on test set\nresults = {}\nfor name, model in models.items():\n    log_probs = []\n    for batch in test_dataloader:\n        batch = preprocess(batch)\n        log_prob = model.log_prob(batch, rngs=rngs)\n        log_probs.append(log_prob)\n\n    avg_log_prob = jnp.mean(jnp.concatenate(log_probs))\n    results[name] = avg_log_prob\n\n    print(f\"{name}: {avg_log_prob:.3f} (higher is better)\")\n\n# Best model\nbest_model = max(results, key=results.get)\nprint(f\"Best model: {best_model}\")\n</code></pre>"},{"location":"user-guide/models/flow-guide/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/models/flow-guide/#multi-modal-data-distribution","title":"Multi-Modal Data Distribution","text":"<p>For data with multiple modes, increase model capacity:</p> <pre><code>from artifex.generative_models.core.configuration.flow_config import (\n    RealNVPConfig,\n    NeuralSplineConfig,\n    CouplingNetworkConfig,\n)\n\n# Increase number of layers\ncoupling_deep = CouplingNetworkConfig(\n    name=\"deep_coupling\",\n    hidden_dims=(1024, 1024, 1024),  # Deeper networks\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\nconfig = RealNVPConfig(\n    name=\"multimodal_flow\",\n    coupling_network=coupling_deep,\n    input_dim=64,\n    base_distribution=\"normal\",\n    num_coupling_layers=16,          # More layers\n    mask_type=\"checkerboard\",\n)\n\n# Or use Neural Spline Flows for higher expressiveness\ncoupling_spline = CouplingNetworkConfig(\n    name=\"spline_coupling\",\n    hidden_dims=(256, 256),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\nconfig_spline = NeuralSplineConfig(\n    name=\"multimodal_spline\",\n    coupling_network=coupling_spline,\n    input_dim=64,\n    base_distribution=\"normal\",\n    num_layers=12,\n    num_bins=16,                     # More bins for expressiveness\n    tail_bound=3.0,\n)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#high-dimensional-data","title":"High-Dimensional Data","text":"<p>For very high-dimensional data (e.g., high-resolution images):</p> <pre><code>from artifex.generative_models.core.configuration.flow_config import (\n    GlowConfig,\n    CouplingNetworkConfig,\n)\n\n# Use Glow with multi-scale architecture\ncoupling_config = CouplingNetworkConfig(\n    name=\"glow_highres_coupling\",\n    hidden_dims=(512, 512),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\nconfig = GlowConfig(\n    name=\"glow_highres\",\n    coupling_network=coupling_config,\n    input_dim=12288,                 # 64*64*3 flattened\n    base_distribution=\"normal\",\n    image_shape=(64, 64, 3),\n    num_scales=4,                    # More scales for higher resolution\n    blocks_per_scale=8,\n)\n\n# Reduce memory by processing in patches\ndef train_on_patches(model, image, patch_size=32):\n    \"\"\"Train on image patches to reduce memory.\"\"\"\n    H, W, C = image.shape\n    patches = []\n\n    for i in range(0, H, patch_size):\n        for j in range(0, W, patch_size):\n            patch = image[i:i+patch_size, j:j+patch_size, :]\n            patches.append(patch)\n\n    patches = jnp.array(patches)\n    return model(patches, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#tabular-data-with-mixed-types","title":"Tabular Data with Mixed Types","text":"<p>For tabular data with continuous and categorical features:</p> <pre><code># Preprocess mixed types\ndef preprocess_tabular(data, categorical_indices):\n    \"\"\"Preprocess tabular data with mixed types.\"\"\"\n    continuous = data.copy()\n\n    # One-hot encode categorical features\n    for idx in categorical_indices:\n        # One-hot encode\n        n_categories = int(jnp.max(data[:, idx])) + 1\n        one_hot = jax.nn.one_hot(data[:, idx].astype(int), n_categories)\n\n        # Replace categorical column with one-hot\n        continuous = jnp.concatenate([\n            continuous[:, :idx],\n            one_hot,\n            continuous[:, idx+1:],\n        ], axis=1)\n\n    return continuous\n\n# Use MAF for tabular data (good density estimation)\nfrom artifex.generative_models.core.configuration.flow_config import (\n    MAFConfig,\n    CouplingNetworkConfig,\n)\n\ncoupling_config = CouplingNetworkConfig(\n    name=\"tabular_coupling\",\n    hidden_dims=(512, 512),\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\nconfig = MAFConfig(\n    name=\"tabular_maf\",\n    coupling_network=coupling_config,\n    input_dim=processed_dim,\n    base_distribution=\"normal\",\n    num_layers=10,                   # More layers for complex dependencies\n)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#exact-reconstruction","title":"Exact Reconstruction","text":"<p>Verify model invertibility:</p> <pre><code>def test_invertibility(model, x, rngs, tolerance=1e-4):\n    \"\"\"Test that forward and inverse are true inverses.\"\"\"\n    # Forward then inverse\n    z, _ = model.forward(x, rngs=rngs)\n    x_recon, _ = model.inverse(z, rngs=rngs)\n\n    # Compute reconstruction error\n    error = jnp.max(jnp.abs(x - x_recon))\n\n    print(f\"Max reconstruction error: {error:.6f}\")\n    assert error &lt; tolerance, f\"Reconstruction error {error} exceeds tolerance {tolerance}\"\n\n# Test on random data\nx = jax.random.normal(rngs.sample(), (10, 64))\ntest_invertibility(model, x, rngs)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/models/flow-guide/#issue-nan-loss-during-training","title":"Issue: NaN Loss During Training","text":"<p>Symptoms: Loss becomes NaN after a few iterations.</p> <p>Solutions:</p> <ol> <li>Add gradient clipping:</li> </ol> <pre><code>optimizer = nnx.Optimizer(\n    model,\n    optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.adam(1e-4)\n    ),\n    wrt=nnx.Param\n)\n</code></pre> <ol> <li>Check data preprocessing:</li> </ol> <pre><code># Ensure data is properly scaled\nassert jnp.all(jnp.isfinite(batch)), \"Data contains NaN or Inf\"\nassert jnp.abs(jnp.mean(batch)) &lt; 10, \"Data not properly normalized\"\n</code></pre> <ol> <li>Reduce learning rate:</li> </ol> <pre><code>optimizer = nnx.Optimizer(model, optax.adam(1e-5), wrt=nnx.Param)  # Lower LR\n</code></pre> <ol> <li>Check Jacobian stability:</li> </ol> <pre><code># Monitor log-determinant magnitude\nz, log_det = model.forward(batch, rngs=rngs)\nprint(f\"Log-det range: [{jnp.min(log_det):.2f}, {jnp.max(log_det):.2f}]\")\n\n# If log_det has extreme values, reduce model capacity or layers\n</code></pre>"},{"location":"user-guide/models/flow-guide/#issue-poor-sample-quality","title":"Issue: Poor Sample Quality","text":"<p>Symptoms: Generated samples look noisy or unrealistic.</p> <p>Solutions:</p> <ol> <li>Increase model capacity:</li> </ol> <pre><code>coupling_config = CouplingNetworkConfig(\n    name=\"larger_coupling\",\n    hidden_dims=(1024, 1024),        # Larger networks\n    activation=\"relu\",\n    network_type=\"mlp\",\n    scale_activation=\"tanh\",\n)\n\nconfig = RealNVPConfig(\n    name=\"larger_model\",\n    coupling_network=coupling_config,\n    input_dim=784,\n    base_distribution=\"normal\",\n    num_coupling_layers=16,          # More layers\n    mask_type=\"checkerboard\",\n)\n</code></pre> <ol> <li>Use more expressive architecture:</li> </ol> <pre><code># Switch from RealNVP to Neural Spline Flows\nmodel = NeuralSplineFlow(spline_config, rngs=rngs)\n</code></pre> <ol> <li>Improve data preprocessing:</li> </ol> <pre><code># Apply logit transform for bounded data\nbatch = logit_transform(batch)\n</code></pre> <ol> <li>Train longer:</li> </ol> <pre><code>num_epochs = 200  # More epochs\n# Monitor validation likelihood to check for convergence\n</code></pre>"},{"location":"user-guide/models/flow-guide/#issue-slow-training","title":"Issue: Slow Training","text":"<p>Symptoms: Training takes too long per iteration.</p> <p>Solutions:</p> <ol> <li>Use JIT compilation:</li> </ol> <pre><code>@nnx.jit\ndef train_step(model, optimizer, batch, rngs):\n    # ... training step code\n    pass\n</code></pre> <ol> <li>Reduce model complexity:</li> </ol> <pre><code># Fewer coupling layers in RealNVPConfig\nnum_coupling_layers=6  # Instead of 16\n\n# Smaller hidden dimensions in CouplingNetworkConfig\nhidden_dims=(256, 256)  # Instead of (1024, 1024)\n</code></pre> <ol> <li>Use IAF for fast sampling (if sampling is the bottleneck):</li> </ol> <pre><code># IAF has fast sampling\nmodel = IAF(config, rngs=rngs)\n</code></pre> <ol> <li>Batch processing:</li> </ol> <pre><code># Increase batch size (if memory allows)\nbatch_size = 128  # Instead of 32\n</code></pre>"},{"location":"user-guide/models/flow-guide/#issue-mode-collapse","title":"Issue: Mode Collapse","text":"<p>Symptoms: Model generates similar samples repeatedly.</p> <p>Solutions:</p> <ol> <li>Check latent space coverage:</li> </ol> <pre><code># Generate many samples and check latent codes\nsamples = model.generate(n_samples=1000, rngs=rngs)\nz_samples, _ = model.forward(samples, rngs=rngs)\n\n# Check if latents cover the expected distribution\nz_mean = jnp.mean(z_samples, axis=0)\nz_std = jnp.std(z_samples, axis=0)\n\nprint(f\"Latent mean: {jnp.mean(z_mean):.3f} (should be ~0)\")\nprint(f\"Latent std: {jnp.mean(z_std):.3f} (should be ~1)\")\n</code></pre> <ol> <li>Increase model expressiveness:</li> </ol> <pre><code># Use Neural Spline Flows\n# Or increase number of flow layers\n</code></pre> <ol> <li>Check for numerical issues:</li> </ol> <pre><code># Ensure stable training\n# Use gradient clipping and proper LR\n</code></pre>"},{"location":"user-guide/models/flow-guide/#issue-memory-errors","title":"Issue: Memory Errors","text":"<p>Symptoms: Out of memory errors during training.</p> <p>Solutions:</p> <ol> <li>Reduce batch size:</li> </ol> <pre><code>batch_size = 16  # Smaller batches\n</code></pre> <ol> <li>Use gradient checkpointing (if available):</li> </ol> <pre><code># Recompute intermediate activations during backward pass\n# (implementation-specific)\n</code></pre> <ol> <li>Reduce model size:</li> </ol> <pre><code># Fewer layers or smaller hidden dimensions\n# In CouplingNetworkConfig: hidden_dims=(256, 256)\n# In RealNVPConfig: num_coupling_layers=6\n</code></pre> <ol> <li>Use mixed precision training:</li> </ol> <pre><code># Use float16 for some computations (implementation-specific)\n</code></pre>"},{"location":"user-guide/models/flow-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/models/flow-guide/#do","title":"DO","text":"<p>\u2705 Preprocess data properly:</p> <pre><code># Always dequantize discrete data\nbatch = dequantize(batch, rngs)\n\n# Normalize to appropriate range\nbatch = (batch - 0.5) / 0.5\n</code></pre> <p>\u2705 Monitor multiple metrics:</p> <pre><code># Track loss, log_det, base log prob\nmetrics = {\n    \"loss\": loss,\n    \"log_det\": jnp.mean(log_det),\n    \"log_p_z\": jnp.mean(log_p_z),\n}\n</code></pre> <p>\u2705 Use gradient clipping:</p> <pre><code>optimizer = nnx.Optimizer(\n    model,\n    optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.adam(1e-4)\n    ),\n    wrt=nnx.Param\n)\n</code></pre> <p>\u2705 Validate invertibility:</p> <pre><code># Periodically test reconstruction\ntest_invertibility(model, validation_batch, rngs)\n</code></pre> <p>\u2705 Choose architecture for your task:</p> <pre><code># MAF for density estimation\n# IAF for fast sampling\n# RealNVP for balance\n# Glow for high-quality images\n# Spline Flows for expressiveness\n</code></pre>"},{"location":"user-guide/models/flow-guide/#dont","title":"DON'T","text":"<p>\u274c Don't skip data preprocessing:</p> <pre><code># BAD: Using raw discrete images\nmodel(raw_images, rngs=rngs)  # Will perform poorly!\n\n# GOOD: Dequantize and normalize\nprocessed = dequantize(raw_images, rngs)\nprocessed = (processed - 0.5) / 0.5\nmodel(processed, rngs=rngs)\n</code></pre> <p>\u274c Don't ignore numerical stability:</p> <pre><code># BAD: No gradient clipping\n# Can lead to NaN losses\n\n# GOOD: Use gradient clipping (wrt=nnx.Param required in NNX 0.11.0+)\noptimizer = nnx.Optimizer(\n    model,\n    optax.chain(optax.clip_by_global_norm(1.0), optax.adam(1e-4)),\n    wrt=nnx.Param\n)\n</code></pre> <p>\u274c Don't use wrong architecture for task:</p> <pre><code># BAD: Using IAF when density estimation is primary goal\n# IAF has slow forward pass\n\n# GOOD: Use MAF for density estimation\nmodel = MAF(config, rngs=rngs)\n</code></pre> <p>\u274c Don't overtrain on small datasets:</p> <pre><code># Monitor validation likelihood\n# Use early stopping\n</code></pre>"},{"location":"user-guide/models/flow-guide/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":""},{"location":"user-guide/models/flow-guide/#number-of-flow-layers","title":"Number of Flow Layers","text":"Data Type Recommended Layers Notes Tabular (low-dim) 6-10 Simpler distributions Tabular (high-dim) 10-16 More complex dependencies Images (low-res) 8-12 MNIST, CIFAR-10 Images (high-res) 12-24 Higher resolution"},{"location":"user-guide/models/flow-guide/#hidden-dimensions","title":"Hidden Dimensions","text":"Model Recommended Notes RealNVP [512, 512] 2-3 layers sufficient Glow [512, 512] Larger for high-res MAF/IAF [512] - [1024] Single deep network Spline [128, 128] Splines are expressive"},{"location":"user-guide/models/flow-guide/#learning-rates","title":"Learning Rates","text":"Stage Learning Rate Notes Warmup 1e-7 \u2192 1e-4 First 1000 steps Training 1e-4 Standard rate Fine-tuning 1e-5 Near convergence"},{"location":"user-guide/models/flow-guide/#batch-sizes","title":"Batch Sizes","text":"Data Type Batch Size Notes Tabular 256-1024 Can use large batches Images (32\u00d732) 64-128 Memory dependent Images (64\u00d764) 32-64 Reduce for Glow Images (128\u00d7128) 16-32 Limited by memory"},{"location":"user-guide/models/flow-guide/#summary","title":"Summary","text":""},{"location":"user-guide/models/flow-guide/#quick-reference","title":"Quick Reference","text":"<p>Model Selection:</p> <ul> <li>RealNVP: Balanced performance, good for most tasks</li> <li>Glow: Best for high-quality image generation</li> <li>MAF: Optimal for density estimation</li> <li>IAF: Optimal for fast sampling</li> <li>Spline Flows: Most expressive transformations</li> </ul> <p>Training Checklist:</p> <ol> <li>\u2705 Preprocess data (dequantize, normalize)</li> <li>\u2705 Use gradient clipping</li> <li>\u2705 Monitor multiple metrics</li> <li>\u2705 Validate invertibility</li> <li>\u2705 Apply learning rate warmup</li> <li>\u2705 Check for NaN/Inf values</li> </ol> <p>Common Workflows:</p> <pre><code># Density estimation workflow\nmodel = MAF(config, rngs=rngs)\nlog_probs = model.log_prob(data, rngs=rngs)\n\n# Generation workflow\nmodel = RealNVP(config, rngs=rngs)\nsamples = model.generate(n_samples=16, rngs=rngs)\n\n# Anomaly detection workflow\nmodel = MAF(config, rngs=rngs)\nthreshold = jnp.percentile(train_log_probs, 5)\nanomalies = test_log_probs &lt; threshold\n</code></pre>"},{"location":"user-guide/models/flow-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Theory: See Flow Concepts for mathematical foundations</li> <li>API Reference: Check Flow API for complete documentation</li> <li>Tutorial: Follow Flow MNIST Example for hands-on practice</li> </ul>"},{"location":"user-guide/models/flow-guide/#references","title":"References","text":"<ul> <li>Dinh et al. (2016): \"Density estimation using Real NVP\"</li> <li>Kingma &amp; Dhariwal (2018): \"Glow: Generative Flow with Invertible 1x1 Convolutions\"</li> <li>Papamakarios et al. (2017): \"Masked Autoregressive Flow for Density Estimation\"</li> <li>Durkan et al. (2019): \"Neural Spline Flows\"</li> </ul>"},{"location":"user-guide/models/gan-guide/","title":"GAN User Guide","text":"<p>This guide provides practical instructions for training and using Generative Adversarial Networks (GANs) in Artifex. We cover all GAN variants, training strategies, common issues, and best practices.</p>"},{"location":"user-guide/models/gan-guide/#quick-start","title":"Quick Start","text":"<p>Here's a minimal example to get you started:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration.network_configs import (\n    GeneratorConfig,\n    DiscriminatorConfig,\n)\nfrom artifex.generative_models.models.gan import Generator, Discriminator\nfrom artifex.generative_models.core.losses.adversarial import (\n    vanilla_generator_loss,\n    vanilla_discriminator_loss,\n)\n\n# Initialize RNGs (discriminator needs dropout stream)\ngen_rngs = nnx.Rngs(params=0, sample=1, dropout=2)\ndisc_rngs = nnx.Rngs(params=3, dropout=4)\n\n# Create generator config\ngen_config = GeneratorConfig(\n    name=\"simple_generator\",\n    hidden_dims=(256, 512),         # Use tuples, not lists\n    output_shape=(28, 28, 1),       # MNIST shape (H, W, C)\n    latent_dim=100,\n    activation=\"relu\",\n    batch_norm=True,\n    dropout_rate=0.0,\n)\n\n# Create discriminator config\ndisc_config = DiscriminatorConfig(\n    name=\"simple_discriminator\",\n    input_shape=(28, 28, 1),\n    hidden_dims=(512, 256),\n    activation=\"leaky_relu\",\n    batch_norm=False,\n    dropout_rate=0.3,\n)\n\n# Create models\ngenerator = Generator(config=gen_config, rngs=gen_rngs)\ndiscriminator = Discriminator(config=disc_config, rngs=disc_rngs)\n\n# Generate samples\nz = jax.random.normal(jax.random.key(0), (16, 100))\ngenerator.eval()  # Set to evaluation mode\nsamples = generator(z)\nprint(f\"Generated samples shape: {samples.shape}\")  # (16, 28, 28, 1)\n</code></pre>"},{"location":"user-guide/models/gan-guide/#creating-gan-components","title":"Creating GAN Components","text":"<p>Artifex uses a config-based API for creating GAN components. Define configurations first, then create models from them.</p>"},{"location":"user-guide/models/gan-guide/#basic-generator","title":"Basic Generator","text":"<p>The generator transforms random noise into data samples:</p> <pre><code>from artifex.generative_models.core.configuration.network_configs import GeneratorConfig\nfrom artifex.generative_models.models.gan import Generator\n\n# Create generator configuration\ngen_config = GeneratorConfig(\n    name=\"image_generator\",\n    hidden_dims=(128, 256, 512),      # Hidden layer sizes (use tuples)\n    output_shape=(32, 32, 3),         # Output: 32x32 RGB images (H, W, C)\n    latent_dim=100,                    # Latent space dimension\n    activation=\"relu\",                 # Activation function\n    batch_norm=True,                   # Use batch normalization\n    dropout_rate=0.0,                  # Dropout rate (usually 0 for generator)\n)\n\n# Create generator from config\nrngs = nnx.Rngs(params=0, sample=1)\ngenerator = Generator(config=gen_config, rngs=rngs)\n\n# Generate samples from random noise\nz = jax.random.normal(jax.random.key(0), (batch_size, 100))\ngenerator.train()  # Set to training mode\nfake_samples = generator(z)\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>hidden_dims</code>: Tuple of hidden layer dimensions (progressively increases capacity)</li> <li><code>output_shape</code>: Target data shape (height, width, channels)</li> <li><code>latent_dim</code>: Size of input latent vector (typically 64-512)</li> <li><code>batch_norm</code>: Stabilizes training (recommended for generator)</li> <li><code>activation</code>: \"relu\" for generator, \"leaky_relu\" for discriminator</li> </ul>"},{"location":"user-guide/models/gan-guide/#basic-discriminator","title":"Basic Discriminator","text":"<p>The discriminator classifies samples as real or fake:</p> <pre><code>from artifex.generative_models.core.configuration.network_configs import DiscriminatorConfig\nfrom artifex.generative_models.models.gan import Discriminator\n\n# Create discriminator configuration\ndisc_config = DiscriminatorConfig(\n    name=\"image_discriminator\",\n    input_shape=(32, 32, 3),          # Input image shape (H, W, C)\n    hidden_dims=(512, 256, 128),      # Hidden layer sizes (often mirrors generator)\n    activation=\"leaky_relu\",           # LeakyReLU prevents dying neurons\n    batch_norm=False,                  # Usually False for discriminator\n    dropout_rate=0.3,                  # Dropout to prevent overfitting\n)\n\n# Create discriminator from config\ndisc_rngs = nnx.Rngs(params=2)\ndiscriminator = Discriminator(config=disc_config, rngs=disc_rngs)\n\n# Classify samples\ndiscriminator.train()  # Set to training mode\nreal_data = jnp.ones((batch_size, 32, 32, 3))\nfake_data = generator(z)\n\nreal_scores = discriminator(real_data)  # Should be close to 1\nfake_scores = discriminator(fake_data)  # Should be close to 0\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>hidden_dims</code>: Usually mirrors generator in reverse</li> <li><code>activation</code>: \"leaky_relu\" is standard (slope 0.2)</li> <li><code>batch_norm</code>: Usually False (can cause training issues)</li> <li><code>dropout_rate</code>: 0.3-0.5 helps prevent overfitting</li> </ul>"},{"location":"user-guide/models/gan-guide/#gan-variants","title":"GAN Variants","text":""},{"location":"user-guide/models/gan-guide/#1-vanilla-gan","title":"1. Vanilla GAN","text":"<p>The original GAN formulation with binary cross-entropy loss:</p> <pre><code>from artifex.generative_models.core.configuration.network_configs import (\n    GeneratorConfig,\n    DiscriminatorConfig,\n)\nfrom artifex.generative_models.core.configuration.gan_config import GANConfig\nfrom artifex.generative_models.models.gan import GAN\n\n# Create nested configs\ngen_config = GeneratorConfig(\n    name=\"vanilla_generator\",\n    hidden_dims=(256, 512),\n    output_shape=(28, 28, 1),\n    latent_dim=100,\n    activation=\"relu\",\n    batch_norm=True,\n)\n\ndisc_config = DiscriminatorConfig(\n    name=\"vanilla_discriminator\",\n    input_shape=(28, 28, 1),\n    hidden_dims=(512, 256),\n    activation=\"leaky_relu\",\n    batch_norm=False,\n    dropout_rate=0.3,\n)\n\n# Create GAN config\ngan_config = GANConfig(\n    name=\"vanilla_gan\",\n    generator=gen_config,\n    discriminator=disc_config,\n    loss_type=\"vanilla\",\n    generator_lr=0.0002,\n    discriminator_lr=0.0002,\n)\n\n# Create GAN\nrngs = nnx.Rngs(params=0, sample=1, dropout=2)\ngan = GAN(config=gan_config, rngs=rngs)\n\n# Generate samples\nsamples = gan.generate(n_samples=16)\n</code></pre> <p>When to use:</p> <ul> <li>Learning GANs for the first time</li> <li>Simple datasets (MNIST, simple shapes)</li> <li>Proof-of-concept experiments</li> </ul> <p>Pros: Simple, well-understood Cons: Training instability, mode collapse</p>"},{"location":"user-guide/models/gan-guide/#2-deep-convolutional-gan-dcgan","title":"2. Deep Convolutional GAN (DCGAN)","text":"<p>Uses convolutional architecture for images:</p> <pre><code>from artifex.generative_models.core.configuration.network_configs import (\n    ConvGeneratorConfig,\n    ConvDiscriminatorConfig,\n)\nfrom artifex.generative_models.core.configuration.gan_config import DCGANConfig\nfrom artifex.generative_models.models.gan import DCGAN\n\n# Create convolutional generator config\nconv_gen_config = ConvGeneratorConfig(\n    name=\"dcgan_generator\",\n    latent_dim=100,\n    hidden_dims=(512, 256, 128, 64),  # Feature map progression\n    output_shape=(64, 64, 3),         # 64x64 RGB images (H, W, C)\n    activation=\"relu\",\n    batch_norm=True,\n    kernel_size=(4, 4),\n    stride=(2, 2),\n    padding=\"SAME\",\n)\n\n# Create convolutional discriminator config\nconv_disc_config = ConvDiscriminatorConfig(\n    name=\"dcgan_discriminator\",\n    hidden_dims=(64, 128, 256, 512),  # Mirrored progression\n    input_shape=(64, 64, 3),\n    activation=\"leaky_relu\",\n    batch_norm=True,\n    kernel_size=(4, 4),\n    stride=(2, 2),\n    padding=\"SAME\",\n)\n\n# Create DCGAN config with nested network configs\ndcgan_config = DCGANConfig(\n    name=\"dcgan_64\",\n    generator=conv_gen_config,\n    discriminator=conv_disc_config,\n    loss_type=\"vanilla\",\n    generator_lr=0.0002,\n    discriminator_lr=0.0002,\n    beta1=0.5,\n    beta2=0.999,\n)\n\n# Create DCGAN\nrngs = nnx.Rngs(params=0, sample=1, dropout=2)\ndcgan = DCGAN(config=dcgan_config, rngs=rngs)\n\n# Generate high-quality images\ndcgan.eval()  # Set to evaluation mode\nsamples = dcgan.generate(n_samples=64)\n</code></pre> <p>DCGAN Architecture Guidelines:</p> <ol> <li>Replace pooling with strided convolutions</li> <li>Use batch normalization (except discriminator input and generator output)</li> <li>Remove fully connected layers (except for latent projection)</li> <li>Use ReLU in generator, LeakyReLU in discriminator</li> <li>Use Tanh activation in generator output</li> </ol> <p>When to use:</p> <ul> <li>Image generation tasks</li> <li>64\u00d764 to 128\u00d7128 resolution</li> <li>More stable training than vanilla GAN</li> </ul> <p>Pros: More stable, better image quality Cons: Still can suffer from mode collapse</p>"},{"location":"user-guide/models/gan-guide/#3-wasserstein-gan-wgan","title":"3. Wasserstein GAN (WGAN)","text":"<p>Uses Wasserstein distance for more stable training:</p> <pre><code>from artifex.generative_models.models.gan import WGAN, compute_gradient_penalty\nfrom artifex.generative_models.core.configuration.gan_config import WGANConfig\nfrom artifex.generative_models.core.configuration.network_configs import (\n    ConvGeneratorConfig,\n    ConvDiscriminatorConfig,\n)\n\n# Create WGAN with proper typed configs\ngenerator_config = ConvGeneratorConfig(\n    name=\"wgan_generator\",\n    latent_dim=100,\n    hidden_dims=(512, 256, 128, 64),\n    output_shape=(3, 64, 64),\n    activation=\"relu\",\n    batch_norm=True,\n    kernel_size=(4, 4),\n    stride=(2, 2),\n    padding=\"SAME\",\n)\n\ndiscriminator_config = ConvDiscriminatorConfig(\n    name=\"wgan_critic\",\n    hidden_dims=(64, 128, 256, 512),\n    input_shape=(3, 64, 64),\n    activation=\"leaky_relu\",\n    kernel_size=(4, 4),\n    stride=(2, 2),\n    padding=\"SAME\",\n    use_instance_norm=True,\n)\n\nwgan_config = WGANConfig(\n    name=\"wgan\",\n    generator=generator_config,\n    discriminator=discriminator_config,\n    critic_iterations=5,               # Update critic 5x per generator\n    use_gradient_penalty=True,\n    gradient_penalty_weight=10.0,      # Lambda for gradient penalty\n)\n\nwgan = WGAN(config=wgan_config, rngs=rngs)\n\n# Training loop for WGAN\ndef train_wgan_step(wgan, real_samples, rngs, n_critic=5):\n    \"\"\"Train WGAN with proper critic/generator balance.\"\"\"\n\n    # Train critic n_critic times\n    for _ in range(n_critic):\n        z = jax.random.normal(rngs.sample(), (real_samples.shape[0], wgan.latent_dim))\n        fake_samples = wgan.generator(z)\n\n        # Compute discriminator loss with gradient penalty\n        disc_loss = wgan.discriminator_loss(real_samples, fake_samples, rngs)\n\n        # Update discriminator (use nnx.Optimizer in practice)\n\n    # Train generator once\n    z = jax.random.normal(rngs.sample(), (real_samples.shape[0], wgan.latent_dim))\n    fake_samples = wgan.generator(z)\n    gen_loss = wgan.generator_loss(fake_samples)\n\n    return {\"disc_loss\": disc_loss, \"gen_loss\": gen_loss}\n</code></pre> <p>Key Differences from Vanilla GAN:</p> <ol> <li>Critic instead of discriminator (no sigmoid at output)</li> <li>Wasserstein distance instead of JS divergence</li> <li>Gradient penalty enforces Lipschitz constraint</li> <li>Multiple critic updates per generator update (5:1 ratio)</li> <li>Instance normalization instead of batch norm in critic</li> </ol> <p>When to use:</p> <ul> <li>Need stable training</li> <li>Want meaningful loss metric</li> <li>High-resolution images</li> <li>Research experiments</li> </ul> <p>Pros: Very stable, meaningful loss, better mode coverage Cons: Slower training, more complex</p>"},{"location":"user-guide/models/gan-guide/#4-least-squares-gan-lsgan","title":"4. Least Squares GAN (LSGAN)","text":"<p>Uses least squares loss for smoother gradients:</p> <pre><code>from artifex.generative_models.models.gan import LSGAN\nfrom artifex.generative_models.core.configuration.gan_config import LSGANConfig\nfrom artifex.generative_models.core.configuration.network_configs import (\n    ConvGeneratorConfig,\n    ConvDiscriminatorConfig,\n)\n\n# Create LSGAN with typed configs\ngenerator_config = ConvGeneratorConfig(\n    name=\"lsgan_generator\",\n    latent_dim=100,\n    hidden_dims=(512, 256, 128, 64),\n    output_shape=(3, 64, 64),\n    activation=\"relu\",\n    batch_norm=True,\n    kernel_size=(4, 4),\n    stride=(2, 2),\n    padding=\"SAME\",\n)\n\ndiscriminator_config = ConvDiscriminatorConfig(\n    name=\"lsgan_discriminator\",\n    hidden_dims=(64, 128, 256, 512),\n    input_shape=(3, 64, 64),\n    activation=\"leaky_relu\",\n    kernel_size=(4, 4),\n    stride=(2, 2),\n    padding=\"SAME\",\n)\n\nlsgan_config = LSGANConfig(\n    name=\"lsgan\",\n    generator=generator_config,\n    discriminator=discriminator_config,\n    a=0.0,   # Target for fake samples in discriminator\n    b=1.0,   # Target for real samples in discriminator\n    c=1.0,   # Target for fake samples in generator\n)\n\nlsgan = LSGAN(config=lsgan_config, rngs=rngs)\n</code></pre> <p>Key Difference:</p> <p>Loss function uses squared error instead of log loss:</p> <ul> <li>Generator: Minimize \\((D(G(z)) - 1)^2\\)</li> <li>Discriminator: Minimize \\((D(x) - 1)^2 + D(G(z))^2\\)</li> </ul> <p>When to use:</p> <ul> <li>Want smoother gradients than vanilla GAN</li> <li>Need more stable training than vanilla</li> <li>Image generation with less training instability</li> </ul> <p>Pros: More stable than vanilla, penalizes far-from-boundary samples Cons: Still can mode collapse</p>"},{"location":"user-guide/models/gan-guide/#5-conditional-gan-cgan","title":"5. Conditional GAN (cGAN)","text":"<p>Conditions generation on labels or other information:</p> <pre><code>from artifex.generative_models.models.gan import (\n    ConditionalGAN,\n    ConditionalGenerator,\n    ConditionalDiscriminator,\n)\nfrom artifex.generative_models.core.configuration.gan_config import ConditionalGANConfig\nfrom artifex.generative_models.core.configuration.network_configs import (\n    ConditionalParams,\n    ConditionalGeneratorConfig,\n    ConditionalDiscriminatorConfig,\n)\n\n# Shared conditional parameters\ncond_params = ConditionalParams(num_classes=10, embedding_dim=50)\n\n# Create conditional generator config\ngen_config = ConditionalGeneratorConfig(\n    name=\"cgan_generator\",\n    latent_dim=100,\n    hidden_dims=(512, 256, 128, 64),\n    output_shape=(1, 28, 28),\n    activation=\"relu\",\n    batch_norm=True,\n    conditional=cond_params,\n)\n\n# Create conditional discriminator config\ndisc_config = ConditionalDiscriminatorConfig(\n    name=\"cgan_discriminator\",\n    hidden_dims=(64, 128, 256, 512),\n    input_shape=(1, 28, 28),\n    activation=\"leaky_relu\",\n    conditional=cond_params,\n)\n\n# Create the conditional GAN\ncgan_config = ConditionalGANConfig(\n    name=\"conditional_gan\",\n    generator=gen_config,\n    discriminator=disc_config,\n)\ncgan = ConditionalGAN(config=cgan_config, rngs=rngs)\n\n# Generate conditioned on class labels (one-hot encoded)\nlabels = jax.nn.one_hot(jnp.arange(10), 10)  # One of each digit\nz = jax.random.normal(rngs.sample(), (10, 100))\n\n# Generate specific digits\ncgan.eval()\nsamples = cgan.generator(z, labels)\n\n# Discriminate with labels\ncgan.train()\nreal_scores = cgan.discriminator(real_data, real_labels)\nfake_scores = cgan.discriminator(samples, labels)\n</code></pre> <p>Key Features:</p> <ul> <li>Controlled generation: Specify what to generate</li> <li>Class conditioning: Generate specific categories</li> <li>Embedding layer: Maps labels to high-dimensional space</li> <li>Concatenation: Combines embeddings with features</li> </ul> <p>When to use:</p> <ul> <li>Need to control generation (class, attributes)</li> <li>Have labeled data</li> <li>Want to generate specific categories</li> <li>Image-to-image translation with labels</li> </ul> <p>Pros: Controlled generation, useful for labeled datasets Cons: Requires labels, more complex</p>"},{"location":"user-guide/models/gan-guide/#6-cyclegan","title":"6. CycleGAN","text":"<p>Unpaired image-to-image translation:</p> <pre><code>from artifex.generative_models.models.gan import CycleGAN\nfrom artifex.generative_models.core.configuration.gan_config import CycleGANConfig\nfrom artifex.generative_models.core.configuration.network_configs import (\n    CycleGANGeneratorConfig,\n    PatchGANDiscriminatorConfig,\n)\n\n# Create CycleGAN for domain transfer (e.g., horse \u2194 zebra)\ncyclegan_config = CycleGANConfig(\n    name=\"cyclegan_horse2zebra\",\n    generator={\n        \"a_to_b\": CycleGANGeneratorConfig(\n            name=\"horse_to_zebra\",\n            latent_dim=0,\n            hidden_dims=(64, 128, 256),\n            output_shape=(256, 256, 3),\n            input_shape=(256, 256, 3),\n            n_residual_blocks=6,\n            activation=\"relu\",\n        ),\n        \"b_to_a\": CycleGANGeneratorConfig(\n            name=\"zebra_to_horse\",\n            latent_dim=0,\n            hidden_dims=(64, 128, 256),\n            output_shape=(256, 256, 3),\n            input_shape=(256, 256, 3),\n            n_residual_blocks=6,\n            activation=\"relu\",\n        ),\n    },\n    discriminator={\n        \"disc_a\": PatchGANDiscriminatorConfig(\n            name=\"horse_discriminator\",\n            hidden_dims=(64, 128, 256, 512),\n            input_shape=(256, 256, 3),\n            activation=\"leaky_relu\",\n        ),\n        \"disc_b\": PatchGANDiscriminatorConfig(\n            name=\"zebra_discriminator\",\n            hidden_dims=(64, 128, 256, 512),\n            input_shape=(256, 256, 3),\n            activation=\"leaky_relu\",\n        ),\n    },\n    input_shape_a=(256, 256, 3),\n    input_shape_b=(256, 256, 3),\n    lambda_cycle=10.0,\n    lambda_identity=0.5,\n)\n\ncyclegan = CycleGAN(config=cyclegan_config, rngs=rngs)\n\n# Training step\ndef train_cyclegan_step(cyclegan, batch_x, batch_y):\n    \"\"\"Train CycleGAN with cycle consistency.\"\"\"\n    cyclegan.train()\n\n    # Forward cycle: A -&gt; B -&gt; A\n    fake_b = cyclegan.generator_a_to_b(batch_x)\n    reconstructed_a = cyclegan.generator_b_to_a(fake_b)\n\n    # Backward cycle: B -&gt; A -&gt; B\n    fake_a = cyclegan.generator_b_to_a(batch_y)\n    reconstructed_b = cyclegan.generator_a_to_b(fake_a)\n\n    # Adversarial losses\n    disc_b_real = cyclegan.discriminator_b(batch_y)\n    disc_b_fake = cyclegan.discriminator_b(fake_b)\n    disc_a_real = cyclegan.discriminator_a(batch_x)\n    disc_a_fake = cyclegan.discriminator_a(fake_a)\n\n    # Cycle consistency losses\n    cycle_loss_a = jnp.mean(jnp.abs(reconstructed_a - batch_x))\n    cycle_loss_b = jnp.mean(jnp.abs(reconstructed_b - batch_y))\n    total_cycle_loss = cyclegan.lambda_cycle * (cycle_loss_a + cycle_loss_b)\n\n    # Identity losses (helps preserve color)\n    identity_a = cyclegan.generator_b_to_a(batch_x)\n    identity_b = cyclegan.generator_a_to_b(batch_y)\n    identity_loss_a = jnp.mean(jnp.abs(identity_a - batch_x))\n    identity_loss_b = jnp.mean(jnp.abs(identity_b - batch_y))\n    total_identity_loss = cyclegan.lambda_identity * (identity_loss_a + identity_loss_b)\n\n    return {\n        \"cycle_loss\": total_cycle_loss,\n        \"identity_loss\": total_identity_loss,\n    }\n</code></pre> <p>Key Features:</p> <ul> <li>Two generators: G: X\u2192Y and F: Y\u2192X</li> <li>Two discriminators: D_X and D_Y</li> <li>Cycle consistency: x \u2192 G(x) \u2192 F(G(x)) \u2248 x</li> <li>No paired data needed</li> </ul> <p>When to use:</p> <ul> <li>Image-to-image translation without paired data</li> <li>Style transfer (photo \u2194 painting)</li> <li>Domain adaptation (synthetic \u2194 real)</li> <li>Seasonal changes (summer \u2194 winter)</li> </ul> <p>Pros: No paired data needed, flexible Cons: Computationally expensive (4 networks), can fail if domains too different</p>"},{"location":"user-guide/models/gan-guide/#7-patchgan","title":"7. PatchGAN","text":"<p>Discriminator operates on image patches:</p> <pre><code>from artifex.generative_models.models.gan import (\n    PatchGANDiscriminator,\n    MultiScalePatchGANDiscriminator,\n)\n\n# Single-scale PatchGAN\nfrom artifex.generative_models.core.configuration.network_configs import (\n    PatchGANDiscriminatorConfig,\n)\n\npatch_config = PatchGANDiscriminatorConfig(\n    name=\"patchgan_disc\",\n    hidden_dims=(64, 128, 256, 512),\n    input_shape=(256, 256, 3),\n    activation=\"leaky_relu\",\n)\n\npatch_discriminator = PatchGANDiscriminator(config=patch_config, rngs=rngs)\n\n# Returns list of intermediate features with final output last\npatch_features = patch_discriminator(images)\n\n# Multi-scale PatchGAN (better for high-resolution)\nmultiscale_discriminator = MultiScalePatchGANDiscriminator(\n    config=patch_config,\n    num_scales=3,\n    rngs=rngs,\n)\n\n# Returns (outputs_per_scale, features_per_scale)\noutputs, features = multiscale_discriminator(images)\n</code></pre> <p>Key Features:</p> <ul> <li>Patch-based: Classifies overlapping patches</li> <li>Local texture: Better for texture quality</li> <li>Efficient: Fewer parameters than full-image discriminator</li> <li>Multi-scale: Can combine predictions at different resolutions</li> </ul> <p>When to use:</p> <ul> <li>High-resolution images (&gt;256\u00d7256)</li> <li>Image-to-image translation (Pix2Pix)</li> <li>Focus on local texture quality</li> <li>With CycleGAN for better results</li> </ul> <p>Pros: Efficient, good for textures, scales well Cons: May miss global structure issues</p>"},{"location":"user-guide/models/gan-guide/#training-gans","title":"Training GANs","text":""},{"location":"user-guide/models/gan-guide/#basic-training-loop","title":"Basic Training Loop","text":"<p>Here's a complete training loop for a vanilla GAN:</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nfrom artifex.generative_models.models.gan import GAN\n\n# Create model\ngan = GAN(config, rngs=nnx.Rngs(params=0, dropout=1, sample=2))\n\n# Create optimizers (separate for generator and discriminator)\n# wrt=nnx.Param required in NNX 0.11.0+\ngen_optimizer = nnx.Optimizer(\n    gan.generator,\n    optax.adam(learning_rate=0.0002, b1=0.5, b2=0.999),\n    wrt=nnx.Param\n)\n\ndisc_optimizer = nnx.Optimizer(\n    gan.discriminator,\n    optax.adam(learning_rate=0.0002, b1=0.5, b2=0.999),\n    wrt=nnx.Param\n)\n\n# Training step\n@nnx.jit\ndef train_step(gan, gen_opt, disc_opt, batch, rngs):\n    \"\"\"Single training step for vanilla GAN.\"\"\"\n\n    # Discriminator update\n    def disc_loss_fn(disc):\n        # Get generator samples (stop gradient to not update generator)\n        z = jax.random.normal(rngs.sample(), (batch.shape[0], gan.latent_dim))\n        fake_samples = gan.generator(z, training=True)\n        fake_samples = jax.lax.stop_gradient(fake_samples)\n\n        # Discriminator scores\n        real_scores = disc(batch, training=True)\n        fake_scores = disc(fake_samples, training=True)\n\n        # Vanilla GAN discriminator loss\n        real_loss = -jnp.log(jnp.clip(real_scores, 1e-7, 1.0))\n        fake_loss = -jnp.log(jnp.clip(1.0 - fake_scores, 1e-7, 1.0))\n\n        return jnp.mean(real_loss + fake_loss)\n\n    # Compute discriminator loss and update\n    disc_loss, disc_grads = nnx.value_and_grad(disc_loss_fn)(gan.discriminator)\n    disc_opt.update(disc_grads)\n\n    # Generator update\n    def gen_loss_fn(gen):\n        # Generate samples\n        z = jax.random.normal(rngs.sample(), (batch.shape[0], gan.latent_dim))\n        fake_samples = gen(z, training=True)\n\n        # Get discriminator scores (stop gradient on discriminator)\n        disc = jax.lax.stop_gradient(gan.discriminator)\n        fake_scores = disc(fake_samples, training=True)\n\n        # Non-saturating generator loss\n        return -jnp.mean(jnp.log(jnp.clip(fake_scores, 1e-7, 1.0)))\n\n    # Compute generator loss and update\n    gen_loss, gen_grads = nnx.value_and_grad(gen_loss_fn)(gan.generator)\n    gen_opt.update(gen_grads)\n\n    return {\n        \"disc_loss\": disc_loss,\n        \"gen_loss\": gen_loss,\n    }\n\n# Training loop\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Preprocess: scale to [-1, 1] for tanh output\n        batch = (batch / 127.5) - 1.0\n\n        # Training step\n        metrics = train_step(gan, gen_optimizer, disc_optimizer, batch, rngs)\n\n        # Log metrics\n        if step % log_interval == 0:\n            print(f\"Epoch {epoch}, Step {step}\")\n            print(f\"  Discriminator Loss: {metrics['disc_loss']:.4f}\")\n            print(f\"  Generator Loss: {metrics['gen_loss']:.4f}\")\n\n        # Generate samples for visualization\n        if step % sample_interval == 0:\n            samples = gan.generate(n_samples=16, rngs=rngs)\n            save_images(samples, f\"samples_step_{step}.png\")\n</code></pre>"},{"location":"user-guide/models/gan-guide/#wgan-training-loop","title":"WGAN Training Loop","text":"<p>WGAN requires multiple discriminator updates per generator update:</p> <pre><code>@nnx.jit\ndef train_wgan_step(wgan, gen_opt, critic_opt, batch, rngs, n_critic=5):\n    \"\"\"Training step for WGAN-GP.\"\"\"\n\n    # Train critic n_critic times\n    critic_losses = []\n    for i in range(n_critic):\n        def critic_loss_fn(critic):\n            # Generate fake samples\n            z = jax.random.normal(rngs.sample(), (batch.shape[0], wgan.latent_dim))\n            fake_samples = wgan.generator(z, training=True)\n            fake_samples = jax.lax.stop_gradient(fake_samples)\n\n            # Get critic outputs\n            real_validity = critic(batch, training=True)\n            fake_validity = critic(fake_samples, training=True)\n\n            # Wasserstein loss\n            wasserstein_distance = jnp.mean(fake_validity) - jnp.mean(real_validity)\n\n            # Gradient penalty\n            alpha = jax.random.uniform(\n                rngs.sample(),\n                shape=(batch.shape[0], 1, 1, 1),\n                minval=0.0,\n                maxval=1.0\n            )\n            interpolated = alpha * batch + (1 - alpha) * fake_samples\n\n            def critic_interp_fn(x):\n                return jnp.sum(critic(x, training=True))\n\n            gradients = jax.grad(critic_interp_fn)(interpolated)\n            gradients = jnp.reshape(gradients, (batch.shape[0], -1))\n            gradient_norm = jnp.sqrt(jnp.sum(gradients**2, axis=1) + 1e-12)\n            gradient_penalty = jnp.mean((gradient_norm - 1.0) ** 2) * 10.0\n\n            return wasserstein_distance + gradient_penalty\n\n        # Update critic\n        critic_loss, critic_grads = nnx.value_and_grad(critic_loss_fn)(wgan.discriminator)\n        critic_opt.update(critic_grads)\n        critic_losses.append(critic_loss)\n\n    # Train generator once\n    def gen_loss_fn(gen):\n        z = jax.random.normal(rngs.sample(), (batch.shape[0], wgan.latent_dim))\n        fake_samples = gen(z, training=True)\n\n        critic = jax.lax.stop_gradient(wgan.discriminator)\n        fake_validity = critic(fake_samples, training=True)\n\n        # WGAN generator loss: maximize critic output\n        return -jnp.mean(fake_validity)\n\n    gen_loss, gen_grads = nnx.value_and_grad(gen_loss_fn)(wgan.generator)\n    gen_opt.update(gen_grads)\n\n    return {\n        \"critic_loss\": jnp.mean(jnp.array(critic_losses)),\n        \"gen_loss\": gen_loss,\n    }\n</code></pre>"},{"location":"user-guide/models/gan-guide/#two-timescale-update-rule-ttur","title":"Two-Timescale Update Rule (TTUR)","text":"<p>Use different learning rates for generator and discriminator:</p> <pre><code># Generator: slower learning rate (wrt=nnx.Param required in NNX 0.11.0+)\ngen_optimizer = nnx.Optimizer(\n    gan.generator,\n    optax.adam(learning_rate=0.0001, b1=0.5, b2=0.999),  # lr = 0.0001\n    wrt=nnx.Param\n)\n\n# Discriminator: faster learning rate\ndisc_optimizer = nnx.Optimizer(\n    gan.discriminator,\n    optax.adam(learning_rate=0.0004, b1=0.5, b2=0.999),  # lr = 0.0004\n    wrt=nnx.Param\n)\n</code></pre> <p>Why it works:</p> <ul> <li>Discriminator needs to stay ahead to provide useful signal</li> <li>Prevents generator from overwhelming discriminator</li> <li>More stable training dynamics</li> </ul>"},{"location":"user-guide/models/gan-guide/#generation-and-sampling","title":"Generation and Sampling","text":""},{"location":"user-guide/models/gan-guide/#basic-generation","title":"Basic Generation","text":"<pre><code># Generate samples\nn_samples = 64\nsamples = gan.generate(n_samples=n_samples, rngs=rngs)\n\n# Samples are in [-1, 1] range (from Tanh)\n# Convert to [0, 255] for visualization\nsamples = ((samples + 1) / 2 * 255).astype(jnp.uint8)\n</code></pre>"},{"location":"user-guide/models/gan-guide/#latent-space-interpolation","title":"Latent Space Interpolation","text":"<p>Smoothly interpolate between two points in latent space:</p> <pre><code>def interpolate_latent(gan, z1, z2, num_steps=10, rngs=None):\n    \"\"\"Interpolate between two latent vectors.\"\"\"\n    # Create interpolation weights\n    alphas = jnp.linspace(0, 1, num_steps)\n\n    # Interpolate\n    interpolated_samples = []\n    for alpha in alphas:\n        z_interp = alpha * z2 + (1 - alpha) * z1\n        sample = gan.generator(z_interp[None, :], training=False)\n        interpolated_samples.append(sample[0])\n\n    return jnp.stack(interpolated_samples)\n\n# Generate two random latent vectors\nz1 = jax.random.normal(rngs.sample(), (latent_dim,))\nz2 = jax.random.normal(rngs.sample(), (latent_dim,))\n\n# Interpolate\ninterpolated = interpolate_latent(gan, z1, z2, num_steps=20)\n</code></pre>"},{"location":"user-guide/models/gan-guide/#latent-space-exploration","title":"Latent Space Exploration","text":"<p>Explore the latent space by varying dimensions:</p> <pre><code>def explore_latent_dimension(gan, dim_idx, num_samples=10, range_scale=3.0):\n    \"\"\"Explore a specific latent dimension.\"\"\"\n    # Fixed random vector\n    z_base = jax.random.normal(rngs.sample(), (latent_dim,))\n\n    # Vary single dimension\n    values = jnp.linspace(-range_scale, range_scale, num_samples)\n\n    samples = []\n    for value in values:\n        z = z_base.at[dim_idx].set(value)\n        sample = gan.generator(z[None, :], training=False)\n        samples.append(sample[0])\n\n    return jnp.stack(samples)\n\n# Explore dimension 0\nsamples_dim0 = explore_latent_dimension(gan, dim_idx=0, num_samples=10)\n</code></pre>"},{"location":"user-guide/models/gan-guide/#conditional-generation","title":"Conditional Generation","text":"<p>For conditional GANs, specify the condition:</p> <pre><code># Generate specific digits (MNIST)\nlabels = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nz = jax.random.normal(rngs.sample(), (10, latent_dim))\n\nsamples = cond_generator(z, labels, training=False)\n# Each sample corresponds to its label\n</code></pre>"},{"location":"user-guide/models/gan-guide/#evaluation-and-monitoring","title":"Evaluation and Monitoring","text":""},{"location":"user-guide/models/gan-guide/#visual-inspection","title":"Visual Inspection","text":"<p>The most important evaluation method for GANs:</p> <pre><code>import matplotlib.pyplot as plt\n\ndef visualize_samples(samples, nrow=8, title=\"Generated Samples\"):\n    \"\"\"Visualize a grid of samples.\"\"\"\n    n_samples = samples.shape[0]\n    ncol = (n_samples + nrow - 1) // nrow\n\n    # Convert from [-1, 1] to [0, 1]\n    samples = (samples + 1) / 2\n\n    fig, axes = plt.subplots(ncol, nrow, figsize=(nrow * 2, ncol * 2))\n    axes = axes.flatten()\n\n    for i, ax in enumerate(axes):\n        if i &lt; n_samples:\n            # Transpose from (C, H, W) to (H, W, C)\n            img = jnp.transpose(samples[i], (1, 2, 0))\n            # Handle grayscale\n            if img.shape[-1] == 1:\n                img = img[:, :, 0]\n                ax.imshow(img, cmap='gray')\n            else:\n                ax.imshow(img)\n        ax.axis('off')\n\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()\n\n# Generate and visualize\nsamples = gan.generate(n_samples=64, rngs=rngs)\nvisualize_samples(samples)\n</code></pre>"},{"location":"user-guide/models/gan-guide/#loss-monitoring","title":"Loss Monitoring","text":"<p>Track both generator and discriminator losses:</p> <pre><code># During training\nhistory = {\n    \"gen_loss\": [],\n    \"disc_loss\": [],\n    \"real_scores\": [],\n    \"fake_scores\": [],\n}\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        metrics = train_step(gan, gen_opt, disc_opt, batch, rngs)\n\n        history[\"gen_loss\"].append(float(metrics[\"gen_loss\"]))\n        history[\"disc_loss\"].append(float(metrics[\"disc_loss\"]))\n\n# Plot losses\nplt.figure(figsize=(10, 5))\nplt.plot(history[\"gen_loss\"], label=\"Generator Loss\")\nplt.plot(history[\"disc_loss\"], label=\"Discriminator Loss\")\nplt.xlabel(\"Training Step\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"GAN Training Losses\")\nplt.show()\n</code></pre> <p>Healthy training signs:</p> <ul> <li>Both losses decrease initially then stabilize</li> <li>Losses oscillate but don't diverge</li> <li>Real scores stay around 0.7-0.9</li> <li>Fake scores start low, gradually increase</li> <li>Visual quality improves over time</li> </ul> <p>Warning signs:</p> <ul> <li>Discriminator loss \u2192 0 (too strong)</li> <li>Generator loss \u2192 \u221e (gradient vanishing)</li> <li>Mode collapse (all samples look same)</li> <li>Training instability (wild oscillations)</li> </ul>"},{"location":"user-guide/models/gan-guide/#inception-score-is","title":"Inception Score (IS)","text":"<p>Measures quality and diversity:</p> <pre><code>from artifex.generative_models.core.evaluation.metrics.image import InceptionScore\n\n# Create IS metric with a classifier function\nis_metric = InceptionScore(classifier=inception_classifier_fn)\n\n# Compute IS\nresults = is_metric.compute(generated_samples, splits=10)\nprint(f\"Inception Score: {results['is_mean']:.2f} \u00b1 {results['is_std']:.2f}\")\n</code></pre> <p>Higher is better (good models: 8-10 for ImageNet)</p>"},{"location":"user-guide/models/gan-guide/#frechet-inception-distance-fid","title":"Fr\u00e9chet Inception Distance (FID)","text":"<p>Measures similarity to real data:</p> <pre><code>from artifex.generative_models.core.evaluation.metrics.image import FrechetInceptionDistance\n\n# Create FID metric with a feature extractor function\nfid_metric = FrechetInceptionDistance(feature_extractor=feature_extractor_fn)\n\n# Compute FID between real and generated images\nresults = fid_metric.compute(real_images, generated_samples)\nprint(f\"FID Score: {results['fid']:.2f}\")\n</code></pre> <p>Lower is better (good models: &lt; 50, excellent: &lt; 10)</p>"},{"location":"user-guide/models/gan-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/models/gan-guide/#mode-collapse","title":"Mode Collapse","text":"<p>Symptom: Generator produces limited variety of samples.</p> <p>Detection:</p> <pre><code># Check sample diversity\nsamples = gan.generate(n_samples=100, rngs=rngs)\nsamples_flat = samples.reshape(samples.shape[0], -1)\n\n# Compute pairwise distances\nfrom scipy.spatial.distance import pdist\ndistances = pdist(samples_flat)\n\nif jnp.mean(distances) &lt; threshold:\n    print(\"Warning: Possible mode collapse detected!\")\n</code></pre> <p>Solutions:</p> <ol> <li>Use WGAN or LSGAN:</li> </ol> <pre><code>config.loss_type = \"wasserstein\"  # or \"least_squares\"\n</code></pre> <ol> <li>Minibatch discrimination:</li> </ol> <pre><code># Add minibatch statistics to discriminator\ndef minibatch_stddev(x):\n    \"\"\"Compute standard deviation across batch.\"\"\"\n    batch_std = jnp.std(x, axis=0, keepdims=True)\n    return jnp.mean(batch_std)\n</code></pre> <ol> <li>Add noise to discriminator inputs:</li> </ol> <pre><code># Gradually decay noise\nnoise_std = 0.1 * (1 - epoch / num_epochs)\nnoisy_real = real_data + jax.random.normal(key, real_data.shape) * noise_std\nnoisy_fake = fake_data + jax.random.normal(key, fake_data.shape) * noise_std\n</code></pre> <ol> <li>Use feature matching:</li> </ol> <pre><code># Match discriminator feature statistics\ndef feature_matching_loss(real_features, fake_features):\n    return jnp.mean((jnp.mean(real_features, axis=0) -\n                     jnp.mean(fake_features, axis=0)) ** 2)\n</code></pre>"},{"location":"user-guide/models/gan-guide/#training-instability","title":"Training Instability","text":"<p>Symptom: Losses oscillate wildly, training doesn't converge.</p> <p>Solutions:</p> <ol> <li>Use spectral normalization:</li> </ol> <pre><code>discriminator = Discriminator(\n    hidden_dims=[512, 256, 128],\n    use_spectral_norm=True,  # Enable spectral norm\n    rngs=rngs,\n)\n</code></pre> <ol> <li>Two-timescale update rule:</li> </ol> <pre><code># Different learning rates\ngen_lr = 0.0001\ndisc_lr = 0.0004\n</code></pre> <ol> <li>Gradient penalty (WGAN-GP):</li> </ol> <pre><code># Use WGAN with gradient penalty\nwgan_config.gradient_penalty_weight = 10.0\n</code></pre> <ol> <li>Label smoothing:</li> </ol> <pre><code># Smooth labels for discriminator\nreal_labels = jnp.ones((batch_size, 1)) * 0.9  # Instead of 1.0\nfake_labels = jnp.zeros((batch_size, 1)) + 0.1  # Instead of 0.0\n</code></pre>"},{"location":"user-guide/models/gan-guide/#vanishing-gradients","title":"Vanishing Gradients","text":"<p>Symptom: Generator loss stops decreasing, samples don't improve.</p> <p>Solutions:</p> <ol> <li>Use non-saturating loss:</li> </ol> <pre><code># Instead of: -log(1 - D(G(z)))\n# Use: -log(D(G(z)))\ngen_loss = -jnp.mean(jnp.log(jnp.clip(fake_scores, 1e-7, 1.0)))\n</code></pre> <ol> <li>Reduce discriminator capacity:</li> </ol> <pre><code># Make discriminator weaker\nconfig.discriminator.hidden_dims = [256, 128]  # Smaller than [512, 256]\n</code></pre> <ol> <li>Update discriminator less frequently:</li> </ol> <pre><code># Update discriminator every 2 generator updates\nif step % 2 == 0:\n    disc_loss = train_discriminator(...)\ngen_loss = train_generator(...)\n</code></pre>"},{"location":"user-guide/models/gan-guide/#poor-sample-quality","title":"Poor Sample Quality","text":"<p>Symptom: Blurry or unrealistic samples.</p> <p>Solutions:</p> <ol> <li>Use DCGAN architecture:</li> </ol> <pre><code># Replace MLP with convolutional architecture\nfrom artifex.generative_models.models.gan import DCGAN\ngan = DCGAN(config, rngs=rngs)\n</code></pre> <ol> <li>Increase model capacity:</li> </ol> <pre><code>config.generator.hidden_dims = [512, 1024, 2048]  # Larger\n</code></pre> <ol> <li>Train longer:</li> </ol> <pre><code>num_epochs = 200  # GANs need many epochs\n</code></pre> <ol> <li>Better data preprocessing:</li> </ol> <pre><code># Normalize to [-1, 1] for Tanh\ndata = (data / 127.5) - 1.0\n\n# Ensure consistent shape\ndata = jnp.transpose(data, (0, 3, 1, 2))  # NHWC \u2192 NCHW\n</code></pre>"},{"location":"user-guide/models/gan-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/models/gan-guide/#do","title":"DO","text":"<p>\u2705 Use DCGAN guidelines for image generation:</p> <pre><code># Strided convolutions, batch norm, LeakyReLU\ngenerator = DCGANGenerator(...)\ndiscriminator = DCGANDiscriminator(...)\n</code></pre> <p>\u2705 Scale data to [-1, 1] for Tanh output:</p> <pre><code>data = (data / 127.5) - 1.0\n</code></pre> <p>\u2705 Use Adam optimizer with \u03b2\u2081=0.5:</p> <pre><code>optimizer = nnx.adam(learning_rate=0.0002, b1=0.5, b2=0.999)\n</code></pre> <p>\u2705 Monitor both losses and samples:</p> <pre><code>if step % 100 == 0:\n    visualize_samples(gan.generate(16, rngs=rngs))\n</code></pre> <p>\u2705 Use two-timescale updates (TTUR):</p> <pre><code>gen_lr = 0.0001\ndisc_lr = 0.0004\n</code></pre> <p>\u2705 Start with WGAN for stable training:</p> <pre><code>config.loss_type = \"wasserstein\"\n</code></pre> <p>\u2705 Save checkpoints regularly:</p> <pre><code>if epoch % 10 == 0:\n    nnx.save_checkpoint(gan, f\"checkpoints/gan_epoch_{epoch}\")\n</code></pre>"},{"location":"user-guide/models/gan-guide/#dont","title":"DON'T","text":"<p>\u274c Don't use batch norm in discriminator input:</p> <pre><code># BAD\ndiscriminator.layers[0] = BatchNorm(...)\n\n# GOOD\ndiscriminator.batch_norm = False  # Or skip first layer\n</code></pre> <p>\u274c Don't use same learning rate for G and D:</p> <pre><code># BAD\ngen_lr = disc_lr = 0.0002\n\n# GOOD\ngen_lr = 0.0001\ndisc_lr = 0.0004  # Discriminator learns faster\n</code></pre> <p>\u274c Don't forget to scale data:</p> <pre><code># BAD\ndata = data / 255.0  # [0, 1] doesn't match Tanh [-1, 1]\n\n# GOOD\ndata = (data / 127.5) - 1.0  # [-1, 1] matches Tanh\n</code></pre> <p>\u274c Don't ignore mode collapse warnings:</p> <pre><code># Check diversity regularly\nif jnp.std(samples) &lt; 0.1:\n    print(\"Warning: Possible mode collapse!\")\n</code></pre> <p>\u274c Don't use too small batch sizes:</p> <pre><code># BAD\nbatch_size = 8  # Too small, unstable\n\n# GOOD\nbatch_size = 64  # Better stability\n</code></pre>"},{"location":"user-guide/models/gan-guide/#summary","title":"Summary","text":"<p>This guide covered:</p> <ul> <li>Creating GANs: Generators, discriminators, and full GAN models</li> <li>Variants: Vanilla, DCGAN, WGAN, LSGAN, cGAN, CycleGAN, PatchGAN</li> <li>Training: Basic loops, WGAN training, two-timescale updates</li> <li>Generation: Basic sampling, interpolation, conditional generation</li> <li>Evaluation: Visual inspection, IS, FID</li> <li>Troubleshooting: Mode collapse, instability, vanishing gradients</li> <li>Best practices: What to do and what to avoid</li> </ul>"},{"location":"user-guide/models/gan-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Theory: See GAN Concepts for mathematical foundations</li> <li>API Reference: Check GAN API Documentation for detailed specifications</li> <li>Example: Follow MNIST GAN Tutorial for hands-on training</li> <li>Advanced: Explore StyleGAN and Progressive GAN for state-of-the-art results</li> </ul>"},{"location":"user-guide/models/vae-guide/","title":"VAE User Guide","text":"<p>Complete guide to building, training, and using Variational Autoencoders with Artifex.</p>"},{"location":"user-guide/models/vae-guide/#overview","title":"Overview","text":"<p>This guide covers practical usage of VAEs in Artifex, from basic setup to advanced techniques. You'll learn how to:</p> <ul> <li> <p> Configure VAEs</p> <p>Set up encoder/decoder architectures and configure hyperparameters</p> </li> <li> <p> Train Models</p> <p>Train VAEs with proper loss functions and monitoring</p> </li> <li> <p> Generate Samples</p> <p>Sample from the prior and manipulate latent representations</p> </li> <li> <p> Tune &amp; Debug</p> <p>Optimize hyperparameters and troubleshoot common issues</p> </li> </ul>"},{"location":"user-guide/models/vae-guide/#quick-start","title":"Quick Start","text":""},{"location":"user-guide/models/vae-guide/#basic-vae-example","title":"Basic VAE Example","text":"<pre><code>import jax.numpy as jnp\nfrom flax import nnx\nfrom artifex.generative_models.core.configuration.network_configs import (\n    EncoderConfig,\n    DecoderConfig,\n)\nfrom artifex.generative_models.core.configuration.vae_config import VAEConfig\nfrom artifex.generative_models.models.vae import VAE\n\n# Initialize RNGs\nrngs = nnx.Rngs(params=0, dropout=1, sample=2)\n\n# Configuration\nlatent_dim = 20\n\n# Create encoder config\nencoder_config = EncoderConfig(\n    name=\"mlp_encoder\",\n    hidden_dims=(256, 128),\n    latent_dim=latent_dim,\n    activation=\"relu\",\n    input_shape=(28, 28, 1),  # Image shape\n)\n\n# Create decoder config\ndecoder_config = DecoderConfig(\n    name=\"mlp_decoder\",\n    hidden_dims=(128, 256),\n    output_shape=(28, 28, 1),\n    latent_dim=latent_dim,\n    activation=\"relu\",\n)\n\n# Create VAE config\nvae_config = VAEConfig(\n    name=\"basic_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",  # Use MLP encoder/decoder\n    kl_weight=1.0,\n)\n\n# Create model\nvae = VAE(config=vae_config, rngs=rngs)\n\n# Forward pass\nx = jnp.ones((32, 28, 28, 1))\noutputs = vae(x)  # Model uses internal RNGs\n\n# Get outputs\nreconstructed = outputs[\"reconstructed\"]\nmean = outputs[\"mean\"]\nlog_var = outputs[\"log_var\"]\nlatent = outputs[\"z\"]\n\nprint(f\"Reconstruction shape: {reconstructed.shape}\")\nprint(f\"Latent shape: {latent.shape}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#creating-vae-models","title":"Creating VAE Models","text":"<p>Artifex uses a config-based API where you define configurations first, then create models from them. This provides type safety, validation, and easy serialization.</p>"},{"location":"user-guide/models/vae-guide/#1-encoder-configurations","title":"1. Encoder Configurations","text":""},{"location":"user-guide/models/vae-guide/#mlp-encoder-fully-connected","title":"MLP Encoder (Fully-Connected)","text":"<p>Best for tabular data and flattened images:</p> <pre><code>from artifex.generative_models.core.configuration.network_configs import EncoderConfig\nfrom artifex.generative_models.models.vae.encoders import MLPEncoder\n\n# Define encoder configuration\nencoder_config = EncoderConfig(\n    name=\"mlp_encoder\",\n    hidden_dims=(512, 256, 128),  # Network depth (use tuples)\n    latent_dim=32,                 # Latent space dimension\n    activation=\"relu\",             # Activation function\n    input_shape=(784,),            # Flattened input size\n)\n\n# Create encoder from config\nencoder = MLPEncoder(config=encoder_config, rngs=rngs)\n\n# Forward pass returns (mean, log_var)\nmean, log_var = encoder(x)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#cnn-encoder-convolutional","title":"CNN Encoder (Convolutional)","text":"<p>Best for image data with spatial structure:</p> <pre><code>from artifex.generative_models.core.configuration.network_configs import EncoderConfig\nfrom artifex.generative_models.models.vae.encoders import CNNEncoder\n\nencoder_config = EncoderConfig(\n    name=\"cnn_encoder\",\n    hidden_dims=(32, 64, 128, 256),  # Channel progression\n    latent_dim=64,\n    activation=\"relu\",\n    input_shape=(28, 28, 1),          # (H, W, C)\n)\n\nencoder = CNNEncoder(config=encoder_config, rngs=rngs)\n\n# Preserves spatial information through convolutions\nmean, log_var = encoder(x)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#conditional-encoder","title":"Conditional Encoder","text":"<p>Add class conditioning using <code>ConditionalVAEConfig</code>:</p> <pre><code>from artifex.generative_models.core.configuration.network_configs import EncoderConfig\nfrom artifex.generative_models.core.configuration.vae_config import ConditionalVAEConfig\nfrom artifex.generative_models.models.vae import ConditionalVAE\n\n# ConditionalVAE handles label embedding and conditioning automatically.\n# The encoder and decoder are created internally with conditional=True,\n# so you only need to provide the base encoder/decoder configs:\ncvae_config = ConditionalVAEConfig(\n    name=\"conditional_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    num_classes=10,\n    kl_weight=1.0,\n)\ncvae = ConditionalVAE(config=cvae_config, rngs=rngs)\n\n# Integer labels are automatically one-hot encoded:\nlabels = jnp.array([0, 1, 2, 3])  # No need for manual one_hot\noutputs = cvae(x, y=labels)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#2-decoder-configurations","title":"2. Decoder Configurations","text":""},{"location":"user-guide/models/vae-guide/#mlp-decoder","title":"MLP Decoder","text":"<pre><code>from artifex.generative_models.core.configuration.network_configs import DecoderConfig\nfrom artifex.generative_models.models.vae.decoders import MLPDecoder\n\ndecoder_config = DecoderConfig(\n    name=\"mlp_decoder\",\n    hidden_dims=(128, 256, 512),  # Reversed from encoder\n    output_shape=(784,),          # Reconstruction size\n    latent_dim=32,\n    activation=\"relu\",\n)\n\ndecoder = MLPDecoder(config=decoder_config, rngs=rngs)\n\nreconstructed = decoder(z)  # Returns JAX array\n</code></pre>"},{"location":"user-guide/models/vae-guide/#cnn-decoder-transposed-convolutions","title":"CNN Decoder (Transposed Convolutions)","text":"<pre><code>from artifex.generative_models.core.configuration.network_configs import DecoderConfig\nfrom artifex.generative_models.models.vae.decoders import CNNDecoder\n\ndecoder_config = DecoderConfig(\n    name=\"cnn_decoder\",\n    hidden_dims=(256, 128, 64, 32),  # Reversed channel progression\n    output_shape=(28, 28, 1),         # Output image shape\n    latent_dim=64,\n    activation=\"relu\",\n)\n\ndecoder = CNNDecoder(config=decoder_config, rngs=rngs)\n\nreconstructed = decoder(z)  # Returns (batch, 28, 28, 1)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#conditional-decoder","title":"Conditional Decoder","text":"<p>For conditional generation, use the full <code>ConditionalVAE</code> model which handles conditioning in both encoder and decoder:</p> <pre><code># See \"Conditional VAE\" section below for the full config-based approach\n</code></pre>"},{"location":"user-guide/models/vae-guide/#3-complete-vae-models","title":"3. Complete VAE Models","text":""},{"location":"user-guide/models/vae-guide/#standard-vae","title":"Standard VAE","text":"<pre><code>from artifex.generative_models.core.configuration.network_configs import (\n    EncoderConfig,\n    DecoderConfig,\n)\nfrom artifex.generative_models.core.configuration.vae_config import VAEConfig\nfrom artifex.generative_models.models.vae import VAE\n\n# Define configurations\nencoder_config = EncoderConfig(\n    name=\"encoder\",\n    hidden_dims=(256, 128),\n    latent_dim=32,\n    activation=\"relu\",\n    input_shape=(28, 28, 1),\n)\n\ndecoder_config = DecoderConfig(\n    name=\"decoder\",\n    hidden_dims=(128, 256),\n    output_shape=(28, 28, 1),\n    latent_dim=32,\n    activation=\"relu\",\n)\n\nvae_config = VAEConfig(\n    name=\"standard_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",     # \"dense\" for MLP, \"cnn\" for convolutional\n    kl_weight=1.0,            # Beta parameter (1.0 = standard VAE)\n)\n\n# Create model\nvae = VAE(config=vae_config, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#-vae-disentangled-representations","title":"\u03b2-VAE (Disentangled Representations)","text":"<pre><code>from artifex.generative_models.core.configuration.vae_config import BetaVAEConfig\nfrom artifex.generative_models.models.vae import BetaVAE\n\nbeta_config = BetaVAEConfig(\n    name=\"beta_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    beta_default=4.0,                    # Higher beta = more disentanglement\n    beta_warmup_steps=10000,             # Gradual beta annealing\n    reconstruction_loss_type=\"mse\",      # \"mse\" or \"bce\"\n)\n\nbeta_vae = BetaVAE(config=beta_config, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#conditional-vae","title":"Conditional VAE","text":"<pre><code>from artifex.generative_models.core.configuration.vae_config import ConditionalVAEConfig\nfrom artifex.generative_models.models.vae import ConditionalVAE\n\ncvae_config = ConditionalVAEConfig(\n    name=\"conditional_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    num_classes=10,                      # Number of classes for conditioning\n    kl_weight=1.0,\n)\n\ncvae = ConditionalVAE(config=cvae_config, rngs=rngs)\n\n# Forward pass with condition (one-hot encoded labels)\nlabels = jax.nn.one_hot(jnp.array([0, 1, 2]), num_classes=10)\noutputs = cvae(x, y=labels)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#vq-vae-discrete-latents","title":"VQ-VAE (Discrete Latents)","text":"<pre><code>from artifex.generative_models.core.configuration.vae_config import VQVAEConfig\nfrom artifex.generative_models.models.vae import VQVAE\n\nvqvae_config = VQVAEConfig(\n    name=\"vqvae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    num_embeddings=512,                  # Codebook size\n    embedding_dim=64,                    # Embedding dimension\n    commitment_cost=0.25,                # Commitment loss weight\n)\n\nvqvae = VQVAE(config=vqvae_config, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#training-vaes","title":"Training VAEs","text":""},{"location":"user-guide/models/vae-guide/#basic-training-loop","title":"Basic Training Loop","text":"<pre><code>import jax\nimport jax.numpy as jnp\nimport optax\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration.network_configs import (\n    EncoderConfig,\n    DecoderConfig,\n)\nfrom artifex.generative_models.core.configuration.vae_config import VAEConfig\nfrom artifex.generative_models.models.vae import VAE\n\n# 1. Create synthetic data (replace with real data loading)\nkey = jax.random.key(42)\ntrain_data = jax.random.uniform(key, (1000, 28, 28, 1))\n\n# 2. Create model configuration\nrngs = nnx.Rngs(params=0, dropout=1, sample=2)\n\nencoder_config = EncoderConfig(\n    name=\"encoder\",\n    hidden_dims=(256, 128),\n    latent_dim=32,\n    activation=\"relu\",\n    input_shape=(28, 28, 1),\n)\n\ndecoder_config = DecoderConfig(\n    name=\"decoder\",\n    hidden_dims=(128, 256),\n    output_shape=(28, 28, 1),\n    latent_dim=32,\n    activation=\"relu\",\n)\n\nvae_config = VAEConfig(\n    name=\"mnist_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    kl_weight=1.0,\n)\n\n# 3. Initialize model and optimizer\nvae = VAE(config=vae_config, rngs=rngs)\noptimizer = nnx.Optimizer(vae, optax.adam(learning_rate=1e-3), wrt=nnx.Param)\n\n# 4. Training step (JIT-compiled for speed)\n@nnx.jit\ndef train_step(model, optimizer, batch):\n    def loss_fn(model):\n        outputs = model(batch)  # Model uses internal RNGs\n        losses = model.loss_fn(x=batch, outputs=outputs)\n        return losses[\"loss\"], losses\n\n    (loss, losses), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n    optimizer.update(model, grads)\n\n    return losses\n\n# 5. Training loop\nbatch_size = 32\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    # Simple batching\n    for i in range(0, len(train_data), batch_size):\n        batch = train_data[i : i + batch_size]\n        losses = train_step(vae, optimizer, batch)\n\n    print(f\"Epoch {epoch + 1} | Loss: {losses['loss']:.4f}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#training-vae-with-annealing","title":"Training \u03b2-VAE with Annealing","text":"<pre><code>from artifex.generative_models.core.configuration.vae_config import BetaVAEConfig\nfrom artifex.generative_models.models.vae import BetaVAE\n\nbeta_config = BetaVAEConfig(\n    name=\"beta_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    beta_default=4.0,\n    beta_warmup_steps=10000,\n    reconstruction_loss_type=\"mse\",\n)\n\nbeta_vae = BetaVAE(config=beta_config, rngs=rngs)\noptimizer = nnx.Optimizer(beta_vae, optax.adam(learning_rate=1e-3), wrt=nnx.Param)\n\nstep = 0\nfor epoch in range(num_epochs):\n    for i in range(0, len(train_data), batch_size):\n        batch = train_data[i : i + batch_size]\n\n        def loss_fn(model):\n            outputs = model(batch)\n            # Pass current step for beta annealing\n            losses = model.loss_fn(x=batch, outputs=outputs, step=step)\n            return losses[\"loss\"], losses\n\n        (loss, losses), grads = nnx.value_and_grad(loss_fn, has_aux=True)(beta_vae)\n        optimizer.update(beta_vae, grads)\n        step += 1\n\n    print(f\"Epoch {epoch + 1}, Beta: {losses.get('beta', 1.0):.4f}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#training-conditional-vae","title":"Training Conditional VAE","text":"<pre><code>from artifex.generative_models.core.configuration.vae_config import ConditionalVAEConfig\nfrom artifex.generative_models.models.vae import ConditionalVAE\n\ncvae_config = ConditionalVAEConfig(\n    name=\"conditional_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    num_classes=10,\n    kl_weight=1.0,\n)\n\ncvae = ConditionalVAE(config=cvae_config, rngs=rngs)\noptimizer = nnx.Optimizer(cvae, optax.adam(learning_rate=1e-3), wrt=nnx.Param)\n\n# Create synthetic labels (replace with real labels)\ntrain_labels = jax.random.randint(jax.random.key(0), (1000,), 0, 10)\ntrain_labels_onehot = jax.nn.one_hot(train_labels, num_classes=10)\n\nfor epoch in range(num_epochs):\n    for i in range(0, len(train_data), batch_size):\n        batch_x = train_data[i : i + batch_size]\n        batch_y = train_labels_onehot[i : i + batch_size]\n\n        def loss_fn(model):\n            outputs = model(batch_x, y=batch_y)  # Condition on labels\n            losses = model.loss_fn(x=batch_x, outputs=outputs)\n            return losses[\"loss\"], losses\n\n        (loss, losses), grads = nnx.value_and_grad(loss_fn, has_aux=True)(cvae)\n        optimizer.update(cvae, grads)\n\n    print(f\"Epoch {epoch + 1} | Loss: {losses['loss']:.4f}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#training-vq-vae","title":"Training VQ-VAE","text":"<pre><code>from artifex.generative_models.core.configuration.vae_config import VQVAEConfig\nfrom artifex.generative_models.models.vae import VQVAE\n\nvqvae_config = VQVAEConfig(\n    name=\"vqvae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    num_embeddings=512,\n    embedding_dim=64,\n    commitment_cost=0.25,\n)\n\nvqvae = VQVAE(config=vqvae_config, rngs=rngs)\noptimizer = nnx.Optimizer(vqvae, optax.adam(learning_rate=1e-3), wrt=nnx.Param)\n\nfor epoch in range(num_epochs):\n    for i in range(0, len(train_data), batch_size):\n        batch = train_data[i : i + batch_size]\n\n        def loss_fn(model):\n            outputs = model(batch)\n            losses = model.loss_fn(x=batch, outputs=outputs)\n            return losses[\"loss\"], losses\n\n        (loss, losses), grads = nnx.value_and_grad(loss_fn, has_aux=True)(vqvae)\n        optimizer.update(vqvae, grads)\n\n    # VQ-VAE specific metrics\n    print(f\"Epoch {epoch + 1} | Recon: {losses.get('reconstruction_loss', 0.0):.4f}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#generating-and-sampling","title":"Generating and Sampling","text":""},{"location":"user-guide/models/vae-guide/#generate-new-samples","title":"Generate New Samples","text":"<pre><code># Sample from prior distribution\nn_samples = 16\nsamples = vae.sample(n_samples, temperature=1.0)\n\n# Temperature controls diversity\nhot_samples = vae.sample(n_samples, temperature=2.0)   # More diverse\ncold_samples = vae.sample(n_samples, temperature=0.5)  # More focused\n\n# Using generate() method (alias for sample)\nsamples = vae.generate(n_samples, temperature=1.0)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#conditional-generation","title":"Conditional Generation","text":"<pre><code># Generate samples for specific classes\ntarget_classes = jnp.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  # One of each digit\nlabels = jax.nn.one_hot(target_classes, num_classes=10)\n\nsamples = cvae.sample(n_samples=10, y=labels, temperature=1.0)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#reconstruction","title":"Reconstruction","text":"<pre><code># Stochastic reconstruction (uses internal RNGs)\nreconstructed = vae.reconstruct(x, deterministic=False)\n\n# Deterministic reconstruction (use mean of latent distribution)\ndeterministic_recon = vae.reconstruct(x, deterministic=True)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#latent-space-manipulation","title":"Latent Space Manipulation","text":""},{"location":"user-guide/models/vae-guide/#interpolation-between-images","title":"Interpolation Between Images","text":"<pre><code># Linear interpolation in latent space\nx1 = test_images[0:1]  # First image (keep batch dim)\nx2 = test_images[1:2]  # Second image\n\ninterpolated = vae.interpolate(\n    x1=x1,\n    x2=x2,\n    steps=10,  # Number of interpolation steps\n)\n\n# interpolated.shape = (10, *input_shape)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#latent-traversal-disentanglement-analysis","title":"Latent Traversal (Disentanglement Analysis)","text":"<pre><code># Traverse a single latent dimension\nx = test_images[0:1]\ndim_to_traverse = 3  # Which latent dimension to vary\n\ntraversal = vae.latent_traversal(\n    x=x,\n    dim=dim_to_traverse,\n    range_vals=(-3.0, 3.0),  # Range of values\n    steps=10,                 # Number of steps\n)\n\n# traversal.shape = (10, *input_shape)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#manual-latent-manipulation","title":"Manual Latent Manipulation","text":"<pre><code># Encode image to latent space\nmean, log_var = vae.encode(x)\n\n# Manipulate specific dimensions\nmodified_mean = mean.at[:, 5].set(2.0)    # Increase dimension 5\nmodified_mean = modified_mean.at[:, 10].set(-1.5)  # Decrease dimension 10\n\n# Decode modified latent\nmodified_image = vae.decode(modified_mean)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#evaluation-and-analysis","title":"Evaluation and Analysis","text":""},{"location":"user-guide/models/vae-guide/#reconstruction-quality","title":"Reconstruction Quality","text":"<pre><code># Calculate reconstruction error\ntest_batch = test_images[:100]\nreconstructed = vae.reconstruct(test_batch, deterministic=True)\n\nmse = jnp.mean((test_batch - reconstructed) ** 2)\nprint(f\"Reconstruction MSE: {mse:.4f}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#elbo-evidence-lower-bound","title":"ELBO (Evidence Lower Bound)","text":"<pre><code># Full ELBO calculation\noutputs = vae(test_batch)  # Model uses internal RNGs\nlosses = vae.loss_fn(x=test_batch, outputs=outputs)\n\nelbo = -(losses['reconstruction_loss'] + losses['kl_loss'])\nprint(f\"ELBO: {elbo:.4f}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#latent-space-statistics","title":"Latent Space Statistics","text":"<pre><code># Encode test set\nall_means = []\nall_logvars = []\n\nbatch_size = 32\nfor i in range(0, len(test_images), batch_size):\n    batch = test_images[i : i + batch_size]\n    mean, log_var = vae.encode(batch)  # Uses internal RNGs\n    all_means.append(mean)\n    all_logvars.append(log_var)\n\nall_means = jnp.concatenate(all_means, axis=0)\nall_logvars = jnp.concatenate(all_logvars, axis=0)\n\n# Statistics per dimension\nmean_per_dim = jnp.mean(all_means, axis=0)\nstd_per_dim = jnp.std(all_means, axis=0)\nvariance_per_dim = jnp.exp(jnp.mean(all_logvars, axis=0))\n\nprint(f\"Latent mean: {mean_per_dim}\")\nprint(f\"Latent std: {std_per_dim}\")\nprint(f\"Average variance: {variance_per_dim}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#disentanglement-metrics","title":"Disentanglement Metrics","text":"<pre><code># Per-dimension KL divergence (detect posterior collapse)\ndef per_dim_kl(mean, log_var):\n    \"\"\"Calculate KL divergence per dimension.\"\"\"\n    kl_per_dim = -0.5 * (1 + log_var - mean**2 - jnp.exp(log_var))\n    return jnp.mean(kl_per_dim, axis=0)\n\nkl_per_dimension = per_dim_kl(all_means, all_logvars)\n\n# Dimensions with very low KL likely collapsed\ninactive_dims = jnp.sum(kl_per_dimension &lt; 0.01)\nprint(f\"Inactive dimensions: {inactive_dims}/{vae.latent_dim}\")\n</code></pre>"},{"location":"user-guide/models/vae-guide/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"user-guide/models/vae-guide/#key-hyperparameters","title":"Key Hyperparameters","text":"<pre><code># Architecture\nconfig = {\n    # Network architecture\n    \"latent_dim\": 64,              # 10-100 for images, 2-20 for simple data\n    \"hidden_dims\": [512, 256, 128], # Deeper for complex data\n    \"activation\": \"relu\",          # or \"gelu\", \"swish\"\n\n    # Training\n    \"learning_rate\": 1e-3,         # 1e-4 to 1e-3 typical\n    \"batch_size\": 128,             # Larger is more stable\n    \"num_epochs\": 100,\n\n    # VAE-specific\n    \"kl_weight\": 1.0,              # Beta parameter\n    \"reconstruction_loss\": \"mse\",  # \"mse\" or \"bce\"\n}\n</code></pre>"},{"location":"user-guide/models/vae-guide/#beta-tuning-for-vae","title":"Beta Tuning for \u03b2-VAE","text":"<pre><code>from artifex.generative_models.core.configuration.vae_config import BetaVAEConfig\nfrom artifex.generative_models.models.vae import BetaVAE\n\n# Grid search over beta values\nbeta_values = [0.5, 1.0, 2.0, 4.0, 8.0]\nresults = {}\n\nfor beta in beta_values:\n    # Create config with different beta\n    beta_config = BetaVAEConfig(\n        name=f\"beta_vae_{beta}\",\n        encoder=encoder_config,\n        decoder=decoder_config,\n        encoder_type=\"dense\",\n        beta_default=beta,\n    )\n\n    rngs = nnx.Rngs(params=0, dropout=1, sample=2)\n    model = BetaVAE(config=beta_config, rngs=rngs)\n\n    # Train and evaluate (implement your train/evaluate functions)\n    # trained_model = train(model, train_data, num_epochs=50)\n    # recon_error = evaluate_reconstruction(trained_model, test_data)\n\n    results[beta] = {\"beta\": beta}\n\n# Find best trade-off\nprint(results)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<pre><code>import optax\n\n# Cosine decay schedule\nschedule = optax.cosine_decay_schedule(\n    init_value=1e-3,\n    decay_steps=num_train_steps,\n    alpha=0.1,  # Final learning rate = 0.1 * init_value\n)\n\noptimizer = nnx.Optimizer(vae, optax.adam(learning_rate=schedule), wrt=nnx.Param)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"user-guide/models/vae-guide/#problem-1-posterior-collapse","title":"Problem 1: Posterior Collapse","text":"<p>Symptoms: KL divergence near zero, poor generation quality</p> <p>Solutions:</p> <pre><code>from artifex.generative_models.core.configuration.vae_config import BetaVAEConfig\nfrom artifex.generative_models.models.vae import BetaVAE\n\n# Solution 1: Beta annealing - start with \u03b2=0, gradually increase\nbeta_config = BetaVAEConfig(\n    name=\"beta_annealing_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    beta_default=1.0,\n    beta_warmup_steps=10000,  # Gradual warmup\n)\nbeta_vae = BetaVAE(config=beta_config, rngs=rngs)\n\n# Solution 2: Weaker decoder (make it harder to ignore latent)\n# Use smaller hidden_dims in decoder than encoder\nweak_decoder_config = DecoderConfig(\n    name=\"weak_decoder\",\n    hidden_dims=(64, 128),  # Smaller than encoder\n    output_shape=(28, 28, 1),\n    latent_dim=32,\n    activation=\"relu\",\n)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#problem-2-blurry-reconstructions","title":"Problem 2: Blurry Reconstructions","text":"<p>Symptoms: Overly smooth outputs, lack of detail</p> <p>Solutions:</p> <pre><code># Solution 1: Lower kl_weight (emphasize reconstruction)\nvae_config = VAEConfig(\n    name=\"sharp_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    kl_weight=0.5,  # Lower than default 1.0\n)\nvae = VAE(config=vae_config, rngs=rngs)\n\n# Solution 2: Use VQ-VAE (discrete latents often produce sharper outputs)\nfrom artifex.generative_models.core.configuration.vae_config import VQVAEConfig\nfrom artifex.generative_models.models.vae import VQVAE\n\nvqvae_config = VQVAEConfig(\n    name=\"vqvae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    num_embeddings=512,\n    embedding_dim=64,\n)\nvqvae = VQVAE(config=vqvae_config, rngs=rngs)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#problem-3-unstable-training","title":"Problem 3: Unstable Training","text":"<p>Symptoms: Loss oscillations, NaN values</p> <p>Solutions:</p> <pre><code># Solution 1: Gradient clipping\nimport optax\n\noptimizer = nnx.Optimizer(\n    vae,\n    optax.chain(\n        optax.clip_by_global_norm(1.0),  # Clip gradients\n        optax.adam(learning_rate=1e-3),\n    ),\n    wrt=nnx.Param\n)\n\n# Solution 2: Lower learning rate\noptimizer = nnx.Optimizer(vae, optax.adam(learning_rate=1e-4), wrt=nnx.Param)\n\n# Solution 3: Batch normalization in encoder/decoder\n# (implement custom encoder/decoder with normalization)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#problem-4-poor-disentanglement","title":"Problem 4: Poor Disentanglement","text":"<p>Symptoms: Latent dimensions don't correspond to interpretable factors</p> <p>Solutions:</p> <pre><code># Solution 1: Increase beta for more disentanglement\nfrom artifex.generative_models.core.configuration.vae_config import BetaVAEConfig\nfrom artifex.generative_models.models.vae import BetaVAE\n\nbeta_config = BetaVAEConfig(\n    name=\"high_beta_vae\",\n    encoder=encoder_config,\n    decoder=decoder_config,\n    encoder_type=\"dense\",\n    beta_default=4.0,  # Higher beta encourages disentanglement\n)\nbeta_vae = BetaVAE(config=beta_config, rngs=rngs)\n\n# Solution 2: More latent dimensions - give model more capacity\n# Update encoder_config with larger latent_dim\nencoder_config_large = EncoderConfig(\n    name=\"encoder_large_latent\",\n    hidden_dims=(256, 128),\n    latent_dim=128,  # Increased from 32\n    activation=\"relu\",\n    input_shape=(28, 28, 1),\n)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"user-guide/models/vae-guide/#custom-loss-functions","title":"Custom Loss Functions","text":"<pre><code>def custom_loss_fn(predictions, targets):\n    \"\"\"Custom reconstruction loss combining multiple terms.\n\n    Note: follows JAX/Optax convention \u2014 (predictions, targets) order.\n    \"\"\"\n    # L1 loss for sparsity\n    l1_loss = jnp.mean(jnp.abs(predictions - targets))\n\n    # L2 loss for overall quality\n    l2_loss = jnp.mean((predictions - targets) ** 2)\n\n    # Combine\n    return 0.5 * l1_loss + 0.5 * l2_loss\n\n# Use in training\nlosses = vae.loss_fn(\n    x=batch,\n    outputs=outputs,\n    reconstruction_loss_fn=custom_loss_fn,\n)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#multi-gpu-training","title":"Multi-GPU Training","text":"<pre><code>import jax\nfrom jax import devices\n\n# Check available devices\nprint(f\"Available devices: {jax.devices()}\")\n\n# For multi-GPU training, use JAX's sharding API\n# with Artifex's distributed training utilities\nfrom artifex.generative_models.training.distributed import (\n    DataParallel,\n    DeviceMeshManager,\n)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#checkpointing","title":"Checkpointing","text":"<pre><code>import orbax.checkpoint as ocp\n\n# Create checkpointer\ncheckpointer = ocp.StandardCheckpointer()\n\n# Save model state\ngraphdef, state = nnx.split(vae)\ncheckpointer.save(\"/tmp/vae_checkpoint\", state)\n\n# Load model state\nrestored_state = checkpointer.restore(\"/tmp/vae_checkpoint\")\n\n# Create new model and merge state\nnew_vae = VAE(config=vae_config, rngs=nnx.Rngs(0))\n_, new_state = nnx.split(new_vae)\n# Merge restored state into new model\nnnx.update(new_vae, restored_state)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/models/vae-guide/#do","title":"DO \u2705","text":"<ul> <li>Start simple: Begin with standard VAE before trying variants</li> <li>Monitor both losses: Track reconstruction AND KL divergence</li> <li>Use appropriate loss: MSE for continuous, BCE for binary data</li> <li>Visualize latent space: Plot 2D projections to check structure</li> <li>Test interpolation: Smooth interpolation indicates good latent space</li> <li>Check per-dim KL: Detect posterior collapse early</li> <li>Use beta annealing: Helps avoid posterior collapse</li> <li>Larger batch size: More stable training (128+ recommended)</li> </ul>"},{"location":"user-guide/models/vae-guide/#dont","title":"DON'T \u274c","text":"<ul> <li>Don't ignore KL: Zero KL means model ignores latent code</li> <li>Don't use too small latent: Leads to underfitting</li> <li>Don't overtrain: Can lead to posterior collapse</li> <li>Don't skip validation: Regular evaluation prevents surprises</li> <li>Don't forget temperature: Use temperature for diverse sampling</li> <li>Don't compare different betas directly: Higher beta trades reconstruction for disentanglement</li> </ul>"},{"location":"user-guide/models/vae-guide/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/models/vae-guide/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Use gradient checkpointing for large models\nfrom jax import checkpoint\n\n@checkpoint\ndef encoder_forward(encoder, x):\n    return encoder(x)\n\n# Use lower precision for faster training\n# Set precision in config or at JAX level\nimport jax\njax.config.update(\"jax_default_matmul_precision\", \"float32\")  # or \"bfloat16\"\n</code></pre>"},{"location":"user-guide/models/vae-guide/#speed-optimization","title":"Speed Optimization","text":"<pre><code># JIT compile training step\n@nnx.jit\ndef fast_train_step(model, optimizer, batch):\n    def loss_fn(model):\n        outputs = model(batch)  # Model uses internal RNGs\n        losses = model.loss_fn(x=batch, outputs=outputs)\n        return losses[\"loss\"], losses\n\n    (loss, losses), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n    optimizer.update(model, grads)\n    return losses\n\n# Vectorize sampling\nvmapped_decode = jax.vmap(lambda z: vae.decode(z))\nsamples = vmapped_decode(latent_vectors)\n</code></pre>"},{"location":"user-guide/models/vae-guide/#summary","title":"Summary","text":"<p>This guide covered:</p> <ul> <li>\u2705 Creating encoders, decoders, and VAE models</li> <li>\u2705 Training standard VAE, \u03b2-VAE, CVAE, and VQ-VAE</li> <li>\u2705 Generating samples and manipulating latent space</li> <li>\u2705 Evaluation metrics and diagnostics</li> <li>\u2705 Hyperparameter tuning strategies</li> <li>\u2705 Troubleshooting common issues</li> <li>\u2705 Advanced techniques and optimizations</li> </ul>"},{"location":"user-guide/models/vae-guide/#next-steps","title":"Next Steps","text":"<ul> <li>VAE Concepts \u2014 Deep dive into theory</li> <li>VAE API Reference \u2014 Complete API documentation</li> <li>VAE MNIST Example \u2014 Hands-on tutorial</li> <li>Training Guide \u2014 Advanced training techniques</li> <li>Benchmarking \u2014 Evaluate your models</li> </ul>"},{"location":"user-guide/training/advanced-features/","title":"Advanced Training Features","text":"<p>This guide covers advanced training utilities in Artifex for optimizing training performance and handling challenging scenarios like large batch training and mixed-precision optimization.</p>"},{"location":"user-guide/training/advanced-features/#overview","title":"Overview","text":"<p>Artifex provides two key utilities for advanced training:</p> <ul> <li>GradientAccumulator: Enables training with larger effective batch sizes by accumulating gradients across multiple forward/backward passes</li> <li>DynamicLossScaler: Handles numerical stability for mixed-precision training (float16/bfloat16) through automatic loss scaling</li> </ul>"},{"location":"user-guide/training/advanced-features/#gradient-accumulation","title":"Gradient Accumulation","text":""},{"location":"user-guide/training/advanced-features/#why-use-gradient-accumulation","title":"Why Use Gradient Accumulation?","text":"<p>When training large models or using high-resolution inputs, GPU memory often limits the batch size you can use. Gradient accumulation solves this by:</p> <ol> <li>Running multiple forward/backward passes with smaller batches</li> <li>Accumulating the gradients from each pass</li> <li>Applying a single optimizer update with the accumulated gradients</li> </ol> <p>This simulates training with a larger effective batch size without requiring more memory.</p> <p>Effective batch size = micro_batch_size \u00d7 accumulation_steps</p>"},{"location":"user-guide/training/advanced-features/#basic-usage","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.training import (\n    GradientAccumulator,\n    GradientAccumulatorConfig,\n)\n\n# Configure accumulation\nconfig = GradientAccumulatorConfig(\n    accumulation_steps=4,      # Accumulate over 4 micro-batches\n    normalize_gradients=True,  # Average gradients (recommended)\n)\n\n# Create accumulator\naccumulator = GradientAccumulator(config)\n</code></pre>"},{"location":"user-guide/training/advanced-features/#gradientaccumulatorconfig","title":"GradientAccumulatorConfig","text":"Parameter Type Default Description <code>accumulation_steps</code> <code>int</code> <code>1</code> Number of steps to accumulate before update <code>normalize_gradients</code> <code>bool</code> <code>True</code> Whether to average gradients by accumulation_steps"},{"location":"user-guide/training/advanced-features/#training-loop-integration","title":"Training Loop Integration","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nfrom artifex.generative_models.training import (\n    GradientAccumulator,\n    GradientAccumulatorConfig,\n)\n\ndef train_with_accumulation(\n    model: nnx.Module,\n    train_loader,\n    num_epochs: int,\n    micro_batch_size: int = 32,\n    accumulation_steps: int = 4,\n    learning_rate: float = 1e-3,\n):\n    \"\"\"Training with gradient accumulation.\"\"\"\n    # Setup optimizer\n    optimizer = optax.adam(learning_rate)\n    opt_state = optimizer.init(nnx.state(model))\n\n    # Setup accumulator\n    accumulator = GradientAccumulator(\n        GradientAccumulatorConfig(\n            accumulation_steps=accumulation_steps,\n            normalize_gradients=True,\n        )\n    )\n\n    @nnx.jit\n    def compute_gradients(model, batch):\n        \"\"\"Compute gradients for a single micro-batch.\"\"\"\n        def loss_fn(model):\n            outputs = model(batch[\"images\"], training=True)\n            return outputs[\"loss\"], outputs\n\n        (loss, outputs), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n        return grads, loss, outputs\n\n    @nnx.jit\n    def apply_gradients(model, opt_state, grads):\n        \"\"\"Apply accumulated gradients to model.\"\"\"\n        updates, new_opt_state = optimizer.update(grads, opt_state)\n        nnx.update(model, nnx.apply_updates(nnx.state(model), updates))\n        return new_opt_state\n\n    # Training loop\n    global_step = 0\n    for epoch in range(num_epochs):\n        for batch in train_loader(batch_size=micro_batch_size):\n            # Compute and accumulate gradients\n            grads, loss, outputs = compute_gradients(model, batch)\n            accumulator.accumulate(grads)\n            global_step += 1\n\n            # Apply update when accumulation is complete\n            if accumulator.should_update(global_step):\n                accumulated_grads = accumulator.get_gradients()\n                opt_state = apply_gradients(model, opt_state, accumulated_grads)\n                accumulator.reset()\n\n                effective_step = global_step // accumulation_steps\n                if effective_step % 100 == 0:\n                    print(f\"Step {effective_step}: Loss = {loss:.4f}\")\n\n    return model\n</code></pre>"},{"location":"user-guide/training/advanced-features/#key-methods","title":"Key Methods","text":""},{"location":"user-guide/training/advanced-features/#accumulategrads","title":"<code>accumulate(grads)</code>","text":"<p>Add gradients from a micro-batch to the accumulator.</p> <pre><code>grads, loss = compute_gradients(model, batch)\naccumulator.accumulate(grads)\n</code></pre>"},{"location":"user-guide/training/advanced-features/#should_updatestep","title":"<code>should_update(step)</code>","text":"<p>Check if enough gradients have been accumulated. Returns <code>True</code> when <code>step % accumulation_steps == 0</code>.</p> <pre><code>if accumulator.should_update(global_step):\n    # Time to apply optimizer update\n    ...\n</code></pre>"},{"location":"user-guide/training/advanced-features/#get_gradients","title":"<code>get_gradients()</code>","text":"<p>Retrieve the accumulated (and optionally normalized) gradients.</p> <pre><code>accumulated_grads = accumulator.get_gradients()\n# If normalize_gradients=True, returns grads / accumulation_steps\n</code></pre>"},{"location":"user-guide/training/advanced-features/#reset","title":"<code>reset()</code>","text":"<p>Clear the accumulator after applying an update.</p> <pre><code>accumulator.reset()\n</code></pre>"},{"location":"user-guide/training/advanced-features/#dynamic-loss-scaling","title":"Dynamic Loss Scaling","text":""},{"location":"user-guide/training/advanced-features/#why-use-dynamic-loss-scaling","title":"Why Use Dynamic Loss Scaling?","text":"<p>Mixed-precision training with float16 or bfloat16 provides significant speedups but introduces numerical challenges:</p> <ul> <li>Underflow: Small gradients become zero in lower precision</li> <li>Overflow: Large values exceed the representable range</li> </ul> <p>Dynamic loss scaling addresses these issues by:</p> <ol> <li>Scaling up the loss before backward pass (prevents underflow)</li> <li>Unscaling gradients before optimizer update</li> <li>Adjusting scale dynamically based on gradient overflow detection</li> </ol>"},{"location":"user-guide/training/advanced-features/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from artifex.generative_models.training import (\n    DynamicLossScaler,\n    DynamicLossScalerConfig,\n)\n\n# Configure loss scaler\nconfig = DynamicLossScalerConfig(\n    initial_scale=2**15,      # Starting loss scale\n    growth_factor=2.0,        # Scale growth multiplier\n    backoff_factor=0.5,       # Scale reduction on overflow\n    growth_interval=2000,     # Steps before attempting scale increase\n    min_scale=1.0,            # Minimum allowed scale\n    max_scale=2**24,          # Maximum allowed scale\n)\n\n# Create scaler\nscaler = DynamicLossScaler(config)\n</code></pre>"},{"location":"user-guide/training/advanced-features/#dynamiclossscalerconfig","title":"DynamicLossScalerConfig","text":"Parameter Type Default Description <code>initial_scale</code> <code>float</code> <code>2**15</code> Starting loss scale value <code>growth_factor</code> <code>float</code> <code>2.0</code> Multiplier when increasing scale <code>backoff_factor</code> <code>float</code> <code>0.5</code> Multiplier when reducing scale (on overflow) <code>growth_interval</code> <code>int</code> <code>2000</code> Steps without overflow before increasing scale <code>min_scale</code> <code>float</code> <code>1.0</code> Minimum allowed scale value <code>max_scale</code> <code>float</code> <code>2**24</code> Maximum allowed scale value"},{"location":"user-guide/training/advanced-features/#training-loop-integration_1","title":"Training Loop Integration","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nfrom artifex.generative_models.training import (\n    DynamicLossScaler,\n    DynamicLossScalerConfig,\n)\n\ndef train_with_mixed_precision(\n    model: nnx.Module,\n    train_loader,\n    num_epochs: int,\n    learning_rate: float = 1e-3,\n):\n    \"\"\"Training with dynamic loss scaling for mixed precision.\"\"\"\n    # Setup optimizer\n    optimizer = optax.adam(learning_rate)\n    opt_state = optimizer.init(nnx.state(model))\n\n    # Setup loss scaler\n    scaler = DynamicLossScaler(DynamicLossScalerConfig())\n\n    def train_step(model, opt_state, batch):\n        \"\"\"Single training step with loss scaling.\"\"\"\n        def loss_fn(model):\n            # Forward pass (in lower precision if model uses fp16/bf16)\n            outputs = model(batch[\"images\"], training=True)\n            return outputs[\"loss\"], outputs\n\n        # Compute loss and gradients\n        (loss, outputs), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n\n        # Scale loss for numerical stability\n        scaled_loss = scaler.scale_loss(loss)\n\n        # Unscale gradients before optimizer update\n        unscaled_grads = scaler.unscale_gradients(grads)\n\n        # Check for overflow (NaN or Inf in gradients)\n        overflow = scaler.check_overflow(unscaled_grads)\n\n        # Update scale based on overflow status\n        scaler.update_scale(overflow)\n\n        if not overflow:\n            # Apply gradients only if no overflow\n            updates, opt_state = optimizer.update(unscaled_grads, opt_state)\n            nnx.update(model, nnx.apply_updates(nnx.state(model), updates))\n\n        return opt_state, loss, overflow\n\n    # Training loop\n    for epoch in range(num_epochs):\n        total_overflow_steps = 0\n        for step, batch in enumerate(train_loader()):\n            opt_state, loss, overflow = train_step(model, opt_state, batch)\n\n            if overflow:\n                total_overflow_steps += 1\n\n            if step % 100 == 0:\n                print(f\"Step {step}: Loss = {loss:.4f}, Scale = {scaler.scale:.0f}\")\n\n        print(f\"Epoch {epoch}: Overflow steps = {total_overflow_steps}\")\n\n    return model\n</code></pre>"},{"location":"user-guide/training/advanced-features/#key-methods_1","title":"Key Methods","text":""},{"location":"user-guide/training/advanced-features/#scale_lossloss","title":"<code>scale_loss(loss)</code>","text":"<p>Multiply the loss by the current scale factor before backward pass.</p> <pre><code>scaled_loss = scaler.scale_loss(loss)\n# Now compute gradients with respect to scaled_loss\n</code></pre>"},{"location":"user-guide/training/advanced-features/#unscale_gradientsgrads","title":"<code>unscale_gradients(grads)</code>","text":"<p>Divide gradients by the scale factor to get the true gradient values.</p> <pre><code>unscaled_grads = scaler.unscale_gradients(grads)\n</code></pre>"},{"location":"user-guide/training/advanced-features/#check_overflowgrads","title":"<code>check_overflow(grads)</code>","text":"<p>Check if any gradient contains NaN or Inf values.</p> <pre><code>overflow = scaler.check_overflow(unscaled_grads)\nif overflow:\n    # Skip this update, reduce scale\n    pass\n</code></pre>"},{"location":"user-guide/training/advanced-features/#update_scaleoverflow_detected","title":"<code>update_scale(overflow_detected)</code>","text":"<p>Adjust the scale based on whether overflow was detected.</p> <pre><code>scaler.update_scale(overflow)\n# If overflow: scale *= backoff_factor\n# If no overflow for growth_interval steps: scale *= growth_factor\n</code></pre>"},{"location":"user-guide/training/advanced-features/#combining-both-features","title":"Combining Both Features","text":"<p>For optimal training of large models, combine gradient accumulation with dynamic loss scaling:</p> <pre><code>from artifex.generative_models.training import (\n    GradientAccumulator,\n    GradientAccumulatorConfig,\n    DynamicLossScaler,\n    DynamicLossScalerConfig,\n)\n\ndef train_with_accumulation_and_scaling(\n    model: nnx.Module,\n    train_loader,\n    num_epochs: int,\n    accumulation_steps: int = 4,\n    learning_rate: float = 1e-3,\n):\n    \"\"\"Combined gradient accumulation and dynamic loss scaling.\"\"\"\n    optimizer = optax.adam(learning_rate)\n    opt_state = optimizer.init(nnx.state(model))\n\n    # Initialize utilities\n    accumulator = GradientAccumulator(\n        GradientAccumulatorConfig(\n            accumulation_steps=accumulation_steps,\n            normalize_gradients=True,\n        )\n    )\n    scaler = DynamicLossScaler(DynamicLossScalerConfig())\n\n    def compute_scaled_gradients(model, batch):\n        \"\"\"Compute gradients with loss scaling.\"\"\"\n        def loss_fn(model):\n            outputs = model(batch[\"images\"], training=True)\n            # Scale loss before backward pass\n            scaled_loss = scaler.scale_loss(outputs[\"loss\"])\n            return scaled_loss, outputs\n\n        (_, outputs), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n\n        # Unscale gradients immediately\n        unscaled_grads = scaler.unscale_gradients(grads)\n        return unscaled_grads, outputs[\"loss\"]\n\n    # Training loop\n    global_step = 0\n    for epoch in range(num_epochs):\n        for batch in train_loader():\n            # Compute and accumulate gradients\n            grads, loss = compute_scaled_gradients(model, batch)\n            accumulator.accumulate(grads)\n            global_step += 1\n\n            # Apply update when accumulation is complete\n            if accumulator.should_update(global_step):\n                accumulated_grads = accumulator.get_gradients()\n\n                # Check for overflow in accumulated gradients\n                overflow = scaler.check_overflow(accumulated_grads)\n                scaler.update_scale(overflow)\n\n                if not overflow:\n                    updates, opt_state = optimizer.update(\n                        accumulated_grads, opt_state\n                    )\n                    nnx.update(\n                        model,\n                        nnx.apply_updates(nnx.state(model), updates)\n                    )\n\n                accumulator.reset()\n\n    return model\n</code></pre>"},{"location":"user-guide/training/advanced-features/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/training/advanced-features/#gradient-accumulation_1","title":"Gradient Accumulation","text":"<ol> <li>Choose accumulation steps based on target batch size:</li> </ol> <pre><code>target_batch_size = 256\nmicro_batch_size = 32  # What fits in memory\naccumulation_steps = target_batch_size // micro_batch_size  # = 8\n</code></pre> <ol> <li> <p>Always normalize gradients unless you have a specific reason not to. This ensures consistent gradient magnitudes regardless of accumulation steps.</p> </li> <li> <p>Adjust learning rate when changing effective batch size. The linear scaling rule suggests scaling learning rate proportionally with batch size.</p> </li> </ol>"},{"location":"user-guide/training/advanced-features/#dynamic-loss-scaling_1","title":"Dynamic Loss Scaling","text":"<ol> <li> <p>Start with moderate initial scale (default <code>2**15</code> works well for most cases).</p> </li> <li> <p>Monitor overflow frequency. Frequent overflows indicate:</p> </li> <li>Learning rate may be too high</li> <li>Model may have numerical instability</li> <li> <p>Initial scale may be too high</p> </li> <li> <p>Use with bfloat16 when possible. bfloat16 has the same dynamic range as float32, reducing overflow issues compared to float16.</p> </li> <li> <p>Consider gradient clipping as a complementary technique:</p> </li> </ol> <pre><code>optimizer = optax.chain(\n    optax.clip_by_global_norm(1.0),\n    optax.adam(learning_rate),\n)\n</code></pre>"},{"location":"user-guide/training/advanced-features/#integration-with-model-specific-trainers","title":"Integration with Model-Specific Trainers","text":"<p>The advanced features integrate seamlessly with Artifex's model-specific trainers:</p> <pre><code>from artifex.generative_models.training.trainers import VAETrainer\nfrom artifex.generative_models.training import (\n    GradientAccumulator,\n    GradientAccumulatorConfig,\n)\n\n# Create trainer\ntrainer = VAETrainer(model, optimizer, config)\n\n# Use accumulator in custom training loop\naccumulator = GradientAccumulator(\n    GradientAccumulatorConfig(accumulation_steps=4)\n)\n\nfor batch in dataloader:\n    grads, metrics = trainer.compute_gradients(batch)\n    accumulator.accumulate(grads)\n\n    if accumulator.should_update(step):\n        trainer.apply_gradients(accumulator.get_gradients())\n        accumulator.reset()\n</code></pre>"},{"location":"user-guide/training/advanced-features/#api-reference","title":"API Reference","text":"<p>For complete API documentation, see the Trainer API Reference.</p> <p>The gradient accumulation and dynamic loss scaling utilities are exported from the main training module:</p> <pre><code>from artifex.generative_models.training import (\n    GradientAccumulator,\n    GradientAccumulatorConfig,\n    DynamicLossScaler,\n    DynamicLossScalerConfig,\n)\n</code></pre>"},{"location":"user-guide/training/advanced-features/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Guide - Core training patterns and callbacks</li> <li>Logging &amp; Experiment Tracking - W&amp;B, TensorBoard, and progress bar integration</li> <li>Performance Profiling - JAX trace profiling and memory tracking</li> <li>RL Training - Reinforcement learning for model fine-tuning (REINFORCE, PPO, GRPO, DPO)</li> <li>Distributed Training - Multi-device training with gradient accumulation</li> <li>Configuration System - Training configuration options</li> </ul>"},{"location":"user-guide/training/configuration/","title":"Configuration System","text":"<p>Artifex uses a unified, type-safe configuration system built on Pydantic. This system provides validation, serialization, and a consistent interface across all components.</p> <ul> <li> <p> Type-Safe</p> <p>Pydantic validation ensures correctness at creation time</p> </li> <li> <p> Serializable</p> <p>Easy save/load to YAML or JSON formats</p> </li> <li> <p> Composable</p> <p>Hierarchical configs with inheritance and merging</p> </li> <li> <p> Self-Documenting</p> <p>Field descriptions and constraints built-in</p> </li> </ul>"},{"location":"user-guide/training/configuration/#configuration-architecture","title":"Configuration Architecture","text":"<p>The configuration system is organized hierarchically:</p> <pre><code>graph TB\n    subgraph \"Base Configuration\"\n        BC[BaseConfiguration]\n        BC --&gt;|common fields| NAME[name]\n        BC --&gt;|common fields| TYPE[type]\n        BC --&gt;|common fields| DESC[description]\n        BC --&gt;|common fields| META[metadata]\n    end\n\n    subgraph \"Model Configurations\"\n        MC[ModelConfig]\n        BC --&gt;|inherits| MC\n        MC --&gt;|defines| ARCH[Architecture]\n        MC --&gt;|defines| PARAMS[Parameters]\n    end\n\n    subgraph \"Training Configurations\"\n        TC[TrainingConfig]\n        BC --&gt;|inherits| TC\n        TC --&gt;|contains| OC[OptimizerConfig]\n        TC --&gt;|optional| SC[SchedulerConfig]\n    end\n\n    subgraph \"Data Configurations\"\n        DC[DataConfig]\n        BC --&gt;|inherits| DC\n        DC --&gt;|defines| DATASET[Dataset Settings]\n        DC --&gt;|defines| AUG[Augmentation]\n    end\n\n    subgraph \"Experiment Configuration\"\n        EC[ExperimentConfig]\n        BC --&gt;|inherits| EC\n        EC --&gt;|composes| MC\n        EC --&gt;|composes| TC\n        EC --&gt;|composes| DC\n    end</code></pre>"},{"location":"user-guide/training/configuration/#baseconfiguration","title":"BaseConfiguration","text":"<p>All configurations inherit from <code>BaseConfiguration</code>:</p> <pre><code>from artifex.generative_models.core.configuration import BaseConfiguration\nfrom pydantic import Field\n\nclass BaseConfiguration:\n    \"\"\"Base class for all configurations.\"\"\"\n\n    # Core fields\n    name: str = Field(..., description=\"Unique name\")\n    type: ConfigurationType = Field(..., description=\"Configuration type\")\n    description: str | None = Field(None, description=\"Human-readable description\")\n    version: str = Field(\"1.0.0\", description=\"Configuration version\")\n\n    # Metadata\n    tags: list[str] = Field(default_factory=list, description=\"Tags for categorization\")\n    metadata: dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Non-functional metadata for tracking\"\n    )\n</code></pre> <p>Key Features:</p> <ul> <li>Required Fields: <code>name</code> and <code>type</code> are required</li> <li>Optional Metadata: Non-functional information for tracking</li> <li>Version Control: Built-in versioning support</li> <li>Tags: For organizing and searching configurations</li> </ul>"},{"location":"user-guide/training/configuration/#yaml-serialization","title":"YAML Serialization","text":"<p>Save and load configurations from YAML:</p> <pre><code>from artifex.generative_models.core.configuration import ModelConfig\nfrom pathlib import Path\n\n# Create configuration\nconfig = ModelConfig(\n    name=\"vae_config\",\n    model_class=\"artifex.generative_models.models.vae.base.VAE\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 128],\n    output_dim=64,\n    parameters={\"beta\": 1.0},\n)\n\n# Save to YAML\nconfig.to_yaml(\"configs/vae_config.yaml\")\n\n# Load from YAML\nloaded_config = ModelConfig.from_yaml(\"configs/vae_config.yaml\")\n\nassert config.name == loaded_config.name\n</code></pre> <p>Example YAML file:</p> <pre><code>name: vae_config\ntype: model\nmodel_class: artifex.generative_models.models.vae.base.VAE\ninput_dim: [28, 28, 1]\nhidden_dims: [256, 128]\noutput_dim: 64\nactivation: gelu\ndropout_rate: 0.1\nuse_batch_norm: true\nparameters:\n  beta: 1.0\nrngs_seeds:\n  params: 0\n  dropout: 1\n</code></pre>"},{"location":"user-guide/training/configuration/#configuration-merging","title":"Configuration Merging","text":"<p>Merge configurations for easy experimentation:</p> <pre><code># Base configuration\nbase_config = ModelConfig(\n    name=\"base_vae\",\n    model_class=\"artifex.generative_models.models.vae.base.VAE\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 128],\n    output_dim=64,\n)\n\n# Override specific fields\noverrides = {\n    \"hidden_dims\": [512, 256, 128],\n    \"output_dim\": 128,\n    \"parameters\": {\"beta\": 2.0},\n}\n\n# Merge configurations\nnew_config = base_config.merge(overrides)\n\nprint(new_config.hidden_dims)  # [512, 256, 128]\nprint(new_config.parameters)   # {\"beta\": 2.0}\n</code></pre>"},{"location":"user-guide/training/configuration/#modelconfig","title":"ModelConfig","text":"<p><code>ModelConfig</code> defines model architecture and parameters:</p> <pre><code>from artifex.generative_models.core.configuration import ModelConfig\n\nconfig = ModelConfig(\n    name=\"my_vae\",\n    model_class=\"artifex.generative_models.models.vae.base.VAE\",\n\n    # Architecture\n    input_dim=(28, 28, 1),\n    hidden_dims=[512, 256, 128],\n    output_dim=64,\n\n    # Common parameters\n    activation=\"gelu\",\n    dropout_rate=0.1,\n    use_batch_norm=True,\n\n    # RNG seeds for reproducibility\n    rngs_seeds={\"params\": 42, \"dropout\": 1},\n\n    # Model-specific parameters\n    parameters={\n        \"beta\": 1.0,  # VAE beta parameter\n        \"kl_weight\": 0.5,\n    },\n\n    # Optional metadata\n    metadata={\n        \"experiment_id\": \"exp_001\",\n        \"notes\": \"Testing higher latent dim\",\n    },\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#field-reference","title":"Field Reference","text":"Field Type Required Description <code>name</code> <code>str</code> \u2705 Unique configuration name <code>model_class</code> <code>str</code> \u2705 Fully qualified model class <code>input_dim</code> <code>int \\| tuple</code> \u2705 Input dimensions <code>hidden_dims</code> <code>list[int]</code> \u274c Hidden layer dimensions <code>output_dim</code> <code>int \\| tuple</code> \u274c Output dimensions <code>activation</code> <code>str</code> \u274c Activation function <code>dropout_rate</code> <code>float</code> \u274c Dropout rate <code>use_batch_norm</code> <code>bool</code> \u274c Use batch normalization <code>rngs_seeds</code> <code>dict[str, int]</code> \u274c RNG seeds <code>parameters</code> <code>dict[str, Any]</code> \u274c Model-specific params"},{"location":"user-guide/training/configuration/#parameters-vs-metadata","title":"Parameters vs Metadata","text":"<p>Important Distinction:</p> <pre><code># CORRECT: Use parameters for functional configuration\nconfig = ModelConfig(\n    name=\"vae\",\n    model_class=\"...\",\n    input_dim=(28, 28, 1),\n    # Functional parameters that affect model behavior\n    parameters={\n        \"beta\": 1.0,\n        \"kl_weight\": 0.5,\n        \"reconstruction_loss\": \"mse\",\n    },\n    # Non-functional metadata for tracking\n    metadata={\n        \"experiment_id\": \"exp_001\",\n        \"dataset_version\": \"v2.1\",\n        \"notes\": \"Testing lower KL weight\",\n    }\n)\n\n# WRONG: Don't nest model parameters in metadata\nconfig = ModelConfig(\n    name=\"vae\",\n    model_class=\"...\",\n    input_dim=(28, 28, 1),\n    metadata={\n        \"vae_params\": {\"beta\": 1.0}  # DON'T DO THIS\n    }\n)\n</code></pre> <p>Guidelines:</p> <ul> <li><code>parameters</code>: Functional configuration affecting model behavior</li> <li><code>metadata</code>: Non-functional information for experiment tracking</li> </ul>"},{"location":"user-guide/training/configuration/#model-specific-parameters","title":"Model-Specific Parameters","text":"<p>Different models require different parameters:</p> <pre><code># VAE configuration\nvae_config = ModelConfig(\n    name=\"vae\",\n    model_class=\"artifex.generative_models.models.vae.base.VAE\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 128],\n    output_dim=64,\n    parameters={\n        \"beta\": 1.0,              # KL weight\n        \"kl_weight\": 0.5,         # Additional KL scaling\n    },\n)\n\n# GAN configuration\ngan_config = ModelConfig(\n    name=\"gan\",\n    model_class=\"artifex.generative_models.models.gan.base.GAN\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 128],\n    output_dim=(28, 28, 1),\n    parameters={\n        \"noise_dim\": 100,         # Latent noise dimension\n        \"label_smoothing\": 0.1,   # Label smoothing for discriminator\n    },\n)\n\n# Diffusion configuration\ndiffusion_config = ModelConfig(\n    name=\"diffusion\",\n    model_class=\"artifex.generative_models.models.diffusion.ddpm.DDPM\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[128, 256, 512],\n    output_dim=(28, 28, 1),\n    parameters={\n        \"noise_steps\": 1000,      # Diffusion steps\n        \"beta_start\": 0.0001,     # Noise schedule start\n        \"beta_end\": 0.02,         # Noise schedule end\n        \"schedule_type\": \"linear\", # Noise schedule type\n    },\n)\n\n# Flow configuration\nflow_config = ModelConfig(\n    name=\"flow\",\n    model_class=\"artifex.generative_models.models.flow.realnvp.RealNVP\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 256],\n    output_dim=(28, 28, 1),\n    parameters={\n        \"num_coupling_layers\": 8,  # Number of coupling layers\n        \"mask_type\": \"checkerboard\", # Coupling mask pattern\n    },\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#optimizerconfig","title":"OptimizerConfig","text":"<p>Configure optimizers with full type safety:</p> <pre><code>from artifex.generative_models.core.configuration import OptimizerConfig\n\n# Adam optimizer (recommended)\nadam_config = OptimizerConfig(\n    name=\"adam_optimizer\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n    beta1=0.9,\n    beta2=0.999,\n    eps=1e-8,\n)\n\n# AdamW with weight decay\nadamw_config = OptimizerConfig(\n    name=\"adamw_optimizer\",\n    optimizer_type=\"adamw\",\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    beta1=0.9,\n    beta2=0.999,\n)\n\n# SGD with momentum\nsgd_config = OptimizerConfig(\n    name=\"sgd_optimizer\",\n    optimizer_type=\"sgd\",\n    learning_rate=0.1,\n    momentum=0.9,\n    nesterov=True,\n)\n\n# RMSProp\nrmsprop_config = OptimizerConfig(\n    name=\"rmsprop_optimizer\",\n    optimizer_type=\"rmsprop\",\n    learning_rate=1e-3,\n    initial_accumulator_value=0.1,\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#gradient-clipping","title":"Gradient Clipping","text":"<p>Add gradient clipping to any optimizer:</p> <pre><code># Clip by global norm (recommended)\nclipped_adam = OptimizerConfig(\n    name=\"clipped_adam\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n    gradient_clip_norm=1.0,  # Clip to norm of 1.0\n)\n\n# Clip by value\nvalue_clipped = OptimizerConfig(\n    name=\"value_clipped\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n    gradient_clip_value=0.5,  # Clip values to [-0.5, 0.5]\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#optimizer-field-reference","title":"Optimizer Field Reference","text":"Field Type Required Description <code>optimizer_type</code> <code>str</code> \u2705 Optimizer type (adam, adamw, sgd, etc.) <code>learning_rate</code> <code>float</code> \u2705 Learning rate (must be &gt; 0) <code>weight_decay</code> <code>float</code> \u274c Weight decay (L2 penalty) <code>beta1</code> <code>float</code> \u274c Beta1 for Adam (default: 0.9) <code>beta2</code> <code>float</code> \u274c Beta2 for Adam (default: 0.999) <code>eps</code> <code>float</code> \u274c Epsilon for numerical stability <code>momentum</code> <code>float</code> \u274c Momentum for SGD <code>nesterov</code> <code>bool</code> \u274c Use Nesterov momentum <code>gradient_clip_norm</code> <code>float \\| None</code> \u274c Gradient clipping by norm <code>gradient_clip_value</code> <code>float \\| None</code> \u274c Gradient clipping by value"},{"location":"user-guide/training/configuration/#schedulerconfig","title":"SchedulerConfig","text":"<p>Configure learning rate schedules:</p> <pre><code>from artifex.generative_models.core.configuration import SchedulerConfig\n\n# Cosine schedule with warmup (recommended)\ncosine_schedule = SchedulerConfig(\n    name=\"cosine_warmup\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    cycle_length=50000,\n    min_lr_ratio=0.1,\n)\n\n# Linear decay\nlinear_schedule = SchedulerConfig(\n    name=\"linear_decay\",\n    scheduler_type=\"linear\",\n    warmup_steps=500,\n    total_steps=10000,\n    min_lr_ratio=0.0,\n)\n\n# Exponential decay\nexponential_schedule = SchedulerConfig(\n    name=\"exponential\",\n    scheduler_type=\"exponential\",\n    decay_rate=0.95,\n    decay_steps=1000,\n)\n\n# Step decay\nstep_schedule = SchedulerConfig(\n    name=\"step_decay\",\n    scheduler_type=\"step\",\n    step_size=5000,\n    gamma=0.1,\n)\n\n# MultiStep decay\nmultistep_schedule = SchedulerConfig(\n    name=\"multistep\",\n    scheduler_type=\"multistep\",\n    milestones=[10000, 20000, 30000],\n    gamma=0.1,\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#scheduler-field-reference","title":"Scheduler Field Reference","text":"Field Type Required Description <code>scheduler_type</code> <code>str</code> \u2705 Schedule type (cosine, linear, etc.) <code>warmup_steps</code> <code>int</code> \u274c Number of warmup steps <code>min_lr_ratio</code> <code>float</code> \u274c Minimum LR as ratio of initial <code>cycle_length</code> <code>int \\| None</code> \u274c Cycle length for cosine <code>total_steps</code> <code>int \\| None</code> \u274c Total steps for linear <code>decay_rate</code> <code>float</code> \u274c Decay rate for exponential <code>decay_steps</code> <code>int</code> \u274c Decay steps for exponential <code>step_size</code> <code>int</code> \u274c Step size for step schedule <code>gamma</code> <code>float</code> \u274c Gamma for step/multistep <code>milestones</code> <code>list[int]</code> \u274c Milestones for multistep"},{"location":"user-guide/training/configuration/#trainingconfig","title":"TrainingConfig","text":"<p>Compose training configurations from optimizer and scheduler:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    TrainingConfig,\n    OptimizerConfig,\n    SchedulerConfig,\n)\n\n# Create optimizer config\noptimizer = OptimizerConfig(\n    name=\"adamw\",\n    optimizer_type=\"adamw\",\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    gradient_clip_norm=1.0,\n)\n\n# Create scheduler config\nscheduler = SchedulerConfig(\n    name=\"cosine_warmup\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    cycle_length=50000,\n    min_lr_ratio=0.1,\n)\n\n# Compose training configuration\ntraining_config = TrainingConfig(\n    name=\"vae_training\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=optimizer,        # Required\n    scheduler=scheduler,        # Optional\n    save_frequency=5000,\n    log_frequency=100,\n    checkpoint_dir=\"./checkpoints\",\n    max_checkpoints=5,\n    use_wandb=False,\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#training-field-reference","title":"Training Field Reference","text":"Field Type Required Description <code>batch_size</code> <code>int</code> \u2705 Batch size (must be \u2265 1) <code>num_epochs</code> <code>int</code> \u2705 Number of epochs (must be \u2265 1) <code>optimizer</code> <code>OptimizerConfig</code> \u2705 Optimizer configuration <code>scheduler</code> <code>SchedulerConfig \\| None</code> \u274c LR scheduler configuration <code>gradient_clip_norm</code> <code>float \\| None</code> \u274c Global gradient clipping norm <code>checkpoint_dir</code> <code>Path</code> \u274c Checkpoint directory <code>save_frequency</code> <code>int</code> \u274c Save checkpoint every N steps <code>max_checkpoints</code> <code>int</code> \u274c Maximum checkpoints to keep <code>log_frequency</code> <code>int</code> \u274c Log metrics every N steps <code>use_wandb</code> <code>bool</code> \u274c Use Weights &amp; Biases <code>wandb_project</code> <code>str \\| None</code> \u274c W&amp;B project name"},{"location":"user-guide/training/configuration/#dataconfig","title":"DataConfig","text":"<p>Configure data loading and preprocessing:</p> <pre><code>from artifex.generative_models.core.configuration import DataConfig\nfrom pathlib import Path\n\ndata_config = DataConfig(\n    name=\"mnist_data\",\n    dataset_name=\"mnist\",\n    data_dir=Path(\"./data\"),\n    split=\"train\",\n\n    # Data loading\n    num_workers=4,\n    prefetch_factor=2,\n    pin_memory=True,\n\n    # Data splits\n    validation_split=0.1,\n    test_split=0.1,\n\n    # Augmentation\n    augmentation=True,\n    augmentation_params={\n        \"random_flip\": True,\n        \"random_rotation\": 15,\n        \"random_crop\": True,\n    },\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#experimentconfig","title":"ExperimentConfig","text":"<p>Compose complete experiments:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    ExperimentConfig,\n    ModelConfig,\n    TrainingConfig,\n    DataConfig,\n)\n\n# Create component configurations\nmodel_config = ModelConfig(...)\ntraining_config = TrainingConfig(...)\ndata_config = DataConfig(...)\n\n# Compose experiment\nexperiment = ExperimentConfig(\n    name=\"vae_experiment_001\",\n    model_cfg=model_config,\n    training_cfg=training_config,\n    data_cfg=data_config,\n\n    # Experiment settings\n    seed=42,\n    deterministic=True,\n    output_dir=Path(\"./experiments/vae_001\"),\n\n    # Tracking\n    track_carbon=True,\n    track_memory=True,\n\n    # Metadata\n    description=\"Baseline VAE experiment on MNIST\",\n    tags=[\"vae\", \"mnist\", \"baseline\"],\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#configuration-registry","title":"Configuration Registry","text":"<p>Manage configurations centrally:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    ConfigurationRegistry,\n    ConfigurationType,\n)\n\n# Create registry\nregistry = ConfigurationRegistry()\n\n# Register configurations\nregistry.register(model_config)\nregistry.register(training_config)\nregistry.register(data_config)\n\n# Retrieve configurations\nretrieved_model = registry.get(\"vae_config\", ConfigurationType.MODEL)\nretrieved_training = registry.get(\"vae_training\", ConfigurationType.TRAINING)\n\n# List all configurations\nall_configs = registry.list_configs()\nprint(all_configs)  # ['model/vae_config', 'training/vae_training', ...]\n\n# List by type\nmodel_configs = registry.list_configs(ConfigurationType.MODEL)\nprint(model_configs)  # ['vae_config', 'gan_config', ...]\n</code></pre>"},{"location":"user-guide/training/configuration/#loading-from-directory","title":"Loading from Directory","text":"<p>Load all configurations from a directory:</p> <pre><code># Directory structure:\n# configs/\n#   \u251c\u2500\u2500 model_vae.yaml\n#   \u251c\u2500\u2500 model_gan.yaml\n#   \u251c\u2500\u2500 training_default.yaml\n#   \u251c\u2500\u2500 training_fast.yaml\n#   \u2514\u2500\u2500 data_mnist.yaml\n\nregistry = ConfigurationRegistry()\nregistry.load_from_directory(\"./configs\")\n\n# All configurations automatically registered\nvae_config = registry.get(\"vae\", ConfigurationType.MODEL)\ndefault_training = registry.get(\"default\", ConfigurationType.TRAINING)\n</code></pre>"},{"location":"user-guide/training/configuration/#configuration-templates","title":"Configuration Templates","text":"<p>Create reusable configuration templates:</p> <pre><code># Register template\nregistry.register_template(\n    name=\"standard_vae_template\",\n    template={\n        \"model_class\": \"artifex.generative_models.models.vae.base.VAE\",\n        \"activation\": \"gelu\",\n        \"dropout_rate\": 0.1,\n        \"use_batch_norm\": True,\n        \"parameters\": {\n            \"beta\": 1.0,\n        },\n    }\n)\n\n# Create configuration from template\nvae_config = registry.create_from_template(\n    template_name=\"standard_vae_template\",\n    config_class=ModelConfig,\n    # Override specific fields\n    name=\"my_vae\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 128],\n    output_dim=64,\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#custom-configurations","title":"Custom Configurations","text":"<p>Create custom configuration classes:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    BaseConfiguration,\n    ConfigurationType,\n)\nfrom pydantic import Field\n\nclass ProteinModelConfig(BaseConfiguration):\n    \"\"\"Configuration for protein modeling.\"\"\"\n\n    type: ConfigurationType = Field(ConfigurationType.MODEL, frozen=True)\n\n    # Protein-specific fields\n    sequence_length: int = Field(..., description=\"Protein sequence length\")\n    num_residues: int = Field(20, description=\"Number of residue types\")\n    secondary_structure: bool = Field(True, description=\"Use secondary structure\")\n\n    # Structure prediction\n    num_recycles: int = Field(3, description=\"Number of recycle iterations\")\n    use_templates: bool = Field(False, description=\"Use template structures\")\n\n    # Custom validation\n    @field_validator(\"sequence_length\")\n    def validate_sequence_length(cls, v):\n        if v &lt;= 0 or v &gt; 10000:\n            raise ValueError(\"sequence_length must be in (0, 10000]\")\n        return v\n\n# Use custom configuration\nprotein_config = ProteinModelConfig(\n    name=\"alphafold_like\",\n    sequence_length=256,\n    num_residues=20,\n    secondary_structure=True,\n    num_recycles=3,\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>Pydantic automatically validates configurations:</p> <pre><code>from pydantic import ValidationError\n\ntry:\n    # Invalid optimizer type\n    bad_config = OptimizerConfig(\n        name=\"bad_optimizer\",\n        optimizer_type=\"invalid_type\",  # Not a valid optimizer\n        learning_rate=1e-3,\n    )\nexcept ValidationError as e:\n    print(e)\n    # ValidationError: optimizer_type must be one of {...}\n\ntry:\n    # Invalid learning rate\n    bad_config = OptimizerConfig(\n        name=\"bad_lr\",\n        optimizer_type=\"adam\",\n        learning_rate=-1.0,  # Must be &gt; 0\n    )\nexcept ValidationError as e:\n    print(e)\n    # ValidationError: learning_rate must be greater than 0\n\ntry:\n    # Invalid batch size\n    bad_config = TrainingConfig(\n        name=\"bad_batch\",\n        batch_size=0,  # Must be &gt;= 1\n        num_epochs=10,\n        optimizer=optimizer_config,\n    )\nexcept ValidationError as e:\n    print(e)\n    # ValidationError: batch_size must be greater than or equal to 1\n</code></pre>"},{"location":"user-guide/training/configuration/#custom-validators","title":"Custom Validators","text":"<p>Add custom validation logic:</p> <pre><code>from pydantic import field_validator\n\nclass CustomModelConfig(ModelConfig):\n    \"\"\"Model configuration with custom validation.\"\"\"\n\n    @field_validator(\"hidden_dims\")\n    def validate_hidden_dims(cls, v):\n        \"\"\"Ensure hidden dims are decreasing.\"\"\"\n        if not all(v[i] &gt;= v[i+1] for i in range(len(v)-1)):\n            raise ValueError(\"hidden_dims must be non-increasing\")\n        return v\n\n    @field_validator(\"dropout_rate\")\n    def validate_dropout(cls, v):\n        \"\"\"Ensure reasonable dropout rate.\"\"\"\n        if v &lt; 0.0 or v &gt; 0.5:\n            raise ValueError(\"dropout_rate should be in [0.0, 0.5]\")\n        return v\n\n# Valid configuration\nconfig = CustomModelConfig(\n    name=\"valid\",\n    model_class=\"...\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[512, 256, 128],  # Decreasing\n    output_dim=64,\n    dropout_rate=0.2,  # Reasonable\n)\n\n# Invalid configuration\ntry:\n    config = CustomModelConfig(\n        name=\"invalid\",\n        model_class=\"...\",\n        input_dim=(28, 28, 1),\n        hidden_dims=[128, 256, 512],  # Increasing!\n        output_dim=64,\n    )\nexcept ValidationError as e:\n    print(\"Validation failed:\", e)\n</code></pre>"},{"location":"user-guide/training/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/training/configuration/#configuration-organization","title":"Configuration Organization","text":"<p>Organize configurations by purpose:</p> <pre><code>configs/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 vae_small.yaml\n\u2502   \u251c\u2500\u2500 vae_large.yaml\n\u2502   \u251c\u2500\u2500 gan_dcgan.yaml\n\u2502   \u2514\u2500\u2500 diffusion_ddpm.yaml\n\u251c\u2500\u2500 training/\n\u2502   \u251c\u2500\u2500 fast_training.yaml\n\u2502   \u251c\u2500\u2500 default_training.yaml\n\u2502   \u2514\u2500\u2500 long_training.yaml\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 mnist.yaml\n\u2502   \u251c\u2500\u2500 cifar10.yaml\n\u2502   \u2514\u2500\u2500 imagenet.yaml\n\u2514\u2500\u2500 experiments/\n    \u251c\u2500\u2500 exp_001.yaml\n    \u251c\u2500\u2500 exp_002.yaml\n    \u2514\u2500\u2500 exp_003.yaml\n</code></pre>"},{"location":"user-guide/training/configuration/#naming-conventions","title":"Naming Conventions","text":"<p>Use clear, descriptive names:</p> <pre><code># Good names\nconfig = ModelConfig(name=\"vae_beta10_mnist\")\nconfig = TrainingConfig(name=\"fast_training_10epochs\")\nconfig = OptimizerConfig(name=\"adam_lr1e3_clip1\")\n\n# Bad names\nconfig = ModelConfig(name=\"config1\")\nconfig = TrainingConfig(name=\"training\")\nconfig = OptimizerConfig(name=\"opt\")\n</code></pre>"},{"location":"user-guide/training/configuration/#version-control","title":"Version Control","text":"<p>Track configuration versions:</p> <pre><code>config = ModelConfig(\n    name=\"vae_v2\",\n    model_class=\"...\",\n    input_dim=(28, 28, 1),\n    version=\"2.0.0\",  # Semantic versioning\n    description=\"Second iteration with larger latent dimension\",\n    tags=[\"vae\", \"v2\", \"production\"],\n    metadata={\n        \"previous_version\": \"1.0.0\",\n        \"changes\": \"Increased latent dimension from 32 to 64\",\n        \"author\": \"research_team\",\n    },\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#environment-specific-configs","title":"Environment-Specific Configs","text":"<p>Use different configs for different environments:</p> <pre><code># Development config (fast iteration)\ndev_training = TrainingConfig(\n    name=\"dev_training\",\n    batch_size=32,\n    num_epochs=5,\n    optimizer=OptimizerConfig(\n        name=\"dev_adam\",\n        optimizer_type=\"adam\",\n        learning_rate=1e-3,\n    ),\n    save_frequency=100,  # Save frequently\n    log_frequency=10,    # Log often\n)\n\n# Production config (full training)\nprod_training = TrainingConfig(\n    name=\"prod_training\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=OptimizerConfig(\n        name=\"prod_adam\",\n        optimizer_type=\"adam\",\n        learning_rate=3e-4,\n    ),\n    save_frequency=5000,  # Save less often\n    log_frequency=100,    # Log less often\n)\n</code></pre>"},{"location":"user-guide/training/configuration/#configuration-inheritance","title":"Configuration Inheritance","text":"<p>Create base configs and override:</p> <pre><code># Base VAE configuration\nbase_vae = ModelConfig(\n    name=\"base_vae\",\n    model_class=\"artifex.generative_models.models.vae.base.VAE\",\n    activation=\"gelu\",\n    dropout_rate=0.1,\n    use_batch_norm=True,\n)\n\n# Small VAE (for quick experiments)\nsmall_vae = base_vae.merge({\n    \"name\": \"small_vae\",\n    \"input_dim\": (28, 28, 1),\n    \"hidden_dims\": [128, 64],\n    \"output_dim\": 16,\n})\n\n# Large VAE (for final training)\nlarge_vae = base_vae.merge({\n    \"name\": \"large_vae\",\n    \"input_dim\": (28, 28, 1),\n    \"hidden_dims\": [512, 256, 128],\n    \"output_dim\": 128,\n})\n</code></pre>"},{"location":"user-guide/training/configuration/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/training/configuration/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":"<p>Configure hyperparameter searches:</p> <pre><code>def create_config_sweep(base_config, param_ranges):\n    \"\"\"Create configurations for hyperparameter sweep.\"\"\"\n    configs = []\n\n    import itertools\n\n    # Get all parameter combinations\n    keys = list(param_ranges.keys())\n    values = list(param_ranges.values())\n\n    for combination in itertools.product(*values):\n        params = dict(zip(keys, combination))\n\n        # Create config name\n        name_parts = [f\"{k}_{v}\" for k, v in params.items()]\n        config_name = f\"{base_config.name}_{'_'.join(name_parts)}\"\n\n        # Merge with base config\n        new_config = base_config.merge({\n            \"name\": config_name,\n            **params,\n        })\n\n        configs.append(new_config)\n\n    return configs\n\n# Define base configuration\nbase_config = TrainingConfig(\n    name=\"base\",\n    batch_size=128,\n    num_epochs=50,\n    optimizer=OptimizerConfig(\n        name=\"adam\",\n        optimizer_type=\"adam\",\n        learning_rate=1e-3,\n    ),\n)\n\n# Define parameter ranges\nparam_ranges = {\n    \"batch_size\": [32, 64, 128],\n    \"optimizer.learning_rate\": [1e-4, 1e-3, 1e-2],\n}\n\n# Generate sweep configurations\nsweep_configs = create_config_sweep(base_config, param_ranges)\n\n# Train with each configuration\nfor config in sweep_configs:\n    print(f\"Training with config: {config.name}\")\n    trainer = Trainer(model=model, training_config=config)\n    trainer.train_epoch()\n</code></pre>"},{"location":"user-guide/training/configuration/#ab-testing","title":"A/B Testing","text":"<p>Compare different configurations:</p> <pre><code># Configuration A (current)\nconfig_a = TrainingConfig(\n    name=\"config_a_current\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=OptimizerConfig(\n        name=\"adam\",\n        optimizer_type=\"adam\",\n        learning_rate=1e-3,\n    ),\n)\n\n# Configuration B (experimental)\nconfig_b = TrainingConfig(\n    name=\"config_b_experimental\",\n    batch_size=256,\n    num_epochs=100,\n    optimizer=OptimizerConfig(\n        name=\"adamw\",\n        optimizer_type=\"adamw\",\n        learning_rate=3e-4,\n        weight_decay=0.01,\n    ),\n    scheduler=SchedulerConfig(\n        name=\"cosine\",\n        scheduler_type=\"cosine\",\n        warmup_steps=1000,\n    ),\n)\n\n# Train and compare\nresults = {}\nfor config in [config_a, config_b]:\n    trainer = Trainer(model=model, training_config=config)\n    metrics = trainer.train_epoch()\n    results[config.name] = metrics\n\n# Compare results\nprint(f\"Config A Loss: {results['config_a_current']['loss']:.4f}\")\nprint(f\"Config B Loss: {results['config_b_experimental']['loss']:.4f}\")\n</code></pre>"},{"location":"user-guide/training/configuration/#summary","title":"Summary","text":"<p>The Artifex configuration system provides:</p> <ul> <li>\u2705 Type Safety: Pydantic validation catches errors early</li> <li>\u2705 Serialization: Easy YAML/JSON save/load</li> <li>\u2705 Composability: Hierarchical configs with inheritance</li> <li>\u2705 Validation: Built-in and custom validators</li> <li>\u2705 Self-Documentation: Field descriptions and constraints</li> <li>\u2705 Flexibility: Easy to extend with custom configurations</li> </ul>"},{"location":"user-guide/training/configuration/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Training Guide</p> <p>Practical training examples and advanced patterns</p> </li> <li> <p> Training Overview</p> <p>Architecture and core concepts of training system</p> </li> <li> <p> Trainer API</p> <p>Complete API reference for Trainer class</p> </li> </ul> <p>Continue to the Trainer API Reference for complete API documentation.</p>"},{"location":"user-guide/training/logging/","title":"Logging and Experiment Tracking","text":"<p>This guide covers integrating logging backends with Artifex training loops for experiment tracking, metric visualization, and debugging.</p>"},{"location":"user-guide/training/logging/#overview","title":"Overview","text":"<p>Artifex provides logging callbacks that seamlessly integrate with popular experiment tracking platforms:</p> <ul> <li>Weights &amp; Biases (W&amp;B): Full-featured experiment tracking with rich visualizations</li> <li>TensorBoard: Google's visualization toolkit for machine learning</li> <li>Console/Progress Bar: Real-time training feedback with Rich progress bars</li> </ul> <p>All callbacks follow the same interface and can be combined in a single training run.</p>"},{"location":"user-guide/training/logging/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n    TensorBoardLoggerCallback,\n    TensorBoardLoggerConfig,\n    ProgressBarCallback,\n    ProgressBarConfig,\n)\nfrom artifex.generative_models.training import Trainer\n\n# Create callbacks\nwandb_callback = WandbLoggerCallback(WandbLoggerConfig(\n    project=\"my-project\",\n    name=\"experiment-1\",\n))\n\ntensorboard_callback = TensorBoardLoggerCallback(TensorBoardLoggerConfig(\n    log_dir=\"logs/tensorboard\",\n))\n\nprogress_callback = ProgressBarCallback(ProgressBarConfig(\n    show_metrics=True,\n))\n\n# Use with trainer\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    callbacks=[wandb_callback, tensorboard_callback, progress_callback],\n)\n</code></pre>"},{"location":"user-guide/training/logging/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":""},{"location":"user-guide/training/logging/#configuration","title":"Configuration","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n)\n\nconfig = WandbLoggerConfig(\n    project=\"vae-experiments\",           # W&amp;B project name (required)\n    entity=\"my-team\",                     # Team or username (optional)\n    name=\"vae-cifar10-beta4\",            # Run name (auto-generated if None)\n    tags=[\"vae\", \"cifar10\", \"baseline\"],  # Tags for filtering runs\n    notes=\"Testing beta=4.0 with cyclical annealing\",  # Run description\n    config={                              # Hyperparameters to log\n        \"learning_rate\": 1e-4,\n        \"beta\": 4.0,\n        \"kl_annealing\": \"cyclical\",\n    },\n    mode=\"online\",                        # \"online\", \"offline\", or \"disabled\"\n    resume=None,                          # Resume previous run\n    log_every_n_steps=10,                # Log frequency\n    log_on_epoch_end=True,               # Log epoch summaries\n    log_dir=\"./wandb\",                   # Local directory for artifacts\n)\n\ncallback = WandbLoggerCallback(config)\n</code></pre>"},{"location":"user-guide/training/logging/#wandbloggerconfig-parameters","title":"WandbLoggerConfig Parameters","text":"Parameter Type Default Description <code>project</code> <code>str</code> Required W&amp;B project name <code>entity</code> <code>str</code> <code>None</code> W&amp;B team/username <code>name</code> <code>str</code> <code>None</code> Run name (auto-generated if None) <code>tags</code> <code>list[str]</code> <code>[]</code> Tags for filtering <code>notes</code> <code>str</code> <code>None</code> Run description <code>config</code> <code>dict</code> <code>{}</code> Hyperparameters to log <code>mode</code> <code>str</code> <code>\"online\"</code> <code>\"online\"</code>, <code>\"offline\"</code>, or <code>\"disabled\"</code> <code>resume</code> <code>str\\|bool</code> <code>None</code> Resume options: <code>\"allow\"</code>, <code>\"never\"</code>, <code>\"must\"</code>, <code>\"auto\"</code> <code>log_every_n_steps</code> <code>int</code> <code>1</code> Logging frequency <code>log_on_epoch_end</code> <code>bool</code> <code>True</code> Log epoch summaries <code>log_dir</code> <code>str</code> <code>None</code> Local directory for W&amp;B files"},{"location":"user-guide/training/logging/#wb-features","title":"W&amp;B Features","text":"<pre><code># Automatic metric logging\n# All metrics returned by the trainer are logged automatically:\n# - loss, recon_loss, kl_loss (for VAE)\n# - d_loss, g_loss (for GAN)\n# - perplexity, accuracy (for autoregressive models)\n\n# Hyperparameter tracking\nconfig = WandbLoggerConfig(\n    project=\"my-project\",\n    config={\n        \"model_type\": \"VAE\",\n        \"latent_dim\": 128,\n        \"hidden_dims\": [64, 128, 256],\n        \"learning_rate\": 1e-4,\n        \"batch_size\": 64,\n        \"optimizer\": \"adam\",\n    },\n)\n\n# Run comparison\n# W&amp;B automatically enables comparing runs via:\n# - Parallel coordinates plots\n# - Hyperparameter importance analysis\n# - Custom visualizations\n</code></pre>"},{"location":"user-guide/training/logging/#wb-installation","title":"W&amp;B Installation","text":"<pre><code>pip install wandb\n\n# First-time setup\nwandb login\n</code></pre>"},{"location":"user-guide/training/logging/#tensorboard-integration","title":"TensorBoard Integration","text":""},{"location":"user-guide/training/logging/#configuration_1","title":"Configuration","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    TensorBoardLoggerCallback,\n    TensorBoardLoggerConfig,\n)\n\nconfig = TensorBoardLoggerConfig(\n    log_dir=\"logs/tensorboard/experiment-1\",  # Directory for logs\n    flush_secs=120,                            # Flush interval (seconds)\n    max_queue=10,                              # Max queued events\n    log_every_n_steps=1,                       # Logging frequency\n    log_on_epoch_end=True,                     # Log epoch summaries\n    log_graph=False,                           # Log model graph (experimental)\n)\n\ncallback = TensorBoardLoggerCallback(config)\n</code></pre>"},{"location":"user-guide/training/logging/#tensorboardloggerconfig-parameters","title":"TensorBoardLoggerConfig Parameters","text":"Parameter Type Default Description <code>log_dir</code> <code>str</code> <code>\"logs/tensorboard\"</code> Directory for TensorBoard logs <code>flush_secs</code> <code>int</code> <code>120</code> Flush to disk interval (seconds) <code>max_queue</code> <code>int</code> <code>10</code> Maximum queued events <code>log_every_n_steps</code> <code>int</code> <code>1</code> Logging frequency <code>log_on_epoch_end</code> <code>bool</code> <code>True</code> Log epoch summaries <code>log_graph</code> <code>bool</code> <code>False</code> Log model graph (experimental)"},{"location":"user-guide/training/logging/#viewing-tensorboard-logs","title":"Viewing TensorBoard Logs","text":"<pre><code># Start TensorBoard server\ntensorboard --logdir logs/tensorboard\n\n# View in browser at http://localhost:6006\n</code></pre>"},{"location":"user-guide/training/logging/#tensorboard-installation","title":"TensorBoard Installation","text":"<pre><code>pip install tensorboard\n</code></pre>"},{"location":"user-guide/training/logging/#progress-bar-callback","title":"Progress Bar Callback","text":""},{"location":"user-guide/training/logging/#configuration_2","title":"Configuration","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    ProgressBarCallback,\n    ProgressBarConfig,\n)\n\nconfig = ProgressBarConfig(\n    refresh_rate=10,      # Refresh every N steps\n    show_eta=True,        # Show estimated time remaining\n    show_metrics=True,    # Display metrics in progress bar\n    leave=True,           # Keep progress bar after completion\n    disable=False,        # Disable progress bar entirely\n)\n\ncallback = ProgressBarCallback(config)\n</code></pre>"},{"location":"user-guide/training/logging/#progressbarconfig-parameters","title":"ProgressBarConfig Parameters","text":"Parameter Type Default Description <code>refresh_rate</code> <code>int</code> <code>10</code> Refresh frequency (steps) <code>show_eta</code> <code>bool</code> <code>True</code> Show estimated time remaining <code>show_metrics</code> <code>bool</code> <code>True</code> Display metrics inline <code>leave</code> <code>bool</code> <code>True</code> Keep bar after completion <code>disable</code> <code>bool</code> <code>False</code> Disable progress bar"},{"location":"user-guide/training/logging/#progress-bar-installation","title":"Progress Bar Installation","text":"<pre><code>pip install rich\n</code></pre>"},{"location":"user-guide/training/logging/#generic-logger-callback","title":"Generic Logger Callback","text":"<p>For custom logging backends, use the base <code>LoggerCallback</code>:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    LoggerCallback,\n    LoggerCallbackConfig,\n)\nfrom artifex.generative_models.utils.logging import ConsoleLogger\n\n# Create a custom logger\nlogger = ConsoleLogger(name=\"training\")\n\n# Wrap in callback\nconfig = LoggerCallbackConfig(\n    log_every_n_steps=10,\n    log_on_epoch_end=True,\n    prefix=\"train/\",  # Prefix for metric names\n)\ncallback = LoggerCallback(logger=logger, config=config)\n</code></pre>"},{"location":"user-guide/training/logging/#complete-training-example","title":"Complete Training Example","text":"<pre><code>import jax\nimport optax\nfrom flax import nnx\n\nfrom artifex.generative_models.training import Trainer, TrainingConfig\nfrom artifex.generative_models.training.callbacks import (\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n    TensorBoardLoggerCallback,\n    TensorBoardLoggerConfig,\n    ProgressBarCallback,\n    ProgressBarConfig,\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n)\nfrom artifex.generative_models.training.trainers import VAETrainer, VAETrainingConfig\n\n\ndef train_vae_with_logging(\n    model: nnx.Module,\n    train_data,\n    val_data,\n    num_epochs: int = 100,\n):\n    \"\"\"Train VAE with comprehensive logging.\"\"\"\n\n    # Setup optimizer\n    optimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n    # VAE-specific trainer\n    vae_config = VAETrainingConfig(\n        kl_annealing=\"cyclical\",\n        beta=4.0,\n        free_bits=0.25,\n    )\n    vae_trainer = VAETrainer(model, optimizer, vae_config)\n\n    # Logging callbacks\n    callbacks = [\n        # W&amp;B for experiment tracking\n        WandbLoggerCallback(WandbLoggerConfig(\n            project=\"vae-training\",\n            name=\"vae-experiment\",\n            config={\n                \"kl_annealing\": \"cyclical\",\n                \"beta\": 4.0,\n                \"free_bits\": 0.25,\n                \"learning_rate\": 1e-4,\n            },\n            log_every_n_steps=50,\n        )),\n\n        # TensorBoard for local visualization\n        TensorBoardLoggerCallback(TensorBoardLoggerConfig(\n            log_dir=\"logs/vae-experiment\",\n            log_every_n_steps=50,\n        )),\n\n        # Progress bar for real-time feedback\n        ProgressBarCallback(ProgressBarConfig(\n            show_metrics=True,\n            refresh_rate=10,\n        )),\n\n        # Early stopping\n        EarlyStopping(EarlyStoppingConfig(\n            monitor=\"val_loss\",\n            patience=10,\n            min_delta=1e-4,\n        )),\n\n        # Checkpointing\n        ModelCheckpoint(CheckpointConfig(\n            dirpath=\"checkpoints/vae\",\n            monitor=\"val_loss\",\n            save_top_k=3,\n        )),\n    ]\n\n    # Training configuration\n    training_config = TrainingConfig(\n        num_epochs=num_epochs,\n        batch_size=64,\n    )\n\n    # Create base trainer with VAE loss function\n    trainer = Trainer(\n        model=model,\n        training_config=training_config,\n        loss_fn=vae_trainer.create_loss_fn(step=0),\n        callbacks=callbacks,\n    )\n\n    # Train\n    trainer.train(train_data, val_data)\n\n    return model\n</code></pre>"},{"location":"user-guide/training/logging/#metric-types","title":"Metric Types","text":""},{"location":"user-guide/training/logging/#automatically-logged-metrics","title":"Automatically Logged Metrics","text":"<p>Different trainers log different metrics:</p> <p>VAE Trainer:</p> <ul> <li><code>loss</code>: Total ELBO loss</li> <li><code>recon_loss</code>: Reconstruction loss</li> <li><code>kl_loss</code>: KL divergence</li> <li><code>kl_weight</code>: Current KL annealing weight</li> </ul> <p>GAN Trainer:</p> <ul> <li><code>d_loss</code>: Discriminator loss</li> <li><code>g_loss</code>: Generator loss</li> <li><code>d_real</code>: Mean discriminator output on real data</li> <li><code>d_fake</code>: Mean discriminator output on fake data</li> </ul> <p>Diffusion Trainer:</p> <ul> <li><code>loss</code>: Weighted diffusion loss</li> <li><code>loss_unweighted</code>: Unweighted MSE loss</li> </ul> <p>Flow Trainer:</p> <ul> <li><code>loss</code>: Flow matching loss</li> </ul> <p>Energy Trainer:</p> <ul> <li><code>loss</code>: Contrastive divergence loss</li> <li><code>energy_data</code>: Mean energy on data</li> <li><code>energy_neg</code>: Mean energy on negatives</li> <li><code>energy_gap</code>: Energy gap (neg - data)</li> </ul> <p>Autoregressive Trainer:</p> <ul> <li><code>loss</code>: Cross-entropy loss</li> <li><code>perplexity</code>: exp(loss)</li> <li><code>accuracy</code>: Token prediction accuracy</li> <li><code>teacher_forcing_prob</code>: Current teacher forcing probability</li> </ul>"},{"location":"user-guide/training/logging/#custom-metrics","title":"Custom Metrics","text":"<p>Add custom metrics by returning them from your loss function:</p> <pre><code>def custom_loss_fn(model, batch, rng):\n    loss = compute_loss(model, batch)\n\n    # Return additional metrics\n    metrics = {\n        \"loss\": loss,\n        \"custom_metric_1\": value1,\n        \"custom_metric_2\": value2,\n    }\n    return loss, metrics\n</code></pre>"},{"location":"user-guide/training/logging/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/training/logging/#1-logging-frequency","title":"1. Logging Frequency","text":"<pre><code># For fast training loops (&gt;100 steps/sec)\nlog_every_n_steps=100\n\n# For slow training loops (&lt;10 steps/sec)\nlog_every_n_steps=1\n\n# For validation metrics\nlog_on_epoch_end=True\n</code></pre>"},{"location":"user-guide/training/logging/#2-organizing-runs","title":"2. Organizing Runs","text":"<pre><code># Use meaningful tags\nconfig = WandbLoggerConfig(\n    project=\"my-project\",\n    tags=[\n        \"model:vae\",\n        \"dataset:cifar10\",\n        \"experiment:ablation\",\n    ],\n)\n\n# Use descriptive names\nconfig = WandbLoggerConfig(\n    name=f\"vae-beta{beta}-lr{lr}-{timestamp}\",\n)\n</code></pre>"},{"location":"user-guide/training/logging/#3-hyperparameter-tracking","title":"3. Hyperparameter Tracking","text":"<pre><code># Log all relevant hyperparameters\nconfig = WandbLoggerConfig(\n    config={\n        # Model architecture\n        \"latent_dim\": 128,\n        \"hidden_dims\": [64, 128, 256],\n\n        # Training\n        \"learning_rate\": 1e-4,\n        \"batch_size\": 64,\n        \"optimizer\": \"adam\",\n\n        # VAE-specific\n        \"beta\": 4.0,\n        \"kl_annealing\": \"cyclical\",\n        \"free_bits\": 0.25,\n    },\n)\n</code></pre>"},{"location":"user-guide/training/logging/#4-multiple-loggers","title":"4. Multiple Loggers","text":"<pre><code># Use multiple loggers for different purposes\ncallbacks = [\n    # W&amp;B for long-term tracking and comparison\n    WandbLoggerCallback(WandbLoggerConfig(\n        project=\"my-project\",\n        log_every_n_steps=100,\n    )),\n\n    # TensorBoard for quick local visualization\n    TensorBoardLoggerCallback(TensorBoardLoggerConfig(\n        log_every_n_steps=10,\n    )),\n\n    # Progress bar for real-time feedback\n    ProgressBarCallback(),\n]\n</code></pre>"},{"location":"user-guide/training/logging/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Guide - Core training patterns</li> <li>Advanced Features - Gradient accumulation, mixed precision</li> <li>Profiling - Performance analysis</li> </ul>"},{"location":"user-guide/training/overview/","title":"Training System Overview","text":"<p>Artifex provides a robust training system built on JAX/Flax NNX, working towards production-ready status. The training infrastructure handles the complete training lifecycle, from model initialization to checkpointing and evaluation.</p> <ul> <li> <p> Easy to Use</p> <p>Simple, intuitive API for training any generative model with sensible defaults</p> </li> <li> <p> Highly Configurable</p> <p>Type-safe configuration system with Pydantic validation and YAML support</p> </li> <li> <p> Flexible</p> <p>Support for custom training loops, optimizers, and learning rate schedules</p> </li> <li> <p> Research-Focused</p> <p>Checkpointing, resuming, logging, and metrics tracking built-in for experimentation</p> </li> </ul>"},{"location":"user-guide/training/overview/#training-architecture","title":"Training Architecture","text":"<p>The Artifex training system follows a modular, composable architecture:</p> <pre><code>graph TB\n    subgraph \"Training Configuration\"\n        TC[TrainingConfig]\n        OC[OptimizerConfig]\n        SC[SchedulerConfig]\n\n        TC --&gt;|contains| OC\n        TC --&gt;|optional| SC\n    end\n\n    subgraph \"Trainer\"\n        T[Trainer]\n        TS[TrainingState]\n        OPT[Optimizer]\n\n        T --&gt;|manages| TS\n        T --&gt;|uses| OPT\n    end\n\n    subgraph \"Training Loop\"\n        STEP[train_step]\n        VAL[validate_step]\n        EPOCH[train_epoch]\n        EVAL[evaluate]\n\n        EPOCH --&gt;|calls| STEP\n        EPOCH --&gt;|periodic| VAL\n        EVAL --&gt;|uses| VAL\n    end\n\n    subgraph \"Persistence\"\n        CKPT[Checkpointing]\n        LOG[Logging]\n        METRICS[MetricsLogger]\n\n        T --&gt;|saves| CKPT\n        T --&gt;|writes| LOG\n        T --&gt;|tracks| METRICS\n    end\n\n    TC --&gt;|configures| T\n    T --&gt;|executes| EPOCH\n    STEP --&gt;|updates| TS\n    CKPT --&gt;|saves| TS</code></pre>"},{"location":"user-guide/training/overview/#core-components","title":"Core Components","text":""},{"location":"user-guide/training/overview/#1-trainer","title":"1. Trainer","text":"<p>The <code>Trainer</code> class is the central component for training generative models:</p> <pre><code>from artifex.generative_models.training import Trainer\nfrom artifex.generative_models.core.configuration import (\n    TrainingConfig,\n    OptimizerConfig,\n)\n\n# Create optimizer configuration\noptimizer_config = OptimizerConfig(\n    name=\"adam_optimizer\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n    gradient_clip_norm=1.0,\n)\n\n# Create training configuration\ntraining_config = TrainingConfig(\n    name=\"vae_training\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=optimizer_config,\n    save_frequency=1000,\n    log_frequency=100,\n)\n\n# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    train_data_loader=train_loader,\n    val_data_loader=val_loader,\n    workdir=\"./experiments/vae\",\n)\n</code></pre> <p>Key Features:</p> <ul> <li>Type-Safe Configuration: Uses Pydantic-based configuration with validation</li> <li>Automatic Setup: Handles optimizer initialization, state management, and checkpointing</li> <li>Flexible Training: Supports custom loss functions and training loops</li> <li>Built-in Logging: Integrates with Artifex's logging system and external trackers</li> <li>Checkpointing: Automatic saving and loading of training state</li> </ul>"},{"location":"user-guide/training/overview/#2-trainingstate","title":"2. TrainingState","text":"<p>The <code>TrainingState</code> is a PyTree that holds all training state:</p> <pre><code>from artifex.generative_models.training.trainer import TrainingState\n\nstate = TrainingState.create(\n    params=model_params,\n    opt_state=optimizer.init(model_params),\n    rng=jax.random.PRNGKey(42),\n    step=0,\n    best_loss=float(\"inf\"),\n)\n</code></pre> <p>Components:</p> <ul> <li><code>params</code>: Model parameters</li> <li><code>opt_state</code>: Optimizer state</li> <li><code>rng</code>: JAX random number generator</li> <li><code>step</code>: Current training step</li> <li><code>best_loss</code>: Best validation loss (for early stopping)</li> </ul> <p>Benefits:</p> <ul> <li>JAX-compatible PyTree structure</li> <li>Easy to save/load with checkpointing</li> <li>Immutable updates for functional programming</li> <li>Integrates with JAX transformations (jit, grad, etc.)</li> </ul>"},{"location":"user-guide/training/overview/#3-configuration-system","title":"3. Configuration System","text":"<p>Artifex uses a unified, type-safe configuration system based on Pydantic:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    TrainingConfig,\n    OptimizerConfig,\n    SchedulerConfig,\n)\n\n# Optimizer configuration\noptimizer = OptimizerConfig(\n    name=\"adamw_optimizer\",\n    optimizer_type=\"adamw\",\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    beta1=0.9,\n    beta2=0.999,\n)\n\n# Learning rate scheduler\nscheduler = SchedulerConfig(\n    name=\"cosine_scheduler\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    cycle_length=10000,\n    min_lr_ratio=0.1,\n)\n\n# Complete training configuration\ntraining_config = TrainingConfig(\n    name=\"diffusion_training\",\n    batch_size=64,\n    num_epochs=200,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    gradient_clip_norm=1.0,\n    save_frequency=5000,\n)\n</code></pre> <p>Advantages:</p> <ul> <li>Type Safety: Pydantic validates all fields at creation</li> <li>IDE Support: Full autocompletion and type checking</li> <li>Serialization: Easy YAML/JSON save/load</li> <li>Validation: Built-in constraints and custom validators</li> <li>Documentation: Self-documenting with field descriptions</li> </ul> <p>See Configuration Guide for complete details.</p>"},{"location":"user-guide/training/overview/#training-loop-mechanics","title":"Training Loop Mechanics","text":""},{"location":"user-guide/training/overview/#basic-training-flow","title":"Basic Training Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Trainer\n    participant Model\n    participant Optimizer\n    participant Storage\n\n    User-&gt;&gt;Trainer: trainer.train_epoch()\n\n    loop For each batch\n        Trainer-&gt;&gt;Model: compute loss\n        Model--&gt;&gt;Trainer: loss + metrics\n        Trainer-&gt;&gt;Trainer: compute gradients\n        Trainer-&gt;&gt;Optimizer: update parameters\n        Optimizer--&gt;&gt;Trainer: new parameters\n\n        alt Step % save_frequency == 0\n            Trainer-&gt;&gt;Storage: save checkpoint\n        end\n\n        alt Step % log_frequency == 0\n            Trainer-&gt;&gt;Storage: log metrics\n        end\n    end\n\n    Trainer--&gt;&gt;User: epoch metrics</code></pre>"},{"location":"user-guide/training/overview/#training-step","title":"Training Step","text":"<p>The core training step is JIT-compiled for performance:</p> <pre><code>def _train_step(state, batch):\n    \"\"\"Single training step (JIT-compiled).\"\"\"\n    rng, step_rng = jax.random.split(state[\"rng\"])\n\n    # Define loss function\n    def loss_fn(params):\n        loss, metrics = model.loss_fn(params, batch, step_rng)\n        return loss, metrics\n\n    # Compute gradients\n    (loss, metrics), grads = jax.value_and_grad(\n        loss_fn, has_aux=True\n    )(state[\"params\"])\n\n    # Update parameters\n    updates, opt_state = optimizer.update(\n        grads, state[\"opt_state\"], state[\"params\"]\n    )\n    params = optax.apply_updates(state[\"params\"], updates)\n\n    # Create new state\n    new_state = {\n        \"step\": state[\"step\"] + 1,\n        \"params\": params,\n        \"opt_state\": opt_state,\n        \"rng\": rng,\n    }\n\n    return new_state, metrics\n</code></pre> <p>Key Points:</p> <ol> <li>Functional Style: Pure function with immutable updates</li> <li>JIT Compilation: Compiled once, runs fast</li> <li>RNG Splitting: Proper random number handling</li> <li>Gradient Computation: Uses <code>jax.value_and_grad</code> for efficiency</li> <li>State Updates: Returns new state (immutable)</li> </ol>"},{"location":"user-guide/training/overview/#validation-step","title":"Validation Step","text":"<p>Validation uses the same loss function without updates:</p> <pre><code>def _validate_step(state, batch):\n    \"\"\"Single validation step.\"\"\"\n    _, val_rng = jax.random.split(state[\"rng\"])\n\n    # Compute validation loss (no gradients)\n    loss, metrics = model.loss_fn(state[\"params\"], batch, val_rng)\n    metrics[\"loss\"] = loss\n\n    return metrics\n</code></pre>"},{"location":"user-guide/training/overview/#epoch-training","title":"Epoch Training","text":"<p>An epoch iterates over the entire dataset:</p> <pre><code>def train_epoch(trainer):\n    \"\"\"Train for one epoch.\"\"\"\n    data_iter = trainer.train_data_loader(trainer.training_config.batch_size)\n    epoch_metrics = []\n\n    for _ in range(trainer.steps_per_epoch):\n        batch = next(data_iter)\n\n        # Training step\n        trainer.state, metrics = trainer.train_step_fn(trainer.state, batch)\n        epoch_metrics.append(metrics)\n\n        # Periodic checkpointing\n        if trainer.state[\"step\"] % trainer.training_config.save_frequency == 0:\n            trainer.save_checkpoint()\n\n    # Average metrics\n    avg_metrics = {\n        key: sum(m[key] for m in epoch_metrics) / len(epoch_metrics)\n        for key in epoch_metrics[0].keys()\n        if key != \"step\"\n    }\n\n    return avg_metrics\n</code></pre>"},{"location":"user-guide/training/overview/#checkpointing","title":"Checkpointing","text":""},{"location":"user-guide/training/overview/#automatic-checkpointing","title":"Automatic Checkpointing","text":"<p>Checkpoints are automatically saved during training:</p> <pre><code># Configure checkpointing\ntraining_config = TrainingConfig(\n    name=\"my_training\",\n    batch_size=32,\n    num_epochs=100,\n    optimizer=optimizer_config,\n    checkpoint_dir=\"./checkpoints\",\n    save_frequency=1000,  # Save every 1000 steps\n    max_checkpoints=5,    # Keep last 5 checkpoints\n)\n\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n)\n\n# Training automatically saves checkpoints\ntrainer.train_epoch()\n</code></pre>"},{"location":"user-guide/training/overview/#manual-checkpointing","title":"Manual Checkpointing","text":"<p>Save and load checkpoints manually:</p> <pre><code># Save checkpoint\ntrainer.save_checkpoint(\"./checkpoints/my_checkpoint.pkl\")\n\n# Load checkpoint\ntrainer.load_checkpoint(\"./checkpoints/my_checkpoint.pkl\")\n\n# Resume training\ntrainer.train_epoch()  # Continues from loaded state\n</code></pre>"},{"location":"user-guide/training/overview/#checkpoint-contents","title":"Checkpoint Contents","text":"<p>Each checkpoint contains the complete training state:</p> <pre><code>{\n    \"step\": 5000,\n    \"params\": {...},      # Model parameters\n    \"opt_state\": {...},   # Optimizer state\n    \"rng\": Array(...),    # RNG state\n}\n</code></pre> <p>Best Practices:</p> <ul> <li>Save checkpoints to fast storage (SSD) for quick I/O</li> <li>Use <code>max_checkpoints</code> to limit disk usage</li> <li>Save best model separately based on validation metrics</li> <li>Include step number in checkpoint filenames</li> <li>Test checkpoint loading before long training runs</li> </ul>"},{"location":"user-guide/training/overview/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"user-guide/training/overview/#built-in-logging","title":"Built-in Logging","text":"<p>Artifex includes structured logging:</p> <pre><code>from artifex.generative_models.utils.logging import Logger, MetricsLogger\n\n# Create loggers\nlogger = Logger(log_dir=\"./logs\")\nmetrics_logger = MetricsLogger(log_dir=\"./logs/metrics\")\n\n# Initialize trainer with loggers\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    logger=logger,\n    metrics_logger=metrics_logger,\n)\n\n# Logs are written automatically during training\ntrainer.train_epoch()\n</code></pre>"},{"location":"user-guide/training/overview/#custom-logging-callbacks","title":"Custom Logging Callbacks","text":"<p>Implement custom logging with callbacks:</p> <pre><code>def custom_log_callback(step, metrics, prefix=\"train\"):\n    \"\"\"Custom logging function.\"\"\"\n    print(f\"[{prefix}] Step {step}: Loss = {metrics['loss']:.4f}\")\n\n    # Log to external system (e.g., wandb, tensorboard)\n    if wandb_enabled:\n        wandb.log({f\"{prefix}/{k}\": v for k, v in metrics.items()}, step=step)\n\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    log_callback=custom_log_callback,\n)\n</code></pre>"},{"location":"user-guide/training/overview/#metrics-tracking","title":"Metrics Tracking","text":"<p>The trainer automatically tracks:</p> <ul> <li>Training Loss: Loss value for each batch</li> <li>Validation Loss: Periodic validation metrics</li> <li>Learning Rate: Current learning rate (with schedulers)</li> <li>Gradient Norms: L2 norm of gradients</li> <li>Model-Specific Metrics: KL divergence, reconstruction loss, etc.</li> </ul> <p>Example metrics access:</p> <pre><code># Train for an epoch\nmetrics = trainer.train_epoch()\n\nprint(f\"Epoch loss: {metrics['loss']:.4f}\")\n\n# Access training history\nfor step, metric in enumerate(trainer.train_metrics):\n    print(f\"Step {step}: {metric['loss']:.4f}\")\n\n# Validation metrics\nval_metrics = trainer.evaluate(val_data, batch_size=64)\nprint(f\"Validation loss: {val_metrics['loss']:.4f}\")\n</code></pre>"},{"location":"user-guide/training/overview/#optimizers","title":"Optimizers","text":"<p>Artifex supports multiple optimizers through Optax:</p>"},{"location":"user-guide/training/overview/#available-optimizers","title":"Available Optimizers","text":"Optimizer Best For Key Parameters Adam General purpose, most models <code>learning_rate</code>, <code>beta1</code>, <code>beta2</code> AdamW Transformers, weight decay needed <code>learning_rate</code>, <code>weight_decay</code> SGD Large batch training, momentum <code>learning_rate</code>, <code>momentum</code> RMSProp RNNs, non-stationary objectives <code>learning_rate</code>, <code>decay</code> AdaGrad Sparse gradients, NLP <code>learning_rate</code>"},{"location":"user-guide/training/overview/#optimizer-configuration","title":"Optimizer Configuration","text":"<pre><code># Adam optimizer\nadam_config = OptimizerConfig(\n    name=\"adam\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n    beta1=0.9,\n    beta2=0.999,\n    eps=1e-8,\n)\n\n# AdamW with weight decay\nadamw_config = OptimizerConfig(\n    name=\"adamw\",\n    optimizer_type=\"adamw\",\n    learning_rate=3e-4,\n    weight_decay=0.01,\n)\n\n# SGD with momentum\nsgd_config = OptimizerConfig(\n    name=\"sgd\",\n    optimizer_type=\"sgd\",\n    learning_rate=0.1,\n    momentum=0.9,\n    nesterov=True,\n)\n</code></pre>"},{"location":"user-guide/training/overview/#gradient-clipping","title":"Gradient Clipping","text":"<p>Prevent gradient explosion with clipping:</p> <pre><code># Clip by global norm (recommended)\noptimizer_config = OptimizerConfig(\n    name=\"clipped_adam\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n    gradient_clip_norm=1.0,  # Clip to norm of 1.0\n)\n\n# Clip by value\noptimizer_config = OptimizerConfig(\n    name=\"value_clipped_adam\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n    gradient_clip_value=0.5,  # Clip values to [-0.5, 0.5]\n)\n</code></pre>"},{"location":"user-guide/training/overview/#learning-rate-schedules","title":"Learning Rate Schedules","text":""},{"location":"user-guide/training/overview/#available-schedules","title":"Available Schedules","text":"Schedule Description Use Case Constant Fixed learning rate Simple training, debugging Linear Linear decay Short training runs Cosine Cosine annealing Most deep learning (recommended) Exponential Exponential decay Traditional ML Step Step-wise decay Milestone-based training MultiStep Multiple milestones Fine-grained control"},{"location":"user-guide/training/overview/#schedule-configuration","title":"Schedule Configuration","text":"<pre><code># Cosine schedule with warmup (recommended)\ncosine_schedule = SchedulerConfig(\n    name=\"cosine_warmup\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    cycle_length=50000,\n    min_lr_ratio=0.1,  # End at 10% of initial LR\n)\n\n# Linear schedule\nlinear_schedule = SchedulerConfig(\n    name=\"linear_decay\",\n    scheduler_type=\"linear\",\n    warmup_steps=500,\n    total_steps=10000,\n    min_lr_ratio=0.0,  # Decay to 0\n)\n\n# Step schedule\nstep_schedule = SchedulerConfig(\n    name=\"step_decay\",\n    scheduler_type=\"step\",\n    step_size=5000,  # Decay every 5000 steps\n    gamma=0.1,       # Multiply LR by 0.1\n)\n\n# MultiStep schedule\nmultistep_schedule = SchedulerConfig(\n    name=\"multistep\",\n    scheduler_type=\"multistep\",\n    milestones=[10000, 20000, 30000],\n    gamma=0.1,\n)\n</code></pre>"},{"location":"user-guide/training/overview/#schedule-visualization","title":"Schedule Visualization","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_schedule(scheduler_config, base_lr=1e-3, total_steps=10000):\n    \"\"\"Visualize learning rate schedule.\"\"\"\n    schedule = create_schedule(scheduler_config, base_lr)\n    steps = np.arange(total_steps)\n    lrs = [schedule(step) for step in steps]\n\n    plt.figure(figsize=(10, 4))\n    plt.plot(steps, lrs)\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Learning Rate\")\n    plt.title(f\"{scheduler_config.scheduler_type} Schedule\")\n    plt.grid(True)\n    plt.show()\n\n# Visualize cosine schedule\nvisualize_schedule(cosine_schedule)\n</code></pre>"},{"location":"user-guide/training/overview/#evaluation","title":"Evaluation","text":"<p>Evaluate your trained model with validation data and modality-specific metrics:</p> <pre><code># Basic validation during training\nval_metrics = trainer.evaluate(val_data, batch_size=128)\nprint(f\"Validation loss: {val_metrics['loss']:.4f}\")\n\n# Model-specific metrics\nif hasattr(model, 'reconstruction_loss'):\n    print(f\"Reconstruction loss: {val_metrics['reconstruction_loss']:.4f}\")\n    print(f\"KL divergence: {val_metrics['kl_loss']:.4f}\")\n</code></pre> <p>For comprehensive evaluation with FID, Inception Score, and other metrics, see the Benchmarks documentation.</p>"},{"location":"user-guide/training/overview/#complete-training-example","title":"Complete Training Example","text":"<p>Here's a complete training workflow:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    ModelConfig,\n    TrainingConfig,\n    OptimizerConfig,\n    SchedulerConfig,\n)\nfrom artifex.generative_models.factory import create_model\nfrom artifex.generative_models.training import Trainer\nfrom flax import nnx\n\n# 1. Create model configuration\nmodel_config = ModelConfig(\n    name=\"vae_mnist\",\n    model_class=\"artifex.generative_models.models.vae.base.VAE\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[512, 256],\n    output_dim=64,\n    parameters={\"beta\": 1.0},\n)\n\n# 2. Initialize model\nrngs = nnx.Rngs(42)\nmodel = create_model(config=model_config, rngs=rngs)\n\n# 3. Configure optimizer\noptimizer_config = OptimizerConfig(\n    name=\"adamw\",\n    optimizer_type=\"adamw\",\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    gradient_clip_norm=1.0,\n)\n\n# 4. Configure learning rate schedule\nscheduler_config = SchedulerConfig(\n    name=\"cosine_warmup\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,\n    cycle_length=50000,\n    min_lr_ratio=0.1,\n)\n\n# 5. Create training configuration\ntraining_config = TrainingConfig(\n    name=\"vae_training\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=optimizer_config,\n    scheduler=scheduler_config,\n    save_frequency=5000,\n    log_frequency=100,\n    checkpoint_dir=\"./checkpoints/vae\",\n)\n\n# 6. Initialize trainer\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    train_data_loader=train_loader,\n    val_data_loader=val_loader,\n    workdir=\"./experiments/vae\",\n)\n\n# 7. Train\nfor epoch in range(training_config.num_epochs):\n    # Train epoch\n    train_metrics = trainer.train_epoch()\n    print(f\"Epoch {epoch + 1}: Train Loss = {train_metrics['loss']:.4f}\")\n\n    # Validate\n    val_metrics = trainer.evaluate(val_data, batch_size=128)\n    print(f\"Epoch {epoch + 1}: Val Loss = {val_metrics['loss']:.4f}\")\n\n    # Save best model\n    if val_metrics['loss'] &lt; trainer.state.get('best_loss', float('inf')):\n        trainer.save_checkpoint(f\"./checkpoints/vae/best_model.pkl\")\n\n# 8. Generate samples\nsamples = trainer.generate_samples(num_samples=16)\n</code></pre>"},{"location":"user-guide/training/overview/#key-design-principles","title":"Key Design Principles","text":""},{"location":"user-guide/training/overview/#1-functional-programming","title":"1. Functional Programming","text":"<p>The training system uses functional programming principles:</p> <ul> <li>Immutable Updates: States are never modified in-place</li> <li>Pure Functions: Training steps are deterministic</li> <li>JAX Transformations: Compatible with jit, grad, vmap, pmap</li> </ul> <pre><code># Immutable state updates\nnew_state = {**old_state, \"step\": old_state[\"step\"] + 1}\n\n# Pure function (same inputs \u2192 same outputs)\n@jax.jit\ndef train_step(state, batch):\n    # No side effects\n    return new_state, metrics\n</code></pre>"},{"location":"user-guide/training/overview/#2-type-safety","title":"2. Type Safety","text":"<p>All configurations use Pydantic for type safety:</p> <pre><code># Type-safe configuration\nconfig = TrainingConfig(\n    name=\"my_training\",\n    batch_size=32,        # int: validated\n    num_epochs=100,       # int: validated\n    optimizer=opt_config, # OptimizerConfig: validated\n)\n\n# Invalid configuration raises error at creation\ntry:\n    bad_config = TrainingConfig(\n        name=\"bad\",\n        batch_size=\"invalid\",  # TypeError: not an int\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>"},{"location":"user-guide/training/overview/#3-composability","title":"3. Composability","text":"<p>Components are designed to be composable:</p> <pre><code># Compose optimizer with gradient clipping\noptimizer = optax.chain(\n    optax.clip_by_global_norm(1.0),\n    optax.adam(learning_rate=1e-3),\n)\n\n# Compose learning rate schedules\nschedule = optax.join_schedules(\n    schedules=[warmup_schedule, cosine_schedule],\n    boundaries=[warmup_steps],\n)\n\n# Compose training callbacks\ndef composed_callback(step, metrics, prefix=\"train\"):\n    log_to_file(step, metrics, prefix)\n    log_to_wandb(step, metrics, prefix)\n    update_progress_bar(step, metrics)\n</code></pre>"},{"location":"user-guide/training/overview/#4-jax-first-design","title":"4. JAX-First Design","text":"<p>Leverages JAX for performance and scalability:</p> <ul> <li>JIT Compilation: Training steps are JIT-compiled</li> <li>Automatic Differentiation: Gradients computed with <code>jax.grad</code></li> <li>Device Agnostic: Runs on CPU, GPU, or TPU</li> <li>Parallelization: Ready for data and model parallelism</li> </ul>"},{"location":"user-guide/training/overview/#summary","title":"Summary","text":"<p>The Artifex training system provides:</p> <ul> <li>\u2705 Type-Safe Configuration: Pydantic-based with validation</li> <li>\u2705 Flexible Training: Custom loops, optimizers, and schedules</li> <li>\u2705 Research-Ready Features: Checkpointing, logging, monitoring</li> <li>\u2705 High Performance: JIT compilation and JAX optimizations</li> <li>\u2705 Easy to Use: Simple API with sensible defaults</li> <li>\u2705 Well-Tested: Comprehensive test coverage</li> </ul>"},{"location":"user-guide/training/overview/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Training Guide</p> <p>Practical guide with examples for common training scenarios</p> </li> <li> <p> Configuration Guide</p> <p>Deep dive into the configuration system and best practices</p> </li> <li> <p> Trainer API</p> <p>Complete API reference for the Trainer class</p> </li> </ul> <p>Continue to the Training Guide for practical examples and advanced patterns.</p>"},{"location":"user-guide/training/profiling/","title":"Performance Profiling","text":"<p>This guide covers profiling tools in Artifex for analyzing and optimizing training performance, including JAX trace profiling and memory tracking.</p>"},{"location":"user-guide/training/profiling/#overview","title":"Overview","text":"<p>Artifex provides two profiling callbacks:</p> <ul> <li>JAXProfiler: Captures JAX execution traces for visualization in TensorBoard/Perfetto</li> <li>MemoryProfiler: Tracks GPU/TPU memory usage over time</li> </ul> <p>These tools help identify performance bottlenecks, optimize memory usage, and understand training dynamics.</p>"},{"location":"user-guide/training/profiling/#jax-trace-profiling","title":"JAX Trace Profiling","text":""},{"location":"user-guide/training/profiling/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    JAXProfiler,\n    ProfilingConfig,\n)\n\n# Create profiler callback\nprofiler = JAXProfiler(ProfilingConfig(\n    log_dir=\"logs/profiles\",\n    start_step=10,  # Skip warmup/JIT compilation\n    end_step=20,    # Profile 10 steps\n))\n\n# Add to trainer callbacks\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    callbacks=[profiler],\n)\n</code></pre>"},{"location":"user-guide/training/profiling/#configuration","title":"Configuration","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    JAXProfiler,\n    ProfilingConfig,\n)\n\nconfig = ProfilingConfig(\n    log_dir=\"logs/profiles\",    # Directory for trace files\n    start_step=10,              # Step to start profiling\n    end_step=20,                # Step to stop profiling\n    trace_memory=True,          # Include memory in traces\n    trace_python=False,         # Trace Python execution (slower)\n)\n\nprofiler = JAXProfiler(config)\n</code></pre>"},{"location":"user-guide/training/profiling/#profilingconfig-parameters","title":"ProfilingConfig Parameters","text":"Parameter Type Default Description <code>log_dir</code> <code>str</code> <code>\"logs/profiles\"</code> Directory for trace files <code>start_step</code> <code>int</code> <code>10</code> Step to start profiling <code>end_step</code> <code>int</code> <code>20</code> Step to stop profiling <code>trace_memory</code> <code>bool</code> <code>True</code> Include memory in traces <code>trace_python</code> <code>bool</code> <code>False</code> Trace Python (slower, more detail)"},{"location":"user-guide/training/profiling/#viewing-traces","title":"Viewing Traces","text":""},{"location":"user-guide/training/profiling/#tensorboard","title":"TensorBoard","text":"<pre><code># Start TensorBoard\ntensorboard --logdir logs/profiles\n\n# Navigate to \"Profile\" tab in browser\n</code></pre>"},{"location":"user-guide/training/profiling/#perfetto","title":"Perfetto","text":"<ol> <li>Open ui.perfetto.dev in your browser</li> <li>Click \"Open trace file\"</li> <li>Select the <code>.perfetto-trace</code> file from your log directory</li> </ol>"},{"location":"user-guide/training/profiling/#understanding-traces","title":"Understanding Traces","text":"<p>The trace shows:</p> <ul> <li>XLA Compilation: Time spent compiling JAX programs</li> <li>Kernel Execution: Time spent running operations on GPU/TPU</li> <li>Memory Allocation: When and how much memory is allocated</li> <li>Data Transfer: Host-to-device and device-to-host transfers</li> </ul>"},{"location":"user-guide/training/profiling/#common-patterns-to-look-for","title":"Common Patterns to Look For","text":"<p>Good patterns:</p> <ul> <li>Most time in kernel execution</li> <li>Minimal data transfers</li> <li>Steady memory usage</li> </ul> <p>Potential issues:</p> <ul> <li>Repeated XLA compilation (missing JIT)</li> <li>Frequent host-device transfers</li> <li>Memory spikes indicating inefficient allocation</li> </ul>"},{"location":"user-guide/training/profiling/#profiling-best-practices","title":"Profiling Best Practices","text":""},{"location":"user-guide/training/profiling/#1-skip-warmup","title":"1. Skip Warmup","text":"<pre><code>config = ProfilingConfig(\n    start_step=10,  # Skip first 10 steps (JIT compilation)\n    end_step=20,\n)\n</code></pre> <p>The first few steps include JIT compilation overhead. Skip them to get representative performance data.</p>"},{"location":"user-guide/training/profiling/#2-profile-short-windows","title":"2. Profile Short Windows","text":"<pre><code>config = ProfilingConfig(\n    start_step=100,\n    end_step=110,  # Only 10 steps\n)\n</code></pre> <p>Profiling is expensive. Keep the window short (10-20 steps) for manageable trace files.</p>"},{"location":"user-guide/training/profiling/#3-profile-representative-workloads","title":"3. Profile Representative Workloads","text":"<pre><code># Profile at different batch sizes\nfor batch_size in [32, 64, 128]:\n    config = ProfilingConfig(\n        log_dir=f\"logs/profiles/batch_{batch_size}\",\n        start_step=10,\n        end_step=20,\n    )\n    # Run training...\n</code></pre>"},{"location":"user-guide/training/profiling/#memory-profiling","title":"Memory Profiling","text":""},{"location":"user-guide/training/profiling/#quick-start_1","title":"Quick Start","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    MemoryProfiler,\n    MemoryProfileConfig,\n)\n\n# Create memory profiler\nprofiler = MemoryProfiler(MemoryProfileConfig(\n    log_dir=\"logs/memory\",\n    profile_every_n_steps=100,\n))\n\n# Add to trainer callbacks\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    callbacks=[profiler],\n)\n</code></pre>"},{"location":"user-guide/training/profiling/#configuration_1","title":"Configuration","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    MemoryProfiler,\n    MemoryProfileConfig,\n)\n\nconfig = MemoryProfileConfig(\n    log_dir=\"logs/memory\",           # Directory for memory profile\n    profile_every_n_steps=100,       # Frequency of memory checks\n    log_device_memory=True,          # Log GPU/TPU memory stats\n)\n\nprofiler = MemoryProfiler(config)\n</code></pre>"},{"location":"user-guide/training/profiling/#memoryprofileconfig-parameters","title":"MemoryProfileConfig Parameters","text":"Parameter Type Default Description <code>log_dir</code> <code>str</code> <code>\"logs/memory\"</code> Directory for profile output <code>profile_every_n_steps</code> <code>int</code> <code>100</code> Profiling frequency <code>log_device_memory</code> <code>bool</code> <code>True</code> Track device memory"},{"location":"user-guide/training/profiling/#output-format","title":"Output Format","text":"<p>Memory profiles are saved as JSON:</p> <pre><code>[\n  {\n    \"step\": 100,\n    \"memory\": {\n      \"cuda:0\": {\n        \"bytes_in_use\": 1073741824,\n        \"peak_bytes_in_use\": 1610612736\n      }\n    }\n  },\n  {\n    \"step\": 200,\n    \"memory\": {\n      \"cuda:0\": {\n        \"bytes_in_use\": 1073741824,\n        \"peak_bytes_in_use\": 1610612736\n      }\n    }\n  }\n]\n</code></pre>"},{"location":"user-guide/training/profiling/#analyzing-memory-profiles","title":"Analyzing Memory Profiles","text":"<pre><code>import json\nimport matplotlib.pyplot as plt\n\n# Load profile\nwith open(\"logs/memory/memory_profile.json\") as f:\n    profile = json.load(f)\n\n# Extract data\nsteps = [p[\"step\"] for p in profile]\nmemory = [p[\"memory\"][\"cuda:0\"][\"bytes_in_use\"] / 1e9 for p in profile]  # GB\npeak_memory = [p[\"memory\"][\"cuda:0\"][\"peak_bytes_in_use\"] / 1e9 for p in profile]\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(steps, memory, label=\"Current\")\nplt.plot(steps, peak_memory, label=\"Peak\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Memory (GB)\")\nplt.legend()\nplt.title(\"GPU Memory Usage\")\nplt.savefig(\"memory_plot.png\")\n</code></pre>"},{"location":"user-guide/training/profiling/#complete-profiling-example","title":"Complete Profiling Example","text":"<pre><code>from artifex.generative_models.training import Trainer, TrainingConfig\nfrom artifex.generative_models.training.callbacks import (\n    JAXProfiler,\n    ProfilingConfig,\n    MemoryProfiler,\n    MemoryProfileConfig,\n    ProgressBarCallback,\n)\n\n\ndef profile_training(model, train_data, num_steps=1000):\n    \"\"\"Profile a training run.\"\"\"\n\n    callbacks = [\n        # JAX trace profiling (steps 100-110)\n        JAXProfiler(ProfilingConfig(\n            log_dir=\"logs/profiles/trace\",\n            start_step=100,\n            end_step=110,\n        )),\n\n        # Memory profiling (every 50 steps)\n        MemoryProfiler(MemoryProfileConfig(\n            log_dir=\"logs/profiles/memory\",\n            profile_every_n_steps=50,\n        )),\n\n        # Progress bar for feedback\n        ProgressBarCallback(),\n    ]\n\n    trainer = Trainer(\n        model=model,\n        training_config=TrainingConfig(num_epochs=1),\n        callbacks=callbacks,\n    )\n\n    trainer.train(train_data)\n\n    print(\"Profiling complete!\")\n    print(\"- Trace: logs/profiles/trace/\")\n    print(\"- Memory: logs/profiles/memory/memory_profile.json\")\n</code></pre>"},{"location":"user-guide/training/profiling/#common-performance-issues","title":"Common Performance Issues","text":""},{"location":"user-guide/training/profiling/#1-excessive-jit-compilation","title":"1. Excessive JIT Compilation","text":"<p>Symptom: Slow first few steps, traces show XLA compilation</p> <p>Solution:</p> <pre><code># Ensure functions are JIT-compiled once\n@jax.jit\ndef train_step(model, batch, key):\n    ...\n    return loss, metrics\n\n# Don't use Python control flow that changes trace\n# Bad: changes trace every step\nif step % 10 == 0:\n    # Different computation path\n\n# Good: use jax.lax.cond for conditional logic\n</code></pre>"},{"location":"user-guide/training/profiling/#2-memory-leaks","title":"2. Memory Leaks","text":"<p>Symptom: Memory usage increases over time</p> <p>Solution:</p> <pre><code># Check for accumulating state\n# Bad: accumulating in Python list\nall_losses = []\nfor step in range(num_steps):\n    loss = train_step(...)\n    all_losses.append(loss)  # Memory leak!\n\n# Good: aggregate in-place or periodically\nrunning_loss = 0.0\nfor step in range(num_steps):\n    loss = train_step(...)\n    running_loss += float(loss)  # Convert to Python float\n</code></pre>"},{"location":"user-guide/training/profiling/#3-data-transfer-bottlenecks","title":"3. Data Transfer Bottlenecks","text":"<p>Symptom: Long gaps between kernel executions in traces</p> <p>Solution:</p> <pre><code># Pre-transfer data to device\ndata = jax.device_put(data)\n\n# Use asynchronous data loading\n# Prefetch next batch while current batch processes\n</code></pre>"},{"location":"user-guide/training/profiling/#4-inefficient-batch-size","title":"4. Inefficient Batch Size","text":"<p>Symptom: Low GPU utilization</p> <p>Solution:</p> <pre><code># Profile with different batch sizes\nfor batch_size in [32, 64, 128, 256]:\n    # Compare throughput and memory usage\n    ...\n\n# Use gradient accumulation for effective larger batches\nfrom artifex.generative_models.training import GradientAccumulator\n</code></pre>"},{"location":"user-guide/training/profiling/#manual-profiling","title":"Manual Profiling","text":"<p>For custom profiling beyond the callbacks:</p> <pre><code>import jax\n\n# Manual trace profiling\nwith jax.profiler.trace(\"logs/manual_profile\"):\n    for step in range(10):\n        loss = train_step(model, batch, key)\n        jax.block_until_ready(loss)  # Ensure completion\n\n# Check device memory\nfor device in jax.devices():\n    stats = device.memory_stats()\n    if stats:\n        print(f\"{device}: {stats['bytes_in_use'] / 1e9:.2f} GB\")\n\n# Profile specific operations\nwith jax.profiler.StepTraceAnnotation(\"forward_pass\"):\n    logits = model(inputs)\n\nwith jax.profiler.StepTraceAnnotation(\"backward_pass\"):\n    grads = jax.grad(loss_fn)(model)\n</code></pre>"},{"location":"user-guide/training/profiling/#integration-with-other-callbacks","title":"Integration with Other Callbacks","text":"<p>Profiling callbacks work alongside other callbacks:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    JAXProfiler,\n    ProfilingConfig,\n    MemoryProfiler,\n    MemoryProfileConfig,\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n)\n\ncallbacks = [\n    # Profiling\n    JAXProfiler(ProfilingConfig(start_step=100, end_step=110)),\n    MemoryProfiler(MemoryProfileConfig(profile_every_n_steps=100)),\n\n    # Logging\n    WandbLoggerCallback(WandbLoggerConfig(project=\"my-project\")),\n\n    # Training control\n    EarlyStopping(EarlyStoppingConfig(patience=10)),\n    ModelCheckpoint(CheckpointConfig(dirpath=\"checkpoints\")),\n]\n</code></pre>"},{"location":"user-guide/training/profiling/#related-documentation","title":"Related Documentation","text":"<ul> <li>Logging - Experiment tracking and visualization</li> <li>Training Guide - Core training patterns</li> <li>Advanced Features - Gradient accumulation, mixed precision</li> <li>Distributed Training - Multi-device training</li> </ul>"},{"location":"user-guide/training/rl-training/","title":"Reinforcement Learning Training","text":"<p>This guide covers reinforcement learning (RL) training in Artifex for fine-tuning generative models using reward signals. RL training enables optimization of models beyond standard likelihood-based objectives, allowing alignment with human preferences, aesthetic quality, or domain-specific metrics.</p>"},{"location":"user-guide/training/rl-training/#overview","title":"Overview","text":"<p>Artifex provides a comprehensive RL training module with four main trainers:</p> Trainer Use Case Memory Best For REINFORCE Simple policy gradients Low Baselines, simple rewards PPO Proximal Policy Optimization Medium Stable training, complex tasks GRPO Group Relative Policy Optimization Low Large models, memory-constrained DPO Direct Preference Optimization Low Preference learning, no reward model"},{"location":"user-guide/training/rl-training/#when-to-use-rl-training","title":"When to Use RL Training","text":"<p>RL training is particularly effective for:</p> <ul> <li>Diffusion Models: Fine-tuning for aesthetic quality, text-image alignment (CLIP scores), or domain-specific attributes</li> <li>GANs: Using discriminator feedback as rewards for generator improvement</li> <li>VAEs: Optimizing reconstruction quality or latent space properties</li> <li>Flow Models: Improving sample quality beyond maximum likelihood</li> </ul>"},{"location":"user-guide/training/rl-training/#reinforce-trainer","title":"REINFORCE Trainer","text":"<p>REINFORCE implements the basic policy gradient algorithm with variance reduction through baseline subtraction.</p>"},{"location":"user-guide/training/rl-training/#configuration","title":"Configuration","text":"<pre><code>from artifex.generative_models.training import REINFORCEConfig, REINFORCETrainer\n\nconfig = REINFORCEConfig(\n    gamma=0.99,              # Discount factor for returns\n    normalize_returns=True,  # Normalize returns for stability\n    entropy_coeff=0.01,      # Entropy bonus for exploration\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#reinforceconfig-parameters","title":"REINFORCEConfig Parameters","text":"Parameter Type Default Description <code>gamma</code> <code>float</code> <code>0.99</code> Discount factor for computing returns <code>normalize_returns</code> <code>bool</code> <code>True</code> Whether to normalize returns to zero mean and unit variance <code>entropy_coeff</code> <code>float</code> <code>0.01</code> Coefficient for entropy bonus (encourages exploration)"},{"location":"user-guide/training/rl-training/#basic-usage","title":"Basic Usage","text":"<pre><code>from flax import nnx\nimport optax\nfrom artifex.generative_models.training import (\n    REINFORCEConfig,\n    REINFORCETrainer,\n)\n\n# Setup model and optimizer\nmodel = YourPolicyModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n# Create trainer\nconfig = REINFORCEConfig(gamma=0.99, entropy_coeff=0.01)\ntrainer = REINFORCETrainer(model, optimizer, config)\n\n# Training step\nbatch = {\n    \"observations\": observations,  # State observations\n    \"actions\": actions,            # Actions taken\n    \"rewards\": rewards,            # Rewards received\n}\nmetrics = trainer.train_step(batch)\nprint(f\"Policy loss: {metrics['policy_loss']:.4f}\")\n</code></pre>"},{"location":"user-guide/training/rl-training/#how-it-works","title":"How It Works","text":"<p>REINFORCE computes the policy gradient:</p> \\[\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t \\right]\\] <p>Where \\(G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k\\) is the discounted return.</p> <p>Key features:</p> <ul> <li>Returns are computed using <code>compute_discounted_returns()</code> from shared utilities</li> <li>Optional return normalization reduces variance</li> <li>Entropy bonus encourages exploration and prevents premature convergence</li> </ul>"},{"location":"user-guide/training/rl-training/#ppo-trainer","title":"PPO Trainer","text":"<p>Proximal Policy Optimization (PPO) provides stable training through clipped surrogate objectives and value function learning.</p>"},{"location":"user-guide/training/rl-training/#configuration_1","title":"Configuration","text":"<pre><code>from artifex.generative_models.training import PPOConfig, PPOTrainer\n\nconfig = PPOConfig(\n    gamma=0.99,           # Discount factor\n    gae_lambda=0.95,      # GAE lambda for advantage estimation\n    clip_param=0.2,       # PPO clipping parameter\n    vf_coeff=0.5,         # Value function loss coefficient\n    entropy_coeff=0.01,   # Entropy bonus coefficient\n    max_grad_norm=0.5,    # Gradient clipping norm\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#ppoconfig-parameters","title":"PPOConfig Parameters","text":"Parameter Type Default Description <code>gamma</code> <code>float</code> <code>0.99</code> Discount factor for GAE computation <code>gae_lambda</code> <code>float</code> <code>0.95</code> Lambda for Generalized Advantage Estimation <code>clip_param</code> <code>float</code> <code>0.2</code> Clipping parameter epsilon for surrogate loss <code>vf_coeff</code> <code>float</code> <code>0.5</code> Coefficient for value function loss <code>entropy_coeff</code> <code>float</code> <code>0.01</code> Coefficient for entropy bonus <code>max_grad_norm</code> <code>float</code> <code>0.5</code> Maximum gradient norm for clipping"},{"location":"user-guide/training/rl-training/#training-with-ppo","title":"Training with PPO","text":"<pre><code>from flax import nnx\nimport optax\nfrom artifex.generative_models.training import PPOConfig, PPOTrainer\n\n# Actor-critic model with separate heads\nmodel = ActorCriticModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(3e-4), wrt=nnx.Param)\n\nconfig = PPOConfig(\n    gamma=0.99,\n    gae_lambda=0.95,\n    clip_param=0.2,\n    vf_coeff=0.5,\n    entropy_coeff=0.01,\n)\ntrainer = PPOTrainer(model, optimizer, config)\n\n# Collect trajectory\ntrajectory = {\n    \"observations\": observations,\n    \"actions\": actions,\n    \"rewards\": rewards,\n    \"values\": values,           # Value estimates V(s)\n    \"log_probs\": old_log_probs, # Log probs from old policy\n    \"dones\": dones,             # Episode termination flags\n}\n\n# Train on trajectory\nmetrics = trainer.train_step(trajectory)\nprint(f\"Policy loss: {metrics['policy_loss']:.4f}\")\nprint(f\"Value loss: {metrics['value_loss']:.4f}\")\nprint(f\"Entropy: {metrics['entropy']:.4f}\")\n</code></pre>"},{"location":"user-guide/training/rl-training/#generalized-advantage-estimation","title":"Generalized Advantage Estimation","text":"<p>PPO uses GAE for variance-reduced advantage estimation:</p> \\[A_t^{GAE} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}\\] <p>Where \\(\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\) is the TD residual.</p> <p>Benefits of GAE:</p> <ul> <li>Balances bias-variance trade-off via \\(\\lambda\\) parameter</li> <li>\\(\\lambda = 0\\) gives TD(0) (low variance, high bias)</li> <li>\\(\\lambda = 1\\) gives Monte Carlo returns (high variance, low bias)</li> <li>\\(\\lambda = 0.95\\) is a good default for most applications</li> </ul>"},{"location":"user-guide/training/rl-training/#grpo-trainer","title":"GRPO Trainer","text":"<p>Group Relative Policy Optimization (GRPO) is a critic-free RL algorithm that normalizes advantages within groups of generations. This approach, pioneered by DeepSeek, provides approximately 50% memory savings compared to PPO by eliminating the value network.</p>"},{"location":"user-guide/training/rl-training/#configuration_2","title":"Configuration","text":"<pre><code>from artifex.generative_models.training import GRPOConfig, GRPOTrainer\n\nconfig = GRPOConfig(\n    num_generations=4,    # Generations per prompt (group size)\n    clip_param=0.2,       # PPO-style clipping\n    beta=0.01,            # KL penalty coefficient\n    entropy_coeff=0.01,   # Entropy bonus\n    gamma=0.99,           # Discount factor\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#grpoconfig-parameters","title":"GRPOConfig Parameters","text":"Parameter Type Default Description <code>num_generations</code> <code>int</code> <code>4</code> Number of generations per prompt (group size G) <code>clip_param</code> <code>float</code> <code>0.2</code> PPO-style clipping parameter <code>beta</code> <code>float</code> <code>0.01</code> KL divergence penalty coefficient <code>entropy_coeff</code> <code>float</code> <code>0.01</code> Entropy bonus coefficient <code>gamma</code> <code>float</code> <code>0.99</code> Discount factor"},{"location":"user-guide/training/rl-training/#how-grpo-works","title":"How GRPO Works","text":"<p>GRPO eliminates the critic by normalizing rewards within groups:</p> <ol> <li>Generate G samples per prompt: For each prompt, generate multiple completions</li> <li>Compute rewards: Evaluate each generation with a reward function</li> <li>Normalize within groups: Compute advantages as \\((r - \\mu_g) / \\sigma_g\\) within each group</li> <li>Apply PPO-style clipping: Use the normalized advantages with clipped surrogate loss</li> </ol> <pre><code>from flax import nnx\nimport optax\nfrom artifex.generative_models.training import GRPOConfig, GRPOTrainer\n\n# Policy model (no value head needed!)\nmodel = PolicyModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\nconfig = GRPOConfig(\n    num_generations=4,  # Generate 4 samples per prompt\n    clip_param=0.2,\n    beta=0.01,\n)\ntrainer = GRPOTrainer(model, optimizer, config)\n\n# Batch with grouped generations\n# If num_prompts=8 and num_generations=4, batch_size=32\nbatch = {\n    \"observations\": observations,  # Shape: (batch_size, ...)\n    \"actions\": actions,            # Shape: (batch_size, ...)\n    \"rewards\": rewards,            # Shape: (batch_size,)\n    \"log_probs\": old_log_probs,    # Shape: (batch_size,)\n    \"group_size\": 4,               # Optional, defaults to config\n}\n\nmetrics = trainer.train_step(batch)\nprint(f\"Policy loss: {metrics['policy_loss']:.4f}\")\nprint(f\"Approx KL: {metrics['approx_kl']:.4f}\")\n</code></pre>"},{"location":"user-guide/training/rl-training/#advantages-of-grpo","title":"Advantages of GRPO","text":"<ol> <li>Memory Efficient: No value network means ~50% memory savings</li> <li>Simple Implementation: No need to train a critic</li> <li>Effective for Generative Models: Naturally fits the \"generate multiple samples\" paradigm</li> <li>Stable Training: Group normalization provides consistent advantage scaling</li> </ol>"},{"location":"user-guide/training/rl-training/#kl-divergence-penalty","title":"KL Divergence Penalty","text":"<p>GRPO can optionally include a KL penalty to prevent the policy from diverging too far from a reference:</p> <pre><code># With reference model for KL penalty\nbatch = {\n    \"observations\": observations,\n    \"actions\": actions,\n    \"rewards\": rewards,\n    \"log_probs\": old_log_probs,\n    \"ref_log_probs\": ref_log_probs,  # From frozen reference model\n}\n\n# KL penalty is automatically applied when ref_log_probs is provided\nmetrics = trainer.train_step(batch)\nprint(f\"KL penalty: {metrics.get('kl_penalty', 0):.4f}\")\n</code></pre>"},{"location":"user-guide/training/rl-training/#dpo-trainer","title":"DPO Trainer","text":"<p>Direct Preference Optimization (DPO) learns from preference pairs without requiring an explicit reward model or RL optimization loop.</p>"},{"location":"user-guide/training/rl-training/#configuration_3","title":"Configuration","text":"<pre><code>from artifex.generative_models.training import DPOConfig, DPOTrainer\n\nconfig = DPOConfig(\n    beta=0.1,              # Reward scaling temperature\n    label_smoothing=0.0,   # Smoothing for preference labels\n    reference_free=False,  # Enable SimPO mode\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#dpoconfig-parameters","title":"DPOConfig Parameters","text":"Parameter Type Default Description <code>beta</code> <code>float</code> <code>0.1</code> Temperature parameter for reward scaling <code>label_smoothing</code> <code>float</code> <code>0.0</code> Label smoothing for robustness <code>reference_free</code> <code>bool</code> <code>False</code> Use SimPO (reference-free) mode"},{"location":"user-guide/training/rl-training/#standard-dpo-training","title":"Standard DPO Training","text":"<pre><code>from flax import nnx\nimport optax\nfrom artifex.generative_models.training import DPOConfig, DPOTrainer\n\nmodel = PolicyModel(rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(1e-5), wrt=nnx.Param)\n\nconfig = DPOConfig(beta=0.1)\ntrainer = DPOTrainer(model, optimizer, config)\n\n# Preference pairs: chosen (preferred) vs rejected\nbatch = {\n    \"chosen_log_probs\": chosen_log_probs,      # Log probs for preferred\n    \"rejected_log_probs\": rejected_log_probs,  # Log probs for rejected\n    \"ref_chosen_log_probs\": ref_chosen,        # Reference model log probs\n    \"ref_rejected_log_probs\": ref_rejected,\n}\n\nmetrics = trainer.train_step(batch)\nprint(f\"DPO loss: {metrics['dpo_loss']:.4f}\")\nprint(f\"Reward accuracy: {metrics['reward_accuracy']:.2%}\")\n</code></pre>"},{"location":"user-guide/training/rl-training/#simpo-reference-free-dpo","title":"SimPO: Reference-Free DPO","text":"<p>SimPO eliminates the need for a reference model by using length-normalized log probabilities:</p> <pre><code>config = DPOConfig(\n    beta=0.1,\n    reference_free=True,  # Enable SimPO mode\n)\ntrainer = DPOTrainer(model, optimizer, config)\n\n# No reference log probs needed\nbatch = {\n    \"chosen_log_probs\": chosen_log_probs,\n    \"rejected_log_probs\": rejected_log_probs,\n}\n\nmetrics = trainer.train_step(batch)\n</code></pre>"},{"location":"user-guide/training/rl-training/#how-dpo-works","title":"How DPO Works","text":"<p>DPO directly optimizes the Bradley-Terry preference model:</p> \\[\\mathcal{L}_{DPO} = -\\log \\sigma\\left(\\beta \\left[ \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right]\\right)\\] <p>Where:</p> <ul> <li>\\(y_w\\) is the preferred (chosen) response</li> <li>\\(y_l\\) is the rejected response</li> <li>\\(\\pi_{ref}\\) is the reference policy</li> <li>\\(\\beta\\) controls the implicit reward scaling</li> </ul>"},{"location":"user-guide/training/rl-training/#reward-functions","title":"Reward Functions","text":"<p>Artifex provides a flexible reward function interface for custom reward computation.</p>"},{"location":"user-guide/training/rl-training/#built-in-reward-functions","title":"Built-in Reward Functions","text":"<pre><code>from artifex.generative_models.training import (\n    ConstantReward,\n    CompositeReward,\n    ThresholdReward,\n    ScaledReward,\n    ClippedReward,\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#constantreward","title":"ConstantReward","text":"<p>Returns a fixed reward value (useful for testing):</p> <pre><code>reward_fn = ConstantReward(value=1.0)\nrewards = reward_fn(samples)  # Returns array of 1.0\n</code></pre>"},{"location":"user-guide/training/rl-training/#compositereward","title":"CompositeReward","text":"<p>Combines multiple reward functions with weights:</p> <pre><code>reward_fn = CompositeReward(\n    reward_fns=[aesthetic_reward, clip_reward],\n    weights=[0.3, 0.7],  # 30% aesthetic, 70% CLIP\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#thresholdreward","title":"ThresholdReward","text":"<p>Applies a threshold to convert scores to binary rewards:</p> <pre><code># Reward = 1.0 if base_reward &gt; 0.5 else 0.0\nreward_fn = ThresholdReward(\n    reward_fn=base_reward,\n    threshold=0.5,\n    above_value=1.0,\n    below_value=0.0,\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#scaledreward","title":"ScaledReward","text":"<p>Scales rewards by a constant factor:</p> <pre><code>reward_fn = ScaledReward(reward_fn=base_reward, scale=10.0)\n</code></pre>"},{"location":"user-guide/training/rl-training/#clippedreward","title":"ClippedReward","text":"<p>Clips rewards to a specified range:</p> <pre><code>reward_fn = ClippedReward(\n    reward_fn=base_reward,\n    min_value=-1.0,\n    max_value=1.0,\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#custom-reward-functions","title":"Custom Reward Functions","text":"<p>Implement the <code>RewardFunction</code> protocol for custom rewards:</p> <pre><code>from typing import Protocol\nimport jax\nimport jax.numpy as jnp\n\nclass RewardFunction(Protocol):\n    \"\"\"Protocol for reward function implementations.\"\"\"\n\n    def __call__(\n        self,\n        samples: jax.Array,\n        conditions: jax.Array | None = None,\n        **kwargs,\n    ) -&gt; jax.Array:\n        \"\"\"Compute rewards for generated samples.\n\n        Args:\n            samples: Generated samples to evaluate.\n            conditions: Optional conditioning information.\n\n        Returns:\n            Reward values with shape (batch_size,).\n        \"\"\"\n        ...\n\n# Example: CLIP-based reward\nclass CLIPReward:\n    def __init__(self, clip_model, target_text):\n        self.clip_model = clip_model\n        self.target_text = target_text\n        self.target_embedding = clip_model.encode_text(target_text)\n\n    def __call__(self, samples, conditions=None, **kwargs):\n        image_embeddings = self.clip_model.encode_image(samples)\n        # Cosine similarity as reward\n        similarity = jnp.sum(\n            image_embeddings * self.target_embedding,\n            axis=-1,\n        )\n        return similarity\n\n\n# Example: Multi-objective reward with learnable weights\nclass MultiObjectiveReward:\n    \"\"\"Complex reward combining multiple objectives with adaptive weighting.\"\"\"\n\n    def __init__(\n        self,\n        clip_model,\n        aesthetic_model,\n        safety_classifier,\n        target_text: str,\n        clip_weight: float = 0.4,\n        aesthetic_weight: float = 0.3,\n        safety_weight: float = 0.3,\n    ):\n        self.clip_model = clip_model\n        self.aesthetic_model = aesthetic_model\n        self.safety_classifier = safety_classifier\n        self.target_embedding = clip_model.encode_text(target_text)\n\n        # Weights can be tuned during training\n        self.weights = {\n            \"clip\": clip_weight,\n            \"aesthetic\": aesthetic_weight,\n            \"safety\": safety_weight,\n        }\n\n    def __call__(self, samples, conditions=None, **kwargs):\n        batch_size = samples.shape[0]\n\n        # CLIP alignment score (cosine similarity)\n        image_emb = self.clip_model.encode_image(samples)\n        clip_score = jnp.sum(image_emb * self.target_embedding, axis=-1)\n\n        # Aesthetic quality score (normalized to [0, 1])\n        aesthetic_score = self.aesthetic_model(samples)\n        aesthetic_score = jax.nn.sigmoid(aesthetic_score)\n\n        # Safety score (1.0 = safe, 0.0 = unsafe)\n        safety_logits = self.safety_classifier(samples)\n        safety_score = jax.nn.softmax(safety_logits, axis=-1)[:, 0]  # P(safe)\n\n        # Combine with penalty for unsafe content\n        combined_reward = (\n            self.weights[\"clip\"] * clip_score +\n            self.weights[\"aesthetic\"] * aesthetic_score +\n            self.weights[\"safety\"] * safety_score\n        )\n\n        # Apply safety penalty: zero reward for unsafe samples\n        safety_mask = safety_score &gt; 0.5\n        combined_reward = jnp.where(safety_mask, combined_reward, -1.0)\n\n        return combined_reward\n\n\n# Example: Sequence-level reward for text generation\nclass SequenceReward:\n    \"\"\"Reward function for autoregressive text models.\"\"\"\n\n    def __init__(self, reward_model, tokenizer):\n        self.reward_model = reward_model\n        self.tokenizer = tokenizer\n\n    def __call__(self, samples, conditions=None, **kwargs):\n        # samples: token IDs of shape (batch, seq_len)\n        # Compute reward at sequence level\n        rewards = self.reward_model(samples)\n\n        # Apply length penalty to avoid reward hacking\n        lengths = jnp.sum(samples != self.tokenizer.pad_id, axis=-1)\n        length_penalty = jnp.log(lengths + 1) / jnp.log(100)  # Normalize\n\n        return rewards - 0.1 * length_penalty\n</code></pre>"},{"location":"user-guide/training/rl-training/#utility-functions","title":"Utility Functions","text":"<p>The RL module provides shared utility functions following the DRY principle:</p> <pre><code>from artifex.generative_models.training.rl.utils import (\n    compute_discounted_returns,\n    compute_gae_advantages,\n    normalize_advantages,\n    compute_policy_entropy,\n    compute_kl_divergence,\n    compute_clipped_surrogate_loss,\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#compute_discounted_returns","title":"compute_discounted_returns","text":"<pre><code># Compute G_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\nreturns = compute_discounted_returns(rewards, gamma=0.99)\n</code></pre>"},{"location":"user-guide/training/rl-training/#compute_gae_advantages","title":"compute_gae_advantages","text":"<pre><code># GAE with bias-variance trade-off\nadvantages = compute_gae_advantages(\n    rewards=rewards,\n    values=values,        # V(s) estimates including V(s_T+1)\n    dones=dones,          # Episode termination flags\n    gamma=0.99,\n    gae_lambda=0.95,\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#normalize_advantages","title":"normalize_advantages","text":"<pre><code># Zero mean, unit variance normalization\nnormalized = normalize_advantages(advantages, eps=1e-8)\n</code></pre>"},{"location":"user-guide/training/rl-training/#compute_policy_entropy","title":"compute_policy_entropy","text":"<pre><code># H = -sum(p * log(p))\nentropy = compute_policy_entropy(log_probs)\n</code></pre>"},{"location":"user-guide/training/rl-training/#compute_kl_divergence","title":"compute_kl_divergence","text":"<pre><code># KL(policy || reference)\nkl = compute_kl_divergence(policy_log_probs, ref_log_probs)\n</code></pre>"},{"location":"user-guide/training/rl-training/#compute_clipped_surrogate_loss","title":"compute_clipped_surrogate_loss","text":"<pre><code># PPO clipped objective\nloss = compute_clipped_surrogate_loss(\n    log_probs=current_log_probs,\n    old_log_probs=old_log_probs,\n    advantages=advantages,\n    clip_param=0.2,\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#integration-with-model-specific-trainers","title":"Integration with Model-Specific Trainers","text":"<p>RL trainers integrate with Artifex's model-specific trainers for fine-tuning:</p>"},{"location":"user-guide/training/rl-training/#diffusion-model-fine-tuning-with-grpo","title":"Diffusion Model Fine-Tuning with GRPO","text":"<pre><code>from artifex.generative_models.training import GRPOConfig, GRPOTrainer\nfrom artifex.generative_models.training.trainers import DiffusionTrainer\n\n# Standard diffusion trainer for pre-training\ndiffusion_trainer = DiffusionTrainer(model, optimizer, diffusion_config)\n\n# Fine-tune with GRPO\ngrpo_config = GRPOConfig(num_generations=4, beta=0.01)\ngrpo_trainer = GRPOTrainer(model, optimizer, grpo_config)\n\n# Generate samples and compute rewards\nfor batch in dataloader:\n    # Generate multiple samples per condition\n    samples = generate_samples(model, batch[\"conditions\"], num_samples=4)\n    rewards = reward_fn(samples, batch[\"conditions\"])\n\n    # GRPO training step\n    rl_batch = {\n        \"observations\": batch[\"conditions\"],\n        \"actions\": samples,\n        \"rewards\": rewards,\n        \"log_probs\": compute_log_probs(model, samples, batch[\"conditions\"]),\n    }\n    metrics = grpo_trainer.train_step(rl_batch)\n</code></pre>"},{"location":"user-guide/training/rl-training/#vae-latent-space-optimization-with-ppo","title":"VAE Latent Space Optimization with PPO","text":"<pre><code>from artifex.generative_models.training import PPOConfig, PPOTrainer\n\n# Use encoder as policy, decoder as environment\nppo_config = PPOConfig(gamma=0.99, gae_lambda=0.95)\nppo_trainer = PPOTrainer(encoder, optimizer, ppo_config)\n\n# Train encoder to produce latents that decode well\nfor batch in dataloader:\n    # Encoder produces latent \"actions\"\n    latents, log_probs = encoder(batch[\"images\"], return_log_prob=True)\n\n    # Decoder reconstructs, computing \"rewards\"\n    reconstructions = decoder(latents)\n    rewards = compute_reconstruction_reward(batch[\"images\"], reconstructions)\n\n    trajectory = {\n        \"observations\": batch[\"images\"],\n        \"actions\": latents,\n        \"rewards\": rewards,\n        \"values\": value_estimates,\n        \"log_probs\": log_probs,\n        \"dones\": jnp.zeros(batch_size),\n    }\n    metrics = ppo_trainer.train_step(trajectory)\n</code></pre>"},{"location":"user-guide/training/rl-training/#gan-discriminator-as-reward-with-reinforce","title":"GAN Discriminator as Reward with REINFORCE","text":"<pre><code>from artifex.generative_models.training import REINFORCEConfig, REINFORCETrainer\n\nreinforce_config = REINFORCEConfig(entropy_coeff=0.01)\nreinforce_trainer = REINFORCETrainer(generator, optimizer, reinforce_config)\n\n# Use discriminator output as reward\nfor batch in dataloader:\n    # Generate samples\n    generated = generator(batch[\"noise\"])\n\n    # Discriminator provides reward signal\n    rewards = discriminator(generated)  # Higher = more realistic\n\n    rl_batch = {\n        \"observations\": batch[\"noise\"],\n        \"actions\": generated,\n        \"rewards\": rewards,\n    }\n    metrics = reinforce_trainer.train_step(rl_batch)\n</code></pre>"},{"location":"user-guide/training/rl-training/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/training/rl-training/#choosing-the-right-algorithm","title":"Choosing the Right Algorithm","text":"Scenario Recommended Algorithm Memory-constrained GRPO (no value network) Stable training needed PPO (clipped updates) Preference data available DPO (no reward model) Simple baseline REINFORCE Large language models GRPO or DPO Image generation GRPO with CLIP reward"},{"location":"user-guide/training/rl-training/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":"<p>REINFORCE:</p> <ul> <li>Start with <code>gamma=0.99</code> for long-horizon tasks</li> <li>Use <code>normalize_returns=True</code> for stability</li> <li><code>entropy_coeff=0.01</code> is a good default</li> </ul> <p>PPO:</p> <ul> <li><code>clip_param=0.2</code> is the standard choice</li> <li><code>gae_lambda=0.95</code> balances bias-variance</li> <li>Multiple epochs per batch (3-10) improve sample efficiency</li> </ul> <p>GRPO:</p> <ul> <li><code>num_generations=4-8</code> typically works well</li> <li>Lower <code>beta</code> (0.001-0.01) for more exploration</li> <li>Higher <code>beta</code> (0.1) to stay close to reference</li> </ul> <p>DPO:</p> <ul> <li><code>beta=0.1</code> is a common starting point</li> <li>Lower <code>beta</code> for stronger preferences</li> <li>Use <code>label_smoothing=0.1</code> for noisy preferences</li> </ul>"},{"location":"user-guide/training/rl-training/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Reward hacking: Models may find unintended ways to maximize rewards</li> <li>Use composite rewards with multiple objectives</li> <li> <p>Monitor sample quality qualitatively</p> </li> <li> <p>Training instability: Large policy updates can destabilize training</p> </li> <li>Use PPO clipping or GRPO's group normalization</li> <li> <p>Apply gradient clipping</p> </li> <li> <p>Forgetting: RL fine-tuning can degrade base model capabilities</p> </li> <li>Use KL penalty (GRPO's <code>beta</code> parameter)</li> <li> <p>Mix RL objectives with supervised loss</p> </li> <li> <p>Sparse rewards: Infrequent rewards make learning difficult</p> </li> <li>Use reward shaping with intermediate signals</li> <li>Consider dense reward proxies</li> </ol>"},{"location":"user-guide/training/rl-training/#api-reference","title":"API Reference","text":"<p>For complete API documentation, see the Trainer API Reference.</p> <p>All RL training components are exported from the main training module:</p> <pre><code>from artifex.generative_models.training import (\n    # Configurations\n    REINFORCEConfig,\n    PPOConfig,\n    GRPOConfig,\n    DPOConfig,\n\n    # Trainers\n    REINFORCETrainer,\n    PPOTrainer,\n    GRPOTrainer,\n    DPOTrainer,\n\n    # Reward functions\n    RewardFunction,\n    ConstantReward,\n    CompositeReward,\n    ThresholdReward,\n    ScaledReward,\n    ClippedReward,\n)\n</code></pre>"},{"location":"user-guide/training/rl-training/#using-callbacks-with-rl-trainers","title":"Using Callbacks with RL Trainers","text":"<p>RL trainers integrate seamlessly with Artifex callbacks for logging, checkpointing, and profiling:</p> <pre><code>from artifex.generative_models.training import GRPOTrainer, GRPOConfig\nfrom artifex.generative_models.training.callbacks import (\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n    JAXProfiler,\n    ProfilingConfig,\n    ProgressBarCallback,\n)\n\n# Configure RL trainer\ngrpo_config = GRPOConfig(\n    num_generations=4,\n    clip_param=0.2,\n    beta=0.01,\n)\ntrainer = GRPOTrainer(model, optimizer, grpo_config)\n\n# Setup callbacks for RL training\ncallbacks = [\n    # Log RL-specific metrics (rewards, advantages, KL divergence)\n    WandbLoggerCallback(WandbLoggerConfig(\n        project=\"rl-finetuning\",\n        name=\"grpo-experiment\",\n        config={\n            \"algorithm\": \"GRPO\",\n            \"num_generations\": 4,\n            \"beta\": 0.01,\n        },\n        log_every_n_steps=10,\n    )),\n\n    # Save best model based on mean reward\n    ModelCheckpoint(CheckpointConfig(\n        dirpath=\"checkpoints/grpo\",\n        monitor=\"mean_reward\",\n        mode=\"max\",  # Higher reward is better\n        save_top_k=1,  # Keep only the best checkpoint\n    )),\n\n    # Profile RL training (useful for debugging generation bottlenecks)\n    JAXProfiler(ProfilingConfig(\n        log_dir=\"logs/rl_profiles\",\n        start_step=50,\n        end_step=60,\n    )),\n\n    # Progress bar with RL metrics\n    ProgressBarCallback(),\n]\n\n# Training loop with callbacks\nfor callback in callbacks:\n    callback.on_train_begin(trainer)\n\nfor step, batch in enumerate(dataloader):\n    # Generate samples and compute rewards\n    metrics = trainer.train_step(batch, reward_fn, key)\n\n    # Log metrics via callbacks\n    for callback in callbacks:\n        callback.on_train_batch_end(trainer, metrics, step)\n\nfor callback in callbacks:\n    callback.on_train_end(trainer)\n</code></pre>"},{"location":"user-guide/training/rl-training/#rl-specific-metrics-logged","title":"RL-Specific Metrics Logged","text":"<p>Different RL trainers log different metrics:</p> Trainer Metrics REINFORCE <code>loss</code>, <code>mean_reward</code>, <code>reward_std</code>, <code>entropy</code> PPO <code>policy_loss</code>, <code>value_loss</code>, <code>mean_reward</code>, <code>advantage_mean</code>, <code>kl_divergence</code> GRPO <code>loss</code>, <code>mean_reward</code>, <code>group_advantage_std</code>, <code>kl_penalty</code> DPO <code>loss</code>, <code>chosen_reward</code>, <code>rejected_reward</code>, <code>reward_margin</code>"},{"location":"user-guide/training/rl-training/#trainer-class-hierarchy","title":"Trainer Class Hierarchy","text":"<p>Artifex uses a hierarchical trainer architecture for flexibility:</p> <pre><code>Trainer (base)\n\u251c\u2500\u2500 VAETrainer        \u2192 ELBO loss, KL annealing, free bits\n\u251c\u2500\u2500 GANTrainer        \u2192 Adversarial training, multiple loss types\n\u251c\u2500\u2500 DiffusionTrainer  \u2192 Denoising, noise scheduling, EMA\n\u251c\u2500\u2500 FlowTrainer       \u2192 Flow matching, OT-CFM, rectified flow\n\u251c\u2500\u2500 EnergyTrainer     \u2192 Contrastive divergence, MCMC sampling\n\u251c\u2500\u2500 AutoregressiveTrainer \u2192 Teacher forcing, scheduled sampling\n\u2514\u2500\u2500 RL Trainers\n    \u251c\u2500\u2500 REINFORCETrainer \u2192 Policy gradient, variance reduction\n    \u251c\u2500\u2500 PPOTrainer       \u2192 Actor-critic, GAE, clipping\n    \u251c\u2500\u2500 GRPOTrainer      \u2192 Critic-free, group normalization\n    \u2514\u2500\u2500 DPOTrainer       \u2192 Preference learning, SimPO mode\n</code></pre> <p>Each model-specific trainer:</p> <ol> <li>Inherits core functionality from the base <code>Trainer</code> class (optimizer management, callbacks, checkpointing)</li> <li>Implements model-specific loss computation via <code>compute_loss()</code> method</li> <li>Provides specialized training utilities (e.g., <code>generate()</code> for autoregressive, <code>sample_negatives()</code> for energy)</li> <li>Exposes model-specific configuration via dataclass configs</li> </ol>"},{"location":"user-guide/training/rl-training/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Guide - Core training patterns and callbacks</li> <li>Advanced Features - Gradient accumulation and loss scaling</li> <li>Logging &amp; Experiment Tracking - W&amp;B, TensorBoard, and progress bar integration</li> <li>Performance Profiling - JAX trace profiling and memory tracking</li> <li>Distributed Training - Multi-device RL training</li> <li>Configuration System - Training configuration options</li> </ul>"},{"location":"user-guide/training/training-guide/","title":"Training Guide","text":"<p>This guide provides practical examples and patterns for training generative models with Artifex. From basic training to advanced techniques, you'll learn how to effectively train models for your specific use case.</p>"},{"location":"user-guide/training/training-guide/#quick-start","title":"Quick Start","text":"<p>The simplest way to train a model:</p> <pre><code>from artifex.generative_models.core.configuration import (\n    ModelConfig,\n    TrainingConfig,\n    OptimizerConfig,\n)\nfrom artifex.generative_models.factory import create_model\nfrom artifex.generative_models.training import Trainer\nfrom flax import nnx\nimport jax.numpy as jnp\n\n# Create model\nmodel_config = ModelConfig(\n    name=\"simple_vae\",\n    model_class=\"artifex.generative_models.models.vae.base.VAE\",\n    input_dim=(28, 28, 1),\n    hidden_dims=[256, 128],\n    output_dim=32,\n)\n\nrngs = nnx.Rngs(42)\nmodel = create_model(config=model_config, rngs=rngs)\n\n# Configure training\noptimizer_config = OptimizerConfig(\n    name=\"adam\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n)\n\ntraining_config = TrainingConfig(\n    name=\"quick_train\",\n    batch_size=128,\n    num_epochs=10,\n    optimizer=optimizer_config,\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    training_config=training_config,\n    train_data_loader=train_loader,\n)\n\n# Train\nfor epoch in range(training_config.num_epochs):\n    metrics = trainer.train_epoch()\n    print(f\"Epoch {epoch + 1}: Loss = {metrics['loss']:.4f}\")\n</code></pre>"},{"location":"user-guide/training/training-guide/#setting-up-training","title":"Setting Up Training","text":""},{"location":"user-guide/training/training-guide/#data-loading","title":"Data Loading","text":"<p>Create efficient data loaders for your models:</p> <pre><code>import numpy as np\nimport jax\nimport jax.numpy as jnp\n\ndef create_data_loader(data, batch_size, shuffle=True):\n    \"\"\"Create a data loader that yields batches.\"\"\"\n    def data_loader(batch_size):\n        num_samples = len(data)\n        num_batches = num_samples // batch_size\n\n        # Shuffle if requested\n        if shuffle:\n            indices = np.random.permutation(num_samples)\n            data_shuffled = jax.tree_map(lambda x: x[indices], data)\n        else:\n            data_shuffled = data\n\n        # Yield batches\n        for i in range(num_batches):\n            batch_start = i * batch_size\n            batch_end = min(batch_start + batch_size, num_samples)\n\n            batch = jax.tree_map(\n                lambda x: x[batch_start:batch_end],\n                data_shuffled\n            )\n            yield batch\n\n    return data_loader\n\n# Example usage with MNIST\nfrom tensorflow.datasets import load\n\n# Load MNIST\nds_train = load('mnist', split='train', as_supervised=True)\nds_val = load('mnist', split='test', as_supervised=True)\n\n# Convert to numpy arrays\ntrain_images = np.array([img for img, _ in ds_train])\ntrain_labels = np.array([label for _, label in ds_train])\n\nval_images = np.array([img for img, _ in ds_val])\nval_labels = np.array([label for _, label in ds_val])\n\n# Normalize to [0, 1]\ntrain_images = train_images.astype(np.float32) / 255.0\nval_images = val_images.astype(np.float32) / 255.0\n\n# Create data dictionaries\ntrain_data = {\"images\": train_images, \"labels\": train_labels}\nval_data = {\"images\": val_images, \"labels\": val_labels}\n\n# Create data loaders\ntrain_loader = create_data_loader(train_data, batch_size=128, shuffle=True)\nval_loader = create_data_loader(val_data, batch_size=128, shuffle=False)\n</code></pre>"},{"location":"user-guide/training/training-guide/#preprocessing","title":"Preprocessing","text":"<p>Apply preprocessing to your data:</p> <pre><code>def preprocess_images(images):\n    \"\"\"Preprocess images for training.\"\"\"\n    # Normalize to [-1, 1]\n    images = (images - 0.5) * 2.0\n\n    # Add channel dimension if needed\n    if images.ndim == 3:\n        images = images[..., None]\n\n    return images\n\ndef dequantize(images, rng):\n    \"\"\"Add uniform noise to discrete images.\"\"\"\n    noise = jax.random.uniform(rng, images.shape, minval=0.0, maxval=1/256.0)\n    return images + noise\n\n# Apply preprocessing\ntrain_images = preprocess_images(train_images)\nval_images = preprocess_images(val_images)\n\n# Apply dequantization during training\ndef train_step_with_dequantization(state, batch, rng):\n    \"\"\"Training step with dequantization.\"\"\"\n    rng, dequant_rng = jax.random.split(rng)\n\n    # Dequantize images\n    images = dequantize(batch[\"images\"], dequant_rng)\n    batch = {**batch, \"images\": images}\n\n    # Regular training step\n    return train_step(state, batch, rng)\n</code></pre>"},{"location":"user-guide/training/training-guide/#model-initialization","title":"Model Initialization","text":"<p>Properly initialize your models:</p> <pre><code>from flax import nnx\nfrom artifex.generative_models.factory import create_model\n\ndef initialize_model(model_config, seed=0):\n    \"\"\"Initialize a model with proper RNG handling.\"\"\"\n    rngs = nnx.Rngs(seed)\n\n    # Create model\n    model = create_model(config=model_config, rngs=rngs)\n\n    # Verify model is initialized\n    dummy_input = jnp.ones((1, *model_config.input_dim))\n\n    try:\n        output = model(dummy_input, rngs=rngs, training=False)\n        print(f\"Model initialized successfully. Output shape: {output.shape}\")\n    except Exception as e:\n        print(f\"Model initialization failed: {e}\")\n        raise\n\n    return model\n\n# Initialize model\nmodel = initialize_model(model_config, seed=42)\n</code></pre>"},{"location":"user-guide/training/training-guide/#custom-training-loops","title":"Custom Training Loops","text":""},{"location":"user-guide/training/training-guide/#basic-custom-loop","title":"Basic Custom Loop","text":"<p>Create a custom training loop for full control:</p> <pre><code>import jax\nimport jax.numpy as jnp\nimport optax\nfrom flax import nnx\n\ndef custom_training_loop(\n    model,\n    train_loader,\n    val_loader,\n    num_epochs,\n    learning_rate=1e-3,\n):\n    \"\"\"Custom training loop with full control.\"\"\"\n    # Create optimizer\n    optimizer = optax.adam(learning_rate)\n    opt_state = optimizer.init(nnx.state(model))\n\n    # Training state\n    rng = jax.random.PRNGKey(0)\n    step = 0\n\n    # Define training step\n    @nnx.jit\n    def train_step(model, opt_state, batch, rng):\n        def loss_fn(model):\n            outputs = model(batch[\"images\"], rngs=nnx.Rngs(rng), training=True)\n            return outputs[\"loss\"], outputs\n\n        # Compute gradients\n        grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n        (loss, outputs), grads = grad_fn(model)\n\n        # Update parameters\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = nnx.apply_updates(model, updates)\n\n        return model, opt_state, loss, outputs\n\n    # Training loop\n    for epoch in range(num_epochs):\n        epoch_losses = []\n\n        # Train epoch\n        for batch in train_loader(batch_size=128):\n            rng, step_rng = jax.random.split(rng)\n\n            model, opt_state, loss, outputs = train_step(\n                model, opt_state, batch, step_rng\n            )\n\n            epoch_losses.append(float(loss))\n            step += 1\n\n            if step % 100 == 0:\n                print(f\"Step {step}: Loss = {loss:.4f}\")\n\n        # Validation\n        val_losses = []\n        for batch in val_loader(batch_size=128):\n            outputs = model(batch[\"images\"], rngs=nnx.Rngs(rng), training=False)\n            val_losses.append(float(outputs[\"loss\"]))\n\n        print(f\"Epoch {epoch + 1}:\")\n        print(f\"  Train Loss: {np.mean(epoch_losses):.4f}\")\n        print(f\"  Val Loss: {np.mean(val_losses):.4f}\")\n\n    return model\n\n# Train with custom loop\nmodel = custom_training_loop(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=10,\n    learning_rate=1e-3,\n)\n</code></pre>"},{"location":"user-guide/training/training-guide/#advanced-custom-loop-with-metrics","title":"Advanced Custom Loop with Metrics","text":"<p>Track detailed metrics during training:</p> <pre><code>from collections import defaultdict\n\ndef advanced_training_loop(\n    model,\n    train_loader,\n    val_loader,\n    num_epochs,\n    optimizer_config,\n    scheduler_config=None,\n):\n    \"\"\"Advanced training loop with metrics tracking.\"\"\"\n    # Create optimizer\n    base_lr = optimizer_config.learning_rate\n\n    if scheduler_config:\n        schedule = create_schedule(scheduler_config, base_lr)\n        optimizer = optax.adam(learning_rate=schedule)\n    else:\n        optimizer = optax.adam(learning_rate=base_lr)\n\n    # Apply gradient clipping if configured\n    if optimizer_config.gradient_clip_norm:\n        optimizer = optax.chain(\n            optax.clip_by_global_norm(optimizer_config.gradient_clip_norm),\n            optimizer,\n        )\n\n    opt_state = optimizer.init(nnx.state(model))\n\n    # Metrics tracking\n    history = defaultdict(list)\n    rng = jax.random.PRNGKey(0)\n    step = 0\n\n    @nnx.jit\n    def train_step(model, opt_state, batch, rng):\n        def loss_fn(model):\n            outputs = model(batch[\"images\"], rngs=nnx.Rngs(rng), training=True)\n            return outputs[\"loss\"], outputs\n\n        (loss, outputs), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n\n        # Compute gradient norm\n        grad_norm = optax.global_norm(grads)\n\n        # Update\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = nnx.apply_updates(model, updates)\n\n        # Add gradient norm to metrics\n        metrics = {**outputs, \"grad_norm\": grad_norm}\n\n        return model, opt_state, loss, metrics\n\n    # Training loop\n    for epoch in range(num_epochs):\n        # Train epoch\n        for batch in train_loader(batch_size=128):\n            rng, step_rng = jax.random.split(rng)\n\n            model, opt_state, loss, metrics = train_step(\n                model, opt_state, batch, step_rng\n            )\n\n            # Track metrics\n            for key, value in metrics.items():\n                history[f\"train_{key}\"].append(float(value))\n\n            step += 1\n\n            # Periodic logging\n            if step % 100 == 0:\n                recent_loss = np.mean(history[\"train_loss\"][-100:])\n                recent_grad_norm = np.mean(history[\"train_grad_norm\"][-100:])\n                print(f\"Step {step}:\")\n                print(f\"  Loss: {recent_loss:.4f}\")\n                print(f\"  Grad Norm: {recent_grad_norm:.4f}\")\n\n        # Validation\n        val_metrics = defaultdict(list)\n        for batch in val_loader(batch_size=128):\n            outputs = model(batch[\"images\"], rngs=nnx.Rngs(rng), training=False)\n            for key, value in outputs.items():\n                val_metrics[key].append(float(value))\n\n        # Log validation metrics\n        print(f\"\\nEpoch {epoch + 1}:\")\n        for key, values in val_metrics.items():\n            mean_value = np.mean(values)\n            history[f\"val_{key}\"].append(mean_value)\n            print(f\"  Val {key}: {mean_value:.4f}\")\n\n    return model, history\n\n# Train with advanced loop\nmodel, history = advanced_training_loop(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=10,\n    optimizer_config=optimizer_config,\n    scheduler_config=scheduler_config,\n)\n\n# Plot training curves\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history[\"train_loss\"], label=\"Train\")\nplt.plot(np.arange(len(history[\"val_loss\"])) * 100, history[\"val_loss\"], label=\"Val\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Training Loss\")\n\nplt.subplot(1, 2, 2)\nplt.plot(history[\"train_grad_norm\"])\nplt.xlabel(\"Step\")\nplt.ylabel(\"Gradient Norm\")\nplt.title(\"Gradient Norm\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"user-guide/training/training-guide/#learning-rate-schedules","title":"Learning Rate Schedules","text":""},{"location":"user-guide/training/training-guide/#warmup-schedule","title":"Warmup Schedule","text":"<p>Gradually increase learning rate at the start:</p> <pre><code>from artifex.generative_models.core.configuration import SchedulerConfig\n\n# Cosine schedule with warmup (recommended)\nwarmup_cosine = SchedulerConfig(\n    name=\"warmup_cosine\",\n    scheduler_type=\"cosine\",\n    warmup_steps=1000,      # 1000 steps of warmup\n    cycle_length=50000,     # Cosine cycle length\n    min_lr_ratio=0.1,       # End at 10% of peak LR\n)\n\ntraining_config = TrainingConfig(\n    name=\"warmup_training\",\n    batch_size=128,\n    num_epochs=100,\n    optimizer=optimizer_config,\n    scheduler=warmup_cosine,\n)\n</code></pre>"},{"location":"user-guide/training/training-guide/#custom-schedules","title":"Custom Schedules","text":"<p>Create custom learning rate schedules:</p> <pre><code>import optax\n\ndef create_custom_schedule(\n    base_lr,\n    warmup_steps,\n    hold_steps,\n    decay_steps,\n    end_lr_ratio=0.1,\n):\n    \"\"\"Create a custom learning rate schedule.\n\n    Schedule: warmup \u2192 hold \u2192 decay\n    \"\"\"\n    schedules = [\n        # Warmup\n        optax.linear_schedule(\n            init_value=0.0,\n            end_value=base_lr,\n            transition_steps=warmup_steps,\n        ),\n        # Hold\n        optax.constant_schedule(base_lr),\n        # Decay\n        optax.cosine_decay_schedule(\n            init_value=base_lr,\n            decay_steps=decay_steps,\n            alpha=end_lr_ratio,\n        ),\n    ]\n\n    boundaries = [warmup_steps, warmup_steps + hold_steps]\n\n    return optax.join_schedules(schedules, boundaries)\n\n# Use custom schedule\ncustom_schedule = create_custom_schedule(\n    base_lr=1e-3,\n    warmup_steps=1000,\n    hold_steps=5000,\n    decay_steps=44000,\n    end_lr_ratio=0.1,\n)\n\noptimizer = optax.adam(learning_rate=custom_schedule)\n</code></pre>"},{"location":"user-guide/training/training-guide/#one-cycle-schedule","title":"One-Cycle Schedule","text":"<p>Implement one-cycle learning rate policy:</p> <pre><code>def create_one_cycle_schedule(\n    max_lr,\n    total_steps,\n    pct_start=0.3,\n    div_factor=25.0,\n    final_div_factor=1e4,\n):\n    \"\"\"Create a one-cycle learning rate schedule.\n\n    Args:\n        max_lr: Maximum learning rate\n        total_steps: Total training steps\n        pct_start: Percentage of cycle spent increasing LR\n        div_factor: Initial LR = max_lr / div_factor\n        final_div_factor: Final LR = max_lr / final_div_factor\n    \"\"\"\n    initial_lr = max_lr / div_factor\n    final_lr = max_lr / final_div_factor\n    step_up = int(total_steps * pct_start)\n    step_down = total_steps - step_up\n\n    schedules = [\n        # Increase phase\n        optax.linear_schedule(\n            init_value=initial_lr,\n            end_value=max_lr,\n            transition_steps=step_up,\n        ),\n        # Decrease phase\n        optax.cosine_decay_schedule(\n            init_value=max_lr,\n            decay_steps=step_down,\n            alpha=final_lr / max_lr,\n        ),\n    ]\n\n    return optax.join_schedules(schedules, [step_up])\n\n# Use one-cycle schedule\none_cycle_schedule = create_one_cycle_schedule(\n    max_lr=1e-3,\n    total_steps=50000,\n    pct_start=0.3,\n)\n\noptimizer = optax.adam(learning_rate=one_cycle_schedule)\n</code></pre>"},{"location":"user-guide/training/training-guide/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Accumulate gradients to simulate larger batch sizes:</p> <pre><code>def training_with_gradient_accumulation(\n    model,\n    train_loader,\n    num_epochs,\n    accumulation_steps=4,\n    learning_rate=1e-3,\n):\n    \"\"\"Training with gradient accumulation.\"\"\"\n    optimizer = optax.adam(learning_rate)\n    opt_state = optimizer.init(nnx.state(model))\n    rng = jax.random.PRNGKey(0)\n\n    @nnx.jit\n    def compute_gradients(model, batch, rng):\n        \"\"\"Compute gradients for a batch.\"\"\"\n        def loss_fn(model):\n            outputs = model(batch[\"images\"], rngs=nnx.Rngs(rng), training=True)\n            return outputs[\"loss\"], outputs\n\n        (loss, outputs), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n        return grads, loss, outputs\n\n    @nnx.jit\n    def apply_accumulated_gradients(model, opt_state, accumulated_grads):\n        \"\"\"Apply accumulated gradients.\"\"\"\n        # Average gradients\n        averaged_grads = jax.tree_map(\n            lambda g: g / accumulation_steps,\n            accumulated_grads\n        )\n\n        # Update model\n        updates, opt_state = optimizer.update(averaged_grads, opt_state)\n        model = nnx.apply_updates(model, updates)\n\n        return model, opt_state\n\n    # Training loop\n    for epoch in range(num_epochs):\n        accumulated_grads = None\n        step = 0\n\n        for batch in train_loader(batch_size=32):  # Smaller batch size\n            rng, step_rng = jax.random.split(rng)\n\n            # Compute gradients\n            grads, loss, outputs = compute_gradients(model, batch, step_rng)\n\n            # Accumulate gradients\n            if accumulated_grads is None:\n                accumulated_grads = grads\n            else:\n                accumulated_grads = jax.tree_map(\n                    lambda acc, g: acc + g,\n                    accumulated_grads,\n                    grads\n                )\n\n            step += 1\n\n            # Apply accumulated gradients\n            if step % accumulation_steps == 0:\n                model, opt_state = apply_accumulated_gradients(\n                    model, opt_state, accumulated_grads\n                )\n                accumulated_grads = None\n\n                if step % 100 == 0:\n                    print(f\"Step {step // accumulation_steps}: Loss = {loss:.4f}\")\n\n    return model\n\n# Train with gradient accumulation\nmodel = training_with_gradient_accumulation(\n    model=model,\n    train_loader=train_loader,\n    num_epochs=10,\n    accumulation_steps=4,  # Effective batch size = 32 * 4 = 128\n)\n</code></pre> <p>Advanced Gradient Accumulation</p> <p>For production use, Artifex provides a <code>GradientAccumulator</code> class with configurable normalization and step tracking. See Advanced Features for details.</p>"},{"location":"user-guide/training/training-guide/#early-stopping","title":"Early Stopping","text":"<p>Artifex provides a built-in <code>EarlyStopping</code> callback to prevent overfitting:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    EarlyStopping,\n    EarlyStoppingConfig,\n    CallbackList,\n)\n\n# Configure early stopping\nearly_stopping_config = EarlyStoppingConfig(\n    monitor=\"val_loss\",      # Metric to monitor\n    min_delta=0.001,         # Minimum change to qualify as improvement\n    patience=10,             # Epochs to wait before stopping\n    mode=\"min\",              # \"min\" for loss, \"max\" for accuracy\n    check_finite=True,       # Stop if metric becomes NaN/Inf\n    stopping_threshold=None, # Stop immediately if metric reaches this value\n    divergence_threshold=10.0,  # Stop if loss exceeds this (prevents divergence)\n)\n\n# Create callback\nearly_stopping = EarlyStopping(early_stopping_config)\n\n# Use in training loop\nfor epoch in range(max_epochs):\n    # Train epoch...\n    train_metrics = train_epoch(model, train_loader)\n\n    # Validate\n    val_metrics = validate(model, val_loader)\n\n    # Check early stopping (call on_epoch_end with metrics)\n    early_stopping.on_epoch_end(trainer, epoch, {\"val_loss\": val_metrics[\"loss\"]})\n\n    if early_stopping.should_stop:\n        print(f\"Early stopping at epoch {epoch + 1}\")\n        break\n</code></pre>"},{"location":"user-guide/training/training-guide/#using-multiple-callbacks","title":"Using Multiple Callbacks","text":"<p>Combine callbacks with <code>CallbackList</code>:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    CallbackList,\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n)\n\n# Create callbacks\ncallbacks = CallbackList([\n    EarlyStopping(EarlyStoppingConfig(patience=10, monitor=\"val_loss\")),\n    ModelCheckpoint(CheckpointConfig(\n        dirpath=\"./checkpoints\",\n        monitor=\"val_loss\",\n        save_top_k=3,\n    )),\n])\n\n# Dispatch events to all callbacks\ncallbacks.on_train_begin(trainer)\nfor epoch in range(max_epochs):\n    callbacks.on_epoch_begin(trainer, epoch)\n    # ... training ...\n    callbacks.on_epoch_end(trainer, epoch, metrics)\ncallbacks.on_train_end(trainer)\n</code></pre>"},{"location":"user-guide/training/training-guide/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Use mixed precision for faster training:</p> <pre><code>def mixed_precision_training(model, train_loader, num_epochs):\n    \"\"\"Training with mixed precision (bfloat16).\"\"\"\n    # Convert model to bfloat16\n    def convert_to_bfloat16(x):\n        if isinstance(x, jnp.ndarray) and x.dtype == jnp.float32:\n            return x.astype(jnp.bfloat16)\n        return x\n\n    model = jax.tree_map(convert_to_bfloat16, model)\n\n    # Use mixed precision optimizer\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.scale_by_adam(),\n        optax.scale(-1e-3),  # Learning rate\n    )\n\n    opt_state = optimizer.init(nnx.state(model))\n    rng = jax.random.PRNGKey(0)\n\n    @nnx.jit\n    def train_step(model, opt_state, batch, rng):\n        # Convert batch to bfloat16\n        batch = jax.tree_map(convert_to_bfloat16, batch)\n\n        def loss_fn(model):\n            outputs = model(batch[\"images\"], rngs=nnx.Rngs(rng), training=True)\n            # Keep loss in float32 for numerical stability\n            return outputs[\"loss\"].astype(jnp.float32), outputs\n\n        (loss, outputs), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n\n        # Update (gradients automatically in bfloat16)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = nnx.apply_updates(model, updates)\n\n        return model, opt_state, loss\n\n    # Training loop\n    for epoch in range(num_epochs):\n        for batch in train_loader(batch_size=128):\n            rng, step_rng = jax.random.split(rng)\n            model, opt_state, loss = train_step(model, opt_state, batch, step_rng)\n\n        print(f\"Epoch {epoch + 1}: Loss = {loss:.4f}\")\n\n    return model\n\n# Train with mixed precision\nmodel = mixed_precision_training(\n    model=model,\n    train_loader=train_loader,\n    num_epochs=10,\n)\n</code></pre> <p>Dynamic Loss Scaling</p> <p>For robust mixed-precision training, Artifex provides a <code>DynamicLossScaler</code> class that automatically adjusts loss scaling to prevent overflow/underflow. See Advanced Features for details.</p>"},{"location":"user-guide/training/training-guide/#model-checkpointing","title":"Model Checkpointing","text":"<p>Artifex provides robust checkpointing utilities using Orbax for saving and loading model state.</p>"},{"location":"user-guide/training/training-guide/#basic-checkpointing","title":"Basic Checkpointing","text":"<pre><code>from artifex.generative_models.core.checkpointing import (\n    setup_checkpoint_manager,\n    save_checkpoint,\n    load_checkpoint,\n)\n\n# Setup checkpoint manager\ncheckpoint_manager, checkpoint_dir = setup_checkpoint_manager(\n    base_dir=\"./checkpoints/experiment_1\"\n)\n\n# Save checkpoint during training\nfor step in range(num_steps):\n    # ... training step ...\n\n    if (step + 1) % 1000 == 0:\n        save_checkpoint(checkpoint_manager, model, step + 1)\n        print(f\"Saved checkpoint at step {step + 1}\")\n\n# Load checkpoint into a model template\nmodel_template = create_model(config, rngs=nnx.Rngs(0))\nrestored_model, loaded_step = load_checkpoint(\n    checkpoint_manager,\n    target_model_template=model_template,\n)\nprint(f\"Restored from step {loaded_step}\")\n</code></pre>"},{"location":"user-guide/training/training-guide/#checkpointing-with-optimizer-state","title":"Checkpointing with Optimizer State","text":"<p>Save and restore both model and optimizer state:</p> <pre><code>from artifex.generative_models.core.checkpointing import (\n    setup_checkpoint_manager,\n    save_checkpoint_with_optimizer,\n    load_checkpoint_with_optimizer,\n)\n\n# Setup\ncheckpoint_manager, _ = setup_checkpoint_manager(\"./checkpoints\")\noptimizer = nnx.Optimizer(model, optax.adam(1e-4), wrt=nnx.Param)\n\n# Save both model and optimizer\nsave_checkpoint_with_optimizer(checkpoint_manager, model, optimizer, step=100)\n\n# Load both model and optimizer\nmodel_template = create_model(config, rngs=nnx.Rngs(0))\noptimizer_template = nnx.Optimizer(model_template, optax.adam(1e-4), wrt=nnx.Param)\n\nrestored_model, restored_optimizer, step = load_checkpoint_with_optimizer(\n    checkpoint_manager,\n    model_template,\n    optimizer_template,\n)\n</code></pre>"},{"location":"user-guide/training/training-guide/#modelcheckpoint-callback","title":"ModelCheckpoint Callback","text":"<p>Use the <code>ModelCheckpoint</code> callback for automatic best-model saving:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    ModelCheckpoint,\n    CheckpointConfig,\n)\n\n# Configure checkpoint callback\ncheckpoint_config = CheckpointConfig(\n    dirpath=\"./checkpoints\",\n    filename=\"model-{epoch:02d}-{val_loss:.4f}\",\n    monitor=\"val_loss\",\n    mode=\"min\",           # Save when val_loss decreases\n    save_top_k=3,         # Keep top 3 checkpoints\n    save_last=True,       # Also save the last checkpoint\n)\n\ncheckpoint_callback = ModelCheckpoint(checkpoint_config)\n\n# Use in training loop\nfor epoch in range(num_epochs):\n    # ... training ...\n    checkpoint_callback.on_epoch_end(trainer, epoch, {\"val_loss\": val_loss})\n</code></pre>"},{"location":"user-guide/training/training-guide/#checkpoint-validation-and-recovery","title":"Checkpoint Validation and Recovery","text":"<p>Validate checkpoints and recover from corruption:</p> <pre><code>from artifex.generative_models.core.checkpointing import (\n    validate_checkpoint,\n    recover_from_corruption,\n)\n\n# Validate a checkpoint produces consistent outputs\nis_valid = validate_checkpoint(\n    checkpoint_manager,\n    model,\n    step=100,\n    validation_data=sample_batch,\n    tolerance=1e-5,\n)\n\n# Recover from corrupted checkpoints (tries newest to oldest)\nrecovered_model, recovered_step = recover_from_corruption(\n    checkpoint_dir=\"./checkpoints\",\n    model_template=model_template,\n)\n</code></pre> <p>For more details, see the Advanced Checkpointing Guide.</p>"},{"location":"user-guide/training/training-guide/#logging-and-monitoring","title":"Logging and Monitoring","text":"<p>Artifex provides built-in logging callbacks for seamless integration with popular experiment tracking tools. These callbacks integrate with the training callback system for automatic metric logging.</p>"},{"location":"user-guide/training/training-guide/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<p>Use <code>WandbLoggerCallback</code> for experiment tracking with Weights &amp; Biases:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n    ProgressBarCallback,\n)\n\n# Configure W&amp;B logging\nwandb_config = WandbLoggerConfig(\n    project=\"vae-experiments\",\n    name=\"experiment-1\",\n    tags=[\"vae\", \"baseline\"],\n    config={\n        \"learning_rate\": 1e-3,\n        \"batch_size\": 32,\n        \"model\": \"VAE\",\n    },\n    log_every_n_steps=10,\n    log_on_epoch_end=True,\n)\n\n# Create callback and add to trainer\nwandb_callback = WandbLoggerCallback(config=wandb_config)\nprogress_callback = ProgressBarCallback()\n\ntrainer.fit(\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=10,\n    callbacks=[wandb_callback, progress_callback],\n)\n</code></pre> <p>Features:</p> <ul> <li>Automatic metric logging at configurable intervals</li> <li>Hyperparameter tracking via <code>config</code> dict</li> <li>Run tagging and notes support</li> <li>Multiple modes: <code>\"online\"</code>, <code>\"offline\"</code>, or <code>\"disabled\"</code></li> <li>Run resumption support</li> </ul>"},{"location":"user-guide/training/training-guide/#tensorboard-integration","title":"TensorBoard Integration","text":"<p>Use <code>TensorBoardLoggerCallback</code> for TensorBoard logging:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    TensorBoardLoggerCallback,\n    TensorBoardLoggerConfig,\n)\n\n# Configure TensorBoard logging\ntb_config = TensorBoardLoggerConfig(\n    log_dir=\"logs/tensorboard/experiment-1\",\n    flush_secs=60,\n    log_every_n_steps=10,\n    log_on_epoch_end=True,\n)\n\n# Create callback\ntb_callback = TensorBoardLoggerCallback(config=tb_config)\n\ntrainer.fit(\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=10,\n    callbacks=[tb_callback],\n)\n\n# View with: tensorboard --logdir logs/tensorboard\n</code></pre> <p>Requirements: Requires <code>tensorboard</code> package (<code>pip install tensorboard</code>)</p>"},{"location":"user-guide/training/training-guide/#progress-bar-display","title":"Progress Bar Display","text":"<p>Use <code>ProgressBarCallback</code> for rich console progress display:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    ProgressBarCallback,\n    ProgressBarConfig,\n)\n\n# Configure progress bar\nprogress_config = ProgressBarConfig(\n    refresh_rate=10,      # Update every 10 steps\n    show_eta=True,        # Show estimated time\n    show_metrics=True,    # Display metrics in progress bar\n    leave=True,           # Keep progress bar after completion\n)\n\nprogress_callback = ProgressBarCallback(config=progress_config)\ntrainer.fit(callbacks=[progress_callback])\n</code></pre> <p>Requirements: Requires <code>rich</code> package (<code>pip install rich</code>)</p>"},{"location":"user-guide/training/training-guide/#combining-multiple-loggers","title":"Combining Multiple Loggers","text":"<p>Logging callbacks can be combined with other callbacks:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    CallbackList,\n    EarlyStopping,\n    EarlyStoppingConfig,\n    ModelCheckpoint,\n    CheckpointConfig,\n    WandbLoggerCallback,\n    WandbLoggerConfig,\n    ProgressBarCallback,\n)\n\n# Configure all callbacks\ncallbacks = CallbackList([\n    # Logging\n    WandbLoggerCallback(WandbLoggerConfig(\n        project=\"my-project\",\n        name=\"experiment-1\",\n    )),\n    ProgressBarCallback(),\n\n    # Training control\n    EarlyStopping(EarlyStoppingConfig(\n        monitor=\"val_loss\",\n        patience=10,\n    )),\n\n    # Checkpointing\n    ModelCheckpoint(CheckpointConfig(\n        dirpath=\"checkpoints\",\n        monitor=\"val_loss\",\n    )),\n])\n\ntrainer.fit(callbacks=callbacks)\n</code></pre>"},{"location":"user-guide/training/training-guide/#custom-logger-callback","title":"Custom Logger Callback","text":"<p>Wrap any custom <code>Logger</code> instance using <code>LoggerCallback</code>:</p> <pre><code>from artifex.generative_models.utils.logging import ConsoleLogger\nfrom artifex.generative_models.training.callbacks import (\n    LoggerCallback,\n    LoggerCallbackConfig,\n)\n\n# Use existing logger infrastructure\nlogger = ConsoleLogger(name=\"training\")\nconfig = LoggerCallbackConfig(\n    log_every_n_steps=50,\n    log_on_epoch_end=True,\n    prefix=\"train/\",\n)\n\ncallback = LoggerCallback(logger=logger, config=config)\ntrainer.fit(callbacks=[callback])\n</code></pre>"},{"location":"user-guide/training/training-guide/#performance-profiling","title":"Performance Profiling","text":"<p>Artifex provides profiling callbacks for performance analysis during training.</p>"},{"location":"user-guide/training/training-guide/#jax-profiler-callback","title":"JAX Profiler Callback","text":"<p>Use <code>JAXProfiler</code> to capture traces for TensorBoard or Perfetto visualization:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    JAXProfiler,\n    ProfilingConfig,\n)\n\n# Configure profiling (skip JIT warmup, profile 10 steps)\nconfig = ProfilingConfig(\n    log_dir=\"logs/profiles\",\n    start_step=10,  # Start after JIT compilation\n    end_step=20,    # Profile for 10 steps\n)\n\nprofiler = JAXProfiler(config)\ntrainer.fit(callbacks=[profiler])\n\n# View traces in TensorBoard:\n# tensorboard --logdir logs/profiles\n</code></pre> <p>Best Practices:</p> <ul> <li>Set <code>start_step</code> after JIT warmup (typically 5-10 steps)</li> <li>Keep profiling window small (10-20 steps) to minimize overhead</li> <li>Traces show XLA compilation, device execution, and memory allocation</li> </ul>"},{"location":"user-guide/training/training-guide/#memory-profiling","title":"Memory Profiling","text":"<p>Track GPU/TPU memory usage with <code>MemoryProfiler</code>:</p> <pre><code>from artifex.generative_models.training.callbacks import (\n    MemoryProfiler,\n    MemoryProfileConfig,\n)\n\nconfig = MemoryProfileConfig(\n    log_dir=\"logs/memory\",\n    profile_every_n_steps=100,  # Collect memory stats every 100 steps\n    log_device_memory=True,\n)\n\nprofiler = MemoryProfiler(config)\ntrainer.fit(callbacks=[profiler])\n\n# Memory profile saved to logs/memory/memory_profile.json\n</code></pre> <p>The memory profile JSON contains:</p> <pre><code>[\n  {\"step\": 0, \"memory\": {\"cuda:0\": {\"bytes_in_use\": 1073741824, \"peak_bytes_in_use\": 2147483648}}},\n  {\"step\": 100, \"memory\": {\"cuda:0\": {\"bytes_in_use\": 1073741824, \"peak_bytes_in_use\": 2147483648}}}\n]\n</code></pre>"},{"location":"user-guide/training/training-guide/#combining-profiling-with-other-callbacks","title":"Combining Profiling with Other Callbacks","text":"<pre><code>from artifex.generative_models.training.callbacks import (\n    CallbackList,\n    JAXProfiler,\n    ProfilingConfig,\n    MemoryProfiler,\n    MemoryProfileConfig,\n    ProgressBarCallback,\n    EarlyStopping,\n    EarlyStoppingConfig,\n)\n\ncallbacks = CallbackList([\n    # Profiling\n    JAXProfiler(ProfilingConfig(log_dir=\"logs/profiles\", start_step=10, end_step=20)),\n    MemoryProfiler(MemoryProfileConfig(log_dir=\"logs/memory\", profile_every_n_steps=100)),\n\n    # Progress display\n    ProgressBarCallback(),\n\n    # Training control\n    EarlyStopping(EarlyStoppingConfig(monitor=\"val_loss\", patience=10)),\n])\n\ntrainer.fit(callbacks=callbacks)\n</code></pre> <p>See Profiling Callbacks for complete documentation.</p>"},{"location":"user-guide/training/training-guide/#common-training-patterns","title":"Common Training Patterns","text":""},{"location":"user-guide/training/training-guide/#progressive-training","title":"Progressive Training","text":"<p>Train with progressively increasing complexity:</p> <pre><code>def progressive_training(model, train_loader, stages):\n    \"\"\"Train with progressive stages.\n\n    Args:\n        model: Model to train\n        train_loader: Data loader\n        stages: List of (num_epochs, learning_rate, batch_size) tuples\n    \"\"\"\n    optimizer_state = None\n\n    for stage_idx, (num_epochs, learning_rate, batch_size) in enumerate(stages):\n        print(f\"\\nStage {stage_idx + 1}: LR={learning_rate}, BS={batch_size}\")\n\n        # Create optimizer for this stage\n        optimizer = optax.adam(learning_rate)\n\n        # Initialize or reuse optimizer state\n        if optimizer_state is None:\n            optimizer_state = optimizer.init(nnx.state(model))\n\n        # Train for this stage\n        for epoch in range(num_epochs):\n            for batch in train_loader(batch_size=batch_size):\n                model, optimizer_state, loss = train_step(\n                    model, optimizer_state, batch, rng\n                )\n\n            print(f\"  Epoch {epoch + 1}: Loss = {loss:.4f}\")\n\n    return model\n\n# Define progressive stages\nstages = [\n    (10, 1e-3, 32),   # Stage 1: High LR, small batch\n    (20, 5e-4, 64),   # Stage 2: Medium LR, medium batch\n    (30, 1e-4, 128),  # Stage 3: Low LR, large batch\n]\n\nmodel = progressive_training(model, train_loader, stages)\n</code></pre>"},{"location":"user-guide/training/training-guide/#curriculum-learning","title":"Curriculum Learning","text":"<p>Train with increasing data difficulty:</p> <pre><code>def curriculum_learning(model, data_loader_fn, difficulty_schedule):\n    \"\"\"Train with curriculum learning.\n\n    Args:\n        model: Model to train\n        data_loader_fn: Function that returns data loader for difficulty level\n        difficulty_schedule: List of (difficulty_level, num_epochs) tuples\n    \"\"\"\n    optimizer = optax.adam(1e-3)\n    opt_state = optimizer.init(nnx.state(model))\n    rng = jax.random.PRNGKey(0)\n\n    for difficulty, num_epochs in difficulty_schedule:\n        print(f\"\\nTraining on difficulty level: {difficulty}\")\n\n        # Get data loader for this difficulty\n        train_loader = data_loader_fn(difficulty)\n\n        # Train\n        for epoch in range(num_epochs):\n            for batch in train_loader(batch_size=128):\n                rng, step_rng = jax.random.split(rng)\n                model, opt_state, loss = train_step(\n                    model, opt_state, batch, step_rng\n                )\n\n            print(f\"  Epoch {epoch + 1}: Loss = {loss:.4f}\")\n\n    return model\n\n# Define curriculum\ndifficulty_schedule = [\n    (\"easy\", 10),      # Train on easy examples first\n    (\"medium\", 20),    # Then medium difficulty\n    (\"hard\", 30),      # Finally hard examples\n    (\"all\", 40),       # Train on all data\n]\n\nmodel = curriculum_learning(model, data_loader_fn, difficulty_schedule)\n</code></pre>"},{"location":"user-guide/training/training-guide/#multi-task-training","title":"Multi-Task Training","text":"<p>Train on multiple tasks simultaneously:</p> <pre><code>def multi_task_training(\n    model,\n    task_loaders,\n    task_weights,\n    num_epochs,\n):\n    \"\"\"Train on multiple tasks.\n\n    Args:\n        model: Model to train\n        task_loaders: Dict of task_name -&gt; data_loader\n        task_weights: Dict of task_name -&gt; weight\n        num_epochs: Number of epochs\n    \"\"\"\n    optimizer = optax.adam(1e-3)\n    opt_state = optimizer.init(nnx.state(model))\n    rng = jax.random.PRNGKey(0)\n\n    @nnx.jit\n    def multi_task_step(model, opt_state, batches, rng):\n        \"\"\"Training step with multiple tasks.\"\"\"\n        def loss_fn(model):\n            total_loss = 0.0\n            metrics = {}\n\n            for task_name, batch in batches.items():\n                # Task-specific forward pass\n                outputs = model(\n                    batch,\n                    task=task_name,\n                    rngs=nnx.Rngs(rng),\n                    training=True\n                )\n\n                # Weighted loss\n                task_loss = outputs[\"loss\"] * task_weights[task_name]\n                total_loss += task_loss\n\n                # Track metrics\n                metrics[f\"{task_name}_loss\"] = outputs[\"loss\"]\n\n            return total_loss, metrics\n\n        (loss, metrics), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model)\n\n        updates, opt_state = optimizer.update(grads, opt_state)\n        model = nnx.apply_updates(model, updates)\n\n        return model, opt_state, loss, metrics\n\n    # Training loop\n    for epoch in range(num_epochs):\n        # Get batches from all tasks\n        task_iters = {\n            name: loader(batch_size=32)\n            for name, loader in task_loaders.items()\n        }\n\n        for step in range(1000):  # Fixed steps per epoch\n            # Get batch from each task\n            batches = {\n                name: next(task_iter)\n                for name, task_iter in task_iters.items()\n            }\n\n            rng, step_rng = jax.random.split(rng)\n            model, opt_state, loss, metrics = multi_task_step(\n                model, opt_state, batches, step_rng\n            )\n\n            if step % 100 == 0:\n                print(f\"Step {step}: Total Loss = {loss:.4f}\")\n                for task_name, task_loss in metrics.items():\n                    print(f\"  {task_name}: {task_loss:.4f}\")\n\n    return model\n\n# Train on multiple tasks\ntask_loaders = {\n    \"reconstruction\": reconstruction_loader,\n    \"generation\": generation_loader,\n    \"classification\": classification_loader,\n}\n\ntask_weights = {\n    \"reconstruction\": 1.0,\n    \"generation\": 0.5,\n    \"classification\": 0.3,\n}\n\nmodel = multi_task_training(\n    model=model,\n    task_loaders=task_loaders,\n    task_weights=task_weights,\n    num_epochs=50,\n)\n</code></pre>"},{"location":"user-guide/training/training-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/training/training-guide/#nan-loss","title":"NaN Loss","text":"<p>If you encounter NaN loss:</p> <pre><code># 1. Add gradient clipping\noptimizer_config = OptimizerConfig(\n    name=\"clipped_adam\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-3,\n    gradient_clip_norm=1.0,  # Clip gradients\n)\n\n# 2. Lower learning rate\noptimizer_config = OptimizerConfig(\n    name=\"lower_lr\",\n    optimizer_type=\"adam\",\n    learning_rate=1e-4,  # Lower LR\n)\n\n# 3. Check for numerical instability\ndef check_for_nans(metrics, step):\n    \"\"\"Check for NaNs in metrics.\"\"\"\n    for key, value in metrics.items():\n        if np.isnan(value):\n            print(f\"NaN detected at step {step} in {key}\")\n            # Save checkpoint before crash\n            save_checkpoint(model, opt_state, step, \"./emergency_checkpoint.pkl\")\n            raise ValueError(f\"NaN in {key}\")\n\n# 4. Use mixed precision with care\n# Avoid bfloat16 for loss computation\nloss = loss.astype(jnp.float32)  # Keep loss in float32\n</code></pre>"},{"location":"user-guide/training/training-guide/#slow-training","title":"Slow Training","text":"<p>If training is slow:</p> <pre><code># 1. Use JIT compilation\n@nnx.jit\ndef train_step(model, opt_state, batch, rng):\n    # Training step logic\n    pass\n\n# 2. Profile your code with JAXProfiler callback\nfrom artifex.generative_models.training.callbacks import JAXProfiler, ProfilingConfig\nprofiler = JAXProfiler(ProfilingConfig(log_dir=\"logs/profiles\", start_step=10, end_step=20))\ntrainer.fit(callbacks=[profiler])\n# View in TensorBoard: tensorboard --logdir logs/profiles\n\n# 3. Increase batch size (if memory allows)\ntraining_config = TrainingConfig(\n    name=\"large_batch\",\n    batch_size=256,  # Larger batch size\n    num_epochs=50,   # Fewer epochs needed\n    optimizer=optimizer_config,\n)\n\n# 4. Use data prefetching\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef prefetch_data_loader(data_loader, prefetch_size=2):\n    \"\"\"Prefetch data in background.\"\"\"\n    with ThreadPoolExecutor(max_workers=1) as executor:\n        iterator = iter(data_loader(batch_size=128))\n        futures = [executor.submit(lambda: next(iterator))\n                   for _ in range(prefetch_size)]\n\n        while True:\n            # Get next batch from future\n            batch = futures.pop(0).result()\n            # Submit new prefetch\n            futures.append(executor.submit(lambda: next(iterator)))\n            yield batch\n</code></pre>"},{"location":"user-guide/training/training-guide/#memory-issues","title":"Memory Issues","text":"<p>If you run out of memory:</p> <pre><code># 1. Reduce batch size\ntraining_config = TrainingConfig(\n    name=\"small_batch\",\n    batch_size=32,  # Smaller batch\n    num_epochs=200,  # More epochs\n    optimizer=optimizer_config,\n)\n\n# 2. Use gradient accumulation\n# See \"Gradient Accumulation\" section above\n\n# 3. Clear cache periodically\nimport jax\n\n# Clear compilation cache\njax.clear_caches()\n\n# 4. Use checkpointing for large models\nfrom jax.checkpoint import checkpoint\n\n@checkpoint\ndef expensive_forward_pass(model, x):\n    \"\"\"Forward pass with gradient checkpointing.\"\"\"\n    return model(x)\n</code></pre>"},{"location":"user-guide/training/training-guide/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/training/training-guide/#do","title":"DO","text":"<ul> <li>\u2705 Use type-safe configuration with validation</li> <li>\u2705 JIT-compile training steps for performance</li> <li>\u2705 Save checkpoints regularly</li> <li>\u2705 Monitor training metrics (loss, gradients)</li> <li>\u2705 Use gradient clipping for stability</li> <li>\u2705 Start with small learning rate and increase</li> <li>\u2705 Validate periodically during training</li> <li>\u2705 Save best model based on validation metrics</li> <li>\u2705 Use warmup for learning rate schedules</li> <li>\u2705 Profile code to find bottlenecks</li> </ul>"},{"location":"user-guide/training/training-guide/#dont","title":"DON'T","text":"<ul> <li>\u274c Skip validation - always validate your model</li> <li>\u274c Use too high learning rate initially</li> <li>\u274c Forget to shuffle training data</li> <li>\u274c Ignore NaN or infinite losses</li> <li>\u274c Train without gradient clipping</li> <li>\u274c Overwrite checkpoints without backup</li> <li>\u274c Use mixed precision for all operations</li> <li>\u274c Forget to split RNG keys properly</li> <li>\u274c Mutate training state in-place</li> <li>\u274c Skip warmup for large learning rates</li> </ul>"},{"location":"user-guide/training/training-guide/#summary","title":"Summary","text":"<p>This guide covered:</p> <ul> <li>Basic Training: Quick start and setup</li> <li>Custom Loops: Full control over training</li> <li>Learning Rate Schedules: Warmup, cosine, one-cycle</li> <li>Advanced Techniques: Gradient accumulation, early stopping, mixed precision</li> <li>Checkpointing: Save and load model state</li> <li>Logging: W&amp;B, TensorBoard integration</li> <li>Profiling: JAXProfiler for performance traces, MemoryProfiler for memory tracking</li> <li>Common Patterns: Progressive training, curriculum learning, multi-task</li> <li>Troubleshooting: NaN loss, slow training, memory issues</li> </ul> <p>For reward-based fine-tuning and alignment, see the RL Training Guide covering REINFORCE, PPO, GRPO, and DPO trainers.</p>"},{"location":"user-guide/training/training-guide/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Configuration Guide</p> <p>Deep dive into configuration system and best practices</p> </li> <li> <p> Training Overview</p> <p>Architecture and core concepts of training system</p> </li> <li> <p> Trainer API</p> <p>Complete API reference for Trainer class</p> </li> <li> <p> RL Training</p> <p>REINFORCE, PPO, GRPO, and DPO for reward-based fine-tuning</p> </li> <li> <p> Advanced Features</p> <p>Gradient accumulation and dynamic loss scaling</p> </li> <li> <p> Logging &amp; Tracking</p> <p>W&amp;B, TensorBoard, and progress bar integration</p> </li> <li> <p> Performance Profiling</p> <p>JAX trace profiling and memory tracking</p> </li> </ul> <p>See the Configuration Guide for detailed configuration options and patterns.</p>"},{"location":"utils/","title":"Utilities","text":"<p>Comprehensive utility modules for JAX operations, logging, visualization, I/O, and development tools.</p>"},{"location":"utils/#overview","title":"Overview","text":"<ul> <li> <p> JAX Utilities</p> <p>Device management, PRNG handling, dtype utilities, and Flax helpers</p> </li> <li> <p> Logging &amp; Metrics</p> <p>MLflow, Weights &amp; Biases, and custom logging integrations</p> </li> <li> <p> Visualization</p> <p>Attention maps, latent space plots, image grids, and protein visualization</p> </li> <li> <p> Profiling</p> <p>Memory profiling, performance tracking, and XProf integration</p> </li> </ul>"},{"location":"utils/#jax-utilities","title":"JAX Utilities","text":""},{"location":"utils/#device-management","title":"Device Management","text":"<pre><code>from artifex.utils.jax import get_device, get_available_devices\n\n# Get default device\ndevice = get_device()  # Returns GPU if available, else CPU\n\n# List all devices\ndevices = get_available_devices()\nprint(f\"Available: {devices}\")\n</code></pre> <p> Device Utilities</p>"},{"location":"utils/#prng-handling","title":"PRNG Handling","text":"<pre><code>from artifex.utils.jax import create_prng_key, split_key\n\n# Create a key\nkey = create_prng_key(42)\n\n# Split for multiple uses\nkey1, key2, key3 = split_key(key, num=3)\n</code></pre> <p> PRNG Utilities</p>"},{"location":"utils/#data-types","title":"Data Types","text":"<pre><code>from artifex.utils.jax import get_dtype, ensure_dtype\n\n# Get appropriate dtype\ndtype = get_dtype(\"float32\")\n\n# Convert array to dtype\narray = ensure_dtype(array, \"bfloat16\")\n</code></pre> <p> Dtype Utilities</p>"},{"location":"utils/#shape-utilities","title":"Shape Utilities","text":"<pre><code>from artifex.utils.jax import flatten_batch, unflatten_batch\n\n# Flatten batch dimensions\nflat, shape = flatten_batch(tensor, num_batch_dims=2)\n\n# Restore batch dimensions\nrestored = unflatten_batch(flat, shape)\n</code></pre> <p> Shape Utilities</p>"},{"location":"utils/#flax-utilities","title":"Flax Utilities","text":"<pre><code>from artifex.utils.jax import count_params, get_param_shapes\n\n# Count model parameters\nnum_params = count_params(model)\n\n# Get parameter shapes\nshapes = get_param_shapes(model)\n</code></pre> <p> Flax Utilities</p>"},{"location":"utils/#logging-metrics","title":"Logging &amp; Metrics","text":""},{"location":"utils/#logger","title":"Logger","text":"<pre><code>from artifex.utils.logging import get_logger\n\nlogger = get_logger(__name__)\nlogger.info(\"Training started\")\nlogger.debug(\"Batch size: 128\")\n</code></pre> <p> Logger Reference</p>"},{"location":"utils/#weights-biases","title":"Weights &amp; Biases","text":"<pre><code>from artifex.utils.logging import WandbLogger\n\nlogger = WandbLogger(\n    project=\"my-project\",\n    name=\"experiment-001\",\n    config=config_dict,\n)\n\nlogger.log_metrics({\"loss\": 0.5, \"accuracy\": 0.9}, step=100)\nlogger.log_image(\"samples\", image_array)\n</code></pre> <p> W&amp;B Integration</p>"},{"location":"utils/#mlflow","title":"MLflow","text":"<pre><code>from artifex.utils.logging import MLflowLogger\n\nlogger = MLflowLogger(\n    experiment_name=\"vae-experiments\",\n    tracking_uri=\"http://localhost:5000\",\n)\n\nlogger.log_params({\"learning_rate\": 1e-3})\nlogger.log_metrics({\"loss\": 0.5}, step=100)\n</code></pre> <p> MLflow Integration</p>"},{"location":"utils/#metrics-tracking","title":"Metrics Tracking","text":"<pre><code>from artifex.utils.logging import MetricsTracker\n\ntracker = MetricsTracker()\ntracker.update(\"loss\", 0.5)\ntracker.update(\"loss\", 0.4)\n\navg = tracker.compute(\"loss\")  # Returns average\ntracker.reset()\n</code></pre> <p> Metrics Tracking</p>"},{"location":"utils/#visualization","title":"Visualization","text":""},{"location":"utils/#image-grids","title":"Image Grids","text":"<pre><code>from artifex.utils.visualization import create_image_grid, save_image_grid\n\n# Create grid from batch\ngrid = create_image_grid(images, nrow=8)\n\n# Save to file\nsave_image_grid(images, \"samples.png\", nrow=8)\n</code></pre> <p> Image Grid</p>"},{"location":"utils/#latent-space-visualization","title":"Latent Space Visualization","text":"<pre><code>from artifex.utils.visualization import plot_latent_space\n\n# Plot 2D latent space with labels\nplot_latent_space(\n    latents,\n    labels=labels,\n    method=\"tsne\",  # or \"pca\", \"umap\"\n    save_path=\"latent_space.png\",\n)\n</code></pre> <p> Latent Space</p>"},{"location":"utils/#attention-visualization","title":"Attention Visualization","text":"<pre><code>from artifex.utils.visualization import visualize_attention\n\n# Visualize attention weights\nvisualize_attention(\n    attention_weights,\n    tokens=tokens,\n    save_path=\"attention.png\",\n)\n</code></pre> <p> Attention Visualization</p>"},{"location":"utils/#plotting","title":"Plotting","text":"<pre><code>from artifex.utils.visualization import plot_training_curves\n\n# Plot loss curves\nplot_training_curves(\n    train_losses=train_losses,\n    val_losses=val_losses,\n    save_path=\"training_curves.png\",\n)\n</code></pre> <p> Plotting Utilities</p>"},{"location":"utils/#protein-visualization","title":"Protein Visualization","text":"<pre><code>from artifex.utils.visualization import visualize_protein_structure\n\n# Visualize protein backbone\nvisualize_protein_structure(\n    coordinates=coords,\n    sequence=sequence,\n    save_path=\"protein.png\",\n)\n</code></pre> <p> Protein Visualization</p>"},{"location":"utils/#io-utilities","title":"I/O Utilities","text":""},{"location":"utils/#file-operations","title":"File Operations","text":"<pre><code>from artifex.utils.io import save_checkpoint, load_checkpoint\n\n# Save model checkpoint\nsave_checkpoint(model, optimizer, \"checkpoint.ckpt\")\n\n# Load checkpoint\nmodel, optimizer = load_checkpoint(\"checkpoint.ckpt\")\n</code></pre> <p> File Utilities</p>"},{"location":"utils/#format-conversion","title":"Format Conversion","text":"<pre><code>from artifex.utils.io import convert_format\n\n# Convert between formats\nconvert_format(\n    input_path=\"model.ckpt\",\n    output_path=\"model.safetensors\",\n    format=\"safetensors\",\n)\n</code></pre> <p> Format Utilities</p>"},{"location":"utils/#serialization","title":"Serialization","text":"<pre><code>from artifex.utils.io import serialize_config, deserialize_config\n\n# Serialize to YAML\nyaml_str = serialize_config(config, format=\"yaml\")\n\n# Deserialize from JSON\nconfig = deserialize_config(json_str, format=\"json\")\n</code></pre> <p> Serialization</p>"},{"location":"utils/#profiling","title":"Profiling","text":""},{"location":"utils/#memory-profiling","title":"Memory Profiling","text":"<pre><code>from artifex.utils.profiling import memory_profiler\n\nwith memory_profiler() as prof:\n    output = model(input)\n\nprint(f\"Peak memory: {prof.peak_memory_mb:.2f} MB\")\n</code></pre> <p> Memory Profiling</p>"},{"location":"utils/#performance-profiling","title":"Performance Profiling","text":"<pre><code>from artifex.utils.profiling import profile_function\n\n@profile_function\ndef train_step(batch):\n    return model(batch)\n\n# Profiling results printed automatically\n</code></pre> <p> Performance Profiling</p>"},{"location":"utils/#xprof-integration","title":"XProf Integration","text":"<pre><code>from artifex.utils.profiling import start_xprof, stop_xprof\n\nstart_xprof(log_dir=\"profiles/\")\n# ... training code ...\nstop_xprof()\n</code></pre> <p> XProf Integration</p>"},{"location":"utils/#image-utilities","title":"Image Utilities","text":""},{"location":"utils/#color-operations","title":"Color Operations","text":"<pre><code>from artifex.utils.image import rgb_to_grayscale, normalize_image\n\ngrayscale = rgb_to_grayscale(image)\nnormalized = normalize_image(image, mean=0.5, std=0.5)\n</code></pre> <p> Color Utilities</p>"},{"location":"utils/#image-metrics","title":"Image Metrics","text":"<pre><code>from artifex.utils.image import compute_psnr, compute_ssim\n\npsnr = compute_psnr(generated, reference)\nssim = compute_ssim(generated, reference)\n</code></pre> <p> Image Metrics</p>"},{"location":"utils/#transforms","title":"Transforms","text":"<pre><code>from artifex.utils.image import resize, center_crop, random_flip\n\nresized = resize(image, size=(256, 256))\ncropped = center_crop(image, size=(224, 224))\nflipped = random_flip(image, key=prng_key)\n</code></pre> <p> Image Transforms</p>"},{"location":"utils/#numerical-utilities","title":"Numerical Utilities","text":""},{"location":"utils/#math-operations","title":"Math Operations","text":"<pre><code>from artifex.utils.numerical import log_sum_exp, softmax_temperature\n\n# Numerically stable log-sum-exp\nresult = log_sum_exp(logits, axis=-1)\n\n# Softmax with temperature\nprobs = softmax_temperature(logits, temperature=0.7)\n</code></pre> <p> Math Utilities</p>"},{"location":"utils/#numerical-stability","title":"Numerical Stability","text":"<pre><code>from artifex.utils.numerical import safe_log, safe_divide\n\n# Safe log with epsilon\nlog_x = safe_log(x, eps=1e-8)\n\n# Safe division\nresult = safe_divide(a, b, eps=1e-8)\n</code></pre> <p> Stability Utilities</p>"},{"location":"utils/#statistics","title":"Statistics","text":"<pre><code>from artifex.utils.numerical import running_mean, exponential_moving_average\n\n# Compute running statistics\nmean = running_mean(values)\nema = exponential_moving_average(values, decay=0.99)\n</code></pre> <p> Statistics Utilities</p>"},{"location":"utils/#text-utilities","title":"Text Utilities","text":"<pre><code>from artifex.utils.text import compute_bleu, compute_rouge\n\nbleu = compute_bleu(predictions, references)\nrouge = compute_rouge(predictions, references)\n</code></pre> <p> Text Metrics</p>"},{"location":"utils/#development-utilities","title":"Development Utilities","text":""},{"location":"utils/#timer","title":"Timer","text":"<pre><code>from artifex.utils import Timer\n\nwith Timer(\"training_step\"):\n    output = train_step(batch)\n# Prints: training_step took 0.123s\n</code></pre> <p> Timer</p>"},{"location":"utils/#registry","title":"Registry","text":"<pre><code>from artifex.utils import Registry\n\nmodels = Registry(\"models\")\n\n@models.register(\"my_model\")\nclass MyModel:\n    pass\n\nmodel_class = models.get(\"my_model\")\n</code></pre> <p> Registry</p>"},{"location":"utils/#environment","title":"Environment","text":"<pre><code>from artifex.utils import get_env, set_env\n\n# Get environment variable with default\nvalue = get_env(\"MY_VAR\", default=\"default_value\")\n</code></pre> <p> Environment</p>"},{"location":"utils/#dependency-analyzer","title":"Dependency Analyzer","text":"<pre><code>from artifex.utils import analyze_dependencies\n\n# Analyze module dependencies\ndeps = analyze_dependencies(\"artifex.generative_models\")\n</code></pre> <p> Dependency Analyzer</p>"},{"location":"utils/#module-reference","title":"Module Reference","text":"Category Modules JAX device, dtype, flax_utils, prng, shapes Logging logger, metrics, mlflow, wandb, file_utils Visualization attention_vis, image_grid, latent_space, plotting, protein I/O file, formats, serialization Profiling memory, performance, xprof Image color, metrics, transforms Numerical math, stability, stats Text metrics, postprocessing, processing Utils env, registry, timer, types"},{"location":"utils/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Guide - Using utilities in training</li> <li>Logging &amp; Tracking - Experiment tracking</li> <li>Performance Profiling - Profiling guide</li> </ul>"},{"location":"utils/attention_vis/","title":"Attention Vis","text":"<p>Module: <code>generative_models.utils.visualization.attention_vis</code></p> <p>Source: <code>generative_models/utils/visualization/attention_vis.py</code></p>"},{"location":"utils/attention_vis/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/color/","title":"Color","text":"<p>Module: <code>generative_models.utils.image.color</code></p> <p>Source: <code>generative_models/utils/image/color.py</code></p>"},{"location":"utils/color/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/dependency_analyzer/","title":"Dependency Analyzer","text":"<p>Module: <code>generative_models.utils.code_analysis.dependency_analyzer</code></p> <p>Source: <code>generative_models/utils/code_analysis/dependency_analyzer.py</code></p>"},{"location":"utils/dependency_analyzer/#overview","title":"Overview","text":"<p>Module for analyzing dependencies between Python modules.</p> <p>This module provides tools for discovering, analyzing, and visualizing dependencies between Python modules in a codebase.</p>"},{"location":"utils/dependency_analyzer/#classes","title":"Classes","text":""},{"location":"utils/dependency_analyzer/#dependencyanalyzer","title":"DependencyAnalyzer","text":"<pre><code>class DependencyAnalyzer\n</code></pre>"},{"location":"utils/dependency_analyzer/#moduledependency","title":"ModuleDependency","text":"<pre><code>class ModuleDependency\n</code></pre>"},{"location":"utils/dependency_analyzer/#functions","title":"Functions","text":""},{"location":"utils/dependency_analyzer/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"utils/dependency_analyzer/#detect_circular_dependencies","title":"detect_circular_dependencies","text":"<pre><code>def detect_circular_dependencies()\n</code></pre>"},{"location":"utils/dependency_analyzer/#dfs","title":"dfs","text":"<pre><code>def dfs()\n</code></pre>"},{"location":"utils/dependency_analyzer/#generate_dependency_report","title":"generate_dependency_report","text":"<pre><code>def generate_dependency_report()\n</code></pre>"},{"location":"utils/dependency_analyzer/#generate_graph","title":"generate_graph","text":"<pre><code>def generate_graph()\n</code></pre>"},{"location":"utils/dependency_analyzer/#get_all_dependencies","title":"get_all_dependencies","text":"<pre><code>def get_all_dependencies()\n</code></pre>"},{"location":"utils/dependency_analyzer/#get_module_dependencies","title":"get_module_dependencies","text":"<pre><code>def get_module_dependencies()\n</code></pre>"},{"location":"utils/dependency_analyzer/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 2</li> <li>Functions: 7</li> <li>Imports: 5</li> </ul>"},{"location":"utils/device/","title":"Device","text":"<p>Module: <code>generative_models.utils.jax.device</code></p> <p>Source: <code>generative_models/utils/jax/device.py</code></p>"},{"location":"utils/device/#overview","title":"Overview","text":"<p>Device utilities for artifex generative models.</p> <p>This module provides a clean interface to the comprehensive device management system. All functionality has been moved to the core device management architecture.</p>"},{"location":"utils/device/#functions","title":"Functions","text":""},{"location":"utils/device/#get_recommended_batch_size","title":"get_recommended_batch_size","text":"<pre><code>def get_recommended_batch_size()\n</code></pre>"},{"location":"utils/device/#setup_device_for_training","title":"setup_device_for_training","text":"<pre><code>def setup_device_for_training()\n</code></pre>"},{"location":"utils/device/#verify_device_setup","title":"verify_device_setup","text":"<pre><code>def verify_device_setup()\n</code></pre>"},{"location":"utils/device/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 3</li> <li>Imports: 2</li> </ul>"},{"location":"utils/dtype/","title":"Dtype","text":"<p>Module: <code>generative_models.utils.jax.dtype</code></p> <p>Source: <code>generative_models/utils/jax/dtype.py</code></p>"},{"location":"utils/dtype/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/env/","title":"Env","text":"<p>Module: <code>generative_models.utils.env</code></p> <p>Source: <code>generative_models/utils/env.py</code></p>"},{"location":"utils/env/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/file/","title":"File","text":"<p>Module: <code>generative_models.utils.io.file</code></p> <p>Source: <code>generative_models/utils/io/file.py</code></p>"},{"location":"utils/file/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/file_utils/","title":"File Utils","text":"<p>Module: <code>utils.file_utils</code></p> <p>Source: <code>utils/file_utils.py</code></p>"},{"location":"utils/file_utils/#overview","title":"Overview","text":"<p>Utilities for file operations.</p>"},{"location":"utils/file_utils/#functions","title":"Functions","text":""},{"location":"utils/file_utils/#ensure_valid_output_path","title":"ensure_valid_output_path","text":"<pre><code>def ensure_valid_output_path()\n</code></pre>"},{"location":"utils/file_utils/#get_valid_output_dir","title":"get_valid_output_dir","text":"<pre><code>def get_valid_output_dir()\n</code></pre>"},{"location":"utils/file_utils/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 2</li> <li>Imports: 2</li> </ul>"},{"location":"utils/flax_utils/","title":"Flax Utils","text":"<p>Module: <code>generative_models.utils.jax.flax_utils</code></p> <p>Source: <code>generative_models/utils/jax/flax_utils.py</code></p>"},{"location":"utils/flax_utils/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/formats/","title":"Formats","text":"<p>Module: <code>generative_models.utils.io.formats</code></p> <p>Source: <code>generative_models/utils/io/formats.py</code></p>"},{"location":"utils/formats/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/image_grid/","title":"Image Grid","text":"<p>Module: <code>generative_models.utils.visualization.image_grid</code></p> <p>Source: <code>generative_models/utils/visualization/image_grid.py</code></p>"},{"location":"utils/image_grid/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/latent_space/","title":"Latent Space","text":"<p>Module: <code>generative_models.utils.visualization.latent_space</code></p> <p>Source: <code>generative_models/utils/visualization/latent_space.py</code></p>"},{"location":"utils/latent_space/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/logger/","title":"Logger","text":"<p>Module: <code>generative_models.utils.logging.logger</code></p> <p>Source: <code>generative_models/utils/logging/logger.py</code></p>"},{"location":"utils/logger/#overview","title":"Overview","text":"<p>Base logger interface for Artifex.</p> <p>This module provides a base logger class for logging training progress, metrics, and other information during model training and evaluation.</p>"},{"location":"utils/logger/#classes","title":"Classes","text":""},{"location":"utils/logger/#consolelogger","title":"ConsoleLogger","text":"<pre><code>class ConsoleLogger\n</code></pre>"},{"location":"utils/logger/#filelogger","title":"FileLogger","text":"<pre><code>class FileLogger\n</code></pre>"},{"location":"utils/logger/#logger_1","title":"Logger","text":"<pre><code>class Logger\n</code></pre>"},{"location":"utils/logger/#functions","title":"Functions","text":""},{"location":"utils/logger/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"utils/logger/#init_1","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"utils/logger/#close","title":"close","text":"<pre><code>def close()\n</code></pre>"},{"location":"utils/logger/#create_logger","title":"create_logger","text":"<pre><code>def create_logger()\n</code></pre>"},{"location":"utils/logger/#critical","title":"critical","text":"<pre><code>def critical()\n</code></pre>"},{"location":"utils/logger/#debug","title":"debug","text":"<pre><code>def debug()\n</code></pre>"},{"location":"utils/logger/#error","title":"error","text":"<pre><code>def error()\n</code></pre>"},{"location":"utils/logger/#info","title":"info","text":"<pre><code>def info()\n</code></pre>"},{"location":"utils/logger/#log","title":"log","text":"<pre><code>def log()\n</code></pre>"},{"location":"utils/logger/#log_histogram","title":"log_histogram","text":"<pre><code>def log_histogram()\n</code></pre>"},{"location":"utils/logger/#log_histogram_1","title":"log_histogram","text":"<pre><code>def log_histogram()\n</code></pre>"},{"location":"utils/logger/#log_histogram_2","title":"log_histogram","text":"<pre><code>def log_histogram()\n</code></pre>"},{"location":"utils/logger/#log_hyperparams","title":"log_hyperparams","text":"<pre><code>def log_hyperparams()\n</code></pre>"},{"location":"utils/logger/#log_hyperparams_1","title":"log_hyperparams","text":"<pre><code>def log_hyperparams()\n</code></pre>"},{"location":"utils/logger/#log_hyperparams_2","title":"log_hyperparams","text":"<pre><code>def log_hyperparams()\n</code></pre>"},{"location":"utils/logger/#log_image","title":"log_image","text":"<pre><code>def log_image()\n</code></pre>"},{"location":"utils/logger/#log_image_1","title":"log_image","text":"<pre><code>def log_image()\n</code></pre>"},{"location":"utils/logger/#log_image_2","title":"log_image","text":"<pre><code>def log_image()\n</code></pre>"},{"location":"utils/logger/#log_scalar","title":"log_scalar","text":"<pre><code>def log_scalar()\n</code></pre>"},{"location":"utils/logger/#log_scalar_1","title":"log_scalar","text":"<pre><code>def log_scalar()\n</code></pre>"},{"location":"utils/logger/#log_scalar_2","title":"log_scalar","text":"<pre><code>def log_scalar()\n</code></pre>"},{"location":"utils/logger/#log_scalars","title":"log_scalars","text":"<pre><code>def log_scalars()\n</code></pre>"},{"location":"utils/logger/#log_scalars_1","title":"log_scalars","text":"<pre><code>def log_scalars()\n</code></pre>"},{"location":"utils/logger/#log_scalars_2","title":"log_scalars","text":"<pre><code>def log_scalars()\n</code></pre>"},{"location":"utils/logger/#log_text","title":"log_text","text":"<pre><code>def log_text()\n</code></pre>"},{"location":"utils/logger/#log_text_1","title":"log_text","text":"<pre><code>def log_text()\n</code></pre>"},{"location":"utils/logger/#log_text_2","title":"log_text","text":"<pre><code>def log_text()\n</code></pre>"},{"location":"utils/logger/#warning","title":"warning","text":"<pre><code>def warning()\n</code></pre>"},{"location":"utils/logger/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 3</li> <li>Functions: 28</li> <li>Imports: 13</li> </ul>"},{"location":"utils/math/","title":"Math","text":"<p>Module: <code>generative_models.utils.numerical.math</code></p> <p>Source: <code>generative_models/utils/numerical/math.py</code></p>"},{"location":"utils/math/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/memory/","title":"Memory","text":"<p>Module: <code>generative_models.utils.profiling.memory</code></p> <p>Source: <code>generative_models/utils/profiling/memory.py</code></p>"},{"location":"utils/memory/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/metrics/","title":"Metrics","text":"<p>Module: <code>generative_models.utils.text.metrics</code></p> <p>Source: <code>generative_models/utils/text/metrics.py</code></p>"},{"location":"utils/metrics/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/mlflow/","title":"Mlflow","text":"<p>Module: <code>generative_models.utils.logging.mlflow</code></p> <p>Source: <code>generative_models/utils/logging/mlflow.py</code></p>"},{"location":"utils/mlflow/#overview","title":"Overview","text":"<p>MLflow logger implementation for the Artifex library.</p> <p>This module provides a logger implementation that integrates with MLflow for experiment tracking, including metrics, parameters, artifacts, and models.</p>"},{"location":"utils/mlflow/#classes","title":"Classes","text":""},{"location":"utils/mlflow/#mlflowlogger","title":"MLFlowLogger","text":"<pre><code>class MLFlowLogger\n</code></pre>"},{"location":"utils/mlflow/#functions","title":"Functions","text":""},{"location":"utils/mlflow/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"utils/mlflow/#close","title":"close","text":"<pre><code>def close()\n</code></pre>"},{"location":"utils/mlflow/#end_run","title":"end_run","text":"<pre><code>def end_run()\n</code></pre>"},{"location":"utils/mlflow/#log_artifact","title":"log_artifact","text":"<pre><code>def log_artifact()\n</code></pre>"},{"location":"utils/mlflow/#log_artifacts","title":"log_artifacts","text":"<pre><code>def log_artifacts()\n</code></pre>"},{"location":"utils/mlflow/#log_histogram","title":"log_histogram","text":"<pre><code>def log_histogram()\n</code></pre>"},{"location":"utils/mlflow/#log_hyperparams","title":"log_hyperparams","text":"<pre><code>def log_hyperparams()\n</code></pre>"},{"location":"utils/mlflow/#log_image","title":"log_image","text":"<pre><code>def log_image()\n</code></pre>"},{"location":"utils/mlflow/#log_model","title":"log_model","text":"<pre><code>def log_model()\n</code></pre>"},{"location":"utils/mlflow/#log_scalar","title":"log_scalar","text":"<pre><code>def log_scalar()\n</code></pre>"},{"location":"utils/mlflow/#log_scalars","title":"log_scalars","text":"<pre><code>def log_scalars()\n</code></pre>"},{"location":"utils/mlflow/#log_text","title":"log_text","text":"<pre><code>def log_text()\n</code></pre>"},{"location":"utils/mlflow/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 12</li> <li>Imports: 11</li> </ul>"},{"location":"utils/performance/","title":"Performance","text":"<p>Module: <code>generative_models.utils.profiling.performance</code></p> <p>Source: <code>generative_models/utils/profiling/performance.py</code></p>"},{"location":"utils/performance/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/plotting/","title":"Plotting","text":"<p>Module: <code>generative_models.utils.visualization.plotting</code></p> <p>Source: <code>generative_models/utils/visualization/plotting.py</code></p>"},{"location":"utils/plotting/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/postprocessing/","title":"Postprocessing","text":"<p>Module: <code>generative_models.utils.text.postprocessing</code></p> <p>Source: <code>generative_models/utils/text/postprocessing.py</code></p>"},{"location":"utils/postprocessing/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/prng/","title":"Prng","text":"<p>Module: <code>generative_models.utils.jax.prng</code></p> <p>Source: <code>generative_models/utils/jax/prng.py</code></p>"},{"location":"utils/prng/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/processing/","title":"Processing","text":"<p>Module: <code>generative_models.utils.text.processing</code></p> <p>Source: <code>generative_models/utils/text/processing.py</code></p>"},{"location":"utils/processing/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/protein/","title":"Protein","text":"<p>Module: <code>generative_models.utils.visualization.protein</code></p> <p>Source: <code>generative_models/utils/visualization/protein.py</code></p>"},{"location":"utils/protein/#overview","title":"Overview","text":"<p>Protein structure visualization utilities.</p> <p>This module provides visualization utilities for protein structures, with support for both interactive visualization in notebooks and exporting to standard formats.</p>"},{"location":"utils/protein/#classes","title":"Classes","text":""},{"location":"utils/protein/#proteinvisualizer","title":"ProteinVisualizer","text":"<pre><code>class ProteinVisualizer\n</code></pre>"},{"location":"utils/protein/#functions","title":"Functions","text":""},{"location":"utils/protein/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"utils/protein/#coords_to_pdb","title":"coords_to_pdb","text":"<pre><code>def coords_to_pdb()\n</code></pre>"},{"location":"utils/protein/#export_to_pdb","title":"export_to_pdb","text":"<pre><code>def export_to_pdb()\n</code></pre>"},{"location":"utils/protein/#visualize","title":"visualize","text":"<pre><code>def visualize()\n</code></pre>"},{"location":"utils/protein/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 4</li> <li>Imports: 6</li> </ul>"},{"location":"utils/registry/","title":"Registry","text":"<p>Module: <code>generative_models.utils.registry</code></p> <p>Source: <code>generative_models/utils/registry.py</code></p>"},{"location":"utils/registry/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/serialization/","title":"Serialization","text":"<p>Module: <code>generative_models.utils.io.serialization</code></p> <p>Source: <code>generative_models/utils/io/serialization.py</code></p>"},{"location":"utils/serialization/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/shapes/","title":"Shapes","text":"<p>Module: <code>generative_models.utils.jax.shapes</code></p> <p>Source: <code>generative_models/utils/jax/shapes.py</code></p>"},{"location":"utils/shapes/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/stability/","title":"Stability","text":"<p>Module: <code>generative_models.utils.numerical.stability</code></p> <p>Source: <code>generative_models/utils/numerical/stability.py</code></p>"},{"location":"utils/stability/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/stats/","title":"Stats","text":"<p>Module: <code>generative_models.utils.numerical.stats</code></p> <p>Source: <code>generative_models/utils/numerical/stats.py</code></p>"},{"location":"utils/stats/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/timer/","title":"Timer","text":"<p>Module: <code>generative_models.utils.timer</code></p> <p>Source: <code>generative_models/utils/timer.py</code></p>"},{"location":"utils/timer/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/transforms/","title":"Transforms","text":"<p>Module: <code>generative_models.utils.image.transforms</code></p> <p>Source: <code>generative_models/utils/image/transforms.py</code></p>"},{"location":"utils/transforms/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/types/","title":"Types","text":"<p>Module: <code>generative_models.utils.types</code></p> <p>Source: <code>generative_models/utils/types.py</code></p>"},{"location":"utils/types/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"utils/wandb/","title":"Wandb","text":"<p>Module: <code>generative_models.utils.logging.wandb</code></p> <p>Source: <code>generative_models/utils/logging/wandb.py</code></p>"},{"location":"utils/wandb/#overview","title":"Overview","text":"<p>Weights &amp; Biases logger implementation for the Artifex library.</p> <p>This module provides a logger implementation that integrates with Weights &amp; Biases for experiment tracking, visualization, and collaboration.</p>"},{"location":"utils/wandb/#classes","title":"Classes","text":""},{"location":"utils/wandb/#wandblogger","title":"WandbLogger","text":"<pre><code>class WandbLogger\n</code></pre>"},{"location":"utils/wandb/#functions","title":"Functions","text":""},{"location":"utils/wandb/#init","title":"init","text":"<pre><code>def __init__()\n</code></pre>"},{"location":"utils/wandb/#close","title":"close","text":"<pre><code>def close()\n</code></pre>"},{"location":"utils/wandb/#finish","title":"finish","text":"<pre><code>def finish()\n</code></pre>"},{"location":"utils/wandb/#log_code","title":"log_code","text":"<pre><code>def log_code()\n</code></pre>"},{"location":"utils/wandb/#log_histogram","title":"log_histogram","text":"<pre><code>def log_histogram()\n</code></pre>"},{"location":"utils/wandb/#log_hyperparams","title":"log_hyperparams","text":"<pre><code>def log_hyperparams()\n</code></pre>"},{"location":"utils/wandb/#log_image","title":"log_image","text":"<pre><code>def log_image()\n</code></pre>"},{"location":"utils/wandb/#log_model","title":"log_model","text":"<pre><code>def log_model()\n</code></pre>"},{"location":"utils/wandb/#log_scalar","title":"log_scalar","text":"<pre><code>def log_scalar()\n</code></pre>"},{"location":"utils/wandb/#log_scalars","title":"log_scalars","text":"<pre><code>def log_scalars()\n</code></pre>"},{"location":"utils/wandb/#log_text","title":"log_text","text":"<pre><code>def log_text()\n</code></pre>"},{"location":"utils/wandb/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 11</li> <li>Imports: 5</li> </ul>"},{"location":"utils/xprof/","title":"Xprof","text":"<p>Module: <code>generative_models.utils.profiling.xprof</code></p> <p>Source: <code>generative_models/utils/profiling/xprof.py</code></p>"},{"location":"utils/xprof/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 0</li> <li>Functions: 0</li> <li>Imports: 0</li> </ul>"},{"location":"visualization/","title":"Visualization","text":"<p>Artifex provides visualization tools for analyzing generative model outputs, training progress, latent spaces, and benchmark results. These tools help you understand model behavior and communicate results effectively.</p>"},{"location":"visualization/#overview","title":"Overview","text":"<ul> <li> <p> Sample Visualization</p> <p>Display generated samples in organized grids for quality assessment</p> <p> Sample Grids</p> </li> <li> <p> Latent Space</p> <p>Visualize and analyze learned latent representations</p> <p> Latent Space</p> </li> <li> <p> Training Metrics</p> <p>Plot loss curves, metrics, and training progress</p> <p> Training Plots</p> </li> <li> <p> Protein Structures</p> <p>3D visualization of protein structures and molecular data</p> <p> Protein Visualization</p> </li> </ul>"},{"location":"visualization/#quick-start","title":"Quick Start","text":"<pre><code>from artifex.utils.visualization import (\n    create_image_grid,\n    plot_latent_space,\n    plot_training_curves,\n)\n\n# Create a grid of generated samples\ngrid = create_image_grid(samples, nrow=8, padding=2)\n\n# Visualize latent space with t-SNE\nplot_latent_space(latents, labels=labels, method=\"tsne\")\n\n# Plot training progress\nplot_training_curves(\n    history,\n    metrics=[\"loss\", \"reconstruction_loss\", \"kl_loss\"],\n)\n</code></pre>"},{"location":"visualization/#sample-grids","title":"Sample Grids","text":"<p>Display generated samples in organized grids for visual inspection.</p>"},{"location":"visualization/#basic-grid","title":"Basic Grid","text":"<pre><code>from artifex.utils.visualization.image_grid import create_image_grid\nimport matplotlib.pyplot as plt\n\n# Generate samples\nsamples = model.sample(num_samples=64, rngs=rngs)\n\n# Create grid\ngrid = create_image_grid(\n    samples,\n    nrow=8,           # Images per row\n    padding=2,        # Padding between images\n    normalize=True,   # Normalize to [0, 1]\n)\n\n# Display\nplt.figure(figsize=(12, 12))\nplt.imshow(grid)\nplt.axis(\"off\")\nplt.title(\"Generated Samples\")\nplt.savefig(\"samples.png\", dpi=150, bbox_inches=\"tight\")\n</code></pre>"},{"location":"visualization/#comparison-grid","title":"Comparison Grid","text":"<pre><code>from artifex.utils.visualization.image_grid import create_comparison_grid\n\n# Create comparison between models\ncomparison = create_comparison_grid(\n    samples_dict={\n        \"VAE\": vae_samples,\n        \"GAN\": gan_samples,\n        \"Diffusion\": diffusion_samples,\n    },\n    nrow=4,\n)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(comparison)\nplt.axis(\"off\")\nplt.savefig(\"model_comparison.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#reconstruction-grid","title":"Reconstruction Grid","text":"<pre><code>from artifex.utils.visualization.image_grid import create_reconstruction_grid\n\n# Show original vs reconstructed\nrecon_grid = create_reconstruction_grid(\n    original=test_images[:16],\n    reconstructed=model.reconstruct(test_images[:16]),\n    nrow=4,\n)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(recon_grid)\nplt.axis(\"off\")\nplt.title(\"Original (top) vs Reconstructed (bottom)\")\nplt.savefig(\"reconstructions.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#latent-space-visualization","title":"Latent Space Visualization","text":"<p>Analyze learned latent representations through dimensionality reduction.</p>"},{"location":"visualization/#t-sne-visualization","title":"t-SNE Visualization","text":"<pre><code>from artifex.utils.visualization.latent_space import plot_latent_tsne\n\n# Encode images to latent space\nlatents = model.encode(images)[\"mean\"]\n\n# Visualize with t-SNE\nfig = plot_latent_tsne(\n    latents,\n    labels=labels,\n    perplexity=30,\n    n_iter=1000,\n    title=\"Latent Space (t-SNE)\",\n)\nfig.savefig(\"latent_tsne.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#pca-visualization","title":"PCA Visualization","text":"<pre><code>from artifex.utils.visualization.latent_space import plot_latent_pca\n\n# Visualize with PCA\nfig = plot_latent_pca(\n    latents,\n    labels=labels,\n    n_components=2,\n    title=\"Latent Space (PCA)\",\n)\nfig.savefig(\"latent_pca.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#latent-traversal","title":"Latent Traversal","text":"<pre><code>from artifex.utils.visualization.latent_space import plot_latent_traversal\n\n# Traverse individual latent dimensions\ntraversal_grid = plot_latent_traversal(\n    model,\n    base_latent=z,\n    dimensions=[0, 1, 2, 3],  # Dimensions to traverse\n    range_vals=(-3, 3),\n    num_steps=10,\n)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(traversal_grid)\nplt.xlabel(\"Latent Value\")\nplt.ylabel(\"Dimension\")\nplt.title(\"Latent Dimension Traversal\")\nplt.savefig(\"latent_traversal.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#interpolation","title":"Interpolation","text":"<pre><code>from artifex.utils.visualization.latent_space import plot_interpolation\n\n# Interpolate between two samples\ninterp_grid = plot_interpolation(\n    model,\n    start_image=image1,\n    end_image=image2,\n    num_steps=10,\n    method=\"slerp\",  # or \"linear\"\n)\n\nplt.figure(figsize=(15, 2))\nplt.imshow(interp_grid)\nplt.axis(\"off\")\nplt.title(\"Latent Space Interpolation\")\nplt.savefig(\"interpolation.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#training-plots","title":"Training Plots","text":"<p>Visualize training progress and metrics.</p>"},{"location":"visualization/#loss-curves","title":"Loss Curves","text":"<pre><code>from artifex.utils.visualization.plotting import plot_losses\n\n# Plot training and validation losses\nfig = plot_losses(\n    train_losses=history[\"train_loss\"],\n    val_losses=history[\"val_loss\"],\n    title=\"Training Progress\",\n)\nfig.savefig(\"loss_curves.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#multi-metric-plot","title":"Multi-Metric Plot","text":"<pre><code>from artifex.utils.visualization.plotting import plot_metrics\n\n# Plot multiple metrics\nfig = plot_metrics(\n    history,\n    metrics=[\"loss\", \"reconstruction_loss\", \"kl_loss\", \"fid\"],\n    smooth=True,\n    window=10,\n)\nfig.savefig(\"training_metrics.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#learning-rate-schedule","title":"Learning Rate Schedule","text":"<pre><code>from artifex.utils.visualization.plotting import plot_lr_schedule\n\n# Visualize learning rate over training\nfig = plot_lr_schedule(\n    lr_history=history[\"learning_rate\"],\n    title=\"Learning Rate Schedule\",\n)\nfig.savefig(\"lr_schedule.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#benchmark-visualization","title":"Benchmark Visualization","text":"<p>Visualize and compare benchmark results.</p>"},{"location":"visualization/#metric-comparison","title":"Metric Comparison","text":"<pre><code>from artifex.benchmarks.visualization.comparison import plot_model_comparison\n\n# Compare models on multiple metrics\nfig = plot_model_comparison(\n    results={\n        \"VAE\": {\"fid\": 45.2, \"is\": 8.1, \"lpips\": 0.12},\n        \"GAN\": {\"fid\": 32.1, \"is\": 9.2, \"lpips\": 0.08},\n        \"Diffusion\": {\"fid\": 18.5, \"is\": 10.1, \"lpips\": 0.05},\n    },\n    metrics=[\"fid\", \"is\", \"lpips\"],\n)\nfig.savefig(\"model_comparison.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#radar-chart","title":"Radar Chart","text":"<pre><code>from artifex.benchmarks.visualization.plots import plot_radar\n\n# Create radar chart for model comparison\nfig = plot_radar(\n    models=[\"VAE\", \"GAN\", \"Diffusion\"],\n    metrics={\n        \"Quality\": [0.7, 0.85, 0.95],\n        \"Diversity\": [0.9, 0.75, 0.88],\n        \"Speed\": [0.95, 0.8, 0.3],\n        \"Stability\": [0.95, 0.6, 0.9],\n    },\n)\nfig.savefig(\"radar_comparison.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#fid-over-training","title":"FID Over Training","text":"<pre><code>from artifex.benchmarks.visualization.plots import plot_fid_progression\n\n# Track FID during training\nfig = plot_fid_progression(\n    fid_values=history[\"fid\"],\n    steps=history[\"step\"],\n    title=\"FID Score During Training\",\n)\nfig.savefig(\"fid_progression.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#protein-visualization","title":"Protein Visualization","text":"<p>Specialized visualization for protein structures.</p>"},{"location":"visualization/#3d-structure-plot","title":"3D Structure Plot","text":"<pre><code>from artifex.utils.visualization.protein_viz import plot_protein_structure\n\n# Visualize protein backbone\nfig = plot_protein_structure(\n    coordinates=protein_coords,  # (num_residues, 4, 3) for N, CA, C, O\n    color_by=\"residue\",          # or \"chain\", \"secondary_structure\"\n    show_bonds=True,\n)\nfig.savefig(\"protein_structure.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#ramachandran-plot","title":"Ramachandran Plot","text":"<pre><code>from artifex.utils.visualization.protein_viz import plot_ramachandran\n\n# Plot backbone dihedral angles\nfig = plot_ramachandran(\n    phi_angles=phi,\n    psi_angles=psi,\n    title=\"Ramachandran Plot\",\n)\nfig.savefig(\"ramachandran.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#bond-length-distribution","title":"Bond Length Distribution","text":"<pre><code>from artifex.utils.visualization.protein_viz import plot_bond_distributions\n\n# Visualize bond length/angle distributions\nfig = plot_bond_distributions(\n    generated_proteins=generated_coords,\n    reference_proteins=real_coords,\n    metrics=[\"bond_length\", \"bond_angle\"],\n)\nfig.savefig(\"bond_distributions.png\", dpi=150)\n</code></pre>"},{"location":"visualization/#documentation","title":"Documentation","text":"<ul> <li>Protein Visualization - Full protein visualization API</li> </ul>"},{"location":"visualization/#interactive-dashboard","title":"Interactive Dashboard","text":"<p>Create interactive dashboards for model analysis.</p> <pre><code>from artifex.benchmarks.visualization.dashboard import create_dashboard\n\n# Create interactive dashboard\ndashboard = create_dashboard(\n    model=model,\n    dataset=test_dataset,\n    metrics=[\"fid\", \"is\", \"reconstruction\"],\n)\n\n# Launch in browser\ndashboard.run(port=8050)\n</code></pre> <p>The dashboard provides:</p> <ul> <li>Real-time sample generation</li> <li>Latent space exploration</li> <li>Metric tracking</li> <li>Model comparison tools</li> </ul>"},{"location":"visualization/#saving-and-exporting","title":"Saving and Exporting","text":""},{"location":"visualization/#high-quality-export","title":"High-Quality Export","text":"<pre><code>from artifex.utils.visualization import save_figure\n\n# Save with publication-quality settings\nsave_figure(\n    fig,\n    path=\"figure.pdf\",\n    dpi=300,\n    bbox_inches=\"tight\",\n    pad_inches=0.1,\n)\n</code></pre>"},{"location":"visualization/#animation-export","title":"Animation Export","text":"<pre><code>from artifex.utils.visualization import create_animation\n\n# Create training animation\nanim = create_animation(\n    frames=sample_frames,  # List of sample grids over training\n    fps=10,\n    title=\"Training Progress\",\n)\nanim.save(\"training.gif\", writer=\"pillow\")\n</code></pre>"},{"location":"visualization/#configuration","title":"Configuration","text":""},{"location":"visualization/#style-settings","title":"Style Settings","text":"<pre><code>from artifex.utils.visualization import set_style\n\n# Set consistent plotting style\nset_style(\n    style=\"seaborn\",\n    font_scale=1.2,\n    palette=\"viridis\",\n)\n</code></pre>"},{"location":"visualization/#figure-defaults","title":"Figure Defaults","text":"<pre><code>from artifex.utils.visualization import configure_defaults\n\nconfigure_defaults(\n    figsize=(10, 8),\n    dpi=150,\n    colormap=\"plasma\",\n    grid=True,\n)\n</code></pre>"},{"location":"visualization/#best-practices","title":"Best Practices","text":"<p>DO</p> <ul> <li>Use consistent color schemes across plots</li> <li>Include axis labels and titles</li> <li>Save figures in vector format (PDF, SVG) for publications</li> <li>Use appropriate resolution for the output medium</li> </ul> <p>DON'T</p> <ul> <li>Don't use rainbow colormaps for sequential data</li> <li>Don't overcrowd plots with too many elements</li> <li>Don't forget to normalize images before display</li> <li>Don't use low resolution for print materials</li> </ul>"},{"location":"visualization/#summary","title":"Summary","text":"<p>Artifex visualization tools provide:</p> <ul> <li>Sample Grids: Display and compare generated samples</li> <li>Latent Space: t-SNE, PCA, traversals, and interpolation</li> <li>Training Plots: Loss curves, metrics, learning rate schedules</li> <li>Benchmarks: Model comparisons, radar charts, progression plots</li> <li>Protein Viz: 3D structures, Ramachandran plots, distributions</li> <li>Dashboard: Interactive exploration and analysis</li> </ul> <p>Use these tools to understand model behavior, diagnose issues, and communicate results effectively.</p>"},{"location":"visualization/protein_viz/","title":"Protein Viz","text":"<p>Module: <code>visualization.protein_viz</code></p> <p>Source: <code>visualization/protein_viz.py</code></p>"},{"location":"visualization/protein_viz/#overview","title":"Overview","text":"<p>Protein visualization utilities.</p> <p>This module provides visualization utilities for protein structures, including 3D visualization with py3Dmol and 2D plots for Ramachandran analysis.</p>"},{"location":"visualization/protein_viz/#classes","title":"Classes","text":""},{"location":"visualization/protein_viz/#proteinvisualizer","title":"ProteinVisualizer","text":"<pre><code>class ProteinVisualizer\n</code></pre>"},{"location":"visualization/protein_viz/#functions","title":"Functions","text":""},{"location":"visualization/protein_viz/#calculate_dihedral","title":"calculate_dihedral","text":"<pre><code>def calculate_dihedral()\n</code></pre>"},{"location":"visualization/protein_viz/#calculate_dihedral_angles","title":"calculate_dihedral_angles","text":"<pre><code>def calculate_dihedral_angles()\n</code></pre>"},{"location":"visualization/protein_viz/#in_region","title":"in_region","text":"<pre><code>def in_region()\n</code></pre>"},{"location":"visualization/protein_viz/#plot_ramachandran","title":"plot_ramachandran","text":"<pre><code>def plot_ramachandran()\n</code></pre>"},{"location":"visualization/protein_viz/#to_pdb_string","title":"to_pdb_string","text":"<pre><code>def to_pdb_string()\n</code></pre>"},{"location":"visualization/protein_viz/#visualize_protein_structure","title":"visualize_protein_structure","text":"<pre><code>def visualize_protein_structure()\n</code></pre>"},{"location":"visualization/protein_viz/#visualize_structure","title":"visualize_structure","text":"<pre><code>def visualize_structure()\n</code></pre>"},{"location":"visualization/protein_viz/#module-statistics","title":"Module Statistics","text":"<ul> <li>Classes: 1</li> <li>Functions: 7</li> <li>Imports: 6</li> </ul>"},{"location":"zoo/","title":"Model Zoo","text":"<p>The Model Zoo provides pre-configured model configurations for common use cases, enabling quick experimentation with state-of-the-art generative models without writing configuration from scratch.</p>"},{"location":"zoo/#overview","title":"Overview","text":"<ul> <li> <p> Pre-configured Models</p> <p>Ready-to-use configurations for VAEs, GANs, Diffusion, and more</p> </li> <li> <p> Easy Customization</p> <p>Override any configuration parameter while keeping defaults</p> </li> <li> <p> YAML-based</p> <p>Configurations stored in human-readable YAML files</p> </li> <li> <p> Categorized</p> <p>Filter models by category (vision, protein, audio, etc.)</p> </li> </ul>"},{"location":"zoo/#quick-start","title":"Quick Start","text":""},{"location":"zoo/#using-pre-configured-models","title":"Using Pre-configured Models","text":"<pre><code>from artifex.generative_models.zoo import zoo\nfrom flax import nnx\n\n# List available configurations\navailable = zoo.list_configs()\nprint(f\"Available models: {available}\")\n\n# Create a model from zoo configuration\nrngs = nnx.Rngs(params=42, dropout=43, sample=44)\nmodel = zoo.create_model(\"vae-mnist\", rngs=rngs)\n\n# Create with overrides\nmodel = zoo.create_model(\n    \"vae-mnist\",\n    rngs=rngs,\n    latent_dim=64,  # Override latent dimension\n)\n</code></pre>"},{"location":"zoo/#getting-configuration-info","title":"Getting Configuration Info","text":"<pre><code># Get detailed info about a configuration\ninfo = zoo.get_info(\"vae-mnist\")\nprint(f\"Model: {info['name']}\")\nprint(f\"Description: {info['description']}\")\nprint(f\"Tags: {info['tags']}\")\n\n# Get the raw configuration\nconfig = zoo.get_config(\"vae-mnist\")\n</code></pre>"},{"location":"zoo/#filtering-by-category","title":"Filtering by Category","text":"<pre><code># List models in a specific category\nvision_models = zoo.list_configs(category=\"vision\")\nprotein_models = zoo.list_configs(category=\"protein\")\n</code></pre>"},{"location":"zoo/#api-reference","title":"API Reference","text":""},{"location":"zoo/#modelzoo","title":"<code>ModelZoo</code>","text":"<p>The main class for managing pre-configured models.</p> <pre><code>class ModelZoo:\n    \"\"\"Registry for pre-configured model configurations.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the model zoo.\"\"\"\n\n    def get_config(self, name: str) -&gt; ModelConfig:\n        \"\"\"Get a pre-configured model configuration.\n\n        Args:\n            name: Name of the configuration\n\n        Returns:\n            Model configuration\n\n        Raises:\n            KeyError: If configuration not found\n        \"\"\"\n\n    def list_configs(self, category: str | None = None) -&gt; list[str]:\n        \"\"\"List all available configurations.\n\n        Args:\n            category: Optional category filter\n\n        Returns:\n            List of configuration names\n        \"\"\"\n\n    def create_model(\n        self,\n        name: str,\n        *,\n        rngs: nnx.Rngs,\n        modality: str | None = None,\n        **overrides\n    ) -&gt; Any:\n        \"\"\"Create a model from zoo configuration.\n\n        Args:\n            name: Name of the configuration\n            rngs: Random number generators\n            modality: Optional modality adaptation\n            **overrides: Configuration overrides\n\n        Returns:\n            Created model instance\n        \"\"\"\n\n    def register_config(self, config: ModelConfig) -&gt; None:\n        \"\"\"Register a configuration in the zoo.\"\"\"\n\n    def get_info(self, name: str) -&gt; dict[str, Any]:\n        \"\"\"Get detailed information about a configuration.\"\"\"\n</code></pre>"},{"location":"zoo/#global-instance","title":"Global Instance","text":"<p>A global zoo instance is available for convenience:</p> <pre><code>from artifex.generative_models.zoo import zoo\n\n# Use the global instance\nmodels = zoo.list_configs()\n</code></pre>"},{"location":"zoo/#adding-custom-configurations","title":"Adding Custom Configurations","text":""},{"location":"zoo/#register-at-runtime","title":"Register at Runtime","text":"<pre><code>from artifex.generative_models.zoo import zoo\nfrom artifex.generative_models.core.configuration import ModelConfig\n\n# Create a custom configuration\nmy_config = ModelConfig(\n    name=\"my-custom-vae\",\n    description=\"Custom VAE for my project\",\n    model_class=\"vae\",\n    input_dim=784,\n    hidden_dims=(512, 256, 128),\n    latent_dim=32,\n    metadata={\"tags\": [\"custom\", \"vision\"]},\n)\n\n# Register in the zoo\nzoo.register_config(my_config)\n\n# Now you can use it\nmodel = zoo.create_model(\"my-custom-vae\", rngs=rngs)\n</code></pre>"},{"location":"zoo/#add-yaml-configuration","title":"Add YAML Configuration","text":"<p>Create a YAML file in <code>zoo/configs/&lt;category&gt;/&lt;name&gt;.yaml</code>:</p> <pre><code># zoo/configs/vision/my-vae.yaml\nname: my-vae\ndescription: Custom VAE for image generation\nmodel_class: vae\nversion: \"1.0\"\ninput_dim: 784\noutput_dim: 784\nhidden_dims: [512, 256, 128]\nlatent_dim: 32\nmetadata:\n  tags:\n    - vision\n    - vae\n  author: My Name\n</code></pre>"},{"location":"zoo/#available-categories","title":"Available Categories","text":"Category Description Example Models <code>vision</code> Image generation models VAE-MNIST, DCGAN-CIFAR <code>protein</code> Protein structure models ProteinVAE, SE3Flow <code>audio</code> Audio generation models WaveNet, AudioVAE <code>text</code> Text generation models GPT-style, PixelCNN <code>molecular</code> Molecular generation MolFlow, GraphVAE"},{"location":"zoo/#configuration-overrides","title":"Configuration Overrides","text":"<p>When creating a model, you can override any configuration parameter:</p> <pre><code># Override multiple parameters\nmodel = zoo.create_model(\n    \"vae-mnist\",\n    rngs=rngs,\n    latent_dim=128,\n    hidden_dims=(1024, 512, 256),\n    kl_weight=0.5,\n)\n</code></pre> <p>Overrides that don't match existing fields are added to metadata:</p> <pre><code># Custom metadata\nmodel = zoo.create_model(\n    \"vae-mnist\",\n    rngs=rngs,\n    experiment_name=\"exp-001\",  # Added to metadata\n)\n</code></pre>"},{"location":"zoo/#integration-with-factory","title":"Integration with Factory","text":"<p>The zoo uses the Model Factory internally:</p> <pre><code># These are equivalent:\nmodel1 = zoo.create_model(\"vae-mnist\", rngs=rngs)\n\n# Manual approach\nconfig = zoo.get_config(\"vae-mnist\")\nmodel2 = create_model(config, rngs=rngs)\n</code></pre>"},{"location":"zoo/#best-practices","title":"Best Practices","text":""},{"location":"zoo/#do","title":"DO","text":"<ul> <li>\u2705 Use the zoo for quick prototyping</li> <li>\u2705 Override only the parameters you need</li> <li>\u2705 Use categories to organize custom configs</li> <li>\u2705 Include descriptive metadata in custom configs</li> </ul>"},{"location":"zoo/#dont","title":"DON'T","text":"<ul> <li>\u274c Modify zoo configs directly (use overrides instead)</li> <li>\u274c Use generic names that might conflict</li> </ul>"},{"location":"zoo/#related-documentation","title":"Related Documentation","text":"<ul> <li>Model Factory - Low-level model creation</li> <li>Configuration System - Configuration details</li> <li>Model Gallery - All available models</li> </ul>"},{"location":"examples/templates/example_template/","title":"[Example Title]","text":"<p>Duration: [X minutes] | Level: [Beginner/Intermediate/Advanced] | GPU Required: [Yes/No]</p>"},{"location":"examples/templates/example_template/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>By the end of this notebook, you will: 1. Understand [concept 1] 2. Be able to [skill 1] 3. Know how to [application 1] 4. Gain intuition about [principle 1]</p>"},{"location":"examples/templates/example_template/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Introduction</li> <li>Setup</li> <li>Core Concepts</li> <li>Implementation</li> <li>Experiments</li> <li>Exercises</li> <li>Summary</li> </ol>"},{"location":"examples/templates/example_template/#i-prerequisites","title":"\u2139\ufe0f Prerequisites","text":"<ul> <li>Basic Python knowledge</li> <li>Understanding of [specific concepts]</li> <li>Artifex installed (see setup instructions below)</li> <li>[Any optional background knowledge]</li> </ul> <pre><code># Cell 2: Environment Setup and Verification\n\"\"\"\nThis cell checks that all required packages are installed and working correctly.\nRun this first to catch any environment issues early.\n\"\"\"\n\nimport sys\nfrom pathlib import Path\n\n\n# Check Python version\nassert sys.version_info &gt;= (3, 9), \"Python 3.9+ required\"\nprint(f\"\u2705 Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n\n# Import and verify JAX\ntry:\n    import jax\n    import jax.numpy as jnp\n\n    print(f\"\u2705 JAX version: {jax.__version__}\")\n    print(f\"\u2705 JAX backend: {jax.default_backend()}\")\n    print(f\"\u2705 Available devices: {jax.device_count()}\")\nexcept ImportError as e:\n    print(f\"\u274c JAX import failed: {e}\")\n    print(\"Please run: source activate.sh\")\n\n# Import Flax NNX\ntry:\n    from flax import nnx\n\n    print(\"\u2705 Flax NNX imported successfully\")\nexcept ImportError as e:\n    print(f\"\u274c Flax import failed: {e}\")\n\n# Import Artifex\ntry:\n    from artifex.generative_models.core.device_manager import DeviceManager\n\n    print(\"\u2705 Artifex imported successfully\")\nexcept ImportError as e:\n    print(f\"\u274c Artifex import failed: {e}\")\n    print(\"Make sure Artifex is installed: pip install -e .\")\n\nprint(\"\\n\ud83c\udf89 Environment setup complete!\")\n</code></pre>"},{"location":"examples/templates/example_template/#1-introduction","title":"1. Introduction","text":"<p>[3-5 paragraphs explaining the example in depth. Cover:]</p> <p>What problem does this solve?</p> <p>[Explain the problem context, real-world applications, and why this matters.]</p> <p>Why is this approach interesting?</p> <p>[Discuss the key innovations, advantages over alternatives, and theoretical foundations.]</p> <p>Where is this used in practice?</p> <p>[Provide examples of practical applications, research use cases, and industry adoption.]</p>"},{"location":"examples/templates/example_template/#background","title":"\ud83d\udcda Background","text":"<p>[Educational content with mathematical foundations if appropriate. For example:]</p> <p>The key mathematical principle behind this approach is:</p> \\[ \\mathcal{L} = \\mathbb{E}_{x \\sim p(x)} [\\text{loss}(x)] \\] <p>[Explain each component of the equation]</p>"},{"location":"examples/templates/example_template/#key-concepts","title":"\ud83d\udd11 Key Concepts","text":"<ul> <li>Concept 1: [Explanation with intuition, not just definition]</li> <li>Concept 2: [How it relates to Concept 1]</li> <li>Concept 3: [Common pitfalls and how to avoid them]</li> </ul>"},{"location":"examples/templates/example_template/#2-setup","title":"2. Setup","text":"<p>Let's set up our environment and define configuration parameters.</p> <pre><code># Cell: Import all required dependencies\n\"\"\"\n\ud83d\udcd6 WHAT THIS CELL DOES:\nImports all libraries and modules needed for the example.\n\n\ud83c\udf93 KEY CONCEPTS:\n- JAX provides automatic differentiation and GPU acceleration\n- Flax NNX is the neural network framework we use\n- Artifex provides high-level abstractions for generative models\n\"\"\"\n\n# Standard library\nfrom typing import Tuple\n\n# JAX and numerical computing\nimport jax\nimport jax.numpy as jnp\n\n# Visualization\nimport matplotlib.pyplot as plt\nfrom flax import nnx\nfrom IPython.display import display, Markdown\n\n# Artifex core\nfrom artifex.generative_models.core.device_manager import DeviceManager\n\n\n# Progress tracking\n\nprint(\"\u2705 All imports successful!\")\n</code></pre> <pre><code># Cell: Configuration parameters\n\"\"\"\n\ud83d\udcd6 WHAT THIS CELL DOES:\nDefines all hyperparameters and configuration for the example.\n\n\ud83d\udca1 TRY EXPERIMENTING:\n- Change LATENT_DIM to see how it affects model capacity\n- Adjust LEARNING_RATE and observe training dynamics\n- Try different HIDDEN_DIMS architectures\n\"\"\"\n\n# Random seed for reproducibility\nRANDOM_SEED = 42\n\n# Model architecture\nINPUT_DIM = (28, 28, 1)  # Example: MNIST images\nHIDDEN_DIMS = [256, 128, 64]\nLATENT_DIM = 32\n\n# Training parameters\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nNUM_EPOCHS = 10\n\n# Output settings\nOUTPUT_DIR = Path(\"outputs/example_notebook\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\ndisplay(\n    Markdown(f\"\"\"\n### Configuration Summary\n\n- **Input dimensions**: {INPUT_DIM}\n- **Latent dimension**: {LATENT_DIM}\n- **Hidden layers**: {HIDDEN_DIMS}\n- **Batch size**: {BATCH_SIZE}\n- **Learning rate**: {LEARNING_RATE}\n- **Training epochs**: {NUM_EPOCHS}\n\"\"\")\n)\n</code></pre> <p></p>"},{"location":"examples/templates/example_template/#3-core-concepts","title":"3. Core Concepts","text":"<p>Before diving into implementation, let's understand the key concepts in depth.</p>"},{"location":"examples/templates/example_template/#concept-1-name","title":"Concept 1: [Name]","text":"<p>[Educational explanation with diagrams or equations if helpful]</p> <p>Why this matters: [Explain the significance and practical implications]</p> <p>Common pitfalls: - [Pitfall 1 and how to avoid it] - [Pitfall 2 and how to avoid it]</p>"},{"location":"examples/templates/example_template/#concept-2-name","title":"Concept 2: [Name]","text":"<p>[Build on Concept 1, showing connections]</p> <p>Implementation details: - [Detail 1] - [Detail 2]</p> <p></p>"},{"location":"examples/templates/example_template/#4-implementation","title":"4. Implementation","text":"<p>Now let's implement the model step by step.</p> <pre><code># Cell: Initialize environment\n\"\"\"\n\ud83d\udcd6 WHAT THIS CELL DOES:\nSets up the random number generator and device manager.\n\n\ud83c\udf93 KEY CONCEPTS:\n- RNG streams ensure reproducibility\n- DeviceManager handles GPU/CPU automatically\n- JAX uses functional random number generation\n\"\"\"\n\n# Initialize RNG\nrngs = nnx.Rngs(RANDOM_SEED)\n\n# Setup device manager\ndevice_manager = DeviceManager()\ndevice = device_manager.get_device()\n\ndisplay(\n    Markdown(f\"\"\"\n### Environment Status\n\n- **Device**: {device}\n- **Backend**: {jax.default_backend()}\n- **Device count**: {jax.device_count()}\n- **Random seed**: {RANDOM_SEED}\n\"\"\")\n)\n</code></pre> <pre><code># Cell: Create sample data\n\"\"\"\n\ud83d\udcd6 WHAT THIS CELL DOES:\nGenerates synthetic data for demonstration.\n\n\ud83d\udca1 TRY EXPERIMENTING:\n- Change num_samples to see effect on training\n- Visualize the data distribution\n\"\"\"\n\nnum_samples = 1000\n\n# Generate random data\nif \"sample\" in rngs:\n    key = rngs.sample()\nelse:\n    key = jax.random.key(0)\n\ndata = jax.random.uniform(key, shape=(num_samples, *INPUT_DIM))\n\nprint(f\"\u2705 Generated {num_samples} samples\")\nprint(f\"   Shape: {data.shape}\")\nprint(f\"   Range: [{data.min():.3f}, {data.max():.3f}]\")\n\n# Visualize a few samples\nfig, axes = plt.subplots(1, 5, figsize=(12, 3))\nfor i, ax in enumerate(axes):\n    ax.imshow(data[i, :, :, 0], cmap=\"gray\")\n    ax.axis(\"off\")\n    ax.set_title(f\"Sample {i + 1}\")\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code># Cell: Define model architecture\n\"\"\"\n\ud83d\udcd6 WHAT THIS CELL DOES:\nDefines the model class following Artifex/Flax NNX patterns.\n\n\ud83c\udf93 KEY CONCEPTS:\n- Always call super().__init__() first\n- Use rngs parameter for initialization\n- No rngs in __call__ for standard modules\n- Use nnx activation functions\n\"\"\"\n\n\nclass ExampleModel(nnx.Module):\n    \"\"\"Example model demonstrating Artifex patterns.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: Tuple[int, ...],\n        hidden_dims: list[int],\n        output_dim: int,\n        *,\n        rngs: nnx.Rngs,\n        dtype: jnp.dtype = jnp.float32,\n    ):\n        # ALWAYS call super().__init__()\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        # Build layers\n        flat_input = int(jnp.prod(jnp.array(input_dim)))\n        layers = []\n        prev_dim = flat_input\n\n        for hidden_dim in hidden_dims:\n            layers.append(nnx.Linear(prev_dim, hidden_dim, rngs=rngs, dtype=dtype))\n            prev_dim = hidden_dim\n\n        layers.append(nnx.Linear(prev_dim, output_dim, rngs=rngs, dtype=dtype))\n\n        self.layers = layers\n\n    def __call__(self, x: jax.Array, *, deterministic: bool = False) -&gt; jax.Array:\n        # Flatten input\n        batch_size = x.shape[0]\n        x = x.reshape(batch_size, -1)\n\n        # Forward pass\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            x = nnx.relu(x)  # Use nnx.relu, not jax.nn.relu\n\n        x = self.layers[-1](x)\n        return x\n\n\n# Create model\nmodel = ExampleModel(\n    input_dim=INPUT_DIM,\n    hidden_dims=HIDDEN_DIMS,\n    output_dim=LATENT_DIM,\n    rngs=rngs,\n)\n\nprint(f\"\u2705 Model created with {LATENT_DIM}-dimensional output\")\n</code></pre>"},{"location":"examples/templates/example_template/#understanding-the-model-architecture","title":"Understanding the Model Architecture","text":"<p>The model we just defined has the following structure:</p> <pre><code>Input \u2192 Flatten \u2192 Hidden Layers \u2192 Output\n</code></pre> <p>Key design decisions:</p> <ol> <li>Input flattening: We flatten the input to work with dense layers</li> <li>ReLU activations: Non-linearity between layers for learning complex patterns</li> <li>No activation on output: Allows unbounded outputs</li> </ol> <p></p>"},{"location":"examples/templates/example_template/#5-experiments","title":"5. Experiments","text":"<p>Let's run some experiments to understand model behavior.</p> <pre><code># Cell: Test forward pass\n\"\"\"\n\ud83d\udcd6 WHAT THIS CELL DOES:\nTests that the model can process a batch of data.\n\n\ud83d\udca1 TRY EXPERIMENTING:\n- Try different batch sizes\n- Observe output statistics\n\"\"\"\n\n# Get a small batch\ntest_batch = data[:BATCH_SIZE]\n\n# Forward pass\noutput = model(test_batch)\n\nprint(\"\u2705 Forward pass successful!\")\nprint(f\"   Input shape: {test_batch.shape}\")\nprint(f\"   Output shape: {output.shape}\")\nprint(f\"   Output range: [{output.min():.3f}, {output.max():.3f}]\")\nprint(f\"   Output mean: {output.mean():.3f}\")\nprint(f\"   Output std: {output.std():.3f}\")\n</code></pre> <p></p>"},{"location":"examples/templates/example_template/#6-exercises","title":"6. Exercises","text":"<p>Now it's your turn to experiment!</p>"},{"location":"examples/templates/example_template/#exercise-1-modify-architecture","title":"\ud83c\udfcb\ufe0f Exercise 1: Modify Architecture","text":"<p>Difficulty: \u2b50\u2b50\u2606\u2606\u2606</p> <p>Goal: Create a model with a different architecture and compare outputs.</p> <p>Hints: - Try adding more layers or changing layer sizes - Compare output statistics with the original model - Think about how architecture affects capacity</p> <p>Your code here:</p> <pre><code># Exercise 1: Your solution\n\n# TODO: Create a model with different HIDDEN_DIMS\n# TODO: Run forward pass and compare with original\n</code></pre> \ud83d\udca1 Click to see solution <pre><code># Exercise 1 Solution\nnew_hidden_dims = [512, 256, 128, 64]\n\nmodel_deeper = ExampleModel(\n    input_dim=INPUT_DIM,\n    hidden_dims=new_hidden_dims,\n    output_dim=LATENT_DIM,\n    rngs=nnx.Rngs(RANDOM_SEED),\n)\n\noutput_deeper = model_deeper(test_batch)\nprint(f\"Deeper model output shape: {output_deeper.shape}\")\nprint(f\"Deeper model output range: [{output_deeper.min():.3f}, {output_deeper.max():.3f}]\")\n</code></pre>"},{"location":"examples/templates/example_template/#exercise-2-another-exercise","title":"\ud83c\udfcb\ufe0f Exercise 2: [Another Exercise]","text":"<p>Difficulty: \u2b50\u2b50\u2b50\u2606\u2606</p> <p>Goal: [Describe the goal]</p> <p>Hints: - [Hint 1] - [Hint 2]</p> <p>Your code here:</p> <pre><code># Exercise 2: Your solution\n</code></pre> <p></p>"},{"location":"examples/templates/example_template/#7-summary-and-next-steps","title":"7. Summary and Next Steps","text":"<p>Congratulations on completing this example!</p>"},{"location":"examples/templates/example_template/#what-you-learned","title":"\ud83c\udf93 What You Learned","text":"<ul> <li>\u2705 [Key learning 1 with brief explanation]</li> <li>\u2705 [Key learning 2 with brief explanation]</li> <li>\u2705 [Key learning 3 with brief explanation]</li> <li>\u2705 [Practical skill gained]</li> </ul>"},{"location":"examples/templates/example_template/#further-experiments","title":"\ud83d\udd2c Further Experiments","text":"<p>Try these modifications to deepen your understanding:</p> <ol> <li>Experiment 1: Change [parameter X] and observe [effect Y]</li> <li>Expected outcome: [Description]</li> <li> <p>Why this matters: [Explanation]</p> </li> <li> <p>Experiment 2: Implement [extension Z]</p> </li> <li>Hints: [Where to start]</li> <li> <p>Resources: [Links to relevant docs]</p> </li> <li> <p>Experiment 3: Compare with [alternative approach W]</p> </li> <li>What to compare: [Metrics or behaviors]</li> <li>Expected insights: [What you'll learn]</li> </ol>"},{"location":"examples/templates/example_template/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<p>Artifex Documentation: - Core concepts - API reference - Best practices</p> <p>Research Papers: - [Paper 1 title] - [Brief description and link] - [Paper 2 title] - [Brief description and link]</p> <p>Related Examples: - <code>[example1].ipynb</code> - For learning about [topic] - <code>[example2].ipynb</code> - For advanced [technique] - <code>[example3].ipynb</code> - For [application domain]</p>"},{"location":"examples/templates/example_template/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":"<p>Problem: [Common issue 1] - Symptoms: [What you'll see] - Solution: [How to fix it] - Prevention: [How to avoid it]</p> <p>Problem: [Common issue 2] - Symptoms: [What you'll see] - Solution: [How to fix it] - Prevention: [How to avoid it]</p>"},{"location":"examples/templates/example_template/#feedback","title":"\ud83d\udcac Feedback","text":"<p>Found a bug or have suggestions for improving this example? Please open an issue on our GitHub repository!</p> <p>Author: Artifex Team Last Updated: [YYYY-MM-DD] License: MIT  </p>"},{"location":"examples/templates/example_template/#thank-you-for-completing-this-example","title":"\ud83c\udf89 Thank you for completing this example!","text":"<p>Continue your learning journey with more Artifex examples and tutorials.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart: Train Your First VAE","text":"<p>This quickstart demonstrates how to train a Variational Autoencoder (VAE) on MNIST using Artifex's high-performance training infrastructure.</p> <p>What you'll learn: - Load data with <code>TFDSEagerSource</code> (pure JAX, no TensorFlow during training) - Configure a CNN-based VAE with <code>VAEConfig</code> - Train using JIT-compiled training loops for maximum performance - Generate and visualize samples</p> <p>Expected runtime: ~30 seconds on GPU, ~2 minutes on CPU</p> <pre><code># Cell 1: Imports\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optax\nfrom datarax.sources import TFDSEagerSource\nfrom datarax.sources.tfds_source import TFDSEagerConfig\nfrom flax import nnx\n\nfrom artifex.generative_models.core.configuration import (\n    DecoderConfig,\n    EncoderConfig,\n    VAEConfig,\n)\nfrom artifex.generative_models.models.vae import VAE\nfrom artifex.generative_models.training import train_epoch_staged\nfrom artifex.generative_models.training.trainers import VAETrainer, VAETrainingConfig\n</code></pre>"},{"location":"getting-started/quickstart/#step-1-load-data-with-tfdseagersource","title":"Step 1: Load Data with TFDSEagerSource","text":"<p><code>TFDSEagerSource</code> loads the entire dataset into JAX arrays at initialization. This eliminates TensorFlow overhead during training - pure JAX from start to finish.</p> <pre><code># Load MNIST with TFDSEagerSource\nprint(\"Loading MNIST...\")\ntfds_config = TFDSEagerConfig(name=\"mnist\", split=\"train\", shuffle=True, seed=42)\nmnist_source = TFDSEagerSource(tfds_config, rngs=nnx.Rngs(0))\n\n# Get images as JAX array and normalize to [0, 1]\nimages = mnist_source.data[\"image\"].astype(jnp.float32) / 255.0\nnum_samples = len(mnist_source)\nprint(f\"Loaded {num_samples} images, shape: {images.shape}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-configure-the-vae-model","title":"Step 2: Configure the VAE Model","text":"<p>We use a CNN architecture for better image quality: - Encoder: 3-layer CNN (32 -&gt; 64 -&gt; 128 channels) mapping images to 20-dim latent space - Decoder: Symmetric CNN reconstructing images from latent codes - KL weight: 1.0 (standard VAE)</p> <pre><code># Configure encoder\nencoder = EncoderConfig(\n    name=\"mnist_cnn_encoder\",\n    input_shape=(28, 28, 1),\n    latent_dim=20,\n    hidden_dims=(32, 64, 128),\n    activation=\"relu\",\n    use_batch_norm=False,\n)\n\n# Configure decoder (symmetric to encoder)\ndecoder = DecoderConfig(\n    name=\"mnist_cnn_decoder\",\n    latent_dim=20,\n    output_shape=(28, 28, 1),\n    hidden_dims=(32, 64, 128),\n    activation=\"relu\",\n    batch_norm=False,\n)\n\n# Combine into VAE config\nmodel_config = VAEConfig(\n    name=\"mnist_cnn_vae\",\n    encoder=encoder,\n    decoder=decoder,\n    encoder_type=\"cnn\",\n    kl_weight=1.0,\n)\n\nprint(\"Model configured:\")\nprint(f\"  Latent dimension: {encoder.latent_dim}\")\nprint(f\"  Encoder type: CNN with dims {encoder.hidden_dims}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-create-model-optimizer-and-trainer","title":"Step 3: Create Model, Optimizer, and Trainer","text":"<ul> <li>Model: VAE with CNN encoder/decoder</li> <li>Optimizer: Adam with learning rate 2e-3</li> <li>Trainer: VAETrainer with linear KL annealing (gradual warmup of KL term)</li> </ul> <p>KL annealing helps training stability by letting the model learn good reconstructions first before the KL regularization kicks in.</p> <pre><code># Create model and optimizer\nmodel = VAE(model_config, rngs=nnx.Rngs(0))\noptimizer = nnx.Optimizer(model, optax.adam(2e-3), wrt=nnx.Param)\n\n# Create trainer with KL annealing\ntrainer = VAETrainer(\n    VAETrainingConfig(\n        kl_annealing=\"linear\",\n        kl_warmup_steps=2000,  # ~4 epochs of warmup\n        beta=1.0,\n    )\n)\n\n# Count parameters\nstate_leaves = jax.tree.leaves(nnx.state(model))\nparam_count = sum(p.size for p in state_leaves if hasattr(p, \"size\"))\nprint(f\"Model created with ~{param_count / 1e3:.1f}K parameters\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-train-with-jit-compiled-training-loop","title":"Step 4: Train with JIT-Compiled Training Loop","text":"<p>We use <code>train_epoch_staged</code> which: 1. Pre-stages data on GPU with <code>jax.device_put()</code> 2. Uses JIT-compiled training steps 3. Achieves 100-500x speedup over naive Python loops</p> <p>The first epoch includes JIT compilation overhead; subsequent epochs are much faster.</p> <pre><code># Stage data on GPU for maximum performance\nprint()\nprint(\"Staging data on GPU...\")\nstaged_data = jax.device_put(images)\n\n# Training configuration\nNUM_EPOCHS = 20\nBATCH_SIZE = 128\n\n# Warmup JIT compilation (don't count this in training time)\nprint(\"Warming up JIT compilation...\")\nwarmup_rng = jax.random.key(999)\n# Create loss function - step is passed dynamically inside train_epoch_staged\nloss_fn = trainer.create_loss_fn(loss_type=\"bce\")\n_ = train_epoch_staged(\n    model,\n    optimizer,\n    staged_data[:256],\n    batch_size=128,\n    rng=warmup_rng,\n    loss_fn=loss_fn,\n)\nprint(\"JIT warmup complete.\")\nprint()\n\n# Training loop\nprint(f\"Training for {NUM_EPOCHS} epochs...\")\nprint(\"-\" * 50)\n\n# IMPORTANT: Reuse the same loss_fn across epochs for JIT cache hits\n# The warmup already created and cached the epoch runner for this loss_fn\nstep = 0\nfor epoch in range(NUM_EPOCHS):\n    rng = jax.random.key(epoch)\n\n    # Train one epoch (reuses JIT-compiled function from warmup)\n    step, metrics = train_epoch_staged(\n        model,\n        optimizer,\n        staged_data,\n        batch_size=BATCH_SIZE,\n        rng=rng,\n        loss_fn=loss_fn,  # Reuse same loss_fn for JIT caching\n        base_step=step,\n    )\n\n    print(f\"Epoch {epoch + 1:2d}/{NUM_EPOCHS} | Loss: {metrics['loss']:7.2f}\")\n\nprint(\"-\" * 50)\nprint(\"Training complete!\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-generate-and-reconstruct-images","title":"Step 5: Generate and Reconstruct Images","text":"<p>Now let's test the trained model: - Generation: Sample from the prior p(z) = N(0, I) and decode - Reconstruction: Encode test images to latent space, then decode back</p> <pre><code># Generate new samples\nprint()\nprint(\"Generating samples...\")\nsamples = model.sample(n_samples=16)\nprint(f\"Generated {samples.shape[0]} samples\")\n\n# Reconstruct test images\nprint(\"Testing reconstruction...\")\ntest_images = jnp.array(images[:8])\nreconstructed = model.reconstruct(test_images, deterministic=True)\nprint(f\"Reconstructed {reconstructed.shape[0]} images\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-6-visualize-results","title":"Step 6: Visualize Results","text":"<p>Let's visualize the generated samples and reconstructions to verify the model learned meaningful representations.</p> <pre><code># Plot generated samples (4x4 grid)\nfig, axes = plt.subplots(4, 4, figsize=(8, 8))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(samples[i].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n    ax.axis(\"off\")\nfig.suptitle(\"Generated Samples from VAE\", fontsize=14, y=0.98)\nplt.tight_layout()\nplt.savefig(\"vae_samples.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\nprint(\"Saved samples to vae_samples.png\")\n\n# Plot reconstructions (original vs reconstructed)\nfig, axes = plt.subplots(2, 8, figsize=(16, 4))\nfig.text(0.02, 0.75, \"Original\", fontsize=12, fontweight=\"bold\", va=\"center\")\nfig.text(0.02, 0.25, \"Reconstructed\", fontsize=12, fontweight=\"bold\", va=\"center\")\n\nfor i in range(8):\n    axes[0, i].imshow(test_images[i].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n    axes[0, i].axis(\"off\")\n    axes[1, i].imshow(reconstructed[i].squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n    axes[1, i].axis(\"off\")\n\nfig.suptitle(\"VAE Reconstruction Quality\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.subplots_adjust(left=0.08)\nplt.savefig(\"vae_reconstruction.png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\nprint(\"Saved reconstruction to vae_reconstruction.png\")\n\nprint()\nprint(\"Success! You've trained your first VAE with Artifex!\")\n</code></pre>"},{"location":"getting-started/quickstart/#what-you-just-did","title":"What You Just Did","text":"<ol> <li>Loaded data efficiently with <code>TFDSEagerSource</code> - pure JAX, no TF overhead</li> <li>Configured a CNN VAE using Artifex's modular config system</li> <li>Used VAETrainer with KL annealing for stable training</li> <li>Trained with JIT-compiled loops for maximum performance</li> <li>Generated new samples from the learned latent space</li> <li>Reconstructed images to verify encoder-decoder quality</li> </ol>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts: Learn about Artifex's architecture and design principles</li> <li>VAE Guide: Advanced techniques like beta-VAE, conditional VAE, VQ-VAE</li> <li>Other Models: Try Diffusion models, GANs, Flow models</li> <li>Custom Data: Load your own datasets with datarax</li> </ul>"}]}